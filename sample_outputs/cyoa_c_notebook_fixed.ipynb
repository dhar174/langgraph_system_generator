{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhar174/langgraph_system_generator/blob/main/sample_outputs/cyoa_c_notebook_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations"
      ],
      "metadata": {
        "id": "Fina8B8ssmOr"
      },
      "id": "Fina8B8ssmOr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e12ec9c9",
      "metadata": {
        "id": "e12ec9c9",
        "section": "config"
      },
      "source": [
        "## Configuration\n",
        "Set runtime parameters and secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8efcf2a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8efcf2a7",
        "outputId": "df1428fb-bc11-495d-a39c-888d2968b08e",
        "section": "config"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model: gpt-5-nano\n",
            "Working directory: /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "\n",
        "# ---- Runtime config ----\n",
        "WORKDIR = Path(os.getenv(\"WORKDIR\", \".\")).resolve()\n",
        "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Pick any compatible OpenAI model (default: gpt-5-nano)\n",
        "MODEL = os.getenv(\"MODEL\", \"gpt-5-nano\")\n",
        "\n",
        "# Defaults used across nodes (you can override per-call)\n",
        "DEFAULT_TEMPERATURE = float(os.getenv(\"TEMPERATURE\", \"0.6\"))\n",
        "DEFAULT_MAX_TOKENS = int(os.getenv(\"MAX_OUTPUT_TOKENS\", \"900\"))\n",
        "DEFAULT_TIMEOUT_S = float(os.getenv(\"OPENAI_TIMEOUT_S\", \"60\"))\n",
        "\n",
        "# ---- API key ----\n",
        "# If you don't set OPENAI_API_KEY in the environment, this will prompt once.\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  if userdata.get('OPENAI_API_KEY'):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "  else:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OPENAI_API_KEY (kept local): \")\n",
        "\n",
        "print(f\"Using model: {MODEL}\")\n",
        "print(f\"Working directory: {WORKDIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe84f9a",
      "metadata": {
        "id": "5fe84f9a",
        "section": "export"
      },
      "source": [
        "## Export Results\n",
        "Persist outputs for downstream use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a70b5a",
      "metadata": {
        "id": "b8a70b5a",
        "section": "export"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Use the graph variable defined in the previous cell\n",
        "# if \"graph\" not in globals():\n",
        "#     raise NameError(\"`graph` is not defined. Run the 'Graph Construction' cell before exporting results.\")\n",
        "\n",
        "# # Use the same thread_id as the UI (interactive-story-1 is the default)\n",
        "# # If the user restarted, they might need to update this, but this is a safe default for the first run.\n",
        "# config = {\"configurable\": {\"thread_id\": \"interactive-story-1\"}}\n",
        "\n",
        "# # Get the latest state\n",
        "# try:\n",
        "#     snapshot = graph.get_state(config)\n",
        "#     output_data = snapshot.values\n",
        "#     if not output_data:\n",
        "#         print(\"Warning: No state found for thread 'interactive-story-1'.\")\n",
        "#         output_data = {}\n",
        "# except Exception as e:\n",
        "#     print(f\"Error retrieving state: {e}\")\n",
        "#     output_data = {}\n",
        "\n",
        "# output_path = Path(\"graph_results.json\")\n",
        "# with output_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
        "#     json.dump(output_data, handle, indent=2, default=str)\n",
        "\n",
        "# print(f\"Saved results to {output_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4d6baf",
      "metadata": {
        "id": "2f4d6baf",
        "section": "troubleshooting"
      },
      "source": [
        "## Troubleshooting\n",
        "- Restart runtime if imports fail in Colab.\n",
        "- Confirm `OPENAI_API_KEY` is set before running graph cells.\n",
        "- Ensure pip installs complete before executing later cells.\n",
        "- Review output JSON for unexpected schema mismatches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f52d91",
      "metadata": {
        "id": "71f52d91",
        "section": "intro"
      },
      "source": [
        "# LangGraph Workflow: # Choose Your Own Adventure Story Engine (LangGrap\n",
        "\n",
        "Generated by LangGraph Notebook Foundry\n",
        "\n",
        "**Architecture**: hybrid  \n",
        "**Patterns Used**: hybrid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d659caa",
      "metadata": {
        "id": "3d659caa",
        "section": "intro"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook implements a LangGraph workflow using the **hybrid** pattern.\n",
        "\n",
        "### Architecture Justification\n",
        "\n",
        "The requirements align best with a hybrid LangGraph architecture that combines a central router for explicit flow control with a supervisor of specialized subagents. This pattern supports the explicit node roles and graph structure (story_gen_node, choice_gen_node, human_input_node, apply_choice_node, should_continue_router) while enabling modular, independent agent contexts for StoryWriter, ContinuityEditor, ChoiceArchitect, Lorekeeper, and an optional SafetyGuard. Rationale by capability:\n",
        "\n",
        "- Task decomposition and modularity: A hybrid approach lets the router orchestrate high-level workflow while subagents encapsulate domain responsibilities. StoryWriter handles narrative generation; ContinuityEditor ensures coherence across turns; ChoiceArchitect validates and formats exactly 3 choices; Lorekeeper maintains world facts and memory; SafetyGuard enforces tone and safety constraints. This separation improves testability and clarity, and it maps cleanly to the explicit node interfaces and data flows required by the design.\n",
        "\n",
        "- Specialized contexts vs shared state: Subagents provide isolated contexts per agent with well-defined interfaces, reducing cross-contamination of state. The Lorekeeper maintains a shared memory surface (facts, memory_summary) that all agents can reference, while each subagent can cache its own local state for efficiency.\n",
        "\n",
        "- Parallel vs sequential execution: The supervisor can run certain subagent tasks in parallel when appropriate (for example, generating draft story and performing lightweight coherence checks concurrently) while the overall graph enforces the necessary sequential order: story_gen_node -> choice_gen_node -> human_input_node (interrupt) -> apply_choice_node -> should_continue_router. The hybrid pattern preserves this flow while enabling parallelizable micro-tasks within a single turn.\n",
        "\n",
        "- State management and checkpointing: The architecture supports a clearly defined state schema and a robust checkpointing strategy. The router orchestrates checkpointing at interrupt points (before human_input_node), and both in-memory and SQLite-backed checkpointers can be used to resume after kernel restarts. Undo can revert to prior checkpoints, satisfying robustness and user-friendly rollback requirements.\n",
        "\n",
        "- Guardrails and runtime constraints: A dedicated SafetyGuard agent enforces exact 3-choice rules, validates non-duplication and clarity of options, enforces the 150–300 word segment length bound, and stops on end conditions. The router enforces end conditions and coordinates transitions, ensuring violations are rejected gracefully and logged for telemetry if desired.\n",
        "\n",
        "- Runtime UX and notebook compatibility: The interrupt-driven resume pattern aligns with a notebook run loop where the user sees a story segment, 3 choices, and then inputs a choice or a command (save/load/undo/quit). The hybrid approach cleanly separates the user-interaction boundary (router/interpreter) from generation/editing tasks (subagents), enabling straightforward implementation in a Jupyter/Colab environment with minimal dependencies.\n",
        "\n",
        "- Extensibility and scalability: New agents or capabilities (inventory/traits, skill checks, style packs, branching saves) can be plugged into the subagent supervisor without destabilizing the core graph or the router, supporting future growth and multi-agent coordination across longer adventures or multiple story branches.\n",
        "\n",
        "In summary, the hybrid pattern best satisfies the requirement for a modular, graph-driven, interruptable, and checkpointable system with clearly delineated agent roles and scalable orchestration. It leverages the router for flow control and subagents for specialized tasks, providing a robust blueprint for implementation and extension.\n",
        "\n",
        "### Sections\n",
        "\n",
        "1. Setup\n",
        "1. State Definition\n",
        "1. Tools\n",
        "1. Nodes\n",
        "1. Graph Construction\n",
        "1. Execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899b2bc2",
      "metadata": {
        "id": "899b2bc2",
        "section": "setup"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24eb2d1f",
      "metadata": {
        "id": "24eb2d1f",
        "section": "setup"
      },
      "outputs": [],
      "source": [
        "# Install required packages (Colab-friendly)\n",
        "# langchain-openai >= 0.3.9 supports forcing the OpenAI Responses API via ChatOpenAI(use_responses_api=True)\n",
        "!pip install -qU langgraph langchain-openai langchain-core langchain-community ipywidgets pypdf PyPDF2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ec1fd0f",
      "metadata": {
        "id": "4ec1fd0f",
        "section": "setup"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Set initial prompt and story configuration:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_descriptors = ['self-discovery', 'Adventure', 'traveling the world', 'Redemption', 'Temptation', 'Justice vs mercy', 'Freedom vs Oppression', 'Liberation', 'Forbidden knowledge', 'intellectual discovery', 'the pursuit of knowledge', 'political intrigue', 'sacrifice', 'family bonds', 'devotion', 'faith', 'betrayal', 'isolation', 'extinction', 'ambition', 'tyranny','fate','time','destiny', 'causality', 'cycles', 'inevitability', 'second chances', 'mortality', 'belonging', 'alienation', 'social order', 'cultural collapse']"
      ],
      "metadata": {
        "id": "e8mc5mAoGT5t"
      },
      "id": "e8mc5mAoGT5t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88a5a40",
      "metadata": {
        "id": "b88a5a40",
        "section": "setup",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "19df9c8b-8b82-4362-c213-3c44a15dca26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your name: Daddy\n",
            "Enter your age (must be a number): 38\n",
            "Enter your pronouns: He/Him\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Enter the genre of the story, such as fantasy, romance, or horror:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Genre: Cosmic Horror\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Enter the setting for the story. \\n The setting can be a place or time: it can be another planet, world, dimension or universe; \\n a real-world location like a city, country, or even very specific place. \\n It can be another time or era, your imagination is the limit, just make sure it represents a place or time. \\n And not too many words!:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting: 19th century Europe\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Enter the theme for the story. This should be a single word or a few words that are abstract and encapsulate the main idea of the story in a way more specific than genre (and not a place or time like setting).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Some examples of good theme descriptors could be things like faith, causality, Freedom vs Oppression, ambition, intellectual discovery'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theme: abject terror, human insignificance, explicit adult body horror\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Input from the user for several aspects of the desired story/scenario that will be used to create the initial prompt.\n",
        "user_name = input(\"Enter your name: \")\n",
        "user_age = input(\"Enter your age (must be a number): \")\n",
        "user_pronouns = input(\"Enter your pronouns: \")\n",
        "\n",
        "display(\"Enter the genre of the story, such as fantasy, romance, or horror:\")\n",
        "story_genre = input(\"Genre: \")\n",
        "\n",
        "display(\"Enter the setting for the story. \\n The setting can be a place or time: it can be another planet, world, dimension or universe; \\n a real-world location like a city, country, or even very specific place. \\n It can be another time or era, your imagination is the limit, just make sure it represents a place or time. \\n And not too many words!:\")\n",
        "story_setting = input(\"Setting: \")\n",
        "\n",
        "display(\"Enter the theme for the story. This should be a single word or a few words that are abstract and encapsulate the main idea of the story in a way more specific than genre (and not a place or time like setting).\")\n",
        "theme_examples = \", \".join(random.sample(base_descriptors, 5))\n",
        "display(f\"Some examples of good theme descriptors could be things like {theme_examples}\")\n",
        "story_theme = input(\"Theme: \")\n",
        "\n",
        "input_data= {\n",
        "    \"user_name\": user_name,\n",
        "    \"user_age\": user_age,\n",
        "    \"user_pronouns\":user_pronouns,\n",
        "    \"story_genre\":story_genre,\n",
        "    \"story_setting\":story_setting,\n",
        "    \"story_theme\":story_theme,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb771a88",
      "metadata": {
        "id": "fb771a88",
        "section": "state"
      },
      "source": [
        "## State Schema\n",
        "\n",
        "Define the workflow state:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29163c3b",
      "metadata": {
        "id": "29163c3b",
        "section": "state"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Optional, Annotated\n",
        "import operator\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "# Extend LangGraph's MessagesState\n",
        "# (Includes 'messages' key with the built-in 'add_messages' reducer)\n",
        "class WorkflowState(MessagesState):\n",
        "    gen_id: str\n",
        "    start_time: str\n",
        "    last_turn_time: str\n",
        "    user_age: int\n",
        "    user_name:str\n",
        "    user_pronouns:str\n",
        "\n",
        "    # Turn / story tracking\n",
        "    turn_index: int                                  # Default (overwrite) behavior is usually best for counters\n",
        "    story_so_far: Annotated[List[str], operator.add] # Appends new segments to the list\n",
        "    segment_summaries: Annotated[Dict[str, str], operator.or_] # Merges summaries, key is a tuple representing the beginning and ending original indices of the summarized segments\n",
        "    current_segment: str\n",
        "    current_segment_index: int\n",
        "\n",
        "    recent_turns: List[dict] # where each dict stores a packed “turn” and its token count\n",
        "    recent_tokens: int\n",
        "    state_ledger: str\n",
        "    # Choice loop\n",
        "    choices: List[str]                               # Overwrite with new set of choices\n",
        "    selected_choice: Optional[int]\n",
        "\n",
        "    # World memory\n",
        "    memory_summary: str\n",
        "    # Uses dictionary union (|) to merge new facts into existing ones\n",
        "    facts: Annotated[Dict[str, Any], operator.or_]\n",
        "    chars: Annotated[Dict[str, Dict[str, Any]], operator.or_] # key: char name, value: dict with keys: relationships with other chars, relationship with player, appearance, backstory, recent char history, personality traits, etc.\n",
        "    # items_lore: Annotated[Dict[str, Dict[str, Any]], operator.or_] # These are items that are known to exist, not necessarily in the players inventory\n",
        "    locations: Annotated[Dict[str, Dict[str, Any]], operator.or_]\n",
        "    events: Annotated[Dict[str, Dict[str, Any]], operator.or_] # Events that have occurred, irreversible\n",
        "    # traits: Annotated[Dict[str, Dict[str, Any]], operator.or_] # player character traits\n",
        "    inventory: Annotated[Dict[str, Dict[str, Any]], operator.or_] # Player current inventory\n",
        "    # skills: Annotated[Dict[str, Dict[str, Any]], operator.or_] # Player skills\n",
        "    # effects: Annotated[Dict[str, Dict[str, Any]], operator.or_] # Effects in play currently\n",
        "    # relationships: Annotated[Dict[str, Dict[str, Any]], operator.or_] # These are the relationship\n",
        "\n",
        "    facts_advice: List[str]\n",
        "    memadvice: Dict[str,Any]\n",
        "    # Tone / constraints\n",
        "    tone: Annotated[Dict[str, Any], operator.or_]\n",
        "    world_rules: Annotated[Dict[str, Any], operator.or_]\n",
        "\n",
        "    # Run lifecycle\n",
        "    run_status: str                                  # 'running' | 'waiting_for_human' | 'ended'\n",
        "    end_reason: Optional[str]\n",
        "    max_turns: int\n",
        "\n",
        "    # Optional telemetry\n",
        "    telemetry: Annotated[List[Dict[str, Any]], operator.add] # Appends new log entries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "9c5wgFHphXua"
      },
      "id": "9c5wgFHphXua",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "W74g-6VLhXuc"
      },
      "id": "W74g-6VLhXuc"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.disable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "Wu_c1CkwhXuc"
      },
      "id": "Wu_c1CkwhXuc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpointer, InMemoryStore and node cache"
      ],
      "metadata": {
        "id": "wqnjXl4QwjTV"
      },
      "id": "wqnjXl4QwjTV"
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, START, StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "# we'll also need a cache and a store\n",
        "from langgraph.cache.memory import InMemoryCache\n",
        "from langgraph.types import CachePolicy\n",
        "from langchain.embeddings import init_embeddings\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "#import message classes\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Callable, Deque, List, Optional, Tuple\n",
        "from collections import deque\n",
        "\n",
        "import tiktoken  # pip install tiktoken  :contentReference[oaicite:1]{index=1}\n",
        "\n",
        "# Create store with semantic search enabled\n",
        "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n",
        "store = InMemoryStore(\n",
        "    index={\n",
        "        \"embed\": embeddings,\n",
        "        \"dims\": 1536,\n",
        "    }\n",
        ")\n",
        "\n",
        "# store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n",
        "# store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n",
        "\n",
        "# items = store.search(\n",
        "#     (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n",
        "# )\n",
        "# Create graph\n",
        "workflow = StateGraph(WorkflowState)\n",
        "memory = MemorySaver()"
      ],
      "metadata": {
        "id": "223_h7Ovwh-s"
      },
      "id": "223_h7Ovwh-s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4aa454fd",
      "metadata": {
        "id": "4aa454fd",
        "section": "tools"
      },
      "source": [
        "## Tools\n",
        "\n",
        "Define tools used in the workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25690fc4",
      "metadata": {
        "id": "25690fc4",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Helper utilities + StoryWriter tool (LangChain ChatOpenAI w/ Responses API)\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage\n",
        "\n",
        "def _ai_message_to_text(ai_msg: Any) -> str:\n",
        "    \"\"\"Best-effort extraction of plain text from LangChain AIMessage across output versions.\"\"\"\n",
        "    if ai_msg is None:\n",
        "        return \"\"\n",
        "    # Some LangChain builds expose .text for Responses API convenience\n",
        "    txt = getattr(ai_msg, \"text\", None)\n",
        "    if isinstance(txt, str) and txt.strip():\n",
        "        return txt.strip()\n",
        "\n",
        "    content = getattr(ai_msg, \"content\", ai_msg)\n",
        "    if isinstance(content, str):\n",
        "        return content.strip()\n",
        "\n",
        "    # responses/v1 often stores a list of content blocks\n",
        "    if isinstance(content, list):\n",
        "        parts: List[str] = []\n",
        "        for item in content:\n",
        "            if isinstance(item, dict):\n",
        "                if item.get(\"type\") == \"text\" and isinstance(item.get(\"text\"), str):\n",
        "                    parts.append(item[\"text\"])\n",
        "                # Some variants nest under item['text']['value']\n",
        "                elif \"text\" in item and isinstance(item[\"text\"], dict) and isinstance(item[\"text\"].get(\"value\"), str):\n",
        "                    parts.append(item[\"text\"][\"value\"])\n",
        "            elif isinstance(item, str):\n",
        "                parts.append(item)\n",
        "        return \"\\n\".join(p for p in parts if p).strip()\n",
        "\n",
        "    return str(content).strip()\n",
        "\n",
        "def make_llm(\n",
        "    model: Optional[str] = None,\n",
        "    temperature: Optional[float] = None,\n",
        "    max_tokens: Optional[int] = None,\n",
        "    timeout: Optional[float] = None,\n",
        ") -> ChatOpenAI:\n",
        "    \"\"\"Create a ChatOpenAI client configured to use the OpenAI Responses API.\"\"\"\n",
        "    model = model or os.getenv(\"MODEL\", \"gpt-5-nano\")\n",
        "    temperature = float(temperature if temperature is not None else os.getenv(\"TEMPERATURE\", \"0.6\"))\n",
        "    max_tokens = int(max_tokens if max_tokens is not None else os.getenv(\"MAX_OUTPUT_TOKENS\", \"900\"))\n",
        "    timeout = float(timeout if timeout is not None else os.getenv(\"OPENAI_TIMEOUT_S\", \"60\"))\n",
        "\n",
        "    return ChatOpenAI(model=model,\n",
        "                           use_responses_api=True,\n",
        "                           output_version=\"responses/v1\",\n",
        "                           max_completion_tokens =max_tokens,  # alias of max_completion_tokens :contentReference[oaicite:5]{index=5}\n",
        "                           timeout=timeout,\n",
        "                           reasoning={'effort': 'low','summary': 'auto'},\n",
        "                           model_kwargs={'text': {'verbosity': 'high'}})\n",
        "\n",
        "def _format_facts(facts: Any) -> str:\n",
        "    try:\n",
        "        if facts is None:\n",
        "            return \"none\"\n",
        "        if isinstance(facts, dict):\n",
        "            return \", \".join([f\"{k}={v}\" for k, v in facts.items()]) or \"none\"\n",
        "        if isinstance(facts, list):\n",
        "            return \"\\n\".join([f\"- {x}\" for x in facts]) or \"none\"\n",
        "        return str(facts)\n",
        "    except Exception:\n",
        "        return \"none\"\n",
        "\n",
        "def LLM_StoryWriter(\n",
        "    turn_index: int,\n",
        "    user_info:str,\n",
        "    story_so_far: Optional[List[str]] = None,\n",
        "    memory_summary: Optional[str] = None,\n",
        "    facts: Optional[Dict[str, Any]] = None,\n",
        "    tone: Optional[Dict[str, Any]] = None,\n",
        "    model: Optional[str] = None,\n",
        "    temperature: float = 0.6,\n",
        "    max_tokens: int = 900,\n",
        "    timeout: float = 60.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Produce a vivid 150–300 word narrative segment for the current turn.\n",
        "    Uses langchain-openai's ChatOpenAI with the OpenAI Responses API.\n",
        "    \"\"\"\n",
        "    story_so_far = story_so_far or []\n",
        "    memory_summary = memory_summary or \"\"\n",
        "    facts = facts or {}\n",
        "    tone = tone or {\"style\": \"adventure\", \"profanity_guard\": True, \"thematic_boundaries\": []}\n",
        "\n",
        "    llm = make_llm(model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout)\n",
        "\n",
        "    system_msg = SystemMessage(content=\"You are StoryWriter, a collaborative narrative AI novelist.\")\n",
        "    user_msg = HumanMessage(content=(\n",
        "        f\"Write a cohesive, vivid, 150–300 word narrative segment for turn {turn_index + 1} of a fantasy adventure. \"\n",
        "        f\"The user is {user_info} \\n\"\n",
        "        f\"Continue the tale from: {' '.join(story_so_far) if story_so_far else 'the beginning of the tale'}. \"\n",
        "        f\"World memory summary: {memory_summary or 'none'}. \"\n",
        "        f\"Known facts: {_format_facts(facts)}. \"\n",
        "        f\"Tone constraints: {tone}. \"\n",
        "        \"End the segment with a clear setup for three player choices.\"\n",
        "    ))\n",
        "\n",
        "    ai = llm.invoke([system_msg, user_msg])\n",
        "    return _ai_message_to_text(ai) or \"(StoryWriter fallback: no content)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "427a7080",
      "metadata": {
        "id": "427a7080",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: llm_continuity_editor (ChatOpenAI + Responses API)\n",
        "\n",
        "from typing import Optional, List, Dict, Any, Union\n",
        "import json\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "def llm_continuity_editor(\n",
        "    segment: str,\n",
        "    memory_facts: Optional[Union[str, List[str], Dict[str, Any]]] = None,\n",
        "    continuity_constraints: Optional[Union[str, List[str]]] = None,\n",
        "    system_prompt: Optional[str] = None,\n",
        "    model: str = None,  # defaults to env MODEL\n",
        "    temperature: float = 0.55,\n",
        "    max_tokens: int = 450,\n",
        "    timeout: float = 60.0,\n",
        ") -> str:\n",
        "    \"\"\"Refine a draft segment to maintain coherence with story/memory and preserve voice.\"\"\"\n",
        "\n",
        "    def _format_collection(value) -> str:\n",
        "        if value is None:\n",
        "            return \"\"\n",
        "        if isinstance(value, str):\n",
        "            return value.strip()\n",
        "        if isinstance(value, dict):\n",
        "            return json.dumps(value, ensure_ascii=False, indent=2)\n",
        "        if isinstance(value, list):\n",
        "            return \"\\n\".join(f\"- {str(item)}\" for item in value)\n",
        "        try:\n",
        "            return str(value)\n",
        "        except Exception:\n",
        "            return json.dumps(value, ensure_ascii=False)\n",
        "\n",
        "    sys = system_prompt or \"Ensure coherence with memory, resolve inconsistencies, and preserve voice.\"\n",
        "    mem_text = _format_collection(memory_facts)\n",
        "    cons_text = _format_collection(continuity_constraints)\n",
        "\n",
        "    llm = make_llm(model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout)\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=f\"You are ContinuityEditor. {sys}\"),\n",
        "        HumanMessage(content=(\n",
        "            \"Refine the following draft to maintain coherence. \"\n",
        "            f\"Memory facts:\\n{mem_text or 'none'}\\n\\n\"\n",
        "            f\"Continuity constraints:\\n{cons_text or 'none'}\\n\\n\"\n",
        "            f\"Draft segment:\\n{segment}\\n\\n\"\n",
        "            \"Output ONLY the refined segment.\"\n",
        "        )),\n",
        "    ]\n",
        "    ai = llm.invoke(messages)\n",
        "    return _ai_message_to_text(ai) or segment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "519713f1",
      "metadata": {
        "id": "519713f1",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: LLM_Lorekeeper (ChatOpenAI + Responses API)\n",
        "\n",
        "from typing import Optional, Dict, Any\n",
        "import json\n",
        "import re\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "def _extract_json_from_text(text: str):\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        pass\n",
        "    m = re.search(r\"\\{.*\\}\", text, re.S)\n",
        "    if m:\n",
        "        try:\n",
        "            return json.loads(m.group(0))\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def LLM_Lorekeeper(\n",
        "    segment: str,\n",
        "    prior_memory_summary: str = \"\",\n",
        "    prior_facts: Optional[Dict[str, Any]] = None,\n",
        "    model: str = None,\n",
        "    temperature: float = 0.25,\n",
        "    max_tokens: int = 450,\n",
        "    timeout: float = 60.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Update memory_summary (1–2 sentences) and facts (dict) based on latest segment.\n",
        "    Returns: {\"memory_summary\": str, \"facts\": dict}\n",
        "    \"\"\"\n",
        "    prior_facts = prior_facts or {}\n",
        "    llm = make_llm(model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout)\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are Lorekeeper. Maintain a compact world memory and explicit facts.\"),\n",
        "        HumanMessage(content=(\n",
        "            \"Given the latest story segment, output STRICT JSON with keys: \"\n",
        "            \"memory_summary (string) and facts (object).\\n\\n\"\n",
        "            f\"Prior memory_summary:\\n{prior_memory_summary or 'none'}\\n\\n\"\n",
        "            f\"Prior facts:\\n{json.dumps(prior_facts, ensure_ascii=False, indent=2)}\\n\\n\"\n",
        "            f\"Latest segment:\\n{segment}\\n\"\n",
        "        ))\n",
        "    ]\n",
        "    ai = llm.invoke(messages)\n",
        "    txt = _ai_message_to_text(ai)\n",
        "    parsed = _extract_json_from_text(txt) if txt else None\n",
        "\n",
        "    if isinstance(parsed, dict) and \"memory_summary\" in parsed and \"facts\" in parsed:\n",
        "        facts_out = parsed.get(\"facts\") if isinstance(parsed.get(\"facts\"), dict) else {}\n",
        "        return {\n",
        "            \"memory_summary\": str(parsed.get(\"memory_summary\", \"\")).strip(),\n",
        "            \"facts\": {**prior_facts, **facts_out},\n",
        "        }\n",
        "\n",
        "    # Fallback if JSON isn't valid\n",
        "    return {\n",
        "        \"memory_summary\": (txt or prior_memory_summary or \"\").strip()[:512],\n",
        "        \"facts\": prior_facts,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f258833",
      "metadata": {
        "id": "6f258833",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: llm_safety_guard (ChatOpenAI + Responses API)\n",
        "\n",
        "from typing import Dict, Any, Optional\n",
        "import json\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "def llm_safety_guard(\n",
        "    segment: str,\n",
        "    tone: Optional[Dict[str, Any]] = None,\n",
        "    model: str = None,\n",
        "    temperature: float = 0.2,\n",
        "    max_tokens: int = 450,\n",
        "    timeout: float = 60.0,\n",
        ") -> str:\n",
        "    \"\"\"Adjust content for safety and tone alignment while preserving narrative flow.\"\"\"\n",
        "    tone = tone or {\"style\": \"adventure\", \"profanity_guard\": True, \"thematic_boundaries\": []}\n",
        "    llm = make_llm(model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout)\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are SafetyGuard. Ensure output is safe and matches the requested tone.\"),\n",
        "        HumanMessage(content=(\n",
        "            \"Review and adjust the segment for safety, language appropriateness, and tone alignment. \"\n",
        "            f\"Constraints (JSON):\\n{json.dumps(tone, ensure_ascii=False)}\\n\\n\"\n",
        "            f\"Segment:\\n{segment}\\n\\n\"\n",
        "            \"Output ONLY the revised segment.\"\n",
        "        ))\n",
        "    ]\n",
        "    ai = llm.invoke(messages)\n",
        "    return _ai_message_to_text(ai) or segment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a68e50",
      "metadata": {
        "id": "24a68e50",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: LLM_ChoiceArchitect (ChatOpenAI + Responses API)\n",
        "\n",
        "from typing import List, Dict, Any, Optional\n",
        "import json\n",
        "import re\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "def _extract_json_choices(text: str) -> List[str]:\n",
        "    # Try strict JSON object {\"choices\":[...]}\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "        if isinstance(obj, dict) and isinstance(obj.get(\"choices\"), list):\n",
        "            return [str(x).strip() for x in obj[\"choices\"] if str(x).strip()][:3]\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Try to extract {...}\n",
        "    m = re.search(r\"\\{.*\\}\", text, re.S)\n",
        "    if m:\n",
        "        try:\n",
        "            obj = json.loads(m.group(0))\n",
        "            if isinstance(obj, dict) and isinstance(obj.get(\"choices\"), list):\n",
        "                return [str(x).strip() for x in obj[\"choices\"] if str(x).strip()][:3]\n",
        "        except Exception:\n",
        "            pass\n",
        "    # Line fallback\n",
        "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
        "    cleaned=[]\n",
        "    for l in lines:\n",
        "        l = re.sub(r\"^[\\-\\*\\d\\.\\)\\s]+\", \"\", l).strip()\n",
        "        if l:\n",
        "            cleaned.append(l)\n",
        "    return cleaned[:3]\n",
        "\n",
        "def LLM_ChoiceArchitect(\n",
        "    current_segment: str,\n",
        "    memory_summary: str = \"\",\n",
        "    facts: Optional[Dict[str, Any]] = None,\n",
        "    model: str = \"gpt-5-nano\",\n",
        "    temperature: float = 0.4,\n",
        "    max_tokens: int = 350,\n",
        "    timeout: float = 60.0,\n",
        ") -> List[str]:\n",
        "    \"\"\"Generate exactly 3 concise action-oriented choices as JSON.\"\"\"\n",
        "    facts = facts or {}\n",
        "    llm = make_llm(model=model, temperature=temperature, max_tokens=max_tokens, timeout=timeout)\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are ChoicesDesigner. Generate 3 distinct next-step options.\"),\n",
        "        HumanMessage(content=(\n",
        "            \"Return STRICT JSON with a top-level key 'choices' whose value is an array of exactly three strings.\\n\\n\"\n",
        "            f\"Memory summary: {memory_summary or 'none'}\\n\"\n",
        "            f\"Facts: {json.dumps(facts, ensure_ascii=False)}\\n\\n\"\n",
        "            f\"Current segment:\\n{current_segment}\\n\"\n",
        "        ))\n",
        "    ]\n",
        "    ai = llm.invoke(messages)\n",
        "    txt = _ai_message_to_text(ai)\n",
        "    choices = _extract_json_choices(txt) if txt else []\n",
        "    if len(choices) < 3:\n",
        "        # Fill deterministically\n",
        "        filler = [\n",
        "            \"Investigate the mysterious gate and its inscriptions.\",\n",
        "            \"Consult a trusted companion about recent omens.\",\n",
        "            \"Pursue the strange light toward the forest's edge.\",\n",
        "        ]\n",
        "        for f in filler:\n",
        "            if len(choices) >= 3:\n",
        "                break\n",
        "            if f not in choices:\n",
        "                choices.append(f)\n",
        "    return choices[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c32272",
      "metadata": {
        "id": "e0c32272",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: PythonSandbox\n",
        "# Purpose: Run the agent orchestration code in a safe sandbox to produce the next state and segment based on the chosen flow.\n",
        "# Category: Code execution\n",
        "\n",
        "def run_agent_sandbox(\n",
        "    flow_name: str,\n",
        "    orchestration_code: str,\n",
        "    initial_state: dict,\n",
        "    timeout_sec: int = 60,\n",
        "    sandbox: str = 'secure',\n",
        "    libraries: list[str] | None = None\n",
        ") :\n",
        "    \"\"\"\n",
        "    Execute agent orchestration code inside a lightweight, soft sandbox to produce the next state\n",
        "    and segment for the given flow.\n",
        "\n",
        "    This function uses a restricted in-process sandbox with a time-limited execution thread.\n",
        "    It enforces a whitelisting strategy for imports and exposes a minimal, safeTools API to the code.\n",
        "\n",
        "    Important:\n",
        "    - The orchestration_code must define a function with the signature:\n",
        "        def orchestrate(state: MessagesState, tools: dict)\n",
        "      and must return a dict containing at least:\n",
        "        {'next_state': ..., 'segment': ...}\n",
        "    - The function returns a dict with keys:\n",
        "        - success: bool\n",
        "        - next_state: dict (if success)\n",
        "        - segment: str (if success)\n",
        "        - execution_time_sec: float (if success)\n",
        "        - error: str (if not success)\n",
        "        - traceback: str (if not success)\n",
        "\n",
        "    Parameters:\n",
        "        flow_name: Name of the flow being executed (used for debugging/context).\n",
        "        orchestration_code: Python source that defines orchestrate(state, tools).\n",
        "        initial_state: The initial state dictionary fed to orchestrate.\n",
        "        timeout_sec: Maximum allowed runtime in seconds.\n",
        "        sandbox: Sandbox mode indicator (present for compatibility; this soft sandbox ignores mode).\n",
        "        libraries: Optional list of whitelisted modules that the orchestration code may import.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary describing the outcome of the sandboxed execution.\n",
        "    \"\"\"\n",
        "    if not isinstance(initial_state, dict):\n",
        "        return {'success': False, 'error': 'initial_state must be a dict.'}\n",
        "\n",
        "    if libraries is None:\n",
        "        libraries = []\n",
        "\n",
        "    # Internal worker executed in a separate thread to allow timeouts.\n",
        "    from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
        "    import time\n",
        "\n",
        "    def _sandbox_task(result_holder):\n",
        "        try:\n",
        "            # Build a restricted set of builtins\n",
        "            import builtins as _builtins\n",
        "            _allowed_names = [\n",
        "                'abs', 'all', 'any', 'bool', 'bytes', 'chr', 'dict', 'enumerate', 'float',\n",
        "                'int', 'len', 'list', 'map', 'max', 'min', 'next', 'object', 'pow',\n",
        "                'range', 'repr', 'reversed', 'round', 'set', 'slice', 'sorted', 'str',\n",
        "                'sum', 'tuple', 'print', 'type', 'isinstance', 'issubclass', 'dir',\n",
        "                'getattr', 'hasattr', 'setattr', 'help'\n",
        "            ]\n",
        "            safe_builtins = {}\n",
        "            for name in _allowed_names:\n",
        "                if hasattr(_builtins, name):\n",
        "                    safe_builtins[name] = getattr(_builtins, name)\n",
        "\n",
        "            # Real import function to delegate allowed imports\n",
        "            _real_import = __import__\n",
        "\n",
        "            allowed_modules = set(libraries)\n",
        "\n",
        "            def _safe_import(name, globals=None, locals=None, fromlist=(), level=0):\n",
        "                top = name.split('.')[0]\n",
        "                if top in allowed_modules or top in {'json', 'typing', 'sqlite3', 'openai'}:\n",
        "                    return _real_import(name, globals, locals, fromlist, level)\n",
        "                raise ImportError(f\"Import of module '{name}' is not allowed in sandbox\")\n",
        "\n",
        "            safe_builtins['__import__'] = _safe_import\n",
        "\n",
        "            sandbox_globals = {\n",
        "                '__builtins__': safe_builtins,\n",
        "                '__name__': '__sandbox__',\n",
        "                '__doc__': None,\n",
        "            }\n",
        "\n",
        "            # Lightweight tools API exposed to orchestrate function\n",
        "            storage = {}\n",
        "            logs = []\n",
        "\n",
        "            def _log(msg):\n",
        "                logs.append(str(msg))\n",
        "\n",
        "            def _storage_get(key, default=None):\n",
        "                return storage.get(key, default)\n",
        "\n",
        "            def _storage_set(key, value):\n",
        "                storage[key] = value\n",
        "\n",
        "            tools = {\n",
        "                'log': _log,\n",
        "                'storage_get': _storage_get,\n",
        "                'storage_set': _storage_set,\n",
        "                'storage': storage,\n",
        "            }\n",
        "\n",
        "            # Execute the orchestration code\n",
        "            exec(orchestration_code, sandbox_globals)\n",
        "\n",
        "            if 'orchestrate' not in sandbox_globals:\n",
        "                raise RuntimeError(\"Sandbox code must define a function named 'orchestrate(state, tools)'\")\n",
        "\n",
        "            orchestrate = sandbox_globals['orchestrate']\n",
        "            if not callable(orchestrate):\n",
        "                raise TypeError(\"'orchestrate' must be a function\")\n",
        "\n",
        "            start = time.time()\n",
        "            result = orchestrate(initial_state, tools)\n",
        "            elapsed = time.time() - start\n",
        "\n",
        "            if not isinstance(result, dict):\n",
        "                raise TypeError(\"orchestrate must return a dict with 'next_state' and 'segment'\")\n",
        "\n",
        "            if 'next_state' not in result or 'segment' not in result:\n",
        "                raise KeyError(\"Result must include 'next_state' and 'segment'\")\n",
        "\n",
        "            if not isinstance(result['next_state'], dict):\n",
        "                raise TypeError(\"'next_state' must be a dict\")\n",
        "\n",
        "            out = {\n",
        "                'success': True,\n",
        "                'next_state': result['next_state'],\n",
        "                'segment': result['segment'],\n",
        "                'execution_time_sec': elapsed\n",
        "            }\n",
        "            result_holder.append(out)\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            tb = traceback.format_exc()\n",
        "            result_holder.append({'success': False, 'error': str(e), 'traceback': tb})\n",
        "\n",
        "    # Normalize input\n",
        "    if initial_state is None:\n",
        "        initial_state = {}\n",
        "\n",
        "    result_holder = []\n",
        "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
        "        future = executor.submit(_sandbox_task, result_holder)\n",
        "        try:\n",
        "            future.result(timeout=timeout_sec)\n",
        "        except TimeoutError:\n",
        "            return {'success': False, 'error': f'Sandbox execution timed out after {timeout_sec} seconds.'}\n",
        "        except Exception as e:\n",
        "            return {'success': False, 'error': f'Unexpected error during sandbox execution: {e}'}\n",
        "\n",
        "    if not result_holder:\n",
        "        return {'success': False, 'error': 'Sandbox did not return any result.'}\n",
        "\n",
        "    res = result_holder[0]\n",
        "    if not isinstance(res, dict):\n",
        "        return {'success': False, 'error': 'Sandbox returned invalid result format.'}\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "637e2ac1",
      "metadata": {
        "id": "637e2ac1",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: PromptOrchestrator\n",
        "# Purpose: Assemble and manage the prompts sent to each agent (StoryWriter, ContinuityEditor, Lorekeeper, SafetyGuard, ChoiceArchitect) and collect their outputs into a unified state.\n",
        "# Category: Data processing\n",
        "from typing import Any, Callable, Dict, List, Optional\n",
        "def prompt_orchestrator(config: dict, seed: str | None = None, agent_executor: Callable | None = None, initial_state: dict | None = None) :\n",
        "    \"\"\"\n",
        "    PromptOrchestrator\n",
        "\n",
        "    Orchestrates prompts across a chain of agents (StoryWriter -> ContinuityEditor -> Lorekeeper -> SafetyGuard)\n",
        "    and a separate ChoiceArchitect process to generate branching choices. Produces a unified, structured\n",
        "    state containing the final story, lore, safety status, and generated choices.\n",
        "\n",
        "    Parameters:\n",
        "    - config: dict containing at least:\n",
        "        - 'prompt_pipeline': list of strings describing prompt chains. Example:\n",
        "          ['StoryWriter -> ContinuityEditor -> Lorekeeper -> SafetyGuard', 'ChoiceArchitect for choices generation']\n",
        "        - 'output_format': currently expected to be 'structured_state'\n",
        "    - seed: optional seed string used to initialize storytelling context.\n",
        "    - agent_executor: optional callable(agent_name: str, prompt: str, context: dict)\n",
        "        If provided, it will be used to invoke agents externally. The callable should return a dict with at least\n",
        "        an 'output' key. It may also return 'lore', 'safety', or 'choices' depending on the agent.\n",
        "        Signature: dict = {'output': str, 'lore': dict, 'safety': dict, 'choices': list, 'notes': str, 'success': bool}\n",
        "    - initial_state: optional dict to seed/override internal state before orchestration.\n",
        "\n",
        "    Returns:\n",
        "    - dict with keys:\n",
        "      {'state': {\n",
        "          'story_seed': str|None,\n",
        "          'story_text': str|None,\n",
        "          'edited_story': str|None,\n",
        "          'lore': dict|None,\n",
        "          'safety': dict|None,\n",
        "          'choices': list|None\n",
        "        },\n",
        "       'config_used': dict,\n",
        "       'logs': list[str],\n",
        "       'success': bool\n",
        "      }\n",
        "    \"\"\"\n",
        "    # Import necessary libraries at function level\n",
        "    import json\n",
        "    import logging\n",
        "    from typing import Any, Callable, Dict, List, Optional\n",
        "\n",
        "    # Basic logger setup\n",
        "    logger = logging.getLogger(__name__)\n",
        "    if not logger.handlers:\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    logs: List[str] = []\n",
        "\n",
        "    # Validation\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"config must be a dictionary with 'prompt_pipeline' and 'output_format' keys.\")\n",
        "    pipeline = config.get('prompt_pipeline')\n",
        "    if not isinstance(pipeline, list) or len(pipeline) == 0:\n",
        "        raise ValueError(\"config['prompt_pipeline'] must be a non-empty list describing the prompt chains.\")\n",
        "\n",
        "    output_format = config.get('output_format')\n",
        "    if output_format != 'structured_state':\n",
        "        # For now, enforce structured_state as the supported output format\n",
        "        logger.info(\"Output format '%s' is not explicitly supported; defaulting to 'structured_state'.\", output_format)\n",
        "    config_used = dict(config)  # shallow copy for reference\n",
        "\n",
        "    # Initialize internal state\n",
        "    state: Dict[str, Any] = {\n",
        "        'story_seed': seed,\n",
        "        'story_text': None,\n",
        "        'edited_story': None,\n",
        "        'lore': None,\n",
        "        'safety': None,\n",
        "        'choices': None\n",
        "    }\n",
        "\n",
        "    if initial_state:\n",
        "        # Merge allowed keys only\n",
        "        for k in ('story_seed', 'story_text', 'edited_story', 'lore', 'safety', 'choices'):\n",
        "            if k in initial_state:\n",
        "                state[k] = initial_state[k]\n",
        "\n",
        "    # Helper: compose prompts for agents (local templates)\n",
        "    def _compose_prompt(agent_name: str, ctx: Dict[str, Any]):\n",
        "        try:\n",
        "            if agent_name == 'StoryWriter':\n",
        "                s = ctx.get('story_seed') or seed or ''\n",
        "                base = \"Write a vivid narrative beginning\"\n",
        "                return f\"{base}. Seed: {s}. Context: {ctx.get('story_text', '')}\".strip()\n",
        "            if agent_name == 'ContinuityEditor':\n",
        "                return f\"Review the following story for logical continuity and tone: {ctx.get('story_text', '')}\".strip()\n",
        "            if agent_name == 'Lorekeeper':\n",
        "                return f\"Extract and summarize lore from the following edited story: {ctx.get('edited_story', '')}\".strip()\n",
        "            if agent_name == 'SafetyGuard':\n",
        "                return f\"Perform a safety review on the following story and propose edits if needed: {ctx.get('story_text', '') or ctx.get('edited_story', '')}\".strip()\n",
        "            if agent_name == 'ChoiceArchitect':\n",
        "                return f\"Generate branching story choices based on the current context. Story: {ctx.get('story_text', '') or ctx.get('edited_story', '')}; Lore: {ctx.get('lore', {})}\".strip()\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    # Helper: agent invocation (external or internal mock)\n",
        "    def _invoke_agent(agent_name: str, prompt: str, context: Dict[str, Any]):\n",
        "        # External executor provided\n",
        "        if callable(agent_executor):\n",
        "            result = agent_executor(agent_name, prompt, dict(context))\n",
        "            if not isinstance(result, dict):\n",
        "                raise ValueError(f\"agent_executor for {agent_name} returned non-dict result.\")\n",
        "            return result\n",
        "\n",
        "        # Internal mock implementations (deterministic, self-contained)\n",
        "        if agent_name == 'StoryWriter':\n",
        "            s_seed = context.get('story_seed') or ''\n",
        "            base_story = f\"{s_seed} A vivid scene unfolds with evocative imagery and a sense of possibility.\"\n",
        "            if not s_seed:\n",
        "                base_story = \"An untold tale begins in a quiet place, inviting curiosity and wonder.\"\n",
        "            return {'output': base_story, 'notes': 'Generated by internal mock StoryWriter.', 'success': True}\n",
        "        elif agent_name == 'ContinuityEditor':\n",
        "            story = context.get('story_text') or ''\n",
        "            if not story:\n",
        "                story = \"A tale with potential begins here.\"\n",
        "            # Simple continuity fix: ensure consistent tense and remove rare placeholders\n",
        "            edited = story.replace(\"tale\", \"story\").replace(\"Story\", \"Story\")\n",
        "            edited = edited.rstrip() + \" [Edited for continuity by ContinuityEditor]\"\n",
        "            return {'output': edited, 'notes': 'Edited for continuity.', 'success': True}\n",
        "        elif agent_name == 'Lorekeeper':\n",
        "            edited_story = context.get('edited_story') or context.get('story_text') or ''\n",
        "            # Simple mock lore extraction\n",
        "            lore = {\n",
        "                'world': 'Altoria',\n",
        "                'characters': ['Ari', 'Kai'],\n",
        "                'locations': ['Everspring Valley'],\n",
        "                'timeline': 'T1-01',\n",
        "                'themes': ['bold journeys', 'found family']\n",
        "            }\n",
        "            return {'lore': lore, 'output': '', 'notes': 'Lore extracted.', 'success': True}\n",
        "        elif agent_name == 'SafetyGuard':\n",
        "            story = context.get('story_text') or context.get('edited_story') or ''\n",
        "            # Simple safety check: if contains prohibited word, flag (very naive)\n",
        "            prohibited = {'hate', 'slur', 'extreme_violence'}\n",
        "            flagged = any(w in story.lower() for w in prohibited)\n",
        "            safety = {'flagged': flagged, 'notes': 'Safety review completed.'}\n",
        "            if flagged:\n",
        "                # naive remediation: append a safety note\n",
        "                story = story + \" [SafetyGuard: content flagged and adjusted]\"\n",
        "            return {'output': story, 'safety': safety, 'notes': 'Safety check complete.', 'success': True}\n",
        "        elif agent_name == 'ChoiceArchitect':\n",
        "            # Provide a deterministic set of choices\n",
        "            choices = [\n",
        "                {'id': 'c1', 'description': 'Travel to the ancient ruins', 'rationale': 'Expands exploration of lore and setting.'},\n",
        "                {'id': 'c2', 'description': 'Introduce a mysterious mentor', 'rationale': 'Adds new guidance and tension.'},\n",
        "                {'id': 'c3', 'description': 'Reveal a hidden truth about the antagonist', 'rationale': 'Raises stakes and suspense.'}\n",
        "            ]\n",
        "            return {'choices': choices, 'output': '', 'notes': 'Generated choices.', 'success': True}\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown agent: {agent_name}\")\n",
        "\n",
        "    # Track whether a ChoiceArchitect step exists\n",
        "    choice_pipeline_present = any('ChoiceArchitect' in item for item in pipeline if isinstance(item, str))\n",
        "\n",
        "    # Build and execute chains\n",
        "    current_context: Dict[str, Any] = dict(state)  # copy to avoid mutating original state\n",
        "\n",
        "    # Process prompt pipelines (excluding explicit ChoiceArchitect steps)\n",
        "    for item in pipeline:\n",
        "        if not isinstance(item, str):\n",
        "            continue\n",
        "        if 'ChoiceArchitect' in item:\n",
        "            # Handled later\n",
        "            continue\n",
        "        # Parse agents in this chain (split by '->')\n",
        "        agents = [tok.strip() for tok in item.split('->') if tok.strip()]\n",
        "        # Filter to known agents\n",
        "        agents = [a for a in agents if a in {'StoryWriter', 'ContinuityEditor', 'Lorekeeper', 'SafetyGuard'}]\n",
        "        if not agents:\n",
        "            continue\n",
        "\n",
        "        for agent in agents:\n",
        "            prompt = _compose_prompt(agent, current_context)\n",
        "            try:\n",
        "                res = _invoke_agent(agent, prompt, current_context)\n",
        "            except Exception as e:\n",
        "                err_msg = f\"Agent {agent} invocation failed: {str(e)}\"\n",
        "                logger.error(err_msg)\n",
        "                logs.append(err_msg)\n",
        "                # Graceful degradation: stop pipeline on failure\n",
        "                raise RuntimeError(err_msg) from e\n",
        "\n",
        "            # Update context/state based on agent output\n",
        "            if agent == 'StoryWriter':\n",
        "                output = res.get('output')\n",
        "                if output is not None:\n",
        "                    current_context['story_text'] = output\n",
        "                    logs.append(f\"StoryWriter produced story_text: {len(str(output))} chars\")\n",
        "            elif agent == 'ContinuityEditor':\n",
        "                edited = res.get('output')\n",
        "                if edited is not None:\n",
        "                    current_context['edited_story'] = edited\n",
        "                    # If we have a story_text, keep it as the base\n",
        "                    if current_context.get('story_text') is None:\n",
        "                        current_context['story_text'] = edited\n",
        "                    logs.append(\"ContinuityEditor produced edited_story\")\n",
        "            elif agent == 'Lorekeeper':\n",
        "                lore = res.get('lore')\n",
        "                if lore is not None:\n",
        "                    current_context['lore'] = lore\n",
        "                    logs.append(\"Lorekeeper extracted lore\")\n",
        "            elif agent == 'SafetyGuard':\n",
        "                safety = res.get('safety')\n",
        "                if safety is not None:\n",
        "                    current_context['safety'] = safety\n",
        "                # SafetyGuard might also return updated story text\n",
        "                if res.get('output'):\n",
        "                    current_context['story_text'] = res['output']\n",
        "                logs.append(\"SafetyGuard performed safety check\")\n",
        "\n",
        "    # After chain, commit outputs to state\n",
        "    state['story_text'] = current_context.get('story_text')\n",
        "    state['edited_story'] = current_context.get('edited_story')\n",
        "    state['lore'] = current_context.get('lore')\n",
        "    state['safety'] = current_context.get('safety')\n",
        "\n",
        "    # Process ChoiceArchitect if present\n",
        "    if choice_pipeline_present:\n",
        "        # Use current context to inform choices\n",
        "        prompt = _compose_prompt('ChoiceArchitect', current_context)\n",
        "        try:\n",
        "            res = _invoke_agent('ChoiceArchitect', prompt, current_context)\n",
        "        except Exception as e:\n",
        "            err_msg = f\"ChoiceArchitect invocation failed: {str(e)}\"\n",
        "            logger.error(err_msg)\n",
        "            logs.append(err_msg)\n",
        "            raise RuntimeError(err_msg) from e\n",
        "\n",
        "        choices = res.get('choices')\n",
        "        if choices is None:\n",
        "            # If external agent didn't return choices under 'choices', try 'output'\n",
        "            choices = res.get('output')\n",
        "            if isinstance(choices, list):\n",
        "                pass\n",
        "        state['choices'] = choices if isinstance(choices, list) else []\n",
        "\n",
        "        logs.append(f\"ChoiceArchitect generated {len(state['choices']) if state['choices'] else 0} choices\")\n",
        "\n",
        "    # Final structured state\n",
        "    structured_state = {\n",
        "        'story_seed': state.get('story_seed'),\n",
        "        'story_text': state.get('story_text'),\n",
        "        'edited_story': state.get('edited_story'),\n",
        "        'lore': state.get('lore'),\n",
        "        'safety': state.get('safety'),\n",
        "        'choices': state.get('choices')\n",
        "    }\n",
        "\n",
        "    result = {\n",
        "        'state': structured_state,\n",
        "        'config_used': config_used,\n",
        "        'logs': logs,\n",
        "        'success': True\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbfb34ae",
      "metadata": {
        "id": "fbfb34ae",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: StoryGraphRouter\n",
        "# Purpose: Enforce graph loop and end conditions; route from story_gen_node to choice_gen_node to interrupt point to apply_choice_node, and finally to should_continue_router or end_node.\n",
        "# Category: Data processing\n",
        "\n",
        "def StoryGraphRouter(graph: dict, state: dict, config: dict | None = None) :\n",
        "    \"\"\"\n",
        "    StoryGraphRouter\n",
        "    Routes through a story graph following the story_loop_v1 schema.\n",
        "\n",
        "    Routing flow (deterministic, per configuration):\n",
        "      - story_gen_node -> choice_gen_node\n",
        "      - choice_gen_node -> one of the interrupt_points (e.g., human_input_node)\n",
        "      - interrupt_point -> apply_choice_node\n",
        "      - apply_choice_node -> should_continue_router\n",
        "      - should_continue_router -> either end_node (if end conditions met) or story_gen_node (continue loop)\n",
        "\n",
        "    End conditions supported:\n",
        "      - quit / The End / user-provided end keywords contained in end_conditions\n",
        "      - max_turns: based on configured max_turns or state.max_turns\n",
        "\n",
        "    State:\n",
        "      - current_node: id of the current node\n",
        "      - turns: int counter for completed turns (defaults to 0)\n",
        "      - last_input: last user input string (optional, used to trigger end conditions)\n",
        "\n",
        "    Args:\n",
        "      graph: Graph representation with keys:\n",
        "        - 'nodes': dict[node_id] -> {'id': node_id, 'type': str, 'name': str?}\n",
        "        - 'edges': dict[source_node_id] -> [target_node_id, ...]\n",
        "      state: Mutable dictionary holding routing state. Will be updated in place.\n",
        "      config: Optional overrides. If None, defaults from the function are used.\n",
        "\n",
        "    Returns:\n",
        "      dict with keys:\n",
        "        - 'success': bool\n",
        "        - 'next_node_id': str | None\n",
        "        - 'state': dict (updated)\n",
        "        - 'reason': str (informational)\n",
        "        - 'error': str (on failure, if any)\n",
        "    \"\"\"\n",
        "    # Local imports (import at function level as requested)\n",
        "    from typing import Any, Dict, Optional, List\n",
        "\n",
        "    # Default configuration\n",
        "    default_config: Dict[str, Any] = {\n",
        "        'graph_schema': 'story_loop_v1',\n",
        "        'interrupt_points': ['human_input_node'],  # Node IDs\n",
        "        'end_conditions': ['quit', 'The End', 'max_turns'],\n",
        "        'max_turns': 6,\n",
        "        'start_node_type': 'story_gen_node',\n",
        "        'choice_node_type': 'choice_gen_node',\n",
        "        'apply_node_type': 'apply_choice_node',\n",
        "        'should_continue_type': 'should_continue_router',\n",
        "        'end_node_type': 'end_node',\n",
        "    }\n",
        "\n",
        "    # Merge provided config\n",
        "    cfg: Dict[str, Any] = dict(default_config)\n",
        "    if config:\n",
        "        cfg.update(config)\n",
        "\n",
        "    # Basic graph validation\n",
        "    try:\n",
        "        if not isinstance(graph, dict):\n",
        "            raise TypeError(\"graph must be a dictionary.\")\n",
        "        nodes = graph.get('nodes')\n",
        "        edges = graph.get('edges')\n",
        "        if not isinstance(nodes, dict) or not isinstance(edges, dict):\n",
        "            raise ValueError(\"Graph must contain 'nodes' and 'edges' dictionaries.\")\n",
        "        # Ensure each node has an id and type\n",
        "        for nid, n in nodes.items():\n",
        "            if not isinstance(n, dict):\n",
        "                raise ValueError(f\"Node {nid} is not a dictionary.\")\n",
        "            if 'id' not in n or 'type' not in n:\n",
        "                raise ValueError(f\"Node {nid} must have 'id' and 'type' fields.\")\n",
        "        # Ensure edges are well-formed\n",
        "        for src, dsts in edges.items():\n",
        "            if not isinstance(dsts, list):\n",
        "                raise ValueError(f\"Edges for {src} must be a list.\")\n",
        "    except Exception as exc:\n",
        "        return {\n",
        "            'success': False,\n",
        "            'next_node_id': None,\n",
        "            'state': state,\n",
        "            'reason': 'Graph validation failed.',\n",
        "            'error': str(exc)\n",
        "        }\n",
        "\n",
        "    def _get_node(node_id: str) -> Optional[Dict[str, Any]]:\n",
        "        return nodes.get(node_id)\n",
        "\n",
        "    def _get_node_id_by_type(node_type: str) -> Optional[str]:\n",
        "        for nid, nd in nodes.items():\n",
        "            if nd.get('type') == node_type:\n",
        "                return nid\n",
        "        return None\n",
        "\n",
        "    def _get_next_ids(node_id: str) -> List[str]:\n",
        "        return edges.get(node_id, [])\n",
        "\n",
        "    def _get_next_node_of_type(node_id: str, target_type: str) -> Optional[str]:\n",
        "        next_ids = _get_next_ids(node_id)\n",
        "        for nid in next_ids:\n",
        "            nd = _get_node(nid)\n",
        "            if nd and nd.get('type') == target_type:\n",
        "                return nid\n",
        "        return None\n",
        "\n",
        "    # Initialize current node if not provided\n",
        "    try:\n",
        "        current_node_id = state.get('current_node')\n",
        "        if current_node_id is None:\n",
        "            start_id = _get_node_id_by_type(cfg['start_node_type'])\n",
        "            if start_id is None:\n",
        "                # Fallback to story_gen_node if provided\n",
        "                start_id = _get_node_id_by_type('story_gen_node')  # type: ignore\n",
        "            if start_id is None:\n",
        "                raise ValueError(\"Unable to determine start node from graph.\")\n",
        "            current_node_id = start_id\n",
        "            state['current_node'] = current_node_id\n",
        "\n",
        "        current_node = _get_node(current_node_id)\n",
        "        if current_node is None or 'type' not in current_node:\n",
        "            raise ValueError(f\"Current node '{current_node_id}' is invalid or missing 'type'.\")\n",
        "\n",
        "        current_type = current_node.get('type')\n",
        "        next_node_id: Optional[str] = None\n",
        "\n",
        "        # Routing logic per node type\n",
        "        if current_type == 'story_gen_node' or current_type == cfg['start_node_type']:\n",
        "            # Route to the choice_gen_node\n",
        "            next_node_id = _get_next_node_of_type(current_node_id, cfg['choice_node_type'])\n",
        "            if next_node_id is None:\n",
        "                # Fallback to first available edge\n",
        "                next_ids = _get_next_ids(current_node_id)\n",
        "                next_node_id = next_ids[0] if next_ids else None\n",
        "\n",
        "        elif current_type == cfg['choice_node_type']:\n",
        "            # Route to an interrupt point if present; otherwise first next\n",
        "            next_ids = _get_next_ids(current_node_id)\n",
        "            chosen = None\n",
        "            for nid in next_ids:\n",
        "                if nid in cfg['interrupt_points']:\n",
        "                    chosen = nid\n",
        "                    break\n",
        "            if chosen is None and next_ids:\n",
        "                chosen = next_ids[0]\n",
        "            next_node_id = chosen\n",
        "\n",
        "        elif current_type in [*cfg['interrupt_points']]:\n",
        "            # After interrupt point, go to apply_choice_node\n",
        "            next_node_id = _get_next_node_of_type(current_node_id, cfg['apply_node_type'])\n",
        "            if next_node_id is None:\n",
        "                next_ids = _get_next_ids(current_node_id)\n",
        "                next_node_id = next_ids[0] if next_ids else None\n",
        "\n",
        "        elif current_type == cfg['apply_node_type']:\n",
        "            # After applying choice, go to should_continue_router\n",
        "            next_node_id = _get_next_node_of_type(current_node_id, cfg['should_continue_type'])\n",
        "            if next_node_id is None:\n",
        "                next_ids = _get_next_ids(current_node_id)\n",
        "                next_node_id = next_ids[0] if next_ids else None\n",
        "\n",
        "        elif current_type == cfg['should_continue_type']:\n",
        "            # Decide to end or continue loop\n",
        "            last_input = state.get('last_input')\n",
        "            turns = int(state.get('turns', 0))\n",
        "            max_turns = int(cfg.get('max_turns', 6))\n",
        "\n",
        "            end_by_input = False\n",
        "            end_conditions = [str(c).strip() for c in cfg['end_conditions']]\n",
        "\n",
        "            # exclude 'max_turns' for input decision\n",
        "            input_end_conditions = [c for c in end_conditions if c != 'max_turns']\n",
        "\n",
        "            if isinstance(last_input, str) and last_input.strip() in input_end_conditions:\n",
        "                end_by_input = True\n",
        "\n",
        "            end_by_turns = turns >= max_turns\n",
        "\n",
        "            if end_by_input or end_by_turns:\n",
        "                end_node_id = _get_node_id_by_type(cfg['end_node_type'])\n",
        "                if end_node_id is None:\n",
        "                    raise ValueError(f\"End node of type '{cfg['end_node_type']}' not found in graph.\")\n",
        "                next_node_id = end_node_id\n",
        "            else:\n",
        "                # Continue looping back to story_gen_node\n",
        "                next_node_id = _get_node_id_by_type('story_gen_node')\n",
        "                if next_node_id is None:\n",
        "                    # Fallback to first available edge from should_continue_router\n",
        "                    next_ids = _get_next_ids(current_node_id)\n",
        "                    next_node_id = next_ids[0] if next_ids else None\n",
        "\n",
        "        else:\n",
        "            # Default fallback: go to story_gen_node if available\n",
        "            next_node_id = _get_node_id_by_type('story_gen_node')\n",
        "            if next_node_id is None:\n",
        "                next_ids = _get_next_ids(current_node_id)\n",
        "                next_node_id = next_ids[0] if next_ids else None\n",
        "\n",
        "        if next_node_id is None:\n",
        "            raise ValueError(\"Router could not determine a valid next node from current node.\")\n",
        "\n",
        "        # Update state for next iteration\n",
        "        # Increment turns after completing a cycle that includes apply_choice_node or after the should_continue_router decision to continue\n",
        "        # We increment when we move forward after applying a choice (i.e., current_type == apply_choice_node)\n",
        "        if current_type == cfg['apply_node_type']:\n",
        "            state['turns'] = int(state.get('turns', 0)) + 1\n",
        "\n",
        "        # Persist the next node\n",
        "        state['current_node'] = next_node_id\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'next_node_id': next_node_id,\n",
        "            'state': state,\n",
        "            'reason': f\"Routed from {current_type} (node {current_node_id}) to {nodes.get(next_node_id, {}).get('type', 'unknown')} (node {next_node_id}).\"\n",
        "        }\n",
        "\n",
        "    except Exception as exc:\n",
        "        return {\n",
        "            'success': False,\n",
        "            'next_node_id': None,\n",
        "            'state': state,\n",
        "            'reason': 'Routing failed due to an internal error.',\n",
        "            'error': str(exc)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "005dce8e",
      "metadata": {
        "id": "005dce8e",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: StateSchemaValidator\n",
        "# Purpose: Validate the runtime state against the defined schema (turn_index, story_so_far, current_segment, choices, etc.) to catch invalid intermediate states.\n",
        "# Category: Validation\n",
        "\n",
        "def StateSchemaValidator(state: MessagesState, config: dict = None) :\n",
        "    \"\"\"\n",
        "    Validate the given runtime state against a JSON Schema.\n",
        "\n",
        "    This function:\n",
        "    - Loads the JSON Schema from a configured path (default: schemas/state_schema.json).\n",
        "    - Performs a top-level strict-mode check to ensure there are no unknown keys at the root.\n",
        "    - Validates the state against the schema using jsonschema (Draft7).\n",
        "    - Collects all validation errors and returns a structured result.\n",
        "\n",
        "    Parameters:\n",
        "        state (dict): The runtime state to validate. Expected to be a dictionary.\n",
        "        config (dict, optional): Configuration for the validator.\n",
        "            - 'schema_path' (str): Path to the JSON Schema file.\n",
        "            - 'strict_mode' (bool): If True, disallow extra top-level keys not defined in the schema's properties.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            - 'valid' (bool): Whether the state conforms to the schema.\n",
        "            - 'errors' (list[str]): List of human-readable validation error messages.\n",
        "            - 'schema_path' (str): Path to the used JSON Schema.\n",
        "            - 'strict_mode' (bool): Whether strict mode was applied.\n",
        "            - 'state' (dict): The original state that was validated.\n",
        "    \"\"\"\n",
        "    # Local imports to satisfy the requirement of importing libraries at function level\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "    from typing import Dict, List, Any\n",
        "\n",
        "    DEFAULT_CONFIG = {'schema_path': 'schemas/state_schema.json', 'strict_mode': True}\n",
        "\n",
        "    # Normalize input\n",
        "    if config is None:\n",
        "        config = {}\n",
        "    if not isinstance(state, dict):\n",
        "        return {\n",
        "            'valid': False,\n",
        "            'errors': [\"State must be a dictionary.\"],\n",
        "            'schema_path': str(DEFAULT_CONFIG['schema_path']),\n",
        "            'strict_mode': bool(config.get('strict_mode', DEFAULT_CONFIG['strict_mode'])),\n",
        "            'state': state\n",
        "        }\n",
        "\n",
        "    schema_path = config.get('schema_path', DEFAULT_CONFIG['schema_path'])\n",
        "    strict_mode = bool(config.get('strict_mode', DEFAULT_CONFIG['strict_mode']))\n",
        "\n",
        "    errors: List[str] = []\n",
        "    # Load and parse the schema\n",
        "    try:\n",
        "        schema_file = Path(schema_path)\n",
        "        if not schema_file.is_file():\n",
        "            errors.append(f\"Schema file not found at path: {schema_path}\")\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'errors': errors,\n",
        "                'schema_path': str(schema_path),\n",
        "                'strict_mode': strict_mode,\n",
        "                'state': state\n",
        "            }\n",
        "        with open(schema_file, 'r', encoding='utf-8') as f:\n",
        "            schema: Dict[str, Any] = json.load(f)\n",
        "        if not isinstance(schema, dict):\n",
        "            errors.append(\"Schema content must be a JSON object.\")\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'errors': errors,\n",
        "                'schema_path': str(schema_path),\n",
        "                'strict_mode': strict_mode,\n",
        "                'state': state\n",
        "            }\n",
        "    except json.JSONDecodeError as e:\n",
        "        errors.append(f\"Schema JSON decode error: {e}\")\n",
        "        return {\n",
        "            'valid': False,\n",
        "            'errors': errors,\n",
        "            'schema_path': str(schema_path),\n",
        "            'strict_mode': strict_mode,\n",
        "            'state': state\n",
        "        }\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Failed to load schema: {e}\")\n",
        "        return {\n",
        "            'valid': False,\n",
        "            'errors': errors,\n",
        "            'schema_path': str(schema_path),\n",
        "            'strict_mode': strict_mode,\n",
        "            'state': state\n",
        "        }\n",
        "\n",
        "    # Strict mode: ensure no unknown top-level keys are present in the state\n",
        "    if strict_mode:\n",
        "        allowed_keys = set(schema.get('properties', {}).keys())\n",
        "        if not isinstance(allowed_keys, set):\n",
        "            allowed_keys = set(allowed_keys)\n",
        "        state_keys = set(state.keys())\n",
        "        extra_keys = sorted(state_keys - allowed_keys)\n",
        "        if extra_keys:\n",
        "            errors.append(f\"Unknown top-level keys in state (not defined in schema properties): {extra_keys}\")\n",
        "\n",
        "    # Run JSON Schema validation\n",
        "    try:\n",
        "        # Use Draft7Validator for broad compatibility; jsonschema >=3.x supports Draft7 by default\n",
        "        from jsonschema import Draft7Validator\n",
        "        validator = Draft7Validator(schema)\n",
        "        validation_errors = sorted(validator.iter_errors(state), key=lambda e: (list(e.path), e.path))\n",
        "        for err in validation_errors:\n",
        "            # Build a readable path string\n",
        "            path = '.'.join([str(p) for p in err.path]) if err.path else '<root>'\n",
        "            errors.append(f\"{path}: {err.message}\")\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Schema validation error: {e}\")\n",
        "\n",
        "    valid = len(errors) == 0\n",
        "\n",
        "    return {\n",
        "        'valid': valid,\n",
        "        'errors': errors,\n",
        "        'schema_path': str(schema_path),\n",
        "        'strict_mode': strict_mode,\n",
        "        'state': state\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "974ee5db",
      "metadata": {
        "id": "974ee5db",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: WordCountEnforcer\n",
        "# Purpose: Enforce the story_segment length constraint (150–300 words); compute word count and trigger adjustments if outside bounds.\n",
        "# Category: Data processing\n",
        "\n",
        "def enforce_word_count(story_segment: str, min_words: int = 150, max_words: int = 500) :\n",
        "    \"\"\"\n",
        "    Enforce a word-count constraint on a story segment.\n",
        "\n",
        "    This function computes the word count of the provided story_segment and ensures\n",
        "    it lies within [min_words, max_words]. If the segment is too short, it will\n",
        "    be expanded by appending predefined filler sentences until the minimum is reached\n",
        "    (and then trimmed to the maximum if necessary). If the segment is too long, it will\n",
        "    be truncated to max_words.\n",
        "\n",
        "    Returns a dictionary containing:\n",
        "    - original_word_count: Word count before adjustments\n",
        "    - adjusted_word_count: Word count after adjustments\n",
        "    - adjusted_story_segment: The resulting story segment after adjustments\n",
        "    - status: The adjustment outcome (\"within_bounds\", \"expanded_to_min\",\n",
        "              \"trimmed_to_max\", or \"trimmed_to_max_after_expansion\")\n",
        "    - min_words, max_words: The configured bounds\n",
        "\n",
        "    Parameters:\n",
        "    - story_segment: str, input story segment to validate\n",
        "    - min_words: int, minimum allowed word count (default 150)\n",
        "    - max_words: int, maximum allowed word count (default 300)\n",
        "\n",
        "    Raises:\n",
        "    - TypeError: if input types are incorrect\n",
        "    - ValueError: if bounds are invalid (negative or min > max)\n",
        "\n",
        "    The function is designed to be deterministic and side-effect-free apart from the\n",
        "    returned adjusted string.\n",
        "    \"\"\"\n",
        "    # Local imports to satisfy \"import at function level\" requirement\n",
        "    import re\n",
        "    from typing import List\n",
        "\n",
        "    # Simple, readable word counting: count tokens comprised of letters/digits and optional internal apostrophes\n",
        "    def count_words(text: str) -> int:\n",
        "        if not text:\n",
        "            return 0\n",
        "        tokens = re.findall(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?\", text)\n",
        "        return len(tokens)\n",
        "\n",
        "    # Basic whitespace split for trimming/padding operations\n",
        "    def split_words(text: str) -> List[str]:\n",
        "        return text.split()\n",
        "\n",
        "    def trim_to_word_bound(text: str, target_count: int) -> str:\n",
        "        if target_count <= 0:\n",
        "            return \"\"\n",
        "        words = split_words(text)\n",
        "        return \" \".join(words[:target_count])\n",
        "\n",
        "    # Filler sentences used to pad under-length story segments\n",
        "    FILLER_SENTENCES = [\n",
        "        \"The narrative continued with careful description and added depth to the scene.\",\n",
        "        \"Characters' motivations became clearer as the tension mounted.\",\n",
        "        \"Rich sensory detail painted the setting and mood.\",\n",
        "        \"Subtle shifts in pace kept the reader engaged and curious.\",\n",
        "        \"A sense of momentum carried the story toward new possibilities.\"\n",
        "    ]\n",
        "\n",
        "    # Input validation\n",
        "    if not isinstance(story_segment, str):\n",
        "        raise TypeError(\"story_segment must be a string.\")\n",
        "    if not isinstance(min_words, int) or not isinstance(max_words, int):\n",
        "        raise TypeError(\"min_words and max_words must be integers.\")\n",
        "    if min_words < 0 or max_words < 0:\n",
        "        raise ValueError(\"min_words and max_words must be non-negative.\")\n",
        "    if min_words > max_words:\n",
        "        raise ValueError(\"min_words cannot be greater than max_words.\")\n",
        "\n",
        "    original_word_count = count_words(story_segment)\n",
        "\n",
        "    adjusted_story_segment = story_segment\n",
        "    adjusted_word_count = original_word_count\n",
        "    status = \"within_bounds\"\n",
        "\n",
        "    if original_word_count < min_words:\n",
        "        # Pad until we reach at least min_words\n",
        "        idx = 0\n",
        "        max_iterations = 1000  # safeguard against pathological inputs\n",
        "        current = adjusted_story_segment\n",
        "        current_count = original_word_count\n",
        "\n",
        "        while current_count < min_words and max_iterations > 0:\n",
        "            sentence_to_add = FILLER_SENTENCES[idx % len(FILLER_SENTENCES)]\n",
        "            # Ensure we maintain readability by adding a space before appending\n",
        "            current = (current + \" \" + sentence_to_add).strip()\n",
        "            current_count = count_words(current)\n",
        "            idx += 1\n",
        "            max_iterations -= 1\n",
        "\n",
        "        adjusted_story_segment = current\n",
        "        adjusted_word_count = current_count\n",
        "\n",
        "        # If padding pushed us over max_words, trim to max_words\n",
        "        if adjusted_word_count > max_words:\n",
        "            adjusted_story_segment = trim_to_word_bound(adjusted_story_segment, max_words)\n",
        "            adjusted_word_count = max_words\n",
        "            status = \"trimmed_to_max_after_expansion\"\n",
        "        else:\n",
        "            status = \"expanded_to_min\"\n",
        "    elif original_word_count > max_words:\n",
        "        # Truncate to max_words\n",
        "        adjusted_story_segment = trim_to_word_bound(story_segment, max_words)\n",
        "        adjusted_word_count = max_words\n",
        "        status = \"trimmed_to_max\"\n",
        "\n",
        "    # If exactly within bounds, leave as-is\n",
        "    if original_word_count >= min_words and original_word_count <= max_words:\n",
        "        adjusted_word_count = original_word_count\n",
        "        adjusted_story_segment = story_segment\n",
        "        status = \"within_bounds\"\n",
        "\n",
        "    return {\n",
        "        \"original_word_count\": original_word_count,\n",
        "        \"adjusted_word_count\": adjusted_word_count,\n",
        "        \"adjusted_story_segment\": adjusted_story_segment,\n",
        "        \"status\": status,\n",
        "        \"min_words\": min_words,\n",
        "        \"max_words\": max_words,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1dc2a45",
      "metadata": {
        "id": "b1dc2a45",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: GuardrailEnforcer\n",
        "# Purpose: Enforce exact 3 choices, validate 1–3 range, reject duplicates, and validate that choices are meaningful and non-vague.\n",
        "# Category: Validation\n",
        "\n",
        "def guardrail_enforce_choices(choices, config=None):\n",
        "    \"\"\"\n",
        "    GuardrailEnforcer: Validates that a provided list of choices adheres to strict guardrail rules.\n",
        "\n",
        "    Requirements enforced:\n",
        "    - Exactly a specified number of choices (default: 3).\n",
        "    - No duplicates unless allowed by configuration.\n",
        "    - Each choice must be meaningful (non-vague) when min_quality is 'meaningful'.\n",
        "    - Proper error handling with descriptive messages.\n",
        "\n",
        "    Parameters:\n",
        "        choices (list[str] | tuple[str, ...]): The user-provided choices to validate.\n",
        "        config (dict | None): Optional configuration dict. Expected structure:\n",
        "            {\n",
        "                'constraints': {\n",
        "                    'choices_count': int,      # e.g., 3\n",
        "                    'allow_duplicates': bool,  # e.g., False\n",
        "                    'min_quality': str         # e.g., 'meaningful'\n",
        "                }\n",
        "            }\n",
        "\n",
        "    Returns:\n",
        "        list[str]: The cleaned and validated list of choices.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If input types are incorrect.\n",
        "        ValueError: If constraints are violated or choices are not meaningful.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Default configuration\n",
        "    defaults = {\n",
        "        'choices_count': 3,\n",
        "        'allow_duplicates': False,\n",
        "        'min_quality': 'meaningful'\n",
        "    }\n",
        "\n",
        "    # Merge user config with defaults\n",
        "    constraints = defaults.copy()\n",
        "    if isinstance(config, dict):\n",
        "        cfg = config.get('constraints')\n",
        "        if isinstance(cfg, dict):\n",
        "            constraints.update(cfg)\n",
        "\n",
        "    # Validate input type\n",
        "    if not isinstance(choices, (list, tuple)):\n",
        "        raise TypeError(\"choices must be a list or tuple of strings.\")\n",
        "    if len(choices) == 0:\n",
        "        raise ValueError(\"choices must contain at least one item.\")\n",
        "\n",
        "    # Normalize and sanitize inputs\n",
        "    cleaned = []\n",
        "    for idx, c in enumerate(choices):\n",
        "        if not isinstance(c, str):\n",
        "            raise TypeError(f\"Choice at index {idx} is not a string.\")\n",
        "        cleaned.append(c.strip())\n",
        "\n",
        "    # Enforce exact number of choices\n",
        "    expected = constraints.get('choices_count')\n",
        "    if not isinstance(expected, int) or expected <= 0:\n",
        "        raise ValueError(\"Invalid configuration: 'choices_count' must be a positive integer.\")\n",
        "    if len(cleaned) != expected:\n",
        "        raise ValueError(f\"Exactly {expected} choices are required. Received {len(cleaned)}.\")\n",
        "\n",
        "    # Enforce duplicates policy\n",
        "    allow_dup = constraints.get('allow_duplicates', False)\n",
        "    if not allow_dup:\n",
        "        normalized = [c.lower() for c in cleaned]\n",
        "        if len(set(normalized)) != len(normalized):\n",
        "            raise ValueError(\"Duplicate choices are not allowed by configuration.\")\n",
        "\n",
        "    # Validate minimum quality if requested\n",
        "    min_quality = constraints.get('min_quality', 'meaningful')\n",
        "    if min_quality == 'meaningful':\n",
        "        def _is_meaningful(text: str) -> bool:\n",
        "            s = text.strip()\n",
        "            if len(s) < 3:\n",
        "                return False\n",
        "            if not re.search(r'[A-Za-z0-9]', s):\n",
        "                return False\n",
        "\n",
        "            lower = s.lower()\n",
        "\n",
        "            vague_terms = {\n",
        "                'option', 'options', 'choose', 'choosing', 'select', 'selection',\n",
        "                'item', 'misc', 'miscellaneous', 'general', 'anything', 'something',\n",
        "                'whatever', 'placeholder', 'to be decided', 'tbd', 'todo',\n",
        "                'unidentified', 'untitled',\n",
        "            }\n",
        "\n",
        "            if lower in vague_terms:\n",
        "                return False\n",
        "            if re.match(r'option\\s*\\d+', lower) or re.match(r'choice\\s*\\d+', lower):\n",
        "                return False\n",
        "\n",
        "            # Remove common stopwords and evaluate for meaningful content\n",
        "            stopwords = {\n",
        "                'the','and','a','an','of','to','in','for','with','on','at','by','or',\n",
        "                'is','are','as','be','this','that','these','those'\n",
        "            }\n",
        "            words = [w for w in re.split(r'\\s+', lower) if w]\n",
        "            meaningful_words = [w for w in words if w not in stopwords]\n",
        "\n",
        "            return len(meaningful_words) >= 1\n",
        "\n",
        "        for idx, ch in enumerate(cleaned):\n",
        "            if not _is_meaningful(ch):\n",
        "                raise ValueError(f\"Choice at index {idx} ('{ch}') is not meaningful or is too vague.\")\n",
        "\n",
        "    # All checks passed; return the cleaned list\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7d8221",
      "metadata": {
        "id": "7f7d8221",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: JSONStateStore\n",
        "# Purpose: Persist and load the in-memory state to disk as JSON for resilience across notebook restarts or kernel interruptions.\n",
        "# Category: File I/O\n",
        "\n",
        "def JSONStateStore(action: str, data=None, path: str = None, pretty_print: bool = None):\n",
        "    \"\"\"\n",
        "    Persist and load in-memory state to disk as JSON.\n",
        "\n",
        "    This function serves as a simple JSON-backed state store with atomic writes for resilience\n",
        "    across notebook restarts or kernel interruptions. It supports two actions:\n",
        "      - 'save': Persist the provided `data` to disk at the configured path.\n",
        "      - 'load': Load and return the JSON state from disk. If the file does not exist, returns an empty dict.\n",
        "\n",
        "    Configuration (defaults):\n",
        "      path: 'state/state.json'\n",
        "      pretty_print: True\n",
        "\n",
        "    Parameters:\n",
        "      action (str): 'save' to write state, 'load' to read state.\n",
        "      data (Any): JSON-serializable object to save when action='save'. Ignored for action='load'.\n",
        "      path (str): Optional override for the file path to store the JSON.\n",
        "      pretty_print (bool): If True, pretty-prints JSON with indentation. If False or None, compact form.\n",
        "\n",
        "    Returns:\n",
        "      If action='save': dict with keys 'status' and 'path'.\n",
        "      If action='load': the deserialized JSON object, or {} if the file does not exist.\n",
        "\n",
        "    Raises:\n",
        "      ValueError, OSError, or IOError on failure to save or load, with descriptive messages.\n",
        "    \"\"\"\n",
        "    # Import libraries locally to satisfy \"import at function level\" requirement\n",
        "    from pathlib import Path\n",
        "    import json\n",
        "    import os\n",
        "    import tempfile\n",
        "    from typing import Any\n",
        "\n",
        "    # Default configuration\n",
        "    DEFAULT_PATH = 'state/state.json'\n",
        "    DEFAULT_PRETTY = True\n",
        "\n",
        "    # Normalize inputs\n",
        "    effective_path = Path(path) if path else Path(DEFAULT_PATH)\n",
        "    is_pretty = DEFAULT_PRETTY if pretty_print is None else bool(pretty_print)\n",
        "\n",
        "    if action not in {'save', 'load'}:\n",
        "        raise ValueError(\"Invalid action. Use 'save' or 'load'.\")\n",
        "\n",
        "    try:\n",
        "        if action == 'save':\n",
        "            if data is None:\n",
        "                raise ValueError(\"No data provided to save. Please provide a JSON-serializable object via the 'data' parameter.\")\n",
        "\n",
        "            # Quick JSON-serializability check\n",
        "            try:\n",
        "                json.dumps(data)\n",
        "            except (TypeError, ValueError) as e:\n",
        "                raise ValueError(f\"Provided data is not JSON-serializable: {e}\")\n",
        "\n",
        "            indent = 2 if is_pretty else None\n",
        "\n",
        "            try:\n",
        "                # Ensure parent directory exists\n",
        "                effective_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                # Atomic write: write to a temp file, then atomically replace\n",
        "                with tempfile.NamedTemporaryFile('w', encoding='utf-8', delete=False, dir=str(effective_path.parent)) as tmp:\n",
        "                    json.dump(data, tmp, indent=indent)\n",
        "                    tmp.flush()\n",
        "                    os.fsync(tmp.fileno())\n",
        "                    temp_path = Path(tmp.name)\n",
        "\n",
        "                os.replace(str(temp_path), str(effective_path))\n",
        "\n",
        "                return {\"status\": \"success\", \"path\": str(effective_path)}\n",
        "            except OSError as e:\n",
        "                raise IOError(f\"Failed to write state to {effective_path}: {e}\")\n",
        "\n",
        "        else:  # action == 'load'\n",
        "            if not effective_path.exists():\n",
        "                # Gracefully return empty state if no persisted state is present\n",
        "                return {}\n",
        "\n",
        "            try:\n",
        "                with effective_path.open('r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except json.JSONDecodeError as e:\n",
        "                raise ValueError(f\"Invalid JSON in state file {effective_path}: {e}\")\n",
        "            except OSError as e:\n",
        "                raise IOError(f\"Failed to read state file {effective_path}: {e}\")\n",
        "\n",
        "    except Exception:\n",
        "        # Re-raise to preserve traceback for debugging\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "362f9aed",
      "metadata": {
        "id": "362f9aed",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: SQLiteCheckpointer\n",
        "# Purpose: Persistent checkpointing using SQLite to enable resume after kernel restart or crash; supports named slots and versioning.\n",
        "# Category: File I/O\n",
        "\n",
        "def SQLiteCheckpointer(config):\n",
        "    \"\"\"\n",
        "    Lightweight persistent checkpointing using SQLite.\n",
        "\n",
        "    This tool provides a simple slot-based checkpointing mechanism with optional\n",
        "    schema migration. It persists checkpoints in a SQLite database, enabling\n",
        "    resumption after kernel restarts or crashes.\n",
        "\n",
        "    Configuration (config dict):\n",
        "      - db_path: Path to the SQLite database file (e.g., 'checkpoints/adventure.db')\n",
        "      - table: Table name to store checkpoints (default: 'checkpoints')\n",
        "      - slot_columns: List of column names for the slot (e.g., ['slot_id', 'timestamp', 'state_json', 'end_status'])\n",
        "      - auto_migrate: If True, will auto-create or migrate the table as needed (default: False)\n",
        "\n",
        "    Returned object exposes methods:\n",
        "      - save_checkpoint(slot_id, state, end_status=None)\n",
        "      - load_checkpoint(slot_id)  or None\n",
        "      - list_slots() -> List[str]\n",
        "      - delete_slot(slot_id) -> bool\n",
        "      - close()\n",
        "    \"\"\"\n",
        "    import sqlite3\n",
        "    import json\n",
        "    import re\n",
        "    from datetime import datetime\n",
        "    from pathlib import Path\n",
        "\n",
        "    # Validate a simple SQL identifier (table/column name)\n",
        "    def _validate_identifier(name: str) -> str:\n",
        "        if not isinstance(name, str) or not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', name):\n",
        "            raise ValueError(f\"Invalid SQL identifier: {name}\")\n",
        "        return name\n",
        "\n",
        "    if not isinstance(config, dict):\n",
        "        raise ValueError(\"config must be a dictionary.\")\n",
        "\n",
        "    db_path = config.get('db_path')\n",
        "    if not db_path:\n",
        "        raise ValueError(\"config must include 'db_path'.\")\n",
        "\n",
        "    table = config.get('table', 'checkpoints')\n",
        "    slot_columns = config.get('slot_columns', ['slot_id', 'timestamp', 'state_json', 'end_status'])\n",
        "    auto_migrate = bool(config.get('auto_migrate', False))\n",
        "\n",
        "    table = _validate_identifier(table)\n",
        "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    class _CheckpointerBackend:\n",
        "        def __init__(self, db_path, table, slot_columns, auto_migrate):\n",
        "            self.db_path = str(db_path)\n",
        "            self.table = table\n",
        "            self.slot_columns = list(slot_columns)\n",
        "            self.auto_migrate = bool(auto_migrate)\n",
        "\n",
        "            self._conn = sqlite3.connect(self.db_path, check_same_thread=False)\n",
        "            self._conn.row_factory = sqlite3.Row\n",
        "            self.SCHEMA_VERSION = 1  # hard-coded current schema version\n",
        "            self._ensure_table()\n",
        "\n",
        "        def _table_exists(self) -> bool:\n",
        "            cur = self._conn.cursor()\n",
        "            try:\n",
        "                cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", (self.table,))\n",
        "                exists = cur.fetchone() is not None\n",
        "                return exists\n",
        "            finally:\n",
        "                cur.close()\n",
        "\n",
        "        def _get_existing_columns(self) -> set:\n",
        "            cur = self._conn.cursor()\n",
        "            try:\n",
        "                cur.execute(f\"PRAGMA table_info({self.table})\")\n",
        "                return {row['name'] for row in cur.fetchall()}\n",
        "            finally:\n",
        "                cur.close()\n",
        "\n",
        "        def _ensure_table(self):\n",
        "            cur = self._conn.cursor()\n",
        "            try:\n",
        "                if not self._table_exists():\n",
        "                    if not self.auto_migrate:\n",
        "                        raise RuntimeError(f\"Table '{self.table}' does not exist and auto_migrate is disabled.\")\n",
        "                    # Create base table with required columns\n",
        "                    cur.execute(f\"\"\"\n",
        "                        CREATE TABLE IF NOT EXISTS {self.table} (\n",
        "                            slot_id TEXT PRIMARY KEY,\n",
        "                            timestamp TEXT NOT NULL,\n",
        "                            state_json TEXT,\n",
        "                            end_status TEXT\n",
        "                        )\n",
        "                    \"\"\")\n",
        "                    # Metadata table for schema versioning\n",
        "                    cur.execute(\"\"\"\n",
        "                        CREATE TABLE IF NOT EXISTS checkpoints_meta (\n",
        "                            key TEXT PRIMARY KEY,\n",
        "                            value TEXT\n",
        "                        )\n",
        "                    \"\"\")\n",
        "                    cur.execute(\"SELECT value FROM checkpoints_meta WHERE key='schema_version'\")\n",
        "                    if cur.fetchone() is None:\n",
        "                        cur.execute(\"INSERT INTO checkpoints_meta (key, value) VALUES ('schema_version', ?)\", (str(self.SCHEMA_VERSION),))\n",
        "                    self._conn.commit()\n",
        "                else:\n",
        "                    existing = self._get_existing_columns()\n",
        "                    if self.auto_migrate:\n",
        "                        changes = []\n",
        "                        if 'slot_id' not in existing:\n",
        "                            raise RuntimeError(f\"Existing table '{self.table}' lacks required 'slot_id' column.\")\n",
        "                        if 'timestamp' not in existing:\n",
        "                            cur.execute(f\"ALTER TABLE {self.table} ADD COLUMN timestamp TEXT\")\n",
        "                            changes.append('timestamp')\n",
        "                        if 'state_json' not in existing:\n",
        "                            cur.execute(f\"ALTER TABLE {self.table} ADD COLUMN state_json TEXT\")\n",
        "                            changes.append('state_json')\n",
        "                        if 'end_status' not in existing:\n",
        "                            cur.execute(f\"ALTER TABLE {self.table} ADD COLUMN end_status TEXT\")\n",
        "                            changes.append('end_status')\n",
        "                        if changes:\n",
        "                            self._conn.commit()\n",
        "                        # Ensure meta table exists\n",
        "                        cur.execute(\"\"\"\n",
        "                            CREATE TABLE IF NOT EXISTS checkpoints_meta (\n",
        "                                key TEXT PRIMARY KEY,\n",
        "                                value TEXT\n",
        "                            )\n",
        "                        \"\"\")\n",
        "                        cur.execute(\"SELECT value FROM checkpoints_meta WHERE key='schema_version'\")\n",
        "                        if cur.fetchone() is None:\n",
        "                            cur.execute(\"INSERT INTO checkpoints_meta (key, value) VALUES ('schema_version', ?)\", (str(self.SCHEMA_VERSION),))\n",
        "                            self._conn.commit()\n",
        "                    else:\n",
        "                        required = {'slot_id','timestamp','state_json','end_status'}\n",
        "                        if not required.issubset(existing):\n",
        "                            missing = required - existing\n",
        "                            raise RuntimeError(f\"Table '{self.table}' is missing required columns: {missing}\")\n",
        "            finally:\n",
        "                cur.close()\n",
        "\n",
        "        def save_checkpoint(self, slot_id, state, end_status=None):\n",
        "            try:\n",
        "                slot_id = str(slot_id)\n",
        "                timestamp = datetime.utcnow().isoformat(timespec='seconds')\n",
        "                state_json = json.dumps(state)\n",
        "                cur = self._conn.cursor()\n",
        "                cur.execute(f\"SELECT 1 FROM {self.table} WHERE slot_id=?\", (slot_id,))\n",
        "                exists = cur.fetchone() is not None\n",
        "                if exists:\n",
        "                    cur.execute(f\"UPDATE {self.table} SET timestamp=?, state_json=?, end_status=? WHERE slot_id=?\",\n",
        "                                (timestamp, state_json, end_status, slot_id))\n",
        "                else:\n",
        "                    cur.execute(f\"INSERT INTO {self.table} (slot_id, timestamp, state_json, end_status) VALUES (?, ?, ?, ?)\",\n",
        "                                (slot_id, timestamp, state_json, end_status))\n",
        "                self._conn.commit()\n",
        "            finally:\n",
        "                cur.close()\n",
        "\n",
        "        def load_checkpoint(self, slot_id):\n",
        "            try:\n",
        "                slot_id = str(slot_id)\n",
        "                cur = self._conn.cursor()\n",
        "                cur.execute(f\"SELECT slot_id, timestamp, state_json, end_status FROM {self.table} WHERE slot_id=?\", (slot_id,))\n",
        "                row = cur.fetchone()\n",
        "                cur.close()\n",
        "                if row is None:\n",
        "                    return None\n",
        "                state = json.loads(row['state_json']) if row['state_json'] else None\n",
        "                return {\n",
        "                    'slot_id': row['slot_id'],\n",
        "                    'timestamp': row['timestamp'],\n",
        "                    'state': state,\n",
        "                    'end_status': row['end_status']\n",
        "                }\n",
        "            except Exception:\n",
        "                raise\n",
        "\n",
        "        def list_slots(self):\n",
        "            cur = self._conn.cursor()\n",
        "            try:\n",
        "                cur.execute(f\"SELECT slot_id FROM {self.table} ORDER BY timestamp DESC\")\n",
        "                rows = cur.fetchall()\n",
        "                return [r['slot_id'] for r in rows]\n",
        "            finally:\n",
        "                cur.close()\n",
        "\n",
        "        def delete_slot(self, slot_id):\n",
        "            cur = None\n",
        "            try:\n",
        "                slot_id = str(slot_id)\n",
        "                cur = self._conn.cursor()\n",
        "                cur.execute(f\"DELETE FROM {self.table} WHERE slot_id=?\", (slot_id,))\n",
        "                self._conn.commit()\n",
        "                return cur.rowcount > 0\n",
        "            finally:\n",
        "                if cur is not None:\n",
        "                    cur.close()\n",
        "\n",
        "        def close(self):\n",
        "            try:\n",
        "                if self._conn:\n",
        "                    self._conn.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    backend = _CheckpointerBackend(db_path, table, slot_columns, auto_migrate)\n",
        "\n",
        "    class PublicAPI:\n",
        "        def __init__(self, backend):\n",
        "            self._backend = backend\n",
        "\n",
        "        def save_checkpoint(self, slot_id, state, end_status=None):\n",
        "            return self._backend.save_checkpoint(slot_id, state, end_status)\n",
        "\n",
        "        def load_checkpoint(self, slot_id):\n",
        "            return self._backend.load_checkpoint(slot_id)\n",
        "\n",
        "        def list_slots(self):\n",
        "            return self._backend.list_slots()\n",
        "\n",
        "        def delete_slot(self, slot_id):\n",
        "            return self._backend.delete_slot(slot_id)\n",
        "\n",
        "        def close(self):\n",
        "            return self._backend.close()\n",
        "\n",
        "    return PublicAPI(backend)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958e26b7",
      "metadata": {
        "id": "958e26b7",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: InMemoryCheckpointer\n",
        "# Purpose: Ephemeral in-memory checkpoint store to enable fast resume within the same notebook session.\n",
        "# Category: File I/O\n",
        "\n",
        "def InMemoryCheckpointer(config: dict):\n",
        "    \"\"\"\n",
        "    Create an in-memory checkpoint store with LRU eviction policy.\n",
        "\n",
        "    This ephemeral store keeps checkpoints in memory for fast resume within the same notebook session.\n",
        "    It enforces a memory usage limit (storage_limit_mb) and uses an LRU eviction strategy to\n",
        "    reclaim space when needed.\n",
        "\n",
        "    Configuration keys:\n",
        "      - storage_limit_mb: float or int, maximum memory to use in megabytes (default 100)\n",
        "      - eviction_policy: str, currently only 'LRU' is supported\n",
        "\n",
        "    Returned object provides the following methods:\n",
        "      - put(key: str, value: Any) -> bool\n",
        "          Stores or updates a checkpoint. Evicts least-recently-used items if necessary.\n",
        "      - get(key: str, default: Any = None) -> Any\n",
        "          Retrieves a checkpoint by key. Returns default if not found.\n",
        "      - delete(key: str) -> bool\n",
        "          Deletes a checkpoint by key. Returns True if deleted, False if not found.\n",
        "      - clear() -> None\n",
        "          Clears all checkpoints.\n",
        "      - list_keys() -> List[str]\n",
        "          Returns a list of keys in LRU order (least recently used first).\n",
        "      - size_info() -> Dict[str, Any]\n",
        "          Returns a summary of memory usage and item count.\n",
        "    \"\"\"\n",
        "    import threading\n",
        "    from collections import OrderedDict\n",
        "    import sys\n",
        "    from typing import Any, Optional, Dict\n",
        "\n",
        "    class _InMemoryCheckpointer:\n",
        "        def __init__(self, storage_limit_mb: float, eviction_policy: str = 'LRU'):\n",
        "            self.storage_limit_bytes = int(storage_limit_mb * 1024 * 1024)\n",
        "            self.eviction_policy = eviction_policy\n",
        "            self._store = OrderedDict()  # key -> (value, size_bytes)\n",
        "            self._current_size = 0\n",
        "            self._lock = threading.RLock()\n",
        "\n",
        "        def _estimate_size(self, obj: Any, seen: Optional[set] = None) -> int:\n",
        "            if seen is None:\n",
        "                seen = set()\n",
        "            obj_id = id(obj)\n",
        "            if obj_id in seen:\n",
        "                return 0\n",
        "            seen.add(obj_id)\n",
        "            try:\n",
        "                size = sys.getsizeof(obj)\n",
        "            except Exception:\n",
        "                size = 0\n",
        "            # Recurse for containers\n",
        "            if isinstance(obj, dict):\n",
        "                for k, v in obj.items():\n",
        "                    size += self._estimate_size(k, seen)\n",
        "                    size += self._estimate_size(v, seen)\n",
        "            elif isinstance(obj, (list, tuple, set, frozenset)):\n",
        "                for item in obj:\n",
        "                    size += self._estimate_size(item, seen)\n",
        "            elif hasattr(obj, '__dict__'):\n",
        "                size += self._estimate_size(getattr(obj, '__dict__'), seen)\n",
        "            return size\n",
        "\n",
        "        def _evict_until_fit(self, target_remaining_bytes: int):\n",
        "            # Evict least-recently-used items until current size <= target_remaining_bytes\n",
        "            with self._lock:\n",
        "                while self._current_size > target_remaining_bytes and len(self._store) > 0:\n",
        "                    k, (v, s) = self._store.popitem(last=False)  # LRU: remove oldest\n",
        "                    self._current_size -= s\n",
        "\n",
        "        def put(self, key: str, value: Any) -> bool:\n",
        "            if not isinstance(key, str):\n",
        "                raise TypeError(\"Checkpoint key must be a string.\")\n",
        "            with self._lock:\n",
        "                try:\n",
        "                    new_size = self._estimate_size(value)\n",
        "                    if key in self._store:\n",
        "                        old_value, old_size = self._store[key]\n",
        "                        delta = new_size - old_size\n",
        "                        self._store[key] = (value, new_size)\n",
        "                        self._current_size += delta\n",
        "                        self._store.move_to_end(key)\n",
        "                    else:\n",
        "                        # If item too large, reject\n",
        "                        if new_size > self.storage_limit_bytes:\n",
        "                            raise MemoryError(\"Checkpoint too large to store in memory.\")\n",
        "                        # Evict as needed to make space\n",
        "                        self._evict_until_fit(self.storage_limit_bytes - new_size)\n",
        "                        self._store[key] = (value, new_size)\n",
        "                        self._store.move_to_end(key)\n",
        "                        self._current_size += new_size\n",
        "                    return True\n",
        "                except Exception as e:\n",
        "                    raise RuntimeError(f\"Failed to store checkpoint '{key}': {e}\")\n",
        "\n",
        "        def get(self, key: str, default: Optional[Any] = None) -> Any:\n",
        "            if not isinstance(key, str):\n",
        "                raise TypeError(\"Checkpoint key must be a string.\")\n",
        "            with self._lock:\n",
        "                if key in self._store:\n",
        "                    value, _size = self._store[key]\n",
        "                    self._store.move_to_end(key)\n",
        "                    return value\n",
        "                return default\n",
        "\n",
        "        def delete(self, key: str) -> bool:\n",
        "            if not isinstance(key, str):\n",
        "                raise TypeError(\"Checkpoint key must be a string.\")\n",
        "            with self._lock:\n",
        "                if key in self._store:\n",
        "                    _, size = self._store.pop(key)\n",
        "                    self._current_size -= size\n",
        "                    return True\n",
        "                return False\n",
        "\n",
        "        def clear(self) -> None:\n",
        "            with self._lock:\n",
        "                self._store.clear()\n",
        "                self._current_size = 0\n",
        "\n",
        "        def list_keys(self) -> list:\n",
        "            with self._lock:\n",
        "                return list(self._store.keys())\n",
        "\n",
        "        def size_info(self) -> Dict[str, Any]:\n",
        "            with self._lock:\n",
        "                return {\n",
        "                    'current_size_bytes': self._current_size,\n",
        "                    'limit_bytes': self.storage_limit_bytes,\n",
        "                    'items': len(self._store)\n",
        "                }\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f\"InMemoryCheckpointer(current={self._current_size}/{self.storage_limit_bytes} bytes, items={len(self._store)})\"\n",
        "\n",
        "    # Validate and instantiate\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"config must be a dictionary with keys 'storage_limit_mb' and 'eviction_policy'.\")\n",
        "\n",
        "    storage_limit_mb = config.get('storage_limit_mb', 100)\n",
        "    eviction_policy = config.get('eviction_policy', 'LRU')\n",
        "\n",
        "    try:\n",
        "        storage_limit_mb = float(storage_limit_mb)\n",
        "    except (TypeError, ValueError):\n",
        "        raise ValueError(\"storage_limit_mb must be a number representing megabytes.\")\n",
        "\n",
        "    if storage_limit_mb <= 0:\n",
        "        raise ValueError(\"storage_limit_mb must be greater than zero.\")\n",
        "\n",
        "    if eviction_policy not in ('LRU',):\n",
        "        raise ValueError(\"Unsupported eviction_policy. Only 'LRU' is supported in this implementation.\")\n",
        "\n",
        "    return _InMemoryCheckpointer(storage_limit_mb, eviction_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af8e56a",
      "metadata": {
        "id": "3af8e56a",
        "section": "tools"
      },
      "outputs": [],
      "source": [
        "# Tool: TelemetryService\n",
        "# Purpose: Optional telemetry/metrics collection for choices, turn progress, and guardrail events.\n",
        "# Category: External APIs\n",
        "\n",
        "def TelemetryService(config=None):\n",
        "    \"\"\"\n",
        "    Creates and returns a TelemetryClient that batches and sends telemetry events\n",
        "    to an external endpoint. Telemetry is optional and controlled by a sampling rate.\n",
        "\n",
        "    Configuration (config dict):\n",
        "      - endpoint (str): HTTP endpoint to send telemetry data to.\n",
        "      - batch_size (int): Number of events to batch before sending.\n",
        "      - sampling_rate (float): Probability [0.0-1.0] to sample an event.\n",
        "\n",
        "    Returns:\n",
        "      An object with methods:\n",
        "        - log_choice(event_id, choice, metadata=None)\n",
        "        - log_turn_progress(turn_id, progress, metadata=None)\n",
        "        - log_guardrail(event_id, guardrail_type, outcome, metadata=None)\n",
        "        - flush()\n",
        "        - close()\n",
        "    \"\"\"\n",
        "    # Local imports to satisfy \"Import necessary libraries at the function level\"\n",
        "    import time\n",
        "    import threading\n",
        "    import random\n",
        "    import logging\n",
        "    from typing import Any, Dict, Optional\n",
        "\n",
        "    # Defaults\n",
        "    DEFAULT_ENDPOINT = 'https://telemetry.example/api/collect'\n",
        "    DEFAULT_BATCH = 20\n",
        "    DEFAULT_SAMPLING = 0.5\n",
        "\n",
        "    # Normalize and validate config\n",
        "    if config is None:\n",
        "        config = {}\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"config must be a dictionary if provided\")\n",
        "\n",
        "    endpoint = config.get('endpoint', DEFAULT_ENDPOINT)\n",
        "    batch_size = config.get('batch_size', DEFAULT_BATCH)\n",
        "    sampling_rate = config.get('sampling_rate', DEFAULT_SAMPLING)\n",
        "\n",
        "    if not isinstance(endpoint, str) or not endpoint:\n",
        "        raise ValueError(\"Invalid 'endpoint' in config.\")\n",
        "    try:\n",
        "        batch_size = int(batch_size)\n",
        "    except (TypeError, ValueError):\n",
        "        raise ValueError(\"'batch_size' must be an integer.\")\n",
        "    if batch_size <= 0:\n",
        "        raise ValueError(\"'batch_size' must be greater than 0.\")\n",
        "    try:\n",
        "        sampling_rate = float(sampling_rate)\n",
        "    except (TypeError, ValueError):\n",
        "        raise ValueError(\"'sampling_rate' must be a float.\")\n",
        "    if not (0.0 <= sampling_rate <= 1.0):\n",
        "        raise ValueError(\"'sampling_rate' must be between 0.0 and 1.0.\")\n",
        "\n",
        "    class TelemetryClient:\n",
        "        def __init__(self, endpoint: str, batch_size: int, sampling_rate: float):\n",
        "            self.endpoint = endpoint\n",
        "            self.batch_size = batch_size\n",
        "            self.sampling_rate = sampling_rate\n",
        "            self._buffer = []\n",
        "            self._lock = threading.Lock()\n",
        "            self._stop_event = threading.Event()\n",
        "            self._thread = threading.Thread(target=self._worker, daemon=True)\n",
        "            self._logger = logging.getLogger(__name__)\n",
        "            # Start background worker\n",
        "            self._thread.start()\n",
        "\n",
        "        def _should_sample(self) -> bool:\n",
        "            # Always sample if sampling rate is 1.0\n",
        "            if self.sampling_rate >= 1.0:\n",
        "                return True\n",
        "            if self.sampling_rate <= 0.0:\n",
        "                return False\n",
        "            return random.random() < self.sampling_rate\n",
        "\n",
        "        def _worker(self):\n",
        "            try:\n",
        "                while not self._stop_event.is_set():\n",
        "                    # Periodic flush to ensure timely delivery\n",
        "                    self._flush()\n",
        "                    self._stop_event.wait(2.0)  # flush every 2 seconds if not already flushed\n",
        "            except Exception as ex:\n",
        "                self._logger.exception(\"Telemetry worker encountered an error: %s\", ex)\n",
        "\n",
        "        def _send_batch(self, batch):\n",
        "            if not batch:\n",
        "                return\n",
        "            # Ensure the batch is JSON-serializable\n",
        "            try:\n",
        "                import json\n",
        "                # Try requests first\n",
        "                try:\n",
        "                    import requests\n",
        "                    response = requests.post(self.endpoint, json=batch, timeout=5)\n",
        "                    response.raise_for_status()\n",
        "                    return\n",
        "                except Exception as req_ex:\n",
        "                    # Fallback to urllib\n",
        "                    try:\n",
        "                        from urllib import request as urllib_request\n",
        "                        data = json.dumps(batch).encode('utf-8')\n",
        "                        req = urllib_request.Request(self.endpoint, data=data, headers={'Content-Type': 'application/json'})\n",
        "                        with urllib_request.urlopen(req, timeout=5) as resp:\n",
        "                            resp.read()\n",
        "                        return\n",
        "                    except Exception as fallback_ex:\n",
        "                        self._logger.warning(\"Telemetry: HTTP post failed (requests and urllib fallback). Error: %s; Fallback error: %s\", req_ex, fallback_ex)\n",
        "                        # As a last resort, just drop the batch\n",
        "                        return\n",
        "            except Exception as e:\n",
        "                self._logger.exception(\"Telemetry: Unexpected error while sending batch: %s\", e)\n",
        "\n",
        "        def _flush(self):\n",
        "            with self._lock:\n",
        "                if not self._buffer:\n",
        "                    return\n",
        "                batch = self._buffer\n",
        "                self._buffer = []\n",
        "            try:\n",
        "                self._send_batch(batch)\n",
        "            except Exception as e:\n",
        "                self._logger.exception(\"Telemetry: Failed to flush batch: %s\", e)\n",
        "\n",
        "        def log_choice(self, event_id: str, choice: Any, metadata: Optional[Dict[str, Any]] = None):\n",
        "            \"\"\"\n",
        "            Log a user choice event.\n",
        "\n",
        "            Parameters:\n",
        "              event_id (str): Unique identifier for the event.\n",
        "              choice (Any): The chosen option.\n",
        "              metadata (dict, optional): Additional contextual information.\n",
        "            \"\"\"\n",
        "            if event_id is None:\n",
        "                self._logger.warning(\"Telemetry: log_choice called with None event_id; ignoring.\")\n",
        "                return\n",
        "            payload = {\n",
        "                'type': 'choice',\n",
        "                'event_id': event_id,\n",
        "                'choice': choice,\n",
        "                'metadata': metadata or {}\n",
        "            }\n",
        "            self._enqueue(payload)\n",
        "\n",
        "        def log_turn_progress(self, turn_id: str, progress: float, metadata: Optional[Dict[str, Any]] = None):\n",
        "            \"\"\"\n",
        "            Log progress on a turn or step.\n",
        "\n",
        "            Parameters:\n",
        "              turn_id (str): Identifier for the turn/step.\n",
        "              progress (float): Progress value between 0.0 and 1.0.\n",
        "              metadata (dict, optional): Additional contextual information.\n",
        "            \"\"\"\n",
        "            if turn_id is None:\n",
        "                self._logger.warning(\"Telemetry: log_turn_progress called with None turn_id; ignoring.\")\n",
        "                return\n",
        "            try:\n",
        "                progress = float(progress)\n",
        "            except (TypeError, ValueError):\n",
        "                self._logger.warning(\"Telemetry: invalid progress value; must be a float. Got: %r\", progress)\n",
        "                return\n",
        "            payload = {\n",
        "                'type': 'turn_progress',\n",
        "                'turn_id': turn_id,\n",
        "                'progress': progress,\n",
        "                'metadata': metadata or {}\n",
        "            }\n",
        "            self._enqueue(payload)\n",
        "\n",
        "        def log_guardrail(self, event_id: str, guardrail_type: str, outcome: str, metadata: Optional[Dict[str, Any]] = None):\n",
        "            \"\"\"\n",
        "            Log a guardrail event.\n",
        "\n",
        "            Parameters:\n",
        "              event_id (str): Identifier for the guardrail event.\n",
        "              guardrail_type (str): Type or name of the guardrail.\n",
        "              outcome (str): Outcome or result (e.g., 'violation', 'passed').\n",
        "              metadata (dict, optional): Additional contextual information.\n",
        "            \"\"\"\n",
        "            if event_id is None:\n",
        "                self._logger.warning(\"Telemetry: log_guardrail called with None event_id; ignoring.\")\n",
        "                return\n",
        "            payload = {\n",
        "                'type': 'guardrail',\n",
        "                'event_id': event_id,\n",
        "                'guardrail_type': guardrail_type,\n",
        "                'outcome': outcome,\n",
        "                'metadata': metadata or {}\n",
        "            }\n",
        "            self._enqueue(payload)\n",
        "\n",
        "        def _enqueue(self, payload: Dict[str, Any]):\n",
        "            if not self._should_sample():\n",
        "                return\n",
        "            entry = {'timestamp': time.time(), 'payload': payload}\n",
        "            with self._lock:\n",
        "                self._buffer.append(entry)\n",
        "                if len(self._buffer) >= self.batch_size:\n",
        "                    self._flush()\n",
        "\n",
        "        def flush(self):\n",
        "            \"\"\"Public method to flush any buffered telemetry events immediately.\"\"\"\n",
        "            self._flush()\n",
        "\n",
        "        def close(self):\n",
        "            \"\"\"Gracefully stop background worker and flush remaining events.\"\"\"\n",
        "            self._stop_event.set()\n",
        "            self._thread.join(timeout=3)\n",
        "            self._flush()\n",
        "\n",
        "    # Instantiate and return a client\n",
        "    client = TelemetryClient(endpoint=endpoint, batch_size=batch_size, sampling_rate=sampling_rate)\n",
        "    return client"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "484f9897",
      "metadata": {
        "id": "484f9897",
        "section": "nodes"
      },
      "source": [
        "## Nodes\n",
        "\n",
        "Implement workflow nodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "937cdeb7",
      "metadata": {
        "id": "937cdeb7",
        "section": "nodes"
      },
      "outputs": [],
      "source": [
        "# Node: story_gen_node (single authoritative generator)\n",
        "# Generates the next narrative segment + 3 choices, updates memory, and then pauses for human input.\n",
        "\n",
        "from typing import Any, Dict, List\n",
        "import time\n",
        "import os\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "def story_gen_node(state: WorkflowState) :\n",
        "    # ---- Pull prior state ----\n",
        "    story_so_far: List[str] = list(state.get(\"story_so_far\") or [])\n",
        "    memory_summary: str = state.get(\"memory_summary\", \"\") or \"\"\n",
        "    facts: Dict[str, Any] = dict(state.get(\"facts\") or {})\n",
        "    tone: Dict[str, Any] = dict(state.get(\"tone\") or {\"style\": \"adventure\", \"profanity_guard\": True, \"thematic_boundaries\": []})\n",
        "\n",
        "    turn_index: int = int(state.get(\"turn_index\") or 0)\n",
        "    max_turns: int = int(state.get(\"max_turns\") or 12)\n",
        "\n",
        "    telemetry: List[Dict[str, Any]] = list(state.get(\"telemetry\") or [])\n",
        "\n",
        "    # ---- LLM settings ----\n",
        "    model = os.getenv(\"MODEL\", \"gpt-5-nano\")\n",
        "    temperature = float(os.getenv(\"TEMPERATURE\", \"0.6\"))\n",
        "    max_tokens = int(os.getenv(\"MAX_OUTPUT_TOKENS\", \"900\"))\n",
        "    timeout = float(os.getenv(\"OPENAI_TIMEOUT_S\", \"60\"))\n",
        "\n",
        "    # ---- Draft segment ----\n",
        "    t0 = time.time()\n",
        "\n",
        "    #parse pronouns from user_pronouns\n",
        "    pronoun_a = [\"he\", \"she\", \"they\"]\n",
        "    pronoun_an = [\"him\", \"her\", \"them\"]\n",
        "\n",
        "    if state.get(\"user_pronouns\"):\n",
        "        user_pronouns = state.get(\"user_pronouns\")\n",
        "        # detect the delimiter, could be commas, backslashes, or even forward slashes\n",
        "        delimiter = \",\"\n",
        "        if \",\" in user_pronouns:\n",
        "            delimiter = \",\"\n",
        "        elif \"\\\\\" in user_pronouns:\n",
        "            delimiter = \"\\\\\"\n",
        "        elif \"/\" in user_pronouns:\n",
        "            delimiter = \"/\"\n",
        "        else:\n",
        "            delimiter = \" \"\n",
        "        user_pronouns = user_pronouns.split(delimiter)\n",
        "        for pa in pronoun_a:\n",
        "            if pa not in user_pronouns:\n",
        "                pronoun_a.remove(pa)\n",
        "        for pan in pronoun_an:\n",
        "            if pan not in user_pronouns:\n",
        "                pronoun_an.remove(pan)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    user_info = f\"\"\"named {state.get('user_name','Player')}. {pronoun_a[0]} is {state.get('user_age','18')} years old and is interested in a story in the {state.get('user_genre','adventure')} genre.\n",
        "                {state.get('user_name','Player')} would like to play in a setting that is {state.get('user_setting','fantasy')}.\n",
        "                {pronoun_a[0]} has requested the story follow the theme(s) of {state.get('user_theme','raucous adventure and enduring friendship')}.\n",
        "                \"\"\"\n",
        "\n",
        "    try:\n",
        "        draft = LLM_StoryWriter(\n",
        "            turn_index=turn_index,\n",
        "            user_info=user_info,\n",
        "            story_so_far=story_so_far,\n",
        "            memory_summary=memory_summary,\n",
        "            facts=facts,\n",
        "            tone=tone,\n",
        "            model=model,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        draft = f\"(StoryWriter error: {e})\"\n",
        "\n",
        "    # ---- Continuity pass ----\n",
        "    t1 = time.time()\n",
        "    try:\n",
        "        refined = llm_continuity_editor(\n",
        "            segment=draft,\n",
        "            memory_facts={\"memory_summary\": memory_summary, \"facts\": facts},\n",
        "            continuity_constraints=[\"Keep names/locations consistent\", \"Preserve tense and voice\"],\n",
        "            model=model,\n",
        "            temperature=0.45,\n",
        "            max_tokens=450,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "    except Exception:\n",
        "        refined = draft\n",
        "\n",
        "    # ---- Safety/tone pass ----\n",
        "    t2 = time.time()\n",
        "    try:\n",
        "        safe_segment = llm_safety_guard(\n",
        "            segment=refined,\n",
        "            tone=tone,\n",
        "            model=model,\n",
        "            temperature=0.2,\n",
        "            max_tokens=450,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "    except Exception:\n",
        "        safe_segment = refined\n",
        "\n",
        "    # ---- Lore update ----\n",
        "    t3 = time.time()\n",
        "    lore = LLM_Lorekeeper(\n",
        "        segment=safe_segment,\n",
        "        prior_memory_summary=memory_summary,\n",
        "        prior_facts=facts,\n",
        "        model=model,\n",
        "        temperature=0.25,\n",
        "        max_tokens=450,\n",
        "        timeout=timeout,\n",
        "    )\n",
        "    new_memory = str(lore.get(\"memory_summary\", memory_summary) or memory_summary)\n",
        "    new_facts = dict(lore.get(\"facts\", facts) or facts)\n",
        "\n",
        "    # ---- Choices ----\n",
        "    t4 = time.time()\n",
        "    try:\n",
        "        choices = LLM_ChoiceArchitect(\n",
        "            current_segment=safe_segment,\n",
        "            memory_summary=new_memory,\n",
        "            facts=new_facts,\n",
        "            model=model,\n",
        "            temperature=0.4,\n",
        "            max_tokens=350,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "    except Exception:\n",
        "        choices = [\n",
        "            \"Investigate the nearest landmark for clues.\",\n",
        "            \"Talk to a local or ally to gather information.\",\n",
        "            \"Press forward into the unknown and take a risk.\",\n",
        "        ]\n",
        "\n",
        "    # ---- Update story log ----\n",
        "    story_so_far.append(safe_segment)\n",
        "\n",
        "    # Update MessagesState chat history (optional)\n",
        "    messages=[AIMessage(content=safe_segment)]\n",
        "\n",
        "    # ---- Telemetry ----\n",
        "    telemetry.append({\n",
        "        \"turn\": turn_index,\n",
        "        \"stage_ms\": {\n",
        "            \"draft\": int((t1 - t0) * 1000),\n",
        "            \"continuity\": int((t2 - t1) * 1000),\n",
        "            \"safety\": int((t3 - t2) * 1000),\n",
        "            \"lore\": int((t4 - t3) * 1000),\n",
        "            \"choices\": int((time.time() - t4) * 1000),\n",
        "        },\n",
        "        \"model\": model,\n",
        "    })\n",
        "\n",
        "    # ---- Run status ----\n",
        "    current_segment_index = turn_index\n",
        "    next_turn_index = turn_index + 1\n",
        "    if next_turn_index >= max_turns:\n",
        "        run_status = \"ended\"\n",
        "        end_reason = \"Reached max_turns\"\n",
        "        # No choices needed if we're done\n",
        "        choices = []\n",
        "    else:\n",
        "        run_status = \"waiting_for_human\"\n",
        "        end_reason = None\n",
        "\n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"turn_index\": next_turn_index,\n",
        "        \"current_segment_index\": current_segment_index,\n",
        "        \"story_so_far\": [safe_segment],\n",
        "        \"current_segment\": safe_segment,\n",
        "        \"choices\": choices,\n",
        "        \"selected_choice\": None,\n",
        "        \"memory_summary\": new_memory,\n",
        "        \"facts\": new_facts,\n",
        "        \"tone\": tone,\n",
        "        \"run_status\": run_status,\n",
        "        \"end_reason\": end_reason,\n",
        "        \"telemetry\": telemetry,\n",
        "        \"max_turns\": max_turns,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fadb37bb",
      "metadata": {
        "id": "fadb37bb",
        "section": "nodes"
      },
      "outputs": [],
      "source": [
        "# (Deprecated) choice_gen_node\n",
        "# Older versions of this notebook had a separate choice generator.\n",
        "# story_gen_node now generates BOTH the segment and the choices.\n",
        "\n",
        "def choice_gen_node(state: WorkflowState) :\n",
        "    return story_gen_node(state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbb6cc5f",
      "metadata": {
        "id": "fbb6cc5f",
        "section": "nodes"
      },
      "outputs": [],
      "source": [
        "def human_input_node(state: MessagesState) :\n",
        "    \"\"\"\n",
        "    human_input_node\n",
        "\n",
        "    Serves as an interrupt point. In an interactive execution with interrupt_before=['human_input_node'],\n",
        "    this node runs AFTER the user has provided input (injected into state).\n",
        "    If input is present (selected_choice), it passes through.\n",
        "    If not, it signals that the system is waiting.\n",
        "    \"\"\"\n",
        "    state_update = {}\n",
        "    selected_choice = state.get(\"selected_choice\")\n",
        "\n",
        "    # If a choice has been selected (e.g. via UI injection before resuming), just pass through.\n",
        "    if selected_choice is not None:\n",
        "        state_update[\"run_status\"] = \"running\"\n",
        "        return state_update\n",
        "\n",
        "    # Otherwise, mark as waiting.\n",
        "    state_update[\"run_status\"] = \"waiting_for_human\"\n",
        "    return state_update"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Callable, Deque, List, Optional, Tuple, Union\n",
        "from collections import deque\n",
        "import json\n",
        "\n",
        "import tiktoken\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Token helpers\n",
        "# ---------------------------\n",
        "\n",
        "import tiktoken\n",
        "from typing import Any, Dict, List, Tuple, Optional\n",
        "\n",
        "def _get_encoder(model: str):\n",
        "    try:\n",
        "        return tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        return tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "def count_tokens(text: str, model: str) -> int:\n",
        "    enc = _get_encoder(model)\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "def pack_turn(turn_index: int, segment_text: str, selected_choice: str) -> str:\n",
        "    return (\n",
        "        f\"TURN {turn_index}\\n\"\n",
        "        f\"STORY:\\n{segment_text.strip()}\\n\"\n",
        "        f\"CHOICE:\\n{selected_choice.strip()}\\n\"\n",
        "        f\"---\\n\"\n",
        "    )\n",
        "\n",
        "# ---------------------------\n",
        "# Rolling Memory Manager\n",
        "# ---------------------------\n",
        "\n",
        "SummarizerBundleFn = Callable[\n",
        "    [str, str, str],\n",
        "    Tuple[str, str]\n",
        "]\n",
        "# (chunk_text, previous_long_summary, previous_state_ledger) -> (new_long_summary, new_state_ledger)\n",
        "\n",
        "\n",
        "\n",
        "def summarize_story_bundle(\n",
        "    to_summarize: Union[str, List[str]],\n",
        "    prev_long_summary: str,\n",
        "    prev_state_ledger: str,\n",
        "    max_summary_tokens: int = 500,\n",
        "    max_ledger_tokens: int = 250,\n",
        "    model: str = \"gpt-5-nano\",\n",
        ") -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Incrementally merges a NEW chunk into (prev_long_summary, prev_state_ledger).\n",
        "\n",
        "    Returns:\n",
        "      (new_long_summary, new_state_ledger)\n",
        "\n",
        "    \"\"\"\n",
        "    # You said you already have this helper:\n",
        "    summary_llm = make_llm(model=model)\n",
        "\n",
        "    if isinstance(to_summarize, list):\n",
        "        chunk_text = \"\\n\\n\".join(s.strip() for s in to_summarize if s and s.strip())\n",
        "    else:\n",
        "        chunk_text = str(to_summarize).strip()\n",
        "\n",
        "    # If ledger tokens set to 0, instruct model to return empty ledger.\n",
        "    want_ledger = max_ledger_tokens > 0\n",
        "\n",
        "    system = SystemMessage(content=f\"\"\"\n",
        "You are a summarizer assistant for a Choose Your Own Adventure story.\n",
        "Your purpose is to read the story so far up to the latest story segment/scene.\n",
        "Then, you carefully and conscientiously summarize the story up to most recent story segment, including all relevant details.\n",
        "DO NOT write the next part of the story, you ONLY summarize!\n",
        "\n",
        "\n",
        "Goal:\n",
        "- Integrate the NEW CHUNK into the existing summary + state ledger.\n",
        "- Do NOT write new story content. Only summarize and extract state.\n",
        "- Be factual, past tense, \"CliffsNotes\" style (NOT prose).\n",
        "\n",
        "Requirements:\n",
        "  Provide a concise summary no longer than a paragraph (no longer than about {max_summary_tokens} tokens max) of the story so far.\n",
        "  The summary is meant to bring a reader up to speed succinctly without missing important information. Dont write it like a story;\n",
        "  it is more like just a clifs notes summarization about the story, told in past tense, narrator-style but factual and to the point.\n",
        "  It should include:\n",
        "    - Canon facts (things that must not change): world rules, identities, irreversible events.\n",
        "    - Major plot beats (what happened, in bullets).\n",
        "    - Open threads (mysteries, promises, threats, timers).\n",
        "    - Known constraints (injuries, missing items, deadlines).\n",
        "\n",
        "Hard constraints:\n",
        "- The `long_summary` MUST be <= ~{max_summary_tokens} tokens.\n",
        "- If you cannot fit everything, prioritize: (1) state_ledger correctness, (2) canon facts, (3) open threads, (4) major beats.\n",
        "\n",
        "\n",
        "Never invent details. If unclear, write 'Unknown' or omit.\n",
        "\"\"\".strip())\n",
        "\n",
        "    human = HumanMessage(content=f\"\"\"\n",
        "PREVIOUS_LONG_SUMMARY:\n",
        "{prev_long_summary.strip() or \"(none)\"}\n",
        "\n",
        "PREVIOUS_STATE_LEDGER:\n",
        "{prev_state_ledger.strip() or \"(none)\"}\n",
        "\n",
        "NEW_CHUNK_TO_INTEGRATE:\n",
        "{chunk_text}\n",
        "\"\"\".strip())\n",
        "\n",
        "    resp = summary_llm.invoke([system, human])\n",
        "    text = _ai_message_to_text(resp)\n",
        "\n",
        "    # Parse JSON robustly\n",
        "    new_long = text\n",
        "    new_ledger = prev_state_ledger\n",
        "    if not want_ledger:\n",
        "        new_ledger = \"\"\n",
        "\n",
        "    # Enforce token budgets with a shrink pass if needed\n",
        "    new_long, new_ledger = _enforce_budgets_with_shrink(\n",
        "        summary_llm=summary_llm,\n",
        "        model=model,\n",
        "        long_summary=new_long,\n",
        "        state_ledger=new_ledger,\n",
        "        max_summary_tokens=max_summary_tokens,\n",
        "        max_ledger_tokens=max_ledger_tokens,\n",
        "        want_ledger=want_ledger,\n",
        "    )\n",
        "\n",
        "    return new_long.strip(), new_ledger.strip()\n",
        "\n",
        "\n",
        "def _parse_summary_bundle_json(text: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Tries strict JSON, then tries to salvage if model wrapped it in code fences.\n",
        "    \"\"\"\n",
        "    cleaned = text.strip()\n",
        "\n",
        "    # remove ```json fences if present\n",
        "    if cleaned.startswith(\"```\"):\n",
        "        cleaned = cleaned.strip(\"`\")\n",
        "        # crude: if it started with json, remove the leading 'json' token\n",
        "        cleaned = cleaned.replace(\"json\\n\", \"\", 1).strip()\n",
        "\n",
        "    try:\n",
        "        obj = json.loads(cleaned)\n",
        "        return str(obj.get(\"long_summary\", \"\")).strip(), str(obj.get(\"state_ledger\", \"\")).strip()\n",
        "    except Exception:\n",
        "        # fallback: treat entire thing as long_summary if JSON failed\n",
        "        return cleaned, \"\"\n",
        "\n",
        "\n",
        "def _enforce_budgets_with_shrink(\n",
        "    summary_llm,\n",
        "    model: str,\n",
        "    long_summary: str,\n",
        "    state_ledger: str,\n",
        "    max_summary_tokens: int,\n",
        "    max_ledger_tokens: int,\n",
        "    want_ledger: bool,\n",
        "    max_passes: int = 2,\n",
        ") -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    If the model overshoots budgets, run a controlled compression pass.\n",
        "    \"\"\"\n",
        "    for _ in range(max_passes):\n",
        "        long_ok = count_tokens(long_summary, model=model) <= max_summary_tokens\n",
        "        ledger_ok = (not want_ledger) or (count_tokens(state_ledger, model=model) <= max_ledger_tokens)\n",
        "\n",
        "        if long_ok and ledger_ok:\n",
        "            return long_summary, state_ledger\n",
        "\n",
        "        system = SystemMessage(content=f\"\"\"\n",
        "You compress summaries WITHOUT losing key state.\n",
        "\n",
        "\n",
        "Constraints:\n",
        "- long_summary <= ~{max_summary_tokens} tokens\n",
        "\n",
        "Rules:\n",
        "- Keep state_ledger accurate and complete (priority #1).\n",
        "- Remove flavor and redundancy first.\n",
        "- Prefer fewer bullets over longer bullets.\n",
        "- If something must be dropped, drop Major Beats details before Canon/Open Threads/Constraints.\n",
        "\"\"\".strip())\n",
        "\n",
        "        human = HumanMessage(content=f\"\"\"\n",
        "CURRENT_LONG_SUMMARY:\n",
        "{long_summary}\n",
        "\n",
        "CURRENT_STATE_LEDGER:\n",
        "{state_ledger}\n",
        "\"\"\".strip())\n",
        "\n",
        "        resp = summary_llm.invoke([system, human])\n",
        "        text = _ai_message_to_text(resp)\n",
        "        long_summary = text\n",
        "\n",
        "    return long_summary, state_ledger\n",
        "\n",
        "\n",
        "def ensure_memory_fields(state: Dict[str, Any]) -> None:\n",
        "    state.setdefault(\"recent_turns\", [])     # list of dicts: {\"text\":..., \"tokens\":..., \"turn_index\":...}\n",
        "    state.setdefault(\"recent_tokens\", 0)\n",
        "    state.setdefault(\"memory_summary\", \"\")   # long summary (older-than-window)\n",
        "    state.setdefault(\"state_ledger\", \"\")     # authoritative state facts\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "def summarize_story(to_summarize: Union[str, List[str]], max_summary_tokens: int = 300) -> str:\n",
        "    \"\"\"\n",
        "    Backward compatible: returns ONLY a concise \"bring you up to speed\" summary.\n",
        "    Under the hood, uses summarize_story_bundle and returns the long_summary portion.\n",
        "    \"\"\"\n",
        "    long_summary, _ledger = summarize_story_bundle(\n",
        "        to_summarize=to_summarize,\n",
        "        prev_long_summary=\"\",\n",
        "        prev_state_ledger=\"\",\n",
        "        max_summary_tokens=max_summary_tokens,\n",
        "        max_ledger_tokens=0,  # no ledger needed here\n",
        "    )\n",
        "    return long_summary\n",
        "\n",
        "def record_turn_and_compact_update(\n",
        "    state: \"WorkflowState\",\n",
        "    *,\n",
        "    segment_text: str,\n",
        "    choice_text: str,\n",
        "    token_model: str = \"gpt-5-nano\",\n",
        "    recent_max_tokens: int = 4000,\n",
        "    recent_target_tokens: int = 3500,\n",
        "    evict_chunk_target: int = 1500,\n",
        "    safety_margin: int = 250,\n",
        "    summary_bundle=None,  # (chunk, prev_summary, prev_ledger) -> (new_summary, new_ledger)\n",
        "    current_ledger=None\n",
        ") -> Dict[str, Any]:\n",
        "    recent_turns: List[dict] = list(state.get(\"recent_turns\") or [])\n",
        "    recent_tokens: int = int(state.get(\"recent_tokens\") or 0)\n",
        "\n",
        "    turn_index = int(state.get(\"turn_index\") or 0)\n",
        "    packed = pack_turn(turn_index, segment_text, choice_text)\n",
        "    packed_tokens = count_tokens(packed, token_model)\n",
        "\n",
        "    recent_turns.append({\"turn_index\": turn_index, \"text\": packed, \"tokens\": packed_tokens})\n",
        "    recent_tokens += packed_tokens\n",
        "\n",
        "    # compact if needed\n",
        "    if (recent_tokens + safety_margin) > recent_max_tokens and summary_bundle is not None:\n",
        "        evicted, evicted_tokens = [], 0\n",
        "\n",
        "        while recent_turns and (\n",
        "            (recent_tokens + safety_margin) > recent_target_tokens\n",
        "            or evicted_tokens < evict_chunk_target\n",
        "        ):\n",
        "            t = recent_turns.pop(0)\n",
        "            evicted.append(t)\n",
        "            recent_tokens -= int(t[\"tokens\"])\n",
        "            evicted_tokens += int(t[\"tokens\"])\n",
        "\n",
        "            if (recent_tokens + safety_margin) <= recent_target_tokens and evicted_tokens >= evict_chunk_target:\n",
        "                break\n",
        "\n",
        "        if evicted:\n",
        "            chunk_text = \"\".join(t[\"text\"] for t in evicted)\n",
        "            prev_summary = state.get(\"memory_summary\") or \"\"\n",
        "            prev_ledger = current_ledger or state.get(\"state_ledger\") or \"\"\n",
        "            new_summary, new_ledger = summarizer_bundle_adapter(chunk_text, prev_summary, prev_ledger)\n",
        "\n",
        "            # Optional: store chunk-only summaries in segment_summaries\n",
        "            # NOTE: tuple keys can be annoying for JSON-based checkpointing;\n",
        "            # consider using f\"{start}:{end}\" instead.\n",
        "            start_i = int(evicted[0][\"turn_index\"])\n",
        "            end_i = int(evicted[-1][\"turn_index\"])\n",
        "            seg_key = f\"{start_i}-{end_i}\"\n",
        "\n",
        "            return {\n",
        "                \"recent_turns\": recent_turns,\n",
        "                \"recent_tokens\": recent_tokens,\n",
        "                \"story_summary\": new_summary,\n",
        "                \"state_ledger\": new_ledger,\n",
        "                \"segment_summaries\": {seg_key: summarize_story(chunk_text, max_summary_tokens=300)},\n",
        "            }\n",
        "\n",
        "    return {\"recent_turns\": recent_turns, \"recent_tokens\": recent_tokens}\n",
        "\n",
        "def summarizer_bundle_adapter(\n",
        "    chunk_text: str,\n",
        "    prev_long_summary: str,\n",
        "    prev_state_ledger: str,\n",
        ") -> Tuple[str, str]:\n",
        "    return summarize_story_bundle(\n",
        "        to_summarize=chunk_text,\n",
        "        prev_long_summary=prev_long_summary,\n",
        "        prev_state_ledger=prev_state_ledger,\n",
        "        max_summary_tokens=650,   # tune\n",
        "        max_ledger_tokens=250,    # tune\n",
        "        model=\"gpt-5-nano\",\n",
        "    )"
      ],
      "metadata": {
        "id": "bkAbN_SdArMF"
      },
      "id": "bkAbN_SdArMF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def summarize_story(to_summarize:Union[str,List[str]],max_summary_tokens=300) -> str:\n",
        "#     \"\"\"\n",
        "#     Summarize the story so far.\n",
        "#     \"\"\"\n",
        "#     summary_llm = make_llm(model=\"gpt-5-nano\")\n",
        "\n",
        "#     msgs = [SystemMessage(content=f\"\"\"\n",
        "#     You are a summarizer assistant to a storyteller telling a 'Choose Your Own Adventure' story.\n",
        "#     Your purpose is to read the story so far up to the latest story segment/scene.\n",
        "#     Then, you carefully and conscientiously summarize the story up to most recent story segment, including all relevant details.\n",
        "#     DO NOT write the next part of the story, you ONLY summarize!\n",
        "\n",
        "#     Requirements:\n",
        "#     Provide a concise summary no longer than a paragraph (no longer than about {max_summary_tokens} tokens max) of the story so far.\n",
        "#     The summary is meant to bring a reader up to speed succinctly without missing important information. Dont write it like a story;\n",
        "#     it is more like just a clifs notes summarization about the story, told in past tense, narrator-style but factual and to the point.\n",
        "#     It should include:\n",
        "#       - Canon facts (things that must not change): world rules, identities, irreversible events.\n",
        "#       - Major plot beats (what happened, in bullets).\n",
        "#       - Open threads (mysteries, promises, threats, timers).\n",
        "#       - Known constraints (injuries, missing items, deadlines).\n",
        "#     \"\"\")]"
      ],
      "metadata": {
        "id": "X1BWSex22Rh-"
      },
      "id": "X1BWSex22Rh-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIP3m3gL4hQL"
      },
      "id": "jIP3m3gL4hQL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0588a298",
      "metadata": {
        "id": "0588a298",
        "section": "nodes"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Dict, List, Optional, Tuple, OrderedDict\n",
        "\n",
        "def apply_choice_node(state: WorkflowState):\n",
        "    \"\"\"\n",
        "    Apply the selected choice: update story history, memory, and facts.\n",
        "    Does NOT generate the next segment (left to story_gen_node).\n",
        "    Does NOT increment turn_index (left to story_gen_node logic).\n",
        "    \"\"\"\n",
        "    new_state = {}\n",
        "\n",
        "    turn_index = state.get(\"turn_index\", 0)\n",
        "    story_so_far = list(state.get(\"story_so_far\", []))\n",
        "    current_segment = state.get(\"current_segment\", \"\")\n",
        "    seg_index = int(state.get(\"current_segment_index\") or turn_index)\n",
        "    choices = state.get(\"choices\", [])\n",
        "    selected_choice = state.get(\"selected_choice\")\n",
        "    memory_summary = state.get(\"memory_summary\", \"\")\n",
        "    facts = state.get(\"facts\", {}).copy()\n",
        "    story_id = state.get(\"gen_id\")\n",
        "    apply_llm = make_llm(model=\"gpt-5-nano\")\n",
        "    # If we have a current segment that wasn't committed yet (it usually is by story_gen, but just in case)\n",
        "    # Actually story_gen commits it. We just need to commit the CHOICE.\n",
        "    facts_str = json.dumps(facts, indent=2)\n",
        "    prev_state_ledger = state.get(\"state_ledger\")\n",
        "    ledger = \"\"\n",
        "\n",
        "    if selected_choice is not None and isinstance(selected_choice, int) and 0 <= selected_choice < len(choices):\n",
        "        choice_text = choices[selected_choice]\n",
        "        # Append choice to story\n",
        "        ledger = str(turn_index) + \":\" + \"\".join(f\"{k}:{v}\\n\" for k, v in facts.items()) + \" \".join(choices) + \"\\n Player Selected choice: \" + choice_text + \"\\n Current Memory Summary: \\n \" + memory_summary\n",
        "        # Update Memory/Facts (Lightweight logic)\n",
        "        if current_segment and choice_text:\n",
        "            new_state.update(\n",
        "                record_turn_and_compact_update(\n",
        "                    state.copy(),\n",
        "                    segment_text=current_segment,\n",
        "                    choice_text=choice_text,\n",
        "                    token_model=\"gpt-5-nano\",\n",
        "                    summary_bundle=summarizer_bundle_adapter,\n",
        "                    current_ledger=ledger\n",
        "                )\n",
        "            )\n",
        "        recent_turns = \"\\n\\n\".join(t.get(\"text\", \"\") for t in new_state.get(\"recent_turns\") or [])\n",
        "        if recent_turns:\n",
        "            recent_turns = \"\\n\\n\" + recent_turns\n",
        "        else:\n",
        "            recent_turns = \"\"\n",
        "        store.put((str(story_id), \"choices\"), str(seg_index), {\"text\": f\"**Turn {seg_index} Choice:** {choice_text}\"})\n",
        "\n",
        "        msgs = [SystemMessage(content=\"\"\"\n",
        "        You are an assistant to the storyteller telling a 'Choose Your Own Adventure' story.\n",
        "        Your purpose is to read the story so far as well as the most recent player choice in response to the latest story segment/scene.\n",
        "        Then, you perform tasks requested of you, which might include: extracting facts from the most recent choice,\n",
        "        determining if existing facts should impact the results of the players choice, determining if particular memories shown are relevant to the choice and its outcome and\n",
        "        if so how they should impact the story, extracting new memories to store that are relevant, and providing a story summary. DO NOT write the next part of the story, you ONLY answer\n",
        "        questions about the story and the players latest choice's impact on the story, and provide tips and hints for the story writer.\n",
        "        \"\"\"),HumanMessage(content=f\"\"\"\n",
        "STORY_SO_FAR:\n",
        "{new_state.get('story_summary') or '(none)'}\n",
        "\n",
        "{recent_turns}\n",
        "\n",
        "PRIOR_FACTS_JSON:\n",
        "{json.dumps(facts or {}, indent=2)}\n",
        "\n",
        "PRIOR_STATE_LEDGER:\n",
        "{ledger or prev_state_ledger}\n",
        "\n",
        "LATEST_SEGMENT:\n",
        "{current_segment}\n",
        "\n",
        "PLAYER_SELECTED_CHOICE:\n",
        "{choice_text}\n",
        "                          \"\"\".strip())]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        memory_search_terms = []\n",
        "        # tokenize choice_text by sentence\n",
        "        for sent in re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', choice_text):\n",
        "            memory_search_terms.append(sent)\n",
        "            for word in sent.split():\n",
        "                memory_search_terms.append(word)\n",
        "\n",
        "        memories_set = set()\n",
        "        memories = []\n",
        "        for term in memory_search_terms:\n",
        "            search_res = store.search((str(story_id), \"memories\"), query=term, limit=20)\n",
        "            memcandidates = {}\n",
        "            for i, r in enumerate(search_res):\n",
        "              memtext = \"\"\n",
        "              memscore = -1\n",
        "              try:\n",
        "                if isinstance(r, (SearchItem,Item)):\n",
        "                    memtext = r.value.get(\"text\")\n",
        "                    memscore = r.score\n",
        "                else:\n",
        "                  memscore = float(json.loads(r).get(\"score\",0.0))\n",
        "\n",
        "                  r_ = json.loads(r.dict()[\"value\"], object_pairs_hook=OrderedDict)\n",
        "\n",
        "                  if r_.get(\"text\"):\n",
        "                    memtext = r_.get(\"text\")\n",
        "                  else:\n",
        "                    try:\n",
        "                      memtext = getattr(r_, \"text\", str(r_)).strip()\n",
        "                      if re.match(r'\\{.*\\}', memtext):\n",
        "                        memtext = json.loads(memtext)\n",
        "                        if isinstance(memtext, dict):\n",
        "                          memtext =memtext.get(\"text\")\n",
        "                        elif isinstance(memtext, str):\n",
        "                          memtext =memtext\n",
        "\n",
        "                    except Exception as e:\n",
        "                      print(f\"Error parsing memory: {e}, trying again\")\n",
        "                      if memscore and isinstance(memscore, float) and memtext and isinstance(memtext, str):\n",
        "                          pass\n",
        "                      else:\n",
        "                          raise Exception(f\"Error parsing memory: {e}\")\n",
        "\n",
        "\n",
        "              except Exception as ee:\n",
        "                print(f\"Error parsing memory: {ee}\")\n",
        "                try:\n",
        "                  if not memscore or memscore < 0 or not isinstance(memscore, float):\n",
        "                    try:\n",
        "                      assert r.score\n",
        "                      memscore = float(r.score)\n",
        "                    except:\n",
        "                      memscore = float(r.dict().get(\"score\",0.0))\n",
        "                  if memtext is None or not isinstance(memtext, str) or not memtext.strip():\n",
        "                    try:\n",
        "                      assert r.value\n",
        "                      memtext = r.value.get(\"text\")\n",
        "                    except:\n",
        "                        r_ = json.loads(r.dict()[\"value\"], object_pairs_hook=OrderedDict)\n",
        "                        if r_.get(\"text\"):\n",
        "                          memtext =r_.get(\"text\")\n",
        "                        else:\n",
        "                          try:\n",
        "                            memtext = getattr(r_, \"text\", str(r_)).strip()\n",
        "                            #determine if the return memtext is a stringified dict\n",
        "                            if re.match(r'\\{.*\\}', memtext):\n",
        "                              memtext = json.loads(memtext)\n",
        "                              if isinstance(memtext, dict):\n",
        "                                print(f\"Found dict in memory: {memtext}\")\n",
        "                                memtext =memtext.get(\"text\")\n",
        "                              elif isinstance(memtext, str):\n",
        "                                memtext =memtext\n",
        "                            else:\n",
        "                              memtext =str(memtext)\n",
        "                          except Exception as e:\n",
        "                            print(f\"Error parsing memory: {e}\")\n",
        "                            pass\n",
        "\n",
        "\n",
        "\n",
        "                except Exception as ee:\n",
        "                  print(f\"Error parsing memory: {ee}\")\n",
        "                  pass\n",
        "              if memscore and isinstance(memscore, float) and memtext and isinstance(memtext, str):\n",
        "                  memcandidates[i] = (memtext, memscore)\n",
        "            ct_twenty_plus = [True for v in memcandidates.values() if float(v[1]) > 0.2].count(True)\n",
        "\n",
        "            if ct_twenty_plus > 0:\n",
        "              memcandidates = {k: v for k, v in memcandidates.items() if (v[1] > 0.18 and ct_twenty_plus <= 2) or (v[1] > 0.2)}\n",
        "            elif [True for v in memcandidates.values() if float(v[1]) > 0.15].count(True) > 0:\n",
        "              memcandidates = {k: v for k, v in memcandidates.items() if v[1] > 0.15}\n",
        "            if memcandidates:\n",
        "                for mc in memcandidates:\n",
        "                    memories_set.add(mc[0])\n",
        "        if memories_set:\n",
        "            memories = list(memories_set)\n",
        "            # sort memories by their scores for the corresponding text on memcandidates, which is Dict[int,Tuple[str,float]]\n",
        "            memories = sorted(memories, key=lambda x: [mc[1] for mc in memcandidates.values() if mc[0] == x][0], reverse=True)\n",
        "            memories = memories[:10]\n",
        "\n",
        "\n",
        "\n",
        "        msgs_a = msgs.copy()\n",
        "        msgs_a.append(HumanMessage(content=f\"\"\"\n",
        "        Now please extract any new facts based on the latest story segment and the latest player choice, separated by newlines for each.\n",
        "        \"\"\"))\n",
        "        resp = apply_llm.invoke(msgs_a)\n",
        "        text = _ai_message_to_text(resp)\n",
        "        new_facts_lst = text.split(\"\\n\")\n",
        "        new_facts = {}\n",
        "        for f in new_facts_lst:\n",
        "            if f and \":\" in f:\n",
        "                k, v = f.split(\":\", 1)\n",
        "                new_facts[k] = v\n",
        "            else:\n",
        "                new_facts[f] = \"\"\n",
        "        msgs_a.append(resp)\n",
        "        mem_str = \"\"\n",
        "        if memories:\n",
        "            for i, m in enumerate(memories):\n",
        "                mem_str += f\"{i+1}: {m}\\n\"\n",
        "        msgs_a.append(HumanMessage(content=f\"\"\"\n",
        "        Next, carefully consider: Are any of these existing memories (before the latest choice) relevant to the latest choice, and if so, how do they impact the outcome of the latest choice?\n",
        "        Specifically:\n",
        "          - Do any of these memories (from before the latest choice) impact the outcome of the latest choice?\n",
        "          - Which ones (by their index) are relevant to the latest choice, and if so, how are they related to the outcome of the latest choice?\n",
        "          - Do any of these memories (from before the latest choice) need to be removed or updated, and if so, why?\n",
        "\n",
        "        Here are the memories, in order of semantic relevance ranking:\n",
        "        {mem_str}\n",
        "\n",
        "        Write your answer ONLY in the form of a JSON object with the following keys and value types:\n",
        "        {{\n",
        "          'index':{{\n",
        "          'relevant':bool,\n",
        "          'impact':str,\n",
        "          'reason':str}}\n",
        "        }}\n",
        "\n",
        "        Return only the JSON object, nothing else, with no extra text.\n",
        "\n",
        "        \"\"\"))\n",
        "\n",
        "        resp = apply_llm.invoke(msgs_a)\n",
        "        text = _ai_message_to_text(resp)\n",
        "        msgs_a.append(resp)\n",
        "        memadvice = {}\n",
        "        try:\n",
        "            memadvice = json.loads(text)\n",
        "        except:\n",
        "            memadvice = {}\n",
        "            # find the first float after re match to \"'index':\", including the decimal and values after the decimal until either a quotation, comma, or bracket\n",
        "            if m := re.search(r\"'index':(\\d+\\.\\d+)\", text):\n",
        "                memadvice[\"index\"] = float(m.group(1))\n",
        "            # find the first bool after re match to \"'relevant':\" until either a quotation, comma, or bracket\n",
        "            if m := re.search(r\"'relevant':(True|False)\", text):\n",
        "\n",
        "                try:\n",
        "                    m_bool = json.loads(m.group(1))\n",
        "                    if isinstance(m_bool, bool):\n",
        "                        memadvice[\"relevant\"] = m_bool\n",
        "                    else:\n",
        "                        memadvice[\"relevant\"] = bool(m_bool)\n",
        "                except:\n",
        "                    m_bool = bool(m.group(1))\n",
        "\n",
        "                    memadvice[\"relevant\"] = m_bool\n",
        "            elif m := re.search(r\"'relevant':(\\w+)\", text):\n",
        "                memadvice[\"relevant\"] = bool(m.group(1))\n",
        "            # find all str text after re match to \"'impact':\" until either a quotation, comma, or bracket\n",
        "            if m := re.findall(r\"'impact':(\\w+)\", text):\n",
        "                memadvice[\"impact\"] = m\n",
        "            # find all str text after re match to \"'reason':\" or \"'reasoning':\" until either a quotation, comma, or bracket\n",
        "            if m := re.findall(r\"'reason':(\\w+)\", text):\n",
        "                memadvice[\"reason\"] = m\n",
        "            elif m := re.findall(r\"'reasoning':(\\w+)\", text):\n",
        "                memadvice[\"reason\"] = m\n",
        "\n",
        "        # remove all memories at indices matching indices of memadvice where \"relevant\" is False\n",
        "        for i, m in enumerate(memories):\n",
        "            if i in memadvice.keys():\n",
        "                if not memadvice[i][\"relevant\"]:\n",
        "                    memories.remove(m)\n",
        "\n",
        "        msgs_a.append(HumanMessage(content=f\"\"\"\n",
        "        Next, carefully consider: Do any of the prior facts (before the latest choice) impact the outcome of the latest choice?\n",
        "          - Do any of the prior facts (before the latest choice) become relevant to the latest choice, and if so, how do they impact the outcome of the latest choice?\n",
        "          - Do any of the prior facts need to be removed or updated, and if so, why?\n",
        "          - Do any of the new facts contradict or alter the prior facts, and if so, how?\n",
        "\n",
        "          Write your answer in the form of a bullet list of relevant advice for the storyteller based on the questions above. Separate each advice item with a newline.\n",
        "          \"\"\"))\n",
        "\n",
        "        resp = apply_llm.invoke(msgs_a)\n",
        "        text = _ai_message_to_text(resp)\n",
        "        msgs_a.append(resp)\n",
        "\n",
        "        facts_advice = text.split(\"\\n\")\n",
        "        facts_advice = [f for f in facts_advice if f]\n",
        "\n",
        "        msgs_b = msgs_a.copy()\n",
        "        msgs_a.append(HumanMessage(content=f\"\"\"\n",
        "        Now it's time to search the memory bank again. Based on the story so far, the facts, the latest choice, and the new facts, please generate a list of search queries to search the memory bank for relevant memories.\n",
        "        Think carefully and step by step to generate a list of search queries, and respond in the form of a simple list of search queries, separated by newlines. These will be used to search the memory bank.\n",
        "        \"\"\"))\n",
        "        old_mems = memories\n",
        "        resp = apply_llm.invoke(msgs_a)\n",
        "        text = _ai_message_to_text(resp)\n",
        "        msgs_a.append(resp)\n",
        "        memory_search_terms = text.split(\"\\n\")\n",
        "        memory_search_terms = [f for f in memory_search_terms if f]\n",
        "\n",
        "        # tokenize choice_text by sentence\n",
        "        for sent in re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', choice_text):\n",
        "            memory_search_terms.append(sent)\n",
        "            for word in sent.split():\n",
        "                memory_search_terms.append(word)\n",
        "\n",
        "        for i,term in memory_search_terms:\n",
        "            search_res = store.search((str(story_id), \"memories\"), query=term, limit=20)\n",
        "            memcandidates = {}\n",
        "            map_text_to_query = {}\n",
        "            for i, r in enumerate(search_res):\n",
        "              memtext = \"\"\n",
        "              memscore = -1\n",
        "              try:\n",
        "                if isinstance(r, (SearchItem,Item)):\n",
        "                    memtext = r.value.get(\"text\")\n",
        "                    memscore = r.score\n",
        "                else:\n",
        "                  memscore = float(json.loads(r).get(\"score\",0.0))\n",
        "\n",
        "                  r_ = json.loads(r.dict()[\"value\"], object_pairs_hook=OrderedDict)\n",
        "\n",
        "                  if r_.get(\"text\"):\n",
        "                    memtext = r_.get(\"text\")\n",
        "                  else:\n",
        "                    try:\n",
        "                      memtext = getattr(r_, \"text\", str(r_)).strip()\n",
        "                      if re.match(r'\\{.*\\}', memtext):\n",
        "                        memtext = json.loads(memtext)\n",
        "                        if isinstance(memtext, dict):\n",
        "                          memtext =memtext.get(\"text\")\n",
        "                        elif isinstance(memtext, str):\n",
        "                          memtext =memtext\n",
        "\n",
        "                    except Exception as e:\n",
        "                      print(f\"Error parsing memory: {e}, trying again\")\n",
        "                      if memscore and isinstance(memscore, float) and memtext and isinstance(memtext, str):\n",
        "                          pass\n",
        "                      else:\n",
        "                          raise Exception(f\"Error parsing memory: {e}\")\n",
        "\n",
        "\n",
        "              except Exception as ee:\n",
        "                print(f\"Error parsing memory: {ee}\")\n",
        "                try:\n",
        "                  if not memscore or memscore < 0 or not isinstance(memscore, float):\n",
        "                    try:\n",
        "                      assert r.score\n",
        "                      memscore = float(r.score)\n",
        "                    except:\n",
        "                      memscore = float(r.dict().get(\"score\",0.0))\n",
        "                  if memtext is None or not isinstance(memtext, str) or not memtext.strip():\n",
        "                    try:\n",
        "                      assert r.value\n",
        "                      memtext = r.value.get(\"text\")\n",
        "                    except:\n",
        "                        r_ = json.loads(r.dict()[\"value\"], object_pairs_hook=OrderedDict)\n",
        "                        if r_.get(\"text\"):\n",
        "                          memtext =r_.get(\"text\")\n",
        "                        else:\n",
        "                          try:\n",
        "                            memtext = getattr(r_, \"text\", str(r_)).strip()\n",
        "                            #determine if the return memtext is a stringified dict\n",
        "                            if re.match(r'\\{.*\\}', memtext):\n",
        "                              memtext = json.loads(memtext)\n",
        "                              if isinstance(memtext, dict):\n",
        "                                print(f\"Found dict in memory: {memtext}\")\n",
        "                                memtext =memtext.get(\"text\")\n",
        "                              elif isinstance(memtext, str):\n",
        "                                memtext =memtext\n",
        "                            else:\n",
        "                              memtext =str(memtext)\n",
        "                          except Exception as e:\n",
        "                            print(f\"Error parsing memory: {e}\")\n",
        "                            pass\n",
        "\n",
        "\n",
        "\n",
        "                except Exception as ee:\n",
        "                  print(f\"Error parsing memory: {ee}\")\n",
        "                  pass\n",
        "              if memscore and isinstance(memscore, float) and memtext and isinstance(memtext, str):\n",
        "                  memcandidates[term] = (memtext, memscore)\n",
        "            ct_twenty_plus = [True for v in memcandidates.items() if float(v[1]) > 0.2].count(True)\n",
        "\n",
        "            if ct_twenty_plus > 0:\n",
        "              memcandidates = {k: v for k, v in memcandidates.items() if (v[1] > 0.18 and ct_twenty_plus <= 2) or (v[1] > 0.2)}\n",
        "            elif [True for v in memcandidates.values() if float(v[1]) > 0.15].count(True) > 0:\n",
        "              memcandidates = {k: v for k, v in memcandidates.items() if v[1] > 0.15}\n",
        "            if memcandidates:\n",
        "                for mc in memcandidates:\n",
        "                    memories_set.add(mc[0])\n",
        "                    if term not in map_text_to_query:\n",
        "                        map_text_to_query[term] = []\n",
        "                    map_text_to_query[term].append(mc[0])\n",
        "        if memories_set:\n",
        "            memories = list(memories_set)\n",
        "            # sort memories by their scores for the corresponding text on memcandidates, which is Dict[int,Tuple[str,float]]\n",
        "            memories = sorted(memories, key=lambda x: [mc[1] for mc in memcandidates.values() if mc[0] == x][0], reverse=True)\n",
        "            memories = memories[:10]\n",
        "\n",
        "        # remove any memories that are in old_mems\n",
        "        memories = [m for m in memories if m not in old_mems]\n",
        "        old_mem_str = mem_str\n",
        "        mem_str = \"\"\n",
        "        if memories and len(memories) > 0 and memory_search_terms:\n",
        "            for i, m in enumerate(memories):\n",
        "                mterms = map_text_to_query.get(m)\n",
        "                if not mterms:\n",
        "                    mem_str = f\"{i+1}: {m}. Unknown search terms.\\n\"\n",
        "                    continue\n",
        "                mem_str += f\"Memory #{i+1}: {m}, resulted from the search terms:\\n      ({', '.join(mterms)})\\n\\n\"\n",
        "                msgs_a.append(HumanMessage(content=f\"\"\"\n",
        "                Okay, here are the memory search results for each of your queries: Are any of these newly pulled memories (before the latest choice) relevant to the latest choice, and if so, how do they impact the outcome of the latest choice?\n",
        "                Specifically:\n",
        "                  - Do any of these memories (from before the latest choice) impact the outcome of the latest choice?\n",
        "                  - Which ones (by their index) are relevant to the latest choice, and if so, how are they related to the outcome of the latest choice?\n",
        "                  - Do any of these memories (from before the latest choice) need to be removed or updated, and if so, why?\n",
        "\n",
        "                Here are the memories, in order of semantic relevance ranking:\n",
        "                {mem_str}\n",
        "\n",
        "                Write your answer ONLY in the form of a JSON object with the following keys and value types:\n",
        "                {{\n",
        "                  'index':{{\n",
        "                  'relevant':bool,\n",
        "                  'impact':str,\n",
        "                  'reason':str,\n",
        "                  }}\n",
        "                }}\n",
        "\n",
        "                Return only the JSON object, nothing else, with no extra text.\n",
        "\n",
        "                \"\"\"))\n",
        "\n",
        "                resp = apply_llm.invoke(msgs_a)\n",
        "                text = _ai_message_to_text(resp)\n",
        "                msgs_a.append(resp)\n",
        "                memadvice = {}\n",
        "                try:\n",
        "                    memadvice = json.loads(text)\n",
        "                except:\n",
        "                    memadvice = {}\n",
        "                    # find the first float after re match to \"'index':\", including the decimal and values after the decimal until either a quotation, comma, or bracket\n",
        "                    if m := re.search(r\"'index':(\\d+\\.\\d+)\", text):\n",
        "                        memadvice[\"index\"] = float(m.group(1))\n",
        "                    # find the first bool after re match to \"'relevant':\" until either a quotation, comma, or bracket\n",
        "                    if m := re.search(r\"'relevant':(True|False)\", text):\n",
        "\n",
        "                        try:\n",
        "                            m_bool = json.loads(m.group(1))\n",
        "                            if isinstance(m_bool, bool):\n",
        "                                memadvice[\"relevant\"] = m_bool\n",
        "                            else:\n",
        "                                memadvice[\"relevant\"] = bool(m_bool)\n",
        "                        except:\n",
        "                            m_bool = bool(m.group(1))\n",
        "\n",
        "                            memadvice[\"relevant\"] = m_bool\n",
        "                    elif m := re.search(r\"'relevant':(\\w+)\", text):\n",
        "                        memadvice[\"relevant\"] = bool(m.group(1))\n",
        "                    # find all str text after re match to \"'impact':\" until either a quotation, comma, or bracket\n",
        "                    if m := re.findall(r\"'impact':(\\w+)\", text):\n",
        "                        memadvice[\"impact\"] = m\n",
        "                    # find all str text after re match to \"'reason':\" or \"'reasoning':\" until either a quotation, comma, or bracket\n",
        "                    if m := re.findall(r\"'reason':(\\w+)\", text):\n",
        "                        memadvice[\"reason\"] = m\n",
        "                    elif m := re.findall(r\"'reasoning':(\\w+)\", text):\n",
        "                        memadvice[\"reason\"] = m\n",
        "\n",
        "        # remove all memories at indices matching indices of memadvice where \"relevant\" is False\n",
        "        for i, m in enumerate(memories):\n",
        "            if i in memadvice.keys():\n",
        "                if not memadvice[i][\"relevant\"]:\n",
        "                    memories.remove(m)\n",
        "\n",
        "        msgs_b.append(HumanMessage(content=f\"\"\"\n",
        "        Now it's time to produce more memories for the memory bank. Based on the latest story events, the facts, the latest choice, and the new facts, please generate a list of sentences to add to the memory bank.\n",
        "        Think carefully and step by step to generate a list of new memories, only based on the most recent story events and in the form of simple, concise statement sentences, separated by newlines. These will be used to add to the memory bank.\n",
        "        \"\"\"))\n",
        "\n",
        "        resp = apply_llm.invoke(msgs_b)\n",
        "        text = _ai_message_to_text(resp)\n",
        "        msgs_b.append(resp)\n",
        "        new_memories = text.split(\"\\n\")\n",
        "        new_memories = [f for f in new_memories if f]\n",
        "        for m in new_memories:\n",
        "            store.put((str(story_id), \"memories\"), str(seg_index), {\"text\": m})\n",
        "        new_mem_str = \"\\n Newly created memories this turn:\"\n",
        "        for i, m in enumerate(new_memories):\n",
        "            new_mem_str += f\"{i+1}: {m}\\n\"\n",
        "        final_mem_str = \"\\n\".join(old_mems) + \"\\n\" + \"\\n\".join(memories) + \"\\n\" + new_mem_str\n",
        "\n",
        "        msgs_c = msgs_b.copy() + msgs_a[6:].copy()\n",
        "        msgs_c.append(HumanMessage(content=f\"\"\"\n",
        "        Now please produce a short, concise summary of the relevant memories that tells the storyteller what the memories are about. Here are the memories:\n",
        "\n",
        "        {final_mem_str}\n",
        "\n",
        "        Your summary should be in the form of a paragraph no more than about 100 tokens/words.\n",
        "        \"\"\"))\n",
        "        resp = apply_llm.invoke(msgs_c)\n",
        "        text = _ai_message_to_text(resp)\n",
        "        msgs_c.append(resp)\n",
        "        memory_summary = text\n",
        "\n",
        "    # Clear transient state for next turn\n",
        "    new_state.update({\n",
        "        \"messages\": msgs_c,\n",
        "        \"facts_advice\": facts_advice if facts_advice else [],\n",
        "        \"story_summary\": \"\",\n",
        "        \"memory_summary\": memory_summary,\n",
        "        \"facts\": new_facts,\n",
        "        \"selected_choice\": None,\n",
        "        \"choices\": [],\n",
        "        \"current_segment\": \"\", # Clear so story_gen generates new one\n",
        "        \"run_status\": \"running\"\n",
        "    })\n",
        "\n",
        "    return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed7604e5",
      "metadata": {
        "id": "ed7604e5",
        "section": "nodes"
      },
      "outputs": [],
      "source": [
        "# Router node: decide whether to continue or end (purely deterministic)\n",
        "\n",
        "def should_continue_router_node(state: WorkflowState) :\n",
        "    # If some node already ended the run, keep it ended\n",
        "    if state.get(\"run_status\") == \"ended\":\n",
        "        return state\n",
        "\n",
        "    # Otherwise, mark running and continue\n",
        "    return {\"run_status\": \"running\"}\n",
        "\n",
        "def route_step(state: WorkflowState) -> str:\n",
        "    return \"end_node\" if state.get(\"run_status\") == \"ended\" else \"story_gen_node\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6263ffe",
      "metadata": {
        "id": "b6263ffe",
        "section": "nodes"
      },
      "outputs": [],
      "source": [
        "# End node: produce an epilogue + recap (optional LLM), then mark ended.\n",
        "\n",
        "from typing import List\n",
        "import os\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "def end_node(state: WorkflowState) :\n",
        "    story_so_far: List[str] = list(state.get(\"story_so_far\") or [])\n",
        "    memory_summary: str = state.get(\"memory_summary\", \"\") or \"\"\n",
        "    end_reason: str = state.get(\"end_reason\", \"\") or \"\"\n",
        "\n",
        "    model = os.getenv(\"MODEL\", \"gpt-5-nano\")\n",
        "    timeout = float(os.getenv(\"OPENAI_TIMEOUT_S\", \"60\"))\n",
        "\n",
        "    # If we have no story, just end.\n",
        "    if not story_so_far:\n",
        "        return {\"run_status\": \"ended\", \"end_reason\": end_reason or \"No story generated\"}\n",
        "\n",
        "    # Ask the model for a short epilogue + recap (kept compact)\n",
        "    try:\n",
        "        llm = make_llm(model=model, temperature=0.35, max_tokens=350, timeout=timeout)\n",
        "        messages = [\n",
        "            SystemMessage(content=\"You are an epilogue writer. Keep it short and satisfying.\"),\n",
        "            HumanMessage(content=(\n",
        "                \"Write a 4–7 sentence epilogue that closes the story and a 3-bullet recap.\\n\\n\"\n",
        "                f\"End reason: {end_reason or 'n/a'}\\n\\n\"\n",
        "                f\"Memory summary: {memory_summary or 'none'}\\n\\n\"\n",
        "                \"Story (most recent last):\\n\" + \"\\n\\n\".join(story_so_far[-6:])\n",
        "            )),\n",
        "        ]\n",
        "        ai = llm.invoke(messages)\n",
        "        epilogue = _ai_message_to_text(ai)\n",
        "    except Exception as e:\n",
        "        epilogue = f\"(Epilogue generation skipped: {e})\"\n",
        "\n",
        "    messages_hist = list(state.get(\"messages\") or [])\n",
        "    if epilogue:\n",
        "        messages_hist.append(AIMessage(content=epilogue))\n",
        "\n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=epilogue)],\n",
        "        \"run_status\": \"ended\",\n",
        "        \"end_reason\": end_reason or \"Ended\",\n",
        "        \"choices\": [],\n",
        "        \"selected_choice\": None,\n",
        "        \"epilogue\": epilogue,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9093638",
      "metadata": {
        "id": "e9093638",
        "section": "graph"
      },
      "source": [
        "## Graph Construction\n",
        "\n",
        "Build the LangGraph workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6042d22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6042d22",
        "outputId": "fe78747f-8d21-4923-d145-15b4068380a1",
        "section": "graph"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph compiled.\n"
          ]
        }
      ],
      "source": [
        "# Build the LangGraph workflow (human-in-the-loop CYOA)\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Conditional routing after story generation:\n",
        "# - If story_gen_node ended the run (e.g., max_turns), go straight to end_node.\n",
        "# - Otherwise pause before human_input_node.\n",
        "def route_after_story_gen(state: WorkflowState) -> str:\n",
        "    return \"end_node\" if state.get(\"run_status\") == \"ended\" else \"human_input_node\"\n",
        "\n",
        "workflow = StateGraph(WorkflowState)\n",
        "\n",
        "# Nodes\n",
        "workflow.add_node(\"story_gen_node\", story_gen_node)\n",
        "workflow.add_node(\"human_input_node\", human_input_node)\n",
        "workflow.add_node(\"apply_choice_node\", apply_choice_node)\n",
        "workflow.add_node(\"should_continue_router\", should_continue_router_node)\n",
        "workflow.add_node(\"end_node\", end_node)\n",
        "\n",
        "# Edges\n",
        "workflow.add_edge(START, \"story_gen_node\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"story_gen_node\",\n",
        "    route_after_story_gen,\n",
        "    {\"human_input_node\": \"human_input_node\", \"end_node\": \"end_node\"},\n",
        ")\n",
        "\n",
        "# Human choice loop\n",
        "workflow.add_edge(\"human_input_node\", \"apply_choice_node\")\n",
        "workflow.add_edge(\"apply_choice_node\", \"should_continue_router\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"should_continue_router\",\n",
        "    route_step,\n",
        "    {\"story_gen_node\": \"story_gen_node\", \"end_node\": \"end_node\"},\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"end_node\", END)\n",
        "\n",
        "# Compile with in-memory checkpointing, and interrupt right before we \"wait\" for the user\n",
        "checkpointer = MemorySaver()\n",
        "graph = workflow.compile(checkpointer=checkpointer, interrupt_before=[\"human_input_node\"])\n",
        "\n",
        "print(\"Graph compiled.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "oPe9iOrENo24",
        "outputId": "14a92edf-193c-47bd-f19b-b5f28d8cfad8"
      },
      "id": "oPe9iOrENo24",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x7e4ef5a9f8c0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "681513e4",
      "metadata": {
        "id": "681513e4",
        "section": "execution"
      },
      "source": [
        "## Execution\n",
        "\n",
        "Run the workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150a8a71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902,
          "referenced_widgets": [
            "3b90b442364244afa1bf7ddbf624f8ba",
            "3b367ccce9564be99fc382c7b7b930ab",
            "7c50089f042941918a14cc8edf6b64e1",
            "bfaf500e5b564a52aefa72402aaa6b47",
            "f501edba459d4cf689e08ec14dd70afe",
            "7440fe12a1214db89fc98b985ea23f3b",
            "bcc4956397fe4cd0a94ddffda6c30fee",
            "c13691aeb5e8472a8d9a6d505075f1a3",
            "5dc1a0271ee342c2b11bf5e51ff8b4ce",
            "339df824c61d4c87aad0f11e5ee0ba4c",
            "f2e1ff68e94445bcaf4c7901ae62a007",
            "922cf51b9aaf40589657dd87269d877f",
            "992d18341c2c4eca80129d3bc78470b3",
            "a358225a05bc44f2855d784d0cbb449d",
            "6abfd1597fa048aba4f88ef2fb7527e5",
            "6c1218da081a45338667024c29c9d8a0",
            "dba1dd71f627431abdc426901fa3f1d6",
            "fb10405b1fa849e0a162f1e6400f2f91",
            "5ef1afc25d4644d5b13bc34a8e80c16f",
            "625741131f204ec8b39442f0a0d1bb4d",
            "984215dcff3348af81a70793e441777f"
          ]
        },
        "id": "150a8a71",
        "outputId": "7b4abd17-1a12-430d-925b-8c44f7c46bb5",
        "section": "execution"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='\\n        <style>\\n          .widget-button .btn {\\n            white-space: normal !important;\\n …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b90b442364244afa1bf7ddbf624f8ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Output(layout=Layout(border_bottom='1px solid #ddd', border_left='1px solid #ddd', border_right…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfaf500e5b564a52aefa72402aaa6b47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1607084293.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_btn, idx)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mButton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{i+1}. {c}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"100%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_click\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0m_btn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0mbtns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbtns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1607084293.py\u001b[0m in \u001b[0;36m_on_choice\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# Update the checkpointed state at the point of interruption, then resume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"selected_choice\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_node\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human_input_node\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_on_restart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_btn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1607084293.py\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(self, initial_state)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Stream until interrupt or completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2644\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2647\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-232456845.py\u001b[0m in \u001b[0;36mapply_choice_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \"\"\"))\n\u001b[1;32m    197\u001b[0m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgs_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mnew_facts_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mnew_facts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'text'"
          ]
        }
      ],
      "source": [
        "# Simple interactive UI (ipywidgets) for the CYOA graph\n",
        "\n",
        "import uuid\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "class AdventureUI:\n",
        "    def __init__(self, graph):\n",
        "        self.graph = graph\n",
        "\n",
        "        self.out = widgets.Output(\n",
        "            layout=widgets.Layout(\n",
        "                width=\"100%\",\n",
        "                border=\"1px solid #ddd\",\n",
        "                padding=\"12px\",\n",
        "                overflow=\"auto\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Make the box stretch children full-width nicely\n",
        "        self.choices_box = widgets.VBox(\n",
        "            layout=widgets.Layout(\n",
        "                width=\"100%\",\n",
        "                align_items=\"stretch\",   # important for consistent full-width buttons in flex layouts\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.restart_btn = widgets.Button(\n",
        "            description=\"Restart\",\n",
        "            button_style=\"warning\",\n",
        "            layout=widgets.Layout(width=\"160px\", margin=\"10px 0 0 0\"),\n",
        "        )\n",
        "        self.restart_btn.on_click(self._on_restart)\n",
        "\n",
        "        self.thread_id = None\n",
        "        self.config = None\n",
        "        self.input_data = None  # <-- store it so Restart works\n",
        "\n",
        "        # Optional: if your button labels are long, this helps them wrap + left-align.\n",
        "        display(widgets.HTML(\"\"\"\n",
        "        <style>\n",
        "          .widget-button .btn {\n",
        "            white-space: normal !important;\n",
        "            text-align: left !important;\n",
        "          }\n",
        "        </style>\n",
        "        \"\"\"))\n",
        "\n",
        "\n",
        "    def _new_thread(self):\n",
        "        self.thread_id = str(uuid.uuid4())\n",
        "        self.config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
        "\n",
        "    def _initial_state(self,input_data):\n",
        "        tone = None\n",
        "        if input_data:\n",
        "            tone = {\"story_genre\": input_data.get(\"story_genre\",\"fantasy adventure\"),\n",
        "                    \"story_theme\": input_data.get(\"story_theme\",\"Epic dungeon exploration\"),\n",
        "                    \"story_setting\": input_data.get(\"story_setting\", \"high fantasy world full of magic, monsters, and maidens\"),\n",
        "                    \"profanity_guard\": False, \"thematic_boundaries\": [],\n",
        "                    \"style\": input_data.get(\"story_genre\",\"fantasy adventure\") + \" \" + input_data.get(\"story_theme\",\"Epic dungeon exploration\") + \" \" + input_data.get(\"story_setting\", \"high fantasy world full of magic, monsters, and maidens\")\n",
        "                    }\n",
        "        user_name = input_data.get(\"user_name\", \"Player\")\n",
        "        user_age = input_data.get(\"user_age\", 18)\n",
        "        user_pronouns = input_data.get(\"user_pronouns\", \"they/them\")\n",
        "        return {\n",
        "            \"turn_index\": 0,\n",
        "            \"user_pronouns\": user_pronouns,\n",
        "            \"user_name\": user_name,\n",
        "            \"user_age\": user_age,\n",
        "            \"story_so_far\": [],\n",
        "            \"messages\": [],\n",
        "            \"current_segment\": \"\",\n",
        "            \"choices\": [],\n",
        "            \"selected_choice\": None,\n",
        "            \"memory_summary\": \"\",\n",
        "            \"facts\": {},\n",
        "            \"tone\": tone or {\"style\": \"adventure\", \"profanity_guard\": False, \"thematic_boundaries\": []},\n",
        "            \"telemetry\": [],\n",
        "            \"max_turns\": 12,\n",
        "            \"run_status\": \"running\",\n",
        "            \"end_reason\": None,\n",
        "        }\n",
        "\n",
        "    def start(self, input_data):\n",
        "        self.input_data = input_data  # <-- save for restart\n",
        "        self._new_thread()\n",
        "        display(widgets.VBox([self.out, self.choices_box, self.restart_btn],\n",
        "                             layout=widgets.Layout(width=\"100%\")))\n",
        "        self.run_loop(initial_state=self._initial_state(input_data))\n",
        "\n",
        "    def run_loop(self, initial_state=None):\n",
        "        last_state = None\n",
        "\n",
        "        # Stream until interrupt or completion\n",
        "        for event in self.graph.stream(initial_state, config=self.config, stream_mode=\"values\"):\n",
        "            last_state = event\n",
        "\n",
        "        if last_state is None:\n",
        "            try:\n",
        "                last_state = self.graph.get_state(self.config).values\n",
        "            except Exception:\n",
        "                last_state = {}\n",
        "\n",
        "        self.render(last_state)\n",
        "\n",
        "    def render(self, state):\n",
        "        status = state.get(\"run_status\", \"unknown\")\n",
        "        turn = state.get(\"turn_index\", 0)\n",
        "        segment = state.get(\"current_segment\", \"\")\n",
        "        epilogue = state.get(\"epilogue\", \"\")\n",
        "\n",
        "        with self.out:\n",
        "            clear_output()\n",
        "            print(f\"Turn: {turn} | Status: {status}\")\n",
        "            if state.get(\"end_reason\"):\n",
        "                print(f\"End reason: {state.get('end_reason')}\")\n",
        "            if segment:\n",
        "                print(\"\\n\" + segment)\n",
        "            if epilogue:\n",
        "                print(\"\\n---\\n\" + epilogue)\n",
        "\n",
        "        choices = state.get(\"choices\") or []\n",
        "        if status == \"waiting_for_human\" and len(choices) >= 1:\n",
        "            btns = []\n",
        "            for i, c in enumerate(choices):\n",
        "                b = widgets.Button(description=f\"{i+1}. {c}\", layout=widgets.Layout(width=\"100%\"))\n",
        "                b.on_click(lambda _btn, idx=i: self._on_choice(idx))\n",
        "                btns.append(b)\n",
        "            self.choices_box.children = tuple(btns)\n",
        "        else:\n",
        "            self.choices_box.children = (widgets.HTML(\"<i>No choices available.</i>\"),)\n",
        "\n",
        "    def _on_choice(self, idx: int):\n",
        "        # Update the checkpointed state at the point of interruption, then resume\n",
        "        self.graph.update_state(self.config, {\"selected_choice\": idx}, as_node=\"human_input_node\")\n",
        "        self.run_loop(initial_state=None)\n",
        "\n",
        "    def _on_restart(self, _btn):\n",
        "        self._new_thread()\n",
        "        self.run_loop(initial_state=self._initial_state(self.input_data))\n",
        "\n",
        "# Run it\n",
        "try:\n",
        "  ui = AdventureUI(graph)\n",
        "  ui.start(input_data)\n",
        "except Exception as e:\n",
        "  print(f\"Error: {e}. \\n Choices: {ui.graph.state.get(\"choices\")}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3b90b442364244afa1bf7ddbf624f8ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_3b367ccce9564be99fc382c7b7b930ab",
            "placeholder": "​",
            "style": "IPY_MODEL_7c50089f042941918a14cc8edf6b64e1",
            "tabbable": null,
            "tooltip": null,
            "value": "\n        <style>\n          .widget-button .btn {\n            white-space: normal !important;\n            text-align: left !important;\n          }\n        </style>\n        "
          }
        },
        "3b367ccce9564be99fc382c7b7b930ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c50089f042941918a14cc8edf6b64e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "bfaf500e5b564a52aefa72402aaa6b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f501edba459d4cf689e08ec14dd70afe",
              "IPY_MODEL_7440fe12a1214db89fc98b985ea23f3b",
              "IPY_MODEL_bcc4956397fe4cd0a94ddffda6c30fee"
            ],
            "layout": "IPY_MODEL_c13691aeb5e8472a8d9a6d505075f1a3",
            "tabbable": null,
            "tooltip": null
          }
        },
        "f501edba459d4cf689e08ec14dd70afe": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_922cf51b9aaf40589657dd87269d877f",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Turn: 1 | Status: waiting_for_human\n",
                  "\n",
                  "Daddy stood on the quay as fog crawled in from the North Sea like a living thing, clutching the compass that had lived in her pocket longer than most friendships had lasted. The city behind her roared with steam, bells, and a choir of sailors cursing the night. It was the year of hard rain and harder rumors, when every shadow seemed to cough up a memory it would rather forget. Her companions—Mara, a nurse with a laugh like broken glass; Njall, a cartographer who could map\n"
                ]
              }
            ],
            "tabbable": null,
            "tooltip": null
          }
        },
        "7440fe12a1214db89fc98b985ea23f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_992d18341c2c4eca80129d3bc78470b3",
              "IPY_MODEL_a358225a05bc44f2855d784d0cbb449d",
              "IPY_MODEL_6abfd1597fa048aba4f88ef2fb7527e5"
            ],
            "layout": "IPY_MODEL_5dc1a0271ee342c2b11bf5e51ff8b4ce",
            "tabbable": null,
            "tooltip": null
          }
        },
        "bcc4956397fe4cd0a94ddffda6c30fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "Restart",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_339df824c61d4c87aad0f11e5ee0ba4c",
            "style": "IPY_MODEL_f2e1ff68e94445bcaf4c7901ae62a007",
            "tabbable": null,
            "tooltip": null
          }
        },
        "c13691aeb5e8472a8d9a6d505075f1a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "5dc1a0271ee342c2b11bf5e51ff8b4ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "stretch",
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "339df824c61d4c87aad0f11e5ee0ba4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": "10px 0 0 0",
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "160px"
          }
        },
        "f2e1ff68e94445bcaf4c7901ae62a007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_family": null,
            "font_size": null,
            "font_style": null,
            "font_variant": null,
            "font_weight": null,
            "text_color": null,
            "text_decoration": null
          }
        },
        "922cf51b9aaf40589657dd87269d877f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": "1px solid #ddd",
            "border_left": "1px solid #ddd",
            "border_right": "1px solid #ddd",
            "border_top": "1px solid #ddd",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": "auto",
            "padding": "12px",
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "992d18341c2c4eca80129d3bc78470b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "1. {",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6c1218da081a45338667024c29c9d8a0",
            "style": "IPY_MODEL_dba1dd71f627431abdc426901fa3f1d6",
            "tabbable": null,
            "tooltip": null
          }
        },
        "a358225a05bc44f2855d784d0cbb449d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "2. \"choices\": [",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_fb10405b1fa849e0a162f1e6400f2f91",
            "style": "IPY_MODEL_5ef1afc25d4644d5b13bc34a8e80c16f",
            "tabbable": null,
            "tooltip": null
          }
        },
        "6abfd1597fa048aba4f88ef2fb7527e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "3. \"Ask Mara to tend to the wounded on the quay while Daddy tests the compass's bearings, setting a course for the secret dock the map hinted at.\",",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_625741131f204ec8b39442f0a0d1bb4d",
            "style": "IPY_MODEL_984215dcff3348af81a70793e441777f",
            "tabbable": null,
            "tooltip": null
          }
        },
        "6c1218da081a45338667024c29c9d8a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "dba1dd71f627431abdc426901fa3f1d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_family": null,
            "font_size": null,
            "font_style": null,
            "font_variant": null,
            "font_weight": null,
            "text_color": null,
            "text_decoration": null
          }
        },
        "fb10405b1fa849e0a162f1e6400f2f91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "5ef1afc25d4644d5b13bc34a8e80c16f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_family": null,
            "font_size": null,
            "font_style": null,
            "font_variant": null,
            "font_weight": null,
            "text_color": null,
            "text_decoration": null
          }
        },
        "625741131f204ec8b39442f0a0d1bb4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "984215dcff3348af81a70793e441777f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_family": null,
            "font_size": null,
            "font_style": null,
            "font_variant": null,
            "font_weight": null,
            "text_color": null,
            "text_decoration": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}