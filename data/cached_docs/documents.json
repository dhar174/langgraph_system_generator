[
  {
    "page_content": "LangGraph overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangGraph overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nInstall\nCore benefits\nLangGraph ecosystem\nAcknowledgements\nLangGraph overview\nCopy page\nGain control with LangGraph to design agents that reliably handle complex tasks\nCopy page\nTrusted by companies shaping the future of agents\u2014 including Klarna, Replit, Elastic, and more\u2014 LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\nLangGraph is very low-level, and focused entirely on agent\norchestration\n. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with\nmodels\nand\ntools\n.\nWe will commonly use\nLangChain\ncomponents throughout the documentation to integrate models and tools, but you don\u2019t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain\u2019s\nagents\nthat provide pre-built architectures for common LLM and tool-calling loops.\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\n\u200b\nInstall\npip\nuv\nCopy\npip\ninstall\n-U\nlanggraph\nThen, create a simple hello world example:\nCopy\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\n,\nEND\ndef\nmock_llm\n(\nstate\n: MessagesState):\nreturn\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"ai\"\n,\n\"content\"\n:\n\"hello world\"\n}]}\ngraph\n=\nStateGraph(MessagesState)\ngraph.add_node(mock_llm)\ngraph.add_edge(\nSTART\n,\n\"mock_llm\"\n)\ngraph.add_edge(\n\"mock_llm\"\n,\nEND\n)\ngraph\n=\ngraph.compile()\ngraph.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi!\"\n}]})\n\u200b\nCore benefits\nLangGraph provides low-level supporting infrastructure for\nany\nlong-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\nDurable execution\n: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\nHuman-in-the-loop\n: Incorporate human oversight by inspecting and modifying agent state at any point.\nComprehensive memory\n: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\nDebugging with LangSmith\n: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nProduction-ready deployment\n: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\n\u200b\nLangGraph ecosystem\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\nLangSmith\nTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\nLearn more\nLangGraph\nDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\nLearn more\nLangChain\nProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\nLearn more\n\u200b\nAcknowledgements\nLangGraph is inspired by\nPregel\nand\nApache Beam\n. The public interface draws inspiration from\nNetworkX\n. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangGraph\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/overview",
      "title": "LangGraph overview - Docs by LangChain",
      "heading": "LangGraph overview"
    }
  },
  {
    "page_content": "Thinking in LangGraph - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nGet started\nThinking in LangGraph\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nStart with the process you want to automate\nStep 1: Map out your workflow as discrete steps\nStep 2: Identify what each step needs to do\nLLM steps\nData steps\nAction steps\nUser input steps\nStep 3: Design your state\nWhat belongs in state?\nKeep state raw, format prompts on-demand\nStep 4: Build your nodes\nHandle errors appropriately\nImplementing our email agent nodes\nStep 5: Wire it together\nTry out your agent\nSummary and next steps\nKey Insights\nAdvanced considerations\nWhere to go from here\nGet started\nThinking in LangGraph\nCopy page\nLearn how to think about building agents with LangGraph\nCopy page\nWhen you build an agent with LangGraph, you will first break it apart into discrete steps called\nnodes\n. Then, you will describe the different decisions and transitions from each of your nodes. Finally, you connect nodes together through a shared\nstate\nthat each node can read from and write to.\nIn this walkthrough, we\u2019ll guide you through the thought process of building a customer support email agent with LangGraph.\n\u200b\nStart with the process you want to automate\nImagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements:\nCopy\nThe agent should:\n- Read incoming customer emails\n- Classify them by urgency and topic\n- Search relevant documentation to answer questions\n- Draft appropriate responses\n- Escalate complex issues to human agents\n- Schedule follow-ups when needed\nExample scenarios to handle:\n1. Simple product question: \"How do I reset my password?\"\n2. Bug report: \"The export feature crashes when I select PDF format\"\n3. Urgent billing issue: \"I was charged twice for my subscription!\"\n4. Feature request: \"Can you add dark mode to the mobile app?\"\n5. Complex technical issue: \"Our API integration fails intermittently with 504 errors\"\nTo implement an agent in LangGraph, you will usually follow the same five steps.\n\u200b\nStep 1: Map out your workflow as discrete steps\nStart by identifying the distinct steps in your process. Each step will become a\nnode\n(a function that does one specific thing). Then, sketch how these steps connect to each other.\nThe arrows in this diagram show possible paths, but the actual decision of which path to take happens inside each node.\nNow that we\u2019ve identified the components in our workflow, let\u2019s understand what each node needs to do:\nRead Email\n: Extract and parse the email content\nClassify Intent\n: Use an LLM to categorize urgency and topic, then route to appropriate action\nDoc Search\n: Query your knowledge base for relevant information\nBug Track\n: Create or update issue in tracking system\nDraft Reply\n: Generate an appropriate response\nHuman Review\n: Escalate to human agent for approval or handling\nSend Reply\n: Dispatch the email response\nNotice that some nodes make decisions about where to go next (\nClassify Intent\n,\nDraft Reply\n,\nHuman Review\n), while others always proceed to the same next step (\nRead Email\nalways goes to\nClassify Intent\n,\nDoc Search\nalways goes to\nDraft Reply\n).\n\u200b\nStep 2: Identify what each step needs to do\nFor each node in your graph, determine what type of operation it represents and what context it needs to work properly.\nLLM steps\nUse when you need to understand, analyze, generate text, or make reasoning decisions\nData steps\nUse when you need to retrieve information from external sources\nAction steps\nUse when you need to perform external actions\nUser input steps\nUse when you need human intervention\n\u200b\nLLM steps\nWhen a step needs to understand, analyze, generate text, or make reasoning decisions:\nClassify intent\nStatic context (prompt): Classification categories, urgency definitions, response format\nDynamic context (from state): Email content, sender information\nDesired outcome: Structured classification that determines routing\nDraft reply\nStatic context (prompt): Tone guidelines, company policies, response templates\nDynamic context (from state): Classification results, search results, customer history\nDesired outcome: Professional email response ready for review\n\u200b\nData steps\nWhen a step needs to retrieve information from external sources:\nDocument search\nParameters: Query built from intent and topic\nRetry strategy: Yes, with exponential backoff for transient failures\nCaching: Could cache common queries to reduce API calls\nCustomer history lookup\nParameters: Customer email or ID from state\nRetry strategy: Yes, but with fallback to basic info if unavailable\nCaching: Yes, with time-to-live to balance freshness and performance\n\u200b\nAction steps\nWhen a step needs to perform an external action:\nSend reply\nWhen to execute node: After approval (human or automated)\nRetry strategy: Yes, with exponential backoff for network issues\nShould not cache: Each send is a unique action\nBug track\nWhen to execute node: Always when intent is \u201cbug\u201d\nRetry strategy: Yes, critical to not lose bug reports\nReturns: Ticket ID to include in response\n\u200b\nUser input steps\nWhen a step needs human intervention:\nHuman review node\nContext for decision: Original email, draft response, urgency, classification\nExpected input format: Approval boolean plus optional edited response\nWhen triggered: High urgency, complex issues, or quality concerns\n\u200b\nStep 3: Design your state\nState is the shared\nmemory\naccessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.\n\u200b\nWhat belongs in state?\nAsk yourself these questions about each piece of data:\nInclude in state\nDoes it need to persist across steps? If yes, it goes in state.\nDon't store\nCan you derive it from other data? If yes, compute it when needed instead of storing it in state.\nFor our email agent, we need to track:\nThe original email and sender info (can\u2019t reconstruct these later)\nClassification results (needed by multiple later/downstream nodes)\nSearch results and customer data (expensive to re-fetch)\nThe draft response (needs to persist through review)\nExecution metadata (for debugging and recovery)\n\u200b\nKeep state raw, format prompts on-demand\nA key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.\nThis separation means:\nDifferent nodes can format the same data differently for their needs\nYou can change prompt templates without modifying your state schema\nDebugging is clearer \u2013 you see exactly what data each node received\nYour agent can evolve without breaking existing state\nLet\u2019s define our state:\nCopy\nfrom\ntyping\nimport\nTypedDict, Literal\n# Define the structure for email classification\nclass\nEmailClassification\n(\nTypedDict\n):\nintent: Literal[\n\"question\"\n,\n\"bug\"\n,\n\"billing\"\n,\n\"feature\"\n,\n\"complex\"\n]\nurgency: Literal[\n\"low\"\n,\n\"medium\"\n,\n\"high\"\n,\n\"critical\"\n]\ntopic:\nstr\nsummary:\nstr\nclass\nEmailAgentState\n(\nTypedDict\n):\n# Raw email data\nemail_content:\nstr\nsender_email:\nstr\nemail_id:\nstr\n# Classification result\nclassification: EmailClassification\n|\nNone\n# Raw search/API results\nsearch_results: list[\nstr\n]\n|\nNone\n# List of raw document chunks\ncustomer_history:\ndict\n|\nNone\n# Raw customer data from CRM\n# Generated content\ndraft_response:\nstr\n|\nNone\nmessages: list[\nstr\n]\n|\nNone\nNotice that the state contains only raw data \u2013 no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.\n\u200b\nStep 4: Build your nodes\nNow we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it.\n\u200b\nHandle errors appropriately\nDifferent errors need different handling strategies:\nError Type\nWho Fixes It\nStrategy\nWhen to Use\nTransient errors (network issues, rate limits)\nSystem (automatic)\nRetry policy\nTemporary failures that usually resolve on retry\nLLM-recoverable errors (tool failures, parsing issues)\nLLM\nStore error in state and loop back\nLLM can see the error and adjust its approach\nUser-fixable errors (missing information, unclear instructions)\nHuman\nPause with\ninterrupt()\nNeed user input to proceed\nUnexpected errors\nDeveloper\nLet them bubble up\nUnknown issues that need debugging\nTransient errors\nLLM-recoverable\nUser-fixable\nUnexpected\nAdd a retry policy to automatically retry network issues and rate limits:\nCopy\nfrom\nlanggraph.types\nimport\nRetryPolicy\nworkflow.add_node(\n\"search_documentation\"\n,\nsearch_documentation,\nretry_policy\n=\nRetryPolicy(\nmax_attempts\n=\n3\n,\ninitial_interval\n=\n1.0\n)\n)\nStore the error in state and loop back so the LLM can see what went wrong and try again:\nCopy\nfrom\nlanggraph.types\nimport\nCommand\ndef\nexecute_tool\n(\nstate\n: State) -> Command[Literal[\n\"agent\"\n,\n\"execute_tool\"\n]]:\ntry\n:\nresult\n=\nrun_tool(state[\n'tool_call'\n])\nreturn\nCommand(\nupdate\n=\n{\n\"tool_result\"\n: result},\ngoto\n=\n\"agent\"\n)\nexcept\nToolError\nas\ne:\n# Let the LLM see what went wrong and try again\nreturn\nCommand(\nupdate\n=\n{\n\"tool_result\"\n:\nf\n\"Tool error:\n{\nstr\n(e)\n}\n\"\n},\ngoto\n=\n\"agent\"\n)\nPause and collect information from the user when needed (like account IDs, order numbers, or clarifications):\nCopy\nfrom\nlanggraph.types\nimport\nCommand\ndef\nlookup_customer_history\n(\nstate\n: State) -> Command[Literal[\n\"draft_response\"\n]]:\nif\nnot\nstate.get(\n'customer_id'\n):\nuser_input\n=\ninterrupt({\n\"message\"\n:\n\"Customer ID needed\"\n,\n\"request\"\n:\n\"Please provide the customer's account ID to look up their subscription history\"\n})\nreturn\nCommand(\nupdate\n=\n{\n\"customer_id\"\n: user_input[\n'customer_id'\n]},\ngoto\n=\n\"lookup_customer_history\"\n)\n# Now proceed with the lookup\ncustomer_data\n=\nfetch_customer_history(state[\n'customer_id'\n])\nreturn\nCommand(\nupdate\n=\n{\n\"customer_history\"\n: customer_data},\ngoto\n=\n\"draft_response\"\n)\nLet them bubble up for debugging. Don\u2019t catch what you can\u2019t handle:\nCopy\ndef\nsend_reply\n(\nstate\n: EmailAgentState):\ntry\n:\nemail_service.send(state[\n\"draft_response\"\n])\nexcept\nException\n:\nraise\n# Surface unexpected errors\n\u200b\nImplementing our email agent nodes\nWe\u2019ll implement each node as a simple function. Remember: nodes take state, do work, and return updates.\nRead and classify nodes\nCopy\nfrom\ntyping\nimport\nLiteral\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.types\nimport\ninterrupt, Command, RetryPolicy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlangchain.messages\nimport\nHumanMessage\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\ndef\nread_email\n(\nstate\n: EmailAgentState) ->\ndict\n:\n\"\"\"Extract and parse email content\"\"\"\n# In production, this would connect to your email service\nreturn\n{\n\"messages\"\n: [HumanMessage(\ncontent\n=\nf\n\"Processing email:\n{\nstate[\n'email_content'\n]\n}\n\"\n)]\n}\ndef\nclassify_intent\n(\nstate\n: EmailAgentState) -> Command[Literal[\n\"search_documentation\"\n,\n\"human_review\"\n,\n\"draft_response\"\n,\n\"bug_tracking\"\n]]:\n\"\"\"Use LLM to classify email intent and urgency, then route accordingly\"\"\"\n# Create structured LLM that returns EmailClassification dict\nstructured_llm\n=\nllm.with_structured_output(EmailClassification)\n# Format the prompt on-demand, not stored in state\nclassification_prompt\n=\nf\n\"\"\"\nAnalyze this customer email and classify it:\nEmail:\n{\nstate[\n'email_content'\n]\n}\nFrom:\n{\nstate[\n'sender_email'\n]\n}\nProvide classification including intent, urgency, topic, and summary.\n\"\"\"\n# Get structured response directly as dict\nclassification\n=\nstructured_llm.invoke(classification_prompt)\n# Determine next node based on classification\nif\nclassification[\n'intent'\n]\n==\n'billing'\nor\nclassification[\n'urgency'\n]\n==\n'critical'\n:\ngoto\n=\n\"human_review\"\nelif\nclassification[\n'intent'\n]\nin\n[\n'question'\n,\n'feature'\n]:\ngoto\n=\n\"search_documentation\"\nelif\nclassification[\n'intent'\n]\n==\n'bug'\n:\ngoto\n=\n\"bug_tracking\"\nelse\n:\ngoto\n=\n\"draft_response\"\n# Store classification as a single dict in state\nreturn\nCommand(\nupdate\n=\n{\n\"classification\"\n: classification},\ngoto\n=\ngoto\n)\nSearch and tracking nodes\nCopy\ndef\nsearch_documentation\n(\nstate\n: EmailAgentState) -> Command[Literal[\n\"draft_response\"\n]]:\n\"\"\"Search knowledge base for relevant information\"\"\"\n# Build search query from classification\nclassification\n=\nstate.get(\n'classification'\n, {})\nquery\n=\nf\n\"\n{\nclassification.get(\n'intent'\n,\n''\n)\n}\n{\nclassification.get(\n'topic'\n,\n''\n)\n}\n\"\ntry\n:\n# Implement your search logic here\n# Store raw search results, not formatted text\nsearch_results\n=\n[\n\"Reset password via Settings > Security > Change Password\"\n,\n\"Password must be at least 12 characters\"\n,\n\"Include uppercase, lowercase, numbers, and symbols\"\n]\nexcept\nSearchAPIError\nas\ne:\n# For recoverable search errors, store error and continue\nsearch_results\n=\n[\nf\n\"Search temporarily unavailable:\n{\nstr\n(e)\n}\n\"\n]\nreturn\nCommand(\nupdate\n=\n{\n\"search_results\"\n: search_results},\n# Store raw results or error\ngoto\n=\n\"draft_response\"\n)\ndef\nbug_tracking\n(\nstate\n: EmailAgentState) -> Command[Literal[\n\"draft_response\"\n]]:\n\"\"\"Create or update bug tracking ticket\"\"\"\n# Create ticket in your bug tracking system\nticket_id\n=\n\"BUG-12345\"\n# Would be created via API\nreturn\nCommand(\nupdate\n=\n{\n\"search_results\"\n: [\nf\n\"Bug ticket\n{\nticket_id\n}\ncreated\"\n],\n\"current_step\"\n:\n\"bug_tracked\"\n},\ngoto\n=\n\"draft_response\"\n)\nResponse nodes\nCopy\ndef\ndraft_response\n(\nstate\n: EmailAgentState) -> Command[Literal[\n\"human_review\"\n,\n\"send_reply\"\n]]:\n\"\"\"Generate response using context and route based on quality\"\"\"\nclassification\n=\nstate.get(\n'classification'\n, {})\n# Format context from raw state data on-demand\ncontext_sections\n=\n[]\nif\nstate.get(\n'search_results'\n):\n# Format search results for the prompt\nformatted_docs\n=\n\"\n\\n\n\"\n.join([\nf\n\"-\n{\ndoc\n}\n\"\nfor\ndoc\nin\nstate[\n'search_results'\n]])\ncontext_sections.append(\nf\n\"Relevant documentation:\n\\n\n{\nformatted_docs\n}\n\"\n)\nif\nstate.get(\n'customer_history'\n):\n# Format customer data for the prompt\ncontext_sections.append(\nf\n\"Customer tier:\n{\nstate[\n'customer_history'\n].get(\n'tier'\n,\n'standard'\n)\n}\n\"\n)\n# Build the prompt with formatted context\ndraft_prompt\n=\nf\n\"\"\"\nDraft a response to this customer email:\n{\nstate[\n'email_content'\n]\n}\nEmail intent:\n{\nclassification.get(\n'intent'\n,\n'unknown'\n)\n}\nUrgency level:\n{\nclassification.get(\n'urgency'\n,\n'medium'\n)\n}\n{\nchr\n(\n10\n).join(context_sections)\n}\nGuidelines:\n- Be professional and helpful\n- Address their specific concern\n- Use the provided documentation when relevant\n\"\"\"\nresponse\n=\nllm.invoke(draft_prompt)\n# Determine if human review needed based on urgency and intent\nneeds_review\n=\n(\nclassification.get(\n'urgency'\n)\nin\n[\n'high'\n,\n'critical'\n]\nor\nclassification.get(\n'intent'\n)\n==\n'complex'\n)\n# Route to appropriate next node\ngoto\n=\n\"human_review\"\nif\nneeds_review\nelse\n\"send_reply\"\nreturn\nCommand(\nupdate\n=\n{\n\"draft_response\"\n: response.content},\n# Store only the raw response\ngoto\n=\ngoto\n)\ndef\nhuman_review\n(\nstate\n: EmailAgentState) -> Command[Literal[\n\"send_reply\"\n,\nEND\n]]:\n\"\"\"Pause for human review using interrupt and route based on decision\"\"\"\nclassification\n=\nstate.get(\n'classification'\n, {})\n# interrupt() must come first - any code before it will re-run on resume\nhuman_decision\n=\ninterrupt({\n\"email_id\"\n: state.get(\n'email_id'\n,\n''\n),\n\"original_email\"\n: state.get(\n'email_content'\n,\n''\n),\n\"draft_response\"\n: state.get(\n'draft_response'\n,\n''\n),\n\"urgency\"\n: classification.get(\n'urgency'\n),\n\"intent\"\n: classification.get(\n'intent'\n),\n\"action\"\n:\n\"Please review and approve/edit this response\"\n})\n# Now process the human's decision\nif\nhuman_decision.get(\n\"approved\"\n):\nreturn\nCommand(\nupdate\n=\n{\n\"draft_response\"\n: human_decision.get(\n\"edited_response\"\n, state.get(\n'draft_response'\n,\n''\n))},\ngoto\n=\n\"send_reply\"\n)\nelse\n:\n# Rejection means human will handle directly\nreturn\nCommand(\nupdate\n=\n{},\ngoto\n=\nEND\n)\ndef\nsend_reply\n(\nstate\n: EmailAgentState) ->\ndict\n:\n\"\"\"Send the email response\"\"\"\n# Integrate with email service\nprint\n(\nf\n\"Sending reply:\n{\nstate[\n'draft_response'\n][:\n100\n]\n}\n...\"\n)\nreturn\n{}\n\u200b\nStep 5: Wire it together\nNow we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges.\nTo enable\nhuman-in-the-loop\nwith\ninterrupt()\n, we need to compile with a\ncheckpointer\nto save state between runs:\nGraph compilation code\nCopy\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\nfrom\nlanggraph.types\nimport\nRetryPolicy\n# Create the graph\nworkflow\n=\nStateGraph(EmailAgentState)\n# Add nodes with appropriate error handling\nworkflow.add_node(\n\"read_email\"\n, read_email)\nworkflow.add_node(\n\"classify_intent\"\n, classify_intent)\n# Add retry policy for nodes that might have transient failures\nworkflow.add_node(\n\"search_documentation\"\n,\nsearch_documentation,\nretry_policy\n=\nRetryPolicy(\nmax_attempts\n=\n3\n)\n)\nworkflow.add_node(\n\"bug_tracking\"\n, bug_tracking)\nworkflow.add_node(\n\"draft_response\"\n, draft_response)\nworkflow.add_node(\n\"human_review\"\n, human_review)\nworkflow.add_node(\n\"send_reply\"\n, send_reply)\n# Add only the essential edges\nworkflow.add_edge(\nSTART\n,\n\"read_email\"\n)\nworkflow.add_edge(\n\"read_email\"\n,\n\"classify_intent\"\n)\nworkflow.add_edge(\n\"send_reply\"\n,\nEND\n)\n# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer\nmemory\n=\nMemorySaver()\napp\n=\nworkflow.compile(\ncheckpointer\n=\nmemory)\nThe graph structure is minimal because routing happens inside nodes through\nCommand\nobjects. Each node declares where it can go using type hints like\nCommand[Literal[\"node1\", \"node2\"]]\n, making the flow explicit and traceable.\n\u200b\nTry out your agent\nLet\u2019s run our agent with an urgent billing issue that needs human review:\nTesting the agent\nCopy\n# Test with an urgent billing issue\ninitial_state\n=\n{\n\"email_content\"\n:\n\"I was charged twice for my subscription! This is urgent!\"\n,\n\"sender_email\"\n:\n\"\n[email\u00a0protected]\n\"\n,\n\"email_id\"\n:\n\"email_123\"\n,\n\"messages\"\n: []\n}\n# Run with a thread_id for persistence\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"customer_123\"\n}}\nresult\n=\napp.invoke(initial_state, config)\n# The graph will pause at human_review\nprint\n(\nf\n\"human review interrupt:\n{\nresult[\n'__interrupt__'\n]\n}\n\"\n)\n# When ready, provide human input to resume\nfrom\nlanggraph.types\nimport\nCommand\nhuman_response\n=\nCommand(\nresume\n=\n{\n\"approved\"\n:\nTrue\n,\n\"edited_response\"\n:\n\"We sincerely apologize for the double charge. I've initiated an immediate refund...\"\n}\n)\n# Resume execution\nfinal_result\n=\napp.invoke(human_response, config)\nprint\n(\nf\n\"Email sent successfully!\"\n)\nThe graph pauses when it hits\ninterrupt()\n, saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The\nthread_id\nensures all state for this conversation is preserved together.\n\u200b\nSummary and next steps\n\u200b\nKey Insights\nBuilding this email agent has shown us the LangGraph way of thinking:\nBreak into discrete steps\nEach node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps.\nState is shared memory\nStore raw data, not formatted text. This lets different nodes use the same information in different ways.\nNodes are functions\nThey take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination.\nErrors are part of the flow\nTransient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging.\nHuman input is first-class\nThe\ninterrupt()\nfunction pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.\nGraph structure emerges naturally\nYou define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.\n\u200b\nAdvanced considerations\nNode granularity trade-offs\nThis section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.\nYou might wonder: why not combine\nRead Email\nand\nClassify Intent\ninto one node?\nOr why separate Doc Search from Draft Reply?\nThe answer involves trade-offs between resilience and observability.\nThe resilience consideration:\nLangGraph\u2019s\ndurable execution\ncreates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.\nWhy we chose this breakdown for the email agent:\nIsolation of external services:\nDoc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.\nIntermediate visibility:\nHaving\nClassify Intent\nas its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring\u2014you can see exactly when and why the agent routes to human review.\nDifferent failure modes:\nLLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.\nReusability and testing:\nSmaller nodes are easier to test in isolation and reuse in other workflows.\nA different valid approach: You could combine\nRead Email\nand\nClassify Intent\ninto a single node. You\u2019d lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off.\nApplication-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements\u2014LangGraph doesn\u2019t prescribe this.\nPerformance considerations: More nodes doesn\u2019t mean slower execution. LangGraph writes checkpoints in the background by default (\nasync durability mode\n), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed\u2014use\n\"exit\"\nmode to checkpoint only at completion, or\n\"sync\"\nmode to block execution until each checkpoint is written.\n\u200b\nWhere to go from here\nThis was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:\nHuman-in-the-loop patterns\nLearn how to add tool approval before execution, batch approval, and other patterns\nSubgraphs\nCreate subgraphs for complex multi-step operations\nStreaming\nAdd streaming to show real-time progress to users\nObservability\nAdd observability with LangSmith for debugging and monitoring\nTool Integration\nIntegrate more tools for web search, database queries, and API calls\nRetry Logic\nImplement retry logic with exponential backoff for failed operations\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nChangelog\nPrevious\nWorkflows and agents\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph",
      "title": "Thinking in LangGraph - Docs by LangChain",
      "heading": "Thinking in LangGraph"
    }
  },
  {
    "page_content": "Durable execution - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCapabilities\nDurable execution\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nRequirements\nDeterminism and Consistent Replay\nDurability modes\nUsing tasks in nodes\nResuming Workflows\nStarting Points for Resuming Workflows\nCapabilities\nDurable execution\nCopy page\nCopy page\nDurable execution\nis a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require\nhuman-in-the-loop\n, where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps \u2014 even after a significant delay (e.g., a week later).\nLangGraph\u2019s built-in\npersistence\nlayer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted \u2014 whether by a system failure or for\nhuman-in-the-loop\ninteractions \u2014 it can be resumed from its last recorded state.\nIf you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.\nTo make the most of durable execution, ensure that your workflow is designed to be\ndeterministic\nand\nidempotent\nand wrap any side effects or non-deterministic operations inside\ntasks\n. You can use\ntasks\nfrom both the\nStateGraph (Graph API)\nand the\nFunctional API\n.\n\u200b\nRequirements\nTo leverage durable execution in LangGraph, you need to:\nEnable\npersistence\nin your workflow by specifying a\ncheckpointer\nthat will save workflow progress.\nSpecify a\nthread identifier\nwhen executing a workflow. This will track the execution history for a particular instance of the workflow.\nWrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside\ntask\nto ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see\nDeterminism and Consistent Replay\n.\n\u200b\nDeterminism and Consistent Replay\nWhen you resume a workflow run, the code does\nNOT\nresume from the\nsame line of code\nwhere execution stopped; instead, it will identify an appropriate\nstarting point\nfrom which to pick up where it left off. This means that the workflow will replay all steps from the\nstarting point\nuntil it reaches the point where it was stopped.\nAs a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside\ntasks\nor\nnodes\n.\nTo ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:\nAvoid Repeating Work\n: If a\nnode\ncontains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate\ntask\n. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.\nEncapsulate Non-Deterministic Operations:\nWrap any code that might yield non-deterministic results (e.g., random number generation) inside\ntasks\nor\nnodes\n. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.\nUse Idempotent Operations\n: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a\ntask\nstarts but fails to complete successfully, the workflow\u2019s resumption will re-run the\ntask\n, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.\nFor some examples of pitfalls to avoid, see the\nCommon Pitfalls\nsection in the functional API, which shows\nhow to structure your code using\ntasks\nto avoid these issues. The same principles apply to the\nStateGraph (Graph API)\n.\n\u200b\nDurability modes\nLangGraph supports three durability modes that allow you to balance performance and data consistency based on your application\u2019s requirements. A higher durability mode adds more overhead to the workflow execution. You can specify the durability mode when calling any graph execution method:\nCopy\ngraph.stream(\n{\n\"input\"\n:\n\"test\"\n},\ndurability\n=\n\"sync\"\n)\nThe durability modes, from least to most durable, are as follows:\n\"exit\"\n: Changes are persisted only when graph execution exits (either successfully, with an error, or due to an interrupt). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from system failures (e.g., process crashes) that occur mid-execution.\n\"async\"\n: Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there\u2019s a small risk that checkpoints might not be written if the process crashes during execution.\n\"sync\"\n: Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.\n\u200b\nUsing tasks in nodes\nIf a\nnode\ncontains multiple operations, you may find it easier to convert each operation into a\ntask\nrather than refactor the operations into individual nodes.\nOriginal\nWith task\nCopy\nfrom\ntyping\nimport\nNotRequired\nfrom\ntyping_extensions\nimport\nTypedDict\nimport\nuuid\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nimport\nrequests\n# Define a TypedDict to represent the state\nclass\nState\n(\nTypedDict\n):\nurl:\nstr\nresult: NotRequired[\nstr\n]\ndef\ncall_api\n(\nstate\n: State):\n\"\"\"Example node that makes an API request.\"\"\"\nresult\n=\nrequests.get(state[\n'url'\n]).text[:\n100\n]\n# Side-effect  #\nreturn\n{\n\"result\"\n: result\n}\n# Create a StateGraph builder and add a node for the call_api function\nbuilder\n=\nStateGraph(State)\nbuilder.add_node(\n\"call_api\"\n, call_api)\n# Connect the start and end nodes to the call_api node\nbuilder.add_edge(\nSTART\n,\n\"call_api\"\n)\nbuilder.add_edge(\n\"call_api\"\n,\nEND\n)\n# Specify a checkpointer\ncheckpointer\n=\nInMemorySaver()\n# Compile the graph with the checkpointer\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\n# Define a config with a thread ID.\nthread_id\n=\nuuid.uuid4()\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n: thread_id}}\n# Invoke the graph\ngraph.invoke({\n\"url\"\n:\n\"https://www.example.com\"\n}, config)\nCopy\nfrom\ntyping\nimport\nNotRequired\nfrom\ntyping_extensions\nimport\nTypedDict\nimport\nuuid\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.func\nimport\ntask\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nimport\nrequests\n# Define a TypedDict to represent the state\nclass\nState\n(\nTypedDict\n):\nurls: list[\nstr\n]\nresult: NotRequired[list[\nstr\n]]\n@task\ndef\n_make_request\n(\nurl\n:\nstr\n):\n\"\"\"Make a request.\"\"\"\nreturn\nrequests.get(url).text[:\n100\n]\ndef\ncall_api\n(\nstate\n: State):\n\"\"\"Example node that makes an API request.\"\"\"\nrequests\n=\n[_make_request(url)\nfor\nurl\nin\nstate[\n'urls'\n]]\nresults\n=\n[request.result()\nfor\nrequest\nin\nrequests]\nreturn\n{\n\"results\"\n: results\n}\n# Create a StateGraph builder and add a node for the call_api function\nbuilder\n=\nStateGraph(State)\nbuilder.add_node(\n\"call_api\"\n, call_api)\n# Connect the start and end nodes to the call_api node\nbuilder.add_edge(\nSTART\n,\n\"call_api\"\n)\nbuilder.add_edge(\n\"call_api\"\n,\nEND\n)\n# Specify a checkpointer\ncheckpointer\n=\nInMemorySaver()\n# Compile the graph with the checkpointer\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\n# Define a config with a thread ID.\nthread_id\n=\nuuid.uuid4()\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n: thread_id}}\n# Invoke the graph\ngraph.invoke({\n\"urls\"\n: [\n\"https://www.example.com\"\n]}, config)\n\u200b\nResuming Workflows\nOnce you have enabled durable execution in your workflow, you can resume execution for the following scenarios:\nPausing and Resuming Workflows:\nUse the\ninterrupt\nfunction to pause a workflow at specific points and the\nCommand\nprimitive to resume it with updated state. See\nInterrupts\nfor more details.\nRecovering from Failures:\nAutomatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a\nNone\nas the input value (see this\nexample\nwith the functional API).\n\u200b\nStarting Points for Resuming Workflows\nIf you\u2019re using a\nStateGraph (Graph API)\n, the starting point is the beginning of the\nnode\nwhere execution stopped.\nIf you\u2019re making a subgraph call inside a node, the starting point will be the\nparent\nnode that called the subgraph that was halted.\nInside the subgraph, the starting point will be the specific\nnode\nwhere execution stopped.\nIf you\u2019re using the Functional API, the starting point is the beginning of the\nentrypoint\nwhere execution stopped.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nPersistence\nPrevious\nStreaming\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/durable-execution",
      "title": "Durable execution - Docs by LangChain",
      "heading": "Durable execution"
    }
  },
  {
    "page_content": "Streaming - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCapabilities\nStreaming\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nSupported stream modes\nBasic usage example\nStream multiple modes\nStream graph state\nStream subgraph outputs\nDebugging\nLLM tokens\nFilter by LLM invocation\nFilter by node\nStream custom data\nUse with any LLM\nDisable streaming for specific chat models\nAsync with Python < 3.11\nCapabilities\nStreaming\nCopy page\nCopy page\nLangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\nWhat\u2019s possible with LangGraph streaming:\nStream graph state\n\u2014 get state updates / values with\nupdates\nand\nvalues\nmodes.\nStream subgraph outputs\n\u2014 include outputs from both the parent graph and any nested subgraphs.\nStream LLM tokens\n\u2014 capture token streams from anywhere: inside nodes, subgraphs, or tools.\nStream custom data\n\u2014 send custom updates or progress signals directly from tool functions.\nUse multiple streaming modes\n\u2014 choose from\nvalues\n(full state),\nupdates\n(state deltas),\nmessages\n(LLM tokens + metadata),\ncustom\n(arbitrary user data), or\ndebug\n(detailed traces).\n\u200b\nSupported stream modes\nPass one or more of the following stream modes as a list to the\nstream\nor\nastream\nmethods:\nMode\nDescription\nvalues\nStreams the full value of the state after each step of the graph.\nupdates\nStreams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.\ncustom\nStreams custom data from inside your graph nodes.\nmessages\nStreams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.\ndebug\nStreams as much information as possible throughout the execution of the graph.\n\u200b\nBasic usage example\nLangGraph graphs expose the\nstream\n(sync) and\nastream\n(async) methods to yield streamed outputs as iterators.\nCopy\nfor\nchunk\nin\ngraph.stream(inputs,\nstream_mode\n=\n\"updates\"\n):\nprint\n(chunk)\nExtended example: streaming updates\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nclass\nState\n(\nTypedDict\n):\ntopic:\nstr\njoke:\nstr\ndef\nrefine_topic\n(\nstate\n: State):\nreturn\n{\n\"topic\"\n: state[\n\"topic\"\n]\n+\n\" and cats\"\n}\ndef\ngenerate_joke\n(\nstate\n: State):\nreturn\n{\n\"joke\"\n:\nf\n\"This is a joke about\n{\nstate[\n'topic'\n]\n}\n\"\n}\ngraph\n=\n(\nStateGraph(State)\n.add_node(refine_topic)\n.add_node(generate_joke)\n.add_edge(\nSTART\n,\n\"refine_topic\"\n)\n.add_edge(\n\"refine_topic\"\n,\n\"generate_joke\"\n)\n.add_edge(\n\"generate_joke\"\n,\nEND\n)\n.compile()\n)\n# The stream() method returns an iterator that yields streamed outputs\nfor\nchunk\nin\ngraph.stream(\n{\n\"topic\"\n:\n\"ice cream\"\n},\n# Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\n# Other stream modes are also available. See supported stream modes for details\nstream_mode\n=\n\"updates\"\n,\n):\nprint\n(chunk)\nCopy\n{\n'refineTopic'\n: {\n'topic'\n:\n'ice cream and cats'\n}}\n{\n'generateJoke'\n: {\n'joke'\n:\n'This is a joke about ice cream and cats'\n}}\n\u200b\nStream multiple modes\nYou can pass a list as the\nstream_mode\nparameter to stream multiple modes at once.\nThe streamed outputs will be tuples of\n(mode, chunk)\nwhere\nmode\nis the name of the stream mode and\nchunk\nis the data streamed by that mode.\nCopy\nfor\nmode, chunk\nin\ngraph.stream(inputs,\nstream_mode\n=\n[\n\"updates\"\n,\n\"custom\"\n]):\nprint\n(chunk)\n\u200b\nStream graph state\nUse the stream modes\nupdates\nand\nvalues\nto stream the state of the graph as it executes.\nupdates\nstreams the\nupdates\nto the state after each step of the graph.\nvalues\nstreams the\nfull value\nof the state after each step of the graph.\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nclass\nState\n(\nTypedDict\n):\ntopic:\nstr\njoke:\nstr\ndef\nrefine_topic\n(\nstate\n: State):\nreturn\n{\n\"topic\"\n: state[\n\"topic\"\n]\n+\n\" and cats\"\n}\ndef\ngenerate_joke\n(\nstate\n: State):\nreturn\n{\n\"joke\"\n:\nf\n\"This is a joke about\n{\nstate[\n'topic'\n]\n}\n\"\n}\ngraph\n=\n(\nStateGraph(State)\n.add_node(refine_topic)\n.add_node(generate_joke)\n.add_edge(\nSTART\n,\n\"refine_topic\"\n)\n.add_edge(\n\"refine_topic\"\n,\n\"generate_joke\"\n)\n.add_edge(\n\"generate_joke\"\n,\nEND\n)\n.compile()\n)\nupdates\nvalues\nUse this to stream only the\nstate updates\nreturned by the nodes after each step. The streamed outputs include the name of the node as well as the update.\nCopy\nfor\nchunk\nin\ngraph.stream(\n{\n\"topic\"\n:\n\"ice cream\"\n},\nstream_mode\n=\n\"updates\"\n,\n):\nprint\n(chunk)\nUse this to stream the\nfull state\nof the graph after each step.\nCopy\nfor\nchunk\nin\ngraph.stream(\n{\n\"topic\"\n:\n\"ice cream\"\n},\nstream_mode\n=\n\"values\"\n,\n):\nprint\n(chunk)\n\u200b\nStream subgraph outputs\nTo include outputs from\nsubgraphs\nin the streamed outputs, you can set\nsubgraphs=True\nin the\n.stream()\nmethod of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\nThe outputs will be streamed as tuples\n(namespace, data)\n, where\nnamespace\nis a tuple with the path to the node where a subgraph is invoked, e.g.\n(\"parent_node:<task_id>\", \"child_node:<task_id>\")\n.\nCopy\nfor\nchunk\nin\ngraph.stream(\n{\n\"foo\"\n:\n\"foo\"\n},\n# Set subgraphs=True to stream outputs from subgraphs\nsubgraphs\n=\nTrue\n,\nstream_mode\n=\n\"updates\"\n,\n):\nprint\n(chunk)\nExtended example: streaming from subgraphs\nCopy\nfrom\nlanggraph.graph\nimport\nSTART\n, StateGraph\nfrom\ntyping\nimport\nTypedDict\n# Define subgraph\nclass\nSubgraphState\n(\nTypedDict\n):\nfoo:\nstr\n# note that this key is shared with the parent graph state\nbar:\nstr\ndef\nsubgraph_node_1\n(\nstate\n: SubgraphState):\nreturn\n{\n\"bar\"\n:\n\"bar\"\n}\ndef\nsubgraph_node_2\n(\nstate\n: SubgraphState):\nreturn\n{\n\"foo\"\n: state[\n\"foo\"\n]\n+\nstate[\n\"bar\"\n]}\nsubgraph_builder\n=\nStateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(\nSTART\n,\n\"subgraph_node_1\"\n)\nsubgraph_builder.add_edge(\n\"subgraph_node_1\"\n,\n\"subgraph_node_2\"\n)\nsubgraph\n=\nsubgraph_builder.compile()\n# Define parent graph\nclass\nParentState\n(\nTypedDict\n):\nfoo:\nstr\ndef\nnode_1\n(\nstate\n: ParentState):\nreturn\n{\n\"foo\"\n:\n\"hi! \"\n+\nstate[\n\"foo\"\n]}\nbuilder\n=\nStateGraph(ParentState)\nbuilder.add_node(\n\"node_1\"\n, node_1)\nbuilder.add_node(\n\"node_2\"\n, subgraph)\nbuilder.add_edge(\nSTART\n,\n\"node_1\"\n)\nbuilder.add_edge(\n\"node_1\"\n,\n\"node_2\"\n)\ngraph\n=\nbuilder.compile()\nfor\nchunk\nin\ngraph.stream(\n{\n\"foo\"\n:\n\"foo\"\n},\nstream_mode\n=\n\"updates\"\n,\n# Set subgraphs=True to stream outputs from subgraphs\nsubgraphs\n=\nTrue\n,\n):\nprint\n(chunk)\nCopy\n((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n((), {'node_2': {'foo': 'hi! foobar'}})\nNote\nthat we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\n\u200b\nDebugging\nUse the\ndebug\nstreaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\nCopy\nfor\nchunk\nin\ngraph.stream(\n{\n\"topic\"\n:\n\"ice cream\"\n},\nstream_mode\n=\n\"debug\"\n,\n):\nprint\n(chunk)\n\u200b\nLLM tokens\nUse the\nmessages\nstreaming mode to stream Large Language Model (LLM) outputs\ntoken by token\nfrom any part of your graph, including nodes, tools, subgraphs, or tasks.\nThe streamed output from\nmessages\nmode\nis a tuple\n(message_chunk, metadata)\nwhere:\nmessage_chunk\n: the token or message segment from the LLM.\nmetadata\n: a dictionary containing details about the graph node and LLM invocation.\nIf your LLM is not available as a LangChain integration, you can stream its outputs using\ncustom\nmode instead. See\nuse with any LLM\nfor details.\nManual config required for async in Python < 3.11\nWhen using Python < 3.11 with async code, you must explicitly pass\nRunnableConfig\nto\nainvoke()\nto enable proper streaming. See\nAsync with Python < 3.11\nfor details or upgrade to Python 3.11+.\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n@dataclass\nclass\nMyState\n:\ntopic:\nstr\njoke:\nstr\n=\n\"\"\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n)\ndef\ncall_model\n(\nstate\n: MyState):\n\"\"\"Call the LLM to generate a joke about a topic\"\"\"\n# Note that message events are emitted even when the LLM is run using .invoke rather than .stream\nmodel_response\n=\nmodel.invoke(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nf\n\"Generate a joke about\n{\nstate.topic\n}\n\"\n}\n]\n)\nreturn\n{\n\"joke\"\n: model_response.content}\ngraph\n=\n(\nStateGraph(MyState)\n.add_node(call_model)\n.add_edge(\nSTART\n,\n\"call_model\"\n)\n.compile()\n)\n# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor\nmessage_chunk, metadata\nin\ngraph.stream(\n{\n\"topic\"\n:\n\"ice cream\"\n},\nstream_mode\n=\n\"messages\"\n,\n):\nif\nmessage_chunk.content:\nprint\n(message_chunk.content,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n\u200b\nFilter by LLM invocation\nYou can associate\ntags\nwith LLM invocations to filter the streamed tokens by LLM invocation.\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# model_1 is tagged with \"joke\"\nmodel_1\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n,\ntags\n=\n[\n'joke'\n])\n# model_2 is tagged with \"poem\"\nmodel_2\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n,\ntags\n=\n[\n'poem'\n])\ngraph\n=\n...\n# define a graph that uses these LLMs\n# The stream_mode is set to \"messages\" to stream LLM tokens\n# The metadata contains information about the LLM invocation, including the tags\nasync\nfor\nmsg, metadata\nin\ngraph.astream(\n{\n\"topic\"\n:\n\"cats\"\n},\nstream_mode\n=\n\"messages\"\n,\n):\n# Filter the streamed tokens by the tags field in the metadata to only include\n# the tokens from the LLM invocation with the \"joke\" tag\nif\nmetadata[\n\"tags\"\n]\n==\n[\n\"joke\"\n]:\nprint\n(msg.content,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nExtended example: filtering by tags\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nSTART\n, StateGraph\n# The joke_model is tagged with \"joke\"\njoke_model\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n,\ntags\n=\n[\n\"joke\"\n])\n# The poem_model is tagged with \"poem\"\npoem_model\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n,\ntags\n=\n[\n\"poem\"\n])\nclass\nState\n(\nTypedDict\n):\ntopic:\nstr\njoke:\nstr\npoem:\nstr\nasync\ndef\ncall_model\n(\nstate\n,\nconfig\n):\ntopic\n=\nstate[\n\"topic\"\n]\nprint\n(\n\"Writing joke...\"\n)\n# Note: Passing the config through explicitly is required for python < 3.11\n# Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n# The config is passed through explicitly to ensure the context vars are propagated correctly\n# This is required for Python < 3.11 when using async code. Please see the async section for more details\njoke_response\n=\nawait\njoke_model.ainvoke(\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nf\n\"Write a joke about\n{\ntopic\n}\n\"\n}],\nconfig,\n)\nprint\n(\n\"\n\\n\\n\nWriting poem...\"\n)\npoem_response\n=\nawait\npoem_model.ainvoke(\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nf\n\"Write a short poem about\n{\ntopic\n}\n\"\n}],\nconfig,\n)\nreturn\n{\n\"joke\"\n: joke_response.content,\n\"poem\"\n: poem_response.content}\ngraph\n=\n(\nStateGraph(State)\n.add_node(call_model)\n.add_edge(\nSTART\n,\n\"call_model\"\n)\n.compile()\n)\n# The stream_mode is set to \"messages\" to stream LLM tokens\n# The metadata contains information about the LLM invocation, including the tags\nasync\nfor\nmsg, metadata\nin\ngraph.astream(\n{\n\"topic\"\n:\n\"cats\"\n},\nstream_mode\n=\n\"messages\"\n,\n):\nif\nmetadata[\n\"tags\"\n]\n==\n[\n\"joke\"\n]:\nprint\n(msg.content,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n\u200b\nFilter by node\nTo stream tokens only from specific nodes, use\nstream_mode=\"messages\"\nand filter the outputs by the\nlanggraph_node\nfield in the streamed metadata:\nCopy\n# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor\nmsg, metadata\nin\ngraph.stream(\ninputs,\nstream_mode\n=\n\"messages\"\n,\n):\n# Filter the streamed tokens by the langgraph_node field in the metadata\n# to only include the tokens from the specified node\nif\nmsg.content\nand\nmetadata[\n\"langgraph_node\"\n]\n==\n\"some_node_name\"\n:\n...\nExtended example: streaming LLM tokens from specific nodes\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nSTART\n, StateGraph\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\nclass\nState\n(\nTypedDict\n):\ntopic:\nstr\njoke:\nstr\npoem:\nstr\ndef\nwrite_joke\n(\nstate\n: State):\ntopic\n=\nstate[\n\"topic\"\n]\njoke_response\n=\nmodel.invoke(\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nf\n\"Write a joke about\n{\ntopic\n}\n\"\n}]\n)\nreturn\n{\n\"joke\"\n: joke_response.content}\ndef\nwrite_poem\n(\nstate\n: State):\ntopic\n=\nstate[\n\"topic\"\n]\npoem_response\n=\nmodel.invoke(\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nf\n\"Write a short poem about\n{\ntopic\n}\n\"\n}]\n)\nreturn\n{\n\"poem\"\n: poem_response.content}\ngraph\n=\n(\nStateGraph(State)\n.add_node(write_joke)\n.add_node(write_poem)\n# write both the joke and the poem concurrently\n.add_edge(\nSTART\n,\n\"write_joke\"\n)\n.add_edge(\nSTART\n,\n\"write_poem\"\n)\n.compile()\n)\n# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor\nmsg, metadata\nin\ngraph.stream(\n{\n\"topic\"\n:\n\"cats\"\n},\nstream_mode\n=\n\"messages\"\n,\n):\n# Filter the streamed tokens by the langgraph_node field in the metadata\n# to only include the tokens from the write_poem node\nif\nmsg.content\nand\nmetadata[\n\"langgraph_node\"\n]\n==\n\"write_poem\"\n:\nprint\n(msg.content,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n\u200b\nStream custom data\nTo send\ncustom user-defined data\nfrom inside a LangGraph node or tool, follow these steps:\nUse\nget_stream_writer\nto access the stream writer and emit custom data.\nSet\nstream_mode=\"custom\"\nwhen calling\n.stream()\nor\n.astream()\nto get the custom data in the stream. You can combine multiple modes (e.g.,\n[\"updates\", \"custom\"]\n), but at least one must be\n\"custom\"\n.\nNo\nget_stream_writer\nin async for Python < 3.11\nIn async code running on Python < 3.11,\nget_stream_writer\nwill not work.\nInstead, add a\nwriter\nparameter to your node or tool and pass it manually.\nSee\nAsync with Python < 3.11\nfor usage examples.\nnode\ntool\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.config\nimport\nget_stream_writer\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\nclass\nState\n(\nTypedDict\n):\nquery:\nstr\nanswer:\nstr\ndef\nnode\n(\nstate\n: State):\n# Get the stream writer to send custom data\nwriter\n=\nget_stream_writer()\n# Emit a custom key-value pair (e.g., progress update)\nwriter({\n\"custom_key\"\n:\n\"Generating custom data inside node\"\n})\nreturn\n{\n\"answer\"\n:\n\"some data\"\n}\ngraph\n=\n(\nStateGraph(State)\n.add_node(node)\n.add_edge(\nSTART\n,\n\"node\"\n)\n.compile()\n)\ninputs\n=\n{\n\"query\"\n:\n\"example\"\n}\n# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor\nchunk\nin\ngraph.stream(inputs,\nstream_mode\n=\n\"custom\"\n):\nprint\n(chunk)\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlanggraph.config\nimport\nget_stream_writer\n@tool\ndef\nquery_database\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Query the database.\"\"\"\n# Access the stream writer to send custom data\nwriter\n=\nget_stream_writer()\n# Emit a custom key-value pair (e.g., progress update)\nwriter({\n\"data\"\n:\n\"Retrieved 0/100 records\"\n,\n\"type\"\n:\n\"progress\"\n})\n# perform query\n# Emit another custom key-value pair\nwriter({\n\"data\"\n:\n\"Retrieved 100/100 records\"\n,\n\"type\"\n:\n\"progress\"\n})\nreturn\n\"some-answer\"\ngraph\n=\n...\n# define a graph that uses this tool\n# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor\nchunk\nin\ngraph.stream(inputs,\nstream_mode\n=\n\"custom\"\n):\nprint\n(chunk)\n\u200b\nUse with any LLM\nYou can use\nstream_mode=\"custom\"\nto stream data from\nany LLM API\n\u2014 even if that API does\nnot\nimplement the LangChain chat model interface.\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\nCopy\nfrom\nlanggraph.config\nimport\nget_stream_writer\ndef\ncall_arbitrary_model\n(\nstate\n):\n\"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\n# Get the stream writer to send custom data\nwriter\n=\nget_stream_writer()\n# Assume you have a streaming client that yields chunks\n# Generate LLM tokens using your custom streaming client\nfor\nchunk\nin\nyour_custom_streaming_client(state[\n\"topic\"\n]):\n# Use the writer to send custom data to the stream\nwriter({\n\"custom_llm_chunk\"\n: chunk})\nreturn\n{\n\"result\"\n:\n\"completed\"\n}\ngraph\n=\n(\nStateGraph(State)\n.add_node(call_arbitrary_model)\n# Add other nodes and edges as needed\n.compile()\n)\n# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor\nchunk\nin\ngraph.stream(\n{\n\"topic\"\n:\n\"cats\"\n},\nstream_mode\n=\n\"custom\"\n,\n):\n# The chunk will contain the custom data streamed from the llm\nprint\n(chunk)\nExtended example: streaming arbitrary chat model\nCopy\nimport\noperator\nimport\njson\nfrom\ntyping\nimport\nTypedDict\nfrom\ntyping_extensions\nimport\nAnnotated\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\nfrom\nopenai\nimport\nAsyncOpenAI\nopenai_client\n=\nAsyncOpenAI()\nmodel_name\n=\n\"gpt-5-mini\"\nasync\ndef\nstream_tokens\n(\nmodel_name\n:\nstr\n,\nmessages\n: list[\ndict\n]):\nresponse\n=\nawait\nopenai_client.chat.completions.create(\nmessages\n=\nmessages,\nmodel\n=\nmodel_name,\nstream\n=\nTrue\n)\nrole\n=\nNone\nasync\nfor\nchunk\nin\nresponse:\ndelta\n=\nchunk.choices[\n0\n].delta\nif\ndelta.role\nis\nnot\nNone\n:\nrole\n=\ndelta.role\nif\ndelta.content:\nyield\n{\n\"role\"\n: role,\n\"content\"\n: delta.content}\n# this is our tool\nasync\ndef\nget_items\n(\nplace\n:\nstr\n) ->\nstr\n:\n\"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\nwriter\n=\nget_stream_writer()\nresponse\n=\n\"\"\nasync\nfor\nmsg_chunk\nin\nstream_tokens(\nmodel_name,\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: (\n\"Can you tell me what kind of items \"\nf\n\"i might find in the following place: '\n{\nplace\n}\n'. \"\n\"List at least 3 such items separating them by a comma. \"\n\"And include a brief description of each item.\"\n),\n}\n],\n):\nresponse\n+=\nmsg_chunk[\n\"content\"\n]\nwriter(msg_chunk)\nreturn\nresponse\nclass\nState\n(\nTypedDict\n):\nmessages: Annotated[list[\ndict\n], operator.add]\n# this is the tool-calling graph node\nasync\ndef\ncall_tool\n(\nstate\n: State):\nai_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\ntool_call\n=\nai_message[\n\"tool_calls\"\n][\n-\n1\n]\nfunction_name\n=\ntool_call[\n\"function\"\n][\n\"name\"\n]\nif\nfunction_name\n!=\n\"get_items\"\n:\nraise\nValueError\n(\nf\n\"Tool\n{\nfunction_name\n}\nnot supported\"\n)\nfunction_arguments\n=\ntool_call[\n\"function\"\n][\n\"arguments\"\n]\narguments\n=\njson.loads(function_arguments)\nfunction_response\n=\nawait\nget_items(\n**\narguments)\ntool_message\n=\n{\n\"tool_call_id\"\n: tool_call[\n\"id\"\n],\n\"role\"\n:\n\"tool\"\n,\n\"name\"\n: function_name,\n\"content\"\n: function_response,\n}\nreturn\n{\n\"messages\"\n: [tool_message]}\ngraph\n=\n(\nStateGraph(State)\n.add_node(call_tool)\n.add_edge(\nSTART\n,\n\"call_tool\"\n)\n.compile()\n)\nLet\u2019s invoke the graph with an\nAIMessage\nthat includes a tool call:\nCopy\ninputs\n=\n{\n\"messages\"\n: [\n{\n\"content\"\n:\nNone\n,\n\"role\"\n:\n\"assistant\"\n,\n\"tool_calls\"\n: [\n{\n\"id\"\n:\n\"1\"\n,\n\"function\"\n: {\n\"arguments\"\n:\n'{\"place\":\"bedroom\"}'\n,\n\"name\"\n:\n\"get_items\"\n,\n},\n\"type\"\n:\n\"function\"\n,\n}\n],\n}\n]\n}\nasync\nfor\nchunk\nin\ngraph.astream(\ninputs,\nstream_mode\n=\n\"custom\"\n,\n):\nprint\n(chunk[\n\"content\"\n],\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\n\u200b\nDisable streaming for specific chat models\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\nmodels that do not support it.\nSet\nstreaming=False\nwhen initializing the model.\ninit_chat_model\nChat model interface\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n,\n# Set streaming=False to disable streaming for the chat model\nstreaming\n=\nFalse\n)\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\n# Set streaming=False to disable streaming for the chat model\nmodel\n=\nChatOpenAI(\nmodel\n=\n\"o1-preview\"\n,\nstreaming\n=\nFalse\n)\nNot all chat model integrations support the\nstreaming\nparameter. If your model doesn\u2019t support it, use\ndisable_streaming=True\ninstead. This parameter is available on all chat models via the base class.\n\u200b\nAsync with Python < 3.11\nIn Python versions < 3.11,\nasyncio tasks\ndo not support the\ncontext\nparameter.\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph\u2019s streaming mechanisms in two key ways:\nYou\nmust\nexplicitly pass\nRunnableConfig\ninto async LLM calls (e.g.,\nainvoke()\n), as callbacks are not automatically propagated.\nYou\ncannot\nuse\nget_stream_writer\nin async nodes or tools \u2014 you must pass a\nwriter\nargument directly.\nExtended example: async LLM call with manual config\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nSTART\n, StateGraph\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n)\nclass\nState\n(\nTypedDict\n):\ntopic:\nstr\njoke:\nstr\n# Accept config as an argument in the async node function\nasync\ndef\ncall_model\n(\nstate\n,\nconfig\n):\ntopic\n=\nstate[\n\"topic\"\n]\nprint\n(\n\"Generating joke...\"\n)\n# Pass config to model.ainvoke() to ensure proper context propagation\njoke_response\n=\nawait\nmodel.ainvoke(\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nf\n\"Write a joke about\n{\ntopic\n}\n\"\n}],\nconfig,\n)\nreturn\n{\n\"joke\"\n: joke_response.content}\ngraph\n=\n(\nStateGraph(State)\n.add_node(call_model)\n.add_edge(\nSTART\n,\n\"call_model\"\n)\n.compile()\n)\n# Set stream_mode=\"messages\" to stream LLM tokens\nasync\nfor\nchunk, metadata\nin\ngraph.astream(\n{\n\"topic\"\n:\n\"ice cream\"\n},\nstream_mode\n=\n\"messages\"\n,\n):\nif\nchunk.content:\nprint\n(chunk.content,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nExtended example: async custom streaming with stream writer\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.types\nimport\nStreamWriter\nclass\nState\n(\nTypedDict\n):\ntopic:\nstr\njoke:\nstr\n# Add writer as an argument in the function signature of the async node or tool\n# LangGraph will automatically pass the stream writer to the function\nasync\ndef\ngenerate_joke\n(\nstate\n: State,\nwriter\n: StreamWriter):\nwriter({\n\"custom_key\"\n:\n\"Streaming custom data while generating a joke\"\n})\nreturn\n{\n\"joke\"\n:\nf\n\"This is a joke about\n{\nstate[\n'topic'\n]\n}\n\"\n}\ngraph\n=\n(\nStateGraph(State)\n.add_node(generate_joke)\n.add_edge(\nSTART\n,\n\"generate_joke\"\n)\n.compile()\n)\n# Set stream_mode=\"custom\" to receive the custom data in the stream  #\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"topic\"\n:\n\"ice cream\"\n},\nstream_mode\n=\n\"custom\"\n,\n):\nprint\n(chunk)\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nDurable execution\nPrevious\nInterrupts\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/streaming",
      "title": "Streaming - Docs by LangChain",
      "heading": "Streaming"
    }
  },
  {
    "page_content": "Interrupts - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCapabilities\nInterrupts\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nPause using interrupt\nResuming interrupts\nCommon patterns\nApprove or reject\nReview and edit state\nInterrupts in tools\nValidating human input\nRules of interrupts\nDo not wrap interrupt calls in try/except\nDo not reorder interrupt calls within a node\nDo not return complex values in interrupt calls\nSide effects called before interrupt must be idempotent\nUsing with subgraphs called as functions\nDebugging with interrupts\nUsing LangGraph Studio\nCapabilities\nInterrupts\nCopy page\nCopy page\nInterrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its\npersistence\nlayer and waits indefinitely until you resume execution.\nInterrupts work by calling the\ninterrupt()\nfunction at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you\u2019re ready to continue, you resume execution by re-invoking the graph using\nCommand\n, which then becomes the return value of the\ninterrupt()\ncall from inside the node.\nUnlike static breakpoints (which pause before or after specific nodes), interrupts are\ndynamic\n\u2014they can be placed anywhere in your code and can be conditional based on your application logic.\nCheckpointing keeps your place:\nthe checkpointer writes the exact graph state so you can resume later, even when in an error state.\nthread_id\nis your pointer:\nset\nconfig={\"configurable\": {\"thread_id\": ...}}\nto tell the checkpointer which state to load.\nInterrupt payloads surface as\n__interrupt__\n:\nthe values you pass to\ninterrupt()\nreturn to the caller in the\n__interrupt__\nfield so you know what the graph is waiting on.\nThe\nthread_id\nyou choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.\n\u200b\nPause using\ninterrupt\nThe\ninterrupt\nfunction pauses graph execution and returns a value to the caller. When you call\ninterrupt\nwithin a node, LangGraph saves the current graph state and waits for you to resume execution with input.\nTo use\ninterrupt\n, you need:\nA\ncheckpointer\nto persist the graph state (use a durable checkpointer in production)\nA\nthread ID\nin your config so the runtime knows which state to resume from\nTo call\ninterrupt()\nwhere you want to pause (payload must be JSON-serializable)\nCopy\nfrom\nlanggraph.types\nimport\ninterrupt\ndef\napproval_node\n(\nstate\n: State):\n# Pause and ask for approval\napproved\n=\ninterrupt(\n\"Do you approve this action?\"\n)\n# When you resume, Command(resume=...) returns that value here\nreturn\n{\n\"approved\"\n: approved}\nWhen you call\ninterrupt\n, here\u2019s what happens:\nGraph execution gets suspended\nat the exact point where\ninterrupt\nis called\nState is saved\nusing the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)\nValue is returned\nto the caller under\n__interrupt__\n; it can be any JSON-serializable value (string, object, array, etc.)\nGraph waits indefinitely\nuntil you resume execution with a response\nResponse is passed back\ninto the node when you resume, becoming the return value of the\ninterrupt()\ncall\n\u200b\nResuming interrupts\nAfter an interrupt pauses execution, you resume the graph by invoking it again with a\nCommand\nthat contains the resume value. The resume value is passed back to the\ninterrupt\ncall, allowing the node to continue execution with the external input.\nCopy\nfrom\nlanggraph.types\nimport\nCommand\n# Initial run - hits the interrupt and pauses\n# thread_id is the persistent pointer (stores a stable ID in production)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"thread-1\"\n}}\nresult\n=\ngraph.invoke({\n\"input\"\n:\n\"data\"\n},\nconfig\n=\nconfig)\n# Check what was interrupted\n# __interrupt__ contains the payload that was passed to interrupt()\nprint\n(result[\n\"__interrupt__\"\n])\n# > [Interrupt(value='Do you approve this action?')]\n# Resume with the human's response\n# The resume payload becomes the return value of interrupt() inside the node\ngraph.invoke(Command(\nresume\n=\nTrue\n),\nconfig\n=\nconfig)\nKey points about resuming:\nYou must use the\nsame thread ID\nwhen resuming that was used when the interrupt occurred\nThe value passed to\nCommand(resume=...)\nbecomes the return value of the\ninterrupt\ncall\nThe node restarts from the beginning of the node where the\ninterrupt\nwas called when resumed, so any code before the\ninterrupt\nruns again\nYou can pass any JSON-serializable value as the resume value\n\u200b\nCommon patterns\nThe key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:\nApproval workflows\n: Pause before executing critical actions (API calls, database changes, financial transactions)\nReview and edit\n: Let humans review and modify LLM outputs or tool calls before continuing\nInterrupting tool calls\n: Pause before executing tool calls to review and edit the tool call before execution\nValidating human input\n: Pause before proceeding to the next step to validate human input\n\u200b\nApprove or reject\nOne of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.\nCopy\nfrom\ntyping\nimport\nLiteral\nfrom\nlanggraph.types\nimport\ninterrupt, Command\ndef\napproval_node\n(\nstate\n: State) -> Command[Literal[\n\"proceed\"\n,\n\"cancel\"\n]]:\n# Pause execution; payload shows up under result[\"__interrupt__\"]\nis_approved\n=\ninterrupt({\n\"question\"\n:\n\"Do you want to proceed with this action?\"\n,\n\"details\"\n: state[\n\"action_details\"\n]\n})\n# Route based on the response\nif\nis_approved:\nreturn\nCommand(\ngoto\n=\n\"proceed\"\n)\n# Runs after the resume payload is provided\nelse\n:\nreturn\nCommand(\ngoto\n=\n\"cancel\"\n)\nWhen you resume the graph, pass\ntrue\nto approve or\nfalse\nto reject:\nCopy\n# To approve\ngraph.invoke(Command(\nresume\n=\nTrue\n),\nconfig\n=\nconfig)\n# To reject\ngraph.invoke(Command(\nresume\n=\nFalse\n),\nconfig\n=\nconfig)\nFull example\nCopy\nfrom\ntyping\nimport\nLiteral, Optional, TypedDict\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.types\nimport\nCommand, interrupt\nclass\nApprovalState\n(\nTypedDict\n):\naction_details:\nstr\nstatus: Optional[Literal[\n\"pending\"\n,\n\"approved\"\n,\n\"rejected\"\n]]\ndef\napproval_node\n(\nstate\n: ApprovalState) -> Command[Literal[\n\"proceed\"\n,\n\"cancel\"\n]]:\n# Expose details so the caller can render them in a UI\ndecision\n=\ninterrupt({\n\"question\"\n:\n\"Approve this action?\"\n,\n\"details\"\n: state[\n\"action_details\"\n],\n})\n# Route to the appropriate node after resume\nreturn\nCommand(\ngoto\n=\n\"proceed\"\nif\ndecision\nelse\n\"cancel\"\n)\ndef\nproceed_node\n(\nstate\n: ApprovalState):\nreturn\n{\n\"status\"\n:\n\"approved\"\n}\ndef\ncancel_node\n(\nstate\n: ApprovalState):\nreturn\n{\n\"status\"\n:\n\"rejected\"\n}\nbuilder\n=\nStateGraph(ApprovalState)\nbuilder.add_node(\n\"approval\"\n, approval_node)\nbuilder.add_node(\n\"proceed\"\n, proceed_node)\nbuilder.add_node(\n\"cancel\"\n, cancel_node)\nbuilder.add_edge(\nSTART\n,\n\"approval\"\n)\nbuilder.add_edge(\n\"proceed\"\n,\nEND\n)\nbuilder.add_edge(\n\"cancel\"\n,\nEND\n)\n# Use a more durable checkpointer in production\ncheckpointer\n=\nMemorySaver()\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"approval-123\"\n}}\ninitial\n=\ngraph.invoke(\n{\n\"action_details\"\n:\n\"Transfer $500\"\n,\n\"status\"\n:\n\"pending\"\n},\nconfig\n=\nconfig,\n)\nprint\n(initial[\n\"__interrupt__\"\n])\n# -> [Interrupt(value={'question': ..., 'details': ...})]\n# Resume with the decision; True routes to proceed, False to cancel\nresumed\n=\ngraph.invoke(Command(\nresume\n=\nTrue\n),\nconfig\n=\nconfig)\nprint\n(resumed[\n\"status\"\n])\n# -> \"approved\"\n\u200b\nReview and edit state\nSometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.\nCopy\nfrom\nlanggraph.types\nimport\ninterrupt\ndef\nreview_node\n(\nstate\n: State):\n# Pause and show the current content for review (surfaces in result[\"__interrupt__\"])\nedited_content\n=\ninterrupt({\n\"instruction\"\n:\n\"Review and edit this content\"\n,\n\"content\"\n: state[\n\"generated_text\"\n]\n})\n# Update the state with the edited version\nreturn\n{\n\"generated_text\"\n: edited_content}\nWhen resuming, provide the edited content:\nCopy\ngraph.invoke(\nCommand(\nresume\n=\n\"The edited and improved text\"\n),\n# Value becomes the return from interrupt()\nconfig\n=\nconfig\n)\nFull example\nCopy\nimport\nsqlite3\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.types\nimport\nCommand, interrupt\nclass\nReviewState\n(\nTypedDict\n):\ngenerated_text:\nstr\ndef\nreview_node\n(\nstate\n: ReviewState):\n# Ask a reviewer to edit the generated content\nupdated\n=\ninterrupt({\n\"instruction\"\n:\n\"Review and edit this content\"\n,\n\"content\"\n: state[\n\"generated_text\"\n],\n})\nreturn\n{\n\"generated_text\"\n: updated}\nbuilder\n=\nStateGraph(ReviewState)\nbuilder.add_node(\n\"review\"\n, review_node)\nbuilder.add_edge(\nSTART\n,\n\"review\"\n)\nbuilder.add_edge(\n\"review\"\n,\nEND\n)\ncheckpointer\n=\nMemorySaver()\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"review-42\"\n}}\ninitial\n=\ngraph.invoke({\n\"generated_text\"\n:\n\"Initial draft\"\n},\nconfig\n=\nconfig)\nprint\n(initial[\n\"__interrupt__\"\n])\n# -> [Interrupt(value={'instruction': ..., 'content': ...})]\n# Resume with the edited text from the reviewer\nfinal_state\n=\ngraph.invoke(\nCommand(\nresume\n=\n\"Improved draft after review\"\n),\nconfig\n=\nconfig,\n)\nprint\n(final_state[\n\"generated_text\"\n])\n# -> \"Improved draft after review\"\n\u200b\nInterrupts in tools\nYou can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it\u2019s called, and allows for human review and editing of the tool call before it is executed.\nFirst, define a tool that uses\ninterrupt\n:\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlanggraph.types\nimport\ninterrupt\n@tool\ndef\nsend_email\n(\nto\n:\nstr\n,\nsubject\n:\nstr\n,\nbody\n:\nstr\n):\n\"\"\"Send an email to a recipient.\"\"\"\n# Pause before sending; payload surfaces in result[\"__interrupt__\"]\nresponse\n=\ninterrupt({\n\"action\"\n:\n\"send_email\"\n,\n\"to\"\n: to,\n\"subject\"\n: subject,\n\"body\"\n: body,\n\"message\"\n:\n\"Approve sending this email?\"\n})\nif\nresponse.get(\n\"action\"\n)\n==\n\"approve\"\n:\n# Resume value can override inputs before executing\nfinal_to\n=\nresponse.get(\n\"to\"\n, to)\nfinal_subject\n=\nresponse.get(\n\"subject\"\n, subject)\nfinal_body\n=\nresponse.get(\n\"body\"\n, body)\nreturn\nf\n\"Email sent to\n{\nfinal_to\n}\nwith subject '\n{\nfinal_subject\n}\n'\"\nreturn\n\"Email cancelled by user\"\nThis approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.\nFull example\nCopy\nimport\nsqlite3\nfrom\ntyping\nimport\nTypedDict\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlanggraph.checkpoint.sqlite\nimport\nSqliteSaver\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.types\nimport\nCommand, interrupt\nclass\nAgentState\n(\nTypedDict\n):\nmessages: list[\ndict\n]\n@tool\ndef\nsend_email\n(\nto\n:\nstr\n,\nsubject\n:\nstr\n,\nbody\n:\nstr\n):\n\"\"\"Send an email to a recipient.\"\"\"\n# Pause before sending; payload surfaces in result[\"__interrupt__\"]\nresponse\n=\ninterrupt({\n\"action\"\n:\n\"send_email\"\n,\n\"to\"\n: to,\n\"subject\"\n: subject,\n\"body\"\n: body,\n\"message\"\n:\n\"Approve sending this email?\"\n,\n})\nif\nresponse.get(\n\"action\"\n)\n==\n\"approve\"\n:\nfinal_to\n=\nresponse.get(\n\"to\"\n, to)\nfinal_subject\n=\nresponse.get(\n\"subject\"\n, subject)\nfinal_body\n=\nresponse.get(\n\"body\"\n, body)\n# Actually send the email (your implementation here)\nprint\n(\nf\n\"[send_email] to=\n{\nfinal_to\n}\nsubject=\n{\nfinal_subject\n}\nbody=\n{\nfinal_body\n}\n\"\n)\nreturn\nf\n\"Email sent to\n{\nfinal_to\n}\n\"\nreturn\n\"Email cancelled by user\"\nmodel\n=\nChatAnthropic(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n).bind_tools([send_email])\ndef\nagent_node\n(\nstate\n: AgentState):\n# LLM may decide to call the tool; interrupt pauses before sending\nresult\n=\nmodel.invoke(state[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: state[\n\"messages\"\n]\n+\n[result]}\nbuilder\n=\nStateGraph(AgentState)\nbuilder.add_node(\n\"agent\"\n, agent_node)\nbuilder.add_edge(\nSTART\n,\n\"agent\"\n)\nbuilder.add_edge(\n\"agent\"\n,\nEND\n)\ncheckpointer\n=\nSqliteSaver(sqlite3.connect(\n\"tool-approval.db\"\n))\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"email-workflow\"\n}}\ninitial\n=\ngraph.invoke(\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Send an email to\n[email\u00a0protected]\nabout the meeting\"\n}\n]\n},\nconfig\n=\nconfig,\n)\nprint\n(initial[\n\"__interrupt__\"\n])\n# -> [Interrupt(value={'action': 'send_email', ...})]\n# Resume with approval and optionally edited arguments\nresumed\n=\ngraph.invoke(\nCommand(\nresume\n=\n{\n\"action\"\n:\n\"approve\"\n,\n\"subject\"\n:\n\"Updated subject\"\n}),\nconfig\n=\nconfig,\n)\nprint\n(resumed[\n\"messages\"\n][\n-\n1\n])\n# -> Tool result returned by send_email\n\u200b\nValidating human input\nSometimes you need to validate input from humans and ask again if it\u2019s invalid. You can do this using multiple\ninterrupt\ncalls in a loop.\nCopy\nfrom\nlanggraph.types\nimport\ninterrupt\ndef\nget_age_node\n(\nstate\n: State):\nprompt\n=\n\"What is your age?\"\nwhile\nTrue\n:\nanswer\n=\ninterrupt(prompt)\n# payload surfaces in result[\"__interrupt__\"]\n# Validate the input\nif\nisinstance\n(answer,\nint\n)\nand\nanswer\n>\n0\n:\n# Valid input - continue\nbreak\nelse\n:\n# Invalid input - ask again with a more specific prompt\nprompt\n=\nf\n\"'\n{\nanswer\n}\n' is not a valid age. Please enter a positive number.\"\nreturn\n{\n\"age\"\n: answer}\nEach time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.\nFull example\nCopy\nimport\nsqlite3\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.checkpoint.sqlite\nimport\nSqliteSaver\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.types\nimport\nCommand, interrupt\nclass\nFormState\n(\nTypedDict\n):\nage:\nint\n|\nNone\ndef\nget_age_node\n(\nstate\n: FormState):\nprompt\n=\n\"What is your age?\"\nwhile\nTrue\n:\nanswer\n=\ninterrupt(prompt)\n# payload surfaces in result[\"__interrupt__\"]\nif\nisinstance\n(answer,\nint\n)\nand\nanswer\n>\n0\n:\nreturn\n{\n\"age\"\n: answer}\nprompt\n=\nf\n\"'\n{\nanswer\n}\n' is not a valid age. Please enter a positive number.\"\nbuilder\n=\nStateGraph(FormState)\nbuilder.add_node(\n\"collect_age\"\n, get_age_node)\nbuilder.add_edge(\nSTART\n,\n\"collect_age\"\n)\nbuilder.add_edge(\n\"collect_age\"\n,\nEND\n)\ncheckpointer\n=\nSqliteSaver(sqlite3.connect(\n\"forms.db\"\n))\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"form-1\"\n}}\nfirst\n=\ngraph.invoke({\n\"age\"\n:\nNone\n},\nconfig\n=\nconfig)\nprint\n(first[\n\"__interrupt__\"\n])\n# -> [Interrupt(value='What is your age?', ...)]\n# Provide invalid data; the node re-prompts\nretry\n=\ngraph.invoke(Command(\nresume\n=\n\"thirty\"\n),\nconfig\n=\nconfig)\nprint\n(retry[\n\"__interrupt__\"\n])\n# -> [Interrupt(value=\"'thirty' is not a valid age...\", ...)]\n# Provide valid data; loop exits and state updates\nfinal\n=\ngraph.invoke(Command(\nresume\n=\n30\n),\nconfig\n=\nconfig)\nprint\n(final[\n\"age\"\n])\n# -> 30\n\u200b\nRules of interrupts\nWhen you call\ninterrupt\nwithin a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.\nWhen execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning\u2014it does not resume from the exact line where\ninterrupt\nwas called. This means any code that ran before the\ninterrupt\nwill execute again. Because of this, there\u2019s a few important rules to follow when working with interrupts to ensure they behave as expected.\n\u200b\nDo not wrap\ninterrupt\ncalls in try/except\nThe way that\ninterrupt\npauses execution at the point of the call is by throwing a special exception. If you wrap the\ninterrupt\ncall in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph.\n\u2705 Separate\ninterrupt\ncalls from error-prone code\n\u2705 Use specific exception types in try/except blocks\nSeparating logic\nExplicit exception handling\nCopy\ndef\nnode_a\n(\nstate\n: State):\n# \u2705 Good: interrupting first, then handling\n# error conditions separately\ninterrupt(\n\"What's your name?\"\n)\ntry\n:\nfetch_data()\n# This can fail\nexcept\nException\nas\ne:\nprint\n(e)\nreturn\nstate\n\ud83d\udd34 Do not wrap\ninterrupt\ncalls in bare try/except blocks\nCopy\ndef\nnode_a\n(\nstate\n: State):\n# \u274c Bad: wrapping interrupt in bare try/except\n# will catch the interrupt exception\ntry\n:\ninterrupt(\n\"What's your name?\"\n)\nexcept\nException\nas\ne:\nprint\n(e)\nreturn\nstate\n\u200b\nDo not reorder\ninterrupt\ncalls within a node\nIt\u2019s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.\nWhen a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task\u2019s resume list. Matching is\nstrictly index-based\n, so the order of interrupt calls within the node is important.\n\u2705 Keep\ninterrupt\ncalls consistent across node executions\nCopy\ndef\nnode_a\n(\nstate\n: State):\n# \u2705 Good: interrupt calls happen in the same order every time\nname\n=\ninterrupt(\n\"What's your name?\"\n)\nage\n=\ninterrupt(\n\"What's your age?\"\n)\ncity\n=\ninterrupt(\n\"What's your city?\"\n)\nreturn\n{\n\"name\"\n: name,\n\"age\"\n: age,\n\"city\"\n: city\n}\n\ud83d\udd34 Do not conditionally skip\ninterrupt\ncalls within a node\n\ud83d\udd34 Do not loop\ninterrupt\ncalls using logic that isn\u2019t deterministic across executions\nSkipping interrupts\nLooping interrupts\nCopy\ndef\nnode_a\n(\nstate\n: State):\n# \u274c Bad: conditionally skipping interrupts changes the order\nname\n=\ninterrupt(\n\"What's your name?\"\n)\n# On first run, this might skip the interrupt\n# On resume, it might not skip it - causing index mismatch\nif\nstate.get(\n\"needs_age\"\n):\nage\n=\ninterrupt(\n\"What's your age?\"\n)\ncity\n=\ninterrupt(\n\"What's your city?\"\n)\nreturn\n{\n\"name\"\n: name,\n\"city\"\n: city}\n\u200b\nDo not return complex values in\ninterrupt\ncalls\nDepending on which checkpointer is used, complex values may not be serializable (e.g. you can\u2019t serialize a function). To make your graphs adaptable to any deployment, it\u2019s best practice to only use values that can be reasonably serialized.\n\u2705 Pass simple, JSON-serializable types to\ninterrupt\n\u2705 Pass dictionaries/objects with simple values\nSimple values\nStructured data\nCopy\ndef\nnode_a\n(\nstate\n: State):\n# \u2705 Good: passing simple types that are serializable\nname\n=\ninterrupt(\n\"What's your name?\"\n)\ncount\n=\ninterrupt(\n42\n)\napproved\n=\ninterrupt(\nTrue\n)\nreturn\n{\n\"name\"\n: name,\n\"count\"\n: count,\n\"approved\"\n: approved}\n\ud83d\udd34 Do not pass functions, class instances, or other complex objects to\ninterrupt\nFunctions\nClass instances\nCopy\ndef\nvalidate_input\n(\nvalue\n):\nreturn\nlen\n(value)\n>\n0\ndef\nnode_a\n(\nstate\n: State):\n# \u274c Bad: passing a function to interrupt\n# The function cannot be serialized\nresponse\n=\ninterrupt({\n\"question\"\n:\n\"What's your name?\"\n,\n\"validator\"\n: validate_input\n# This will fail\n})\nreturn\n{\n\"name\"\n: response}\n\u200b\nSide effects called before\ninterrupt\nmust be idempotent\nBecause interrupts work by re-running the nodes they were called from, side effects called before\ninterrupt\nshould (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.\nAs an example, you might have an API call to update a record inside of a node. If\ninterrupt\nis called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.\n\u2705 Use idempotent operations before\ninterrupt\n\u2705 Place side effects after\ninterrupt\ncalls\n\u2705 Separate side effects into separate nodes when possible\nIdempotent operations\nSide effects after interrupt\nSeparating into different nodes\nCopy\ndef\nnode_a\n(\nstate\n: State):\n# \u2705 Good: using upsert operation which is idempotent\n# Running this multiple times will have the same result\ndb.upsert_user(\nuser_id\n=\nstate[\n\"user_id\"\n],\nstatus\n=\n\"pending_approval\"\n)\napproved\n=\ninterrupt(\n\"Approve this change?\"\n)\nreturn\n{\n\"approved\"\n: approved}\n\ud83d\udd34 Do not perform non-idempotent operations before\ninterrupt\n\ud83d\udd34 Do not create new records without checking if they exist\nCreating records\nAppending to lists\nCopy\ndef\nnode_a\n(\nstate\n: State):\n# \u274c Bad: creating a new record before interrupt\n# This will create duplicate records on each resume\naudit_id\n=\ndb.create_audit_log({\n\"user_id\"\n: state[\n\"user_id\"\n],\n\"action\"\n:\n\"pending_approval\"\n,\n\"timestamp\"\n: datetime.now()\n})\napproved\n=\ninterrupt(\n\"Approve this change?\"\n)\nreturn\n{\n\"approved\"\n: approved,\n\"audit_id\"\n: audit_id}\n\u200b\nUsing with subgraphs called as functions\nWhen invoking a subgraph within a node, the parent graph will resume execution from the\nbeginning of the node\nwhere the subgraph was invoked and the\ninterrupt\nwas triggered. Similarly, the\nsubgraph\nwill also resume from the beginning of the node where\ninterrupt\nwas called.\nCopy\ndef\nnode_in_parent_graph\n(\nstate\n: State):\nsome_code()\n# <-- This will re-execute when resumed\n# Invoke a subgraph as a function.\n# The subgraph contains an `interrupt` call.\nsubgraph_result\n=\nsubgraph.invoke(some_input)\n# ...\ndef\nnode_in_subgraph\n(\nstate\n: State):\nsome_other_code()\n# <-- This will also re-execute when resumed\nresult\n=\ninterrupt(\n\"What's your name?\"\n)\n# ...\n\u200b\nDebugging with interrupts\nTo debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying\ninterrupt_before\nand\ninterrupt_after\nwhen compiling the graph.\nStatic interrupts are\nnot\nrecommended for human-in-the-loop workflows. Use the\ninterrupt\nfunction instead.\nAt compile time\nAt run time\nCopy\ngraph\n=\nbuilder.compile(\ninterrupt_before\n=\n[\n\"node_a\"\n],\ninterrupt_after\n=\n[\n\"node_b\"\n,\n\"node_c\"\n],\ncheckpointer\n=\ncheckpointer,\n)\n# Pass a thread ID to the graph\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"some_thread\"\n}\n}\n# Run the graph until the breakpoint\ngraph.invoke(inputs,\nconfig\n=\nconfig)\n# Resume the graph\ngraph.invoke(\nNone\n,\nconfig\n=\nconfig)\nThe breakpoints are set during\ncompile\ntime.\ninterrupt_before\nspecifies the nodes where execution should pause before the node is executed.\ninterrupt_after\nspecifies the nodes where execution should pause after the node is executed.\nA checkpointer is required to enable breakpoints.\nThe graph is run until the first breakpoint is hit.\nThe graph is resumed by passing in\nNone\nfor the input. This will run the graph until the next breakpoint is hit.\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"some_thread\"\n}\n}\n# Run the graph until the breakpoint\ngraph.invoke(\ninputs,\ninterrupt_before\n=\n[\n\"node_a\"\n],\ninterrupt_after\n=\n[\n\"node_b\"\n,\n\"node_c\"\n],\nconfig\n=\nconfig,\n)\n# Resume the graph\ngraph.invoke(\nNone\n,\nconfig\n=\nconfig)\ngraph.invoke\nis called with the\ninterrupt_before\nand\ninterrupt_after\nparameters. This is a run-time configuration and can be changed for every invocation.\ninterrupt_before\nspecifies the nodes where execution should pause before the node is executed.\ninterrupt_after\nspecifies the nodes where execution should pause after the node is executed.\nThe graph is run until the first breakpoint is hit.\nThe graph is resumed by passing in\nNone\nfor the input. This will run the graph until the next breakpoint is hit.\n\u200b\nUsing LangGraph Studio\nYou can use\nLangGraph Studio\nto set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nStreaming\nPrevious\nUse time-travel\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/interrupts",
      "title": "Interrupts - Docs by LangChain",
      "heading": "Interrupts"
    }
  },
  {
    "page_content": "Subgraphs - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCapabilities\nSubgraphs\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nSetup\nInvoke a graph from a node\nAdd a graph as a node\nAdd persistence\nView subgraph state\nStream subgraph outputs\nCapabilities\nSubgraphs\nCopy page\nCopy page\nThis guide explains the mechanics of using subgraphs. A subgraph is a\ngraph\nthat is used as a\nnode\nin another graph.\nSubgraphs are useful for:\nBuilding\nmulti-agent systems\nRe-using a set of nodes in multiple graphs\nDistributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph\nWhen adding subgraphs, you need to define how the parent graph and the subgraph communicate:\nInvoke a graph from a node\n\u2014 subgraphs are called from inside a node in the parent graph\nAdd a graph as a node\n\u2014 a subgraph is added directly as a node in the parent and\nshares\nstate keys\nwith the parent\n\u200b\nSetup\npip\nuv\nCopy\npip\ninstall\n-U\nlanggraph\nSet up LangSmith for LangGraph development\nSign up for\nLangSmith\nto quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started\nhere\n.\n\u200b\nInvoke a graph from a node\nA simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have\ncompletely different schemas\nfrom the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a\nmulti-agent\nsystem.\nIf that\u2019s the case for your application, you need to define a node\nfunction that invokes the subgraph\n. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.\nCopy\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph.state\nimport\nStateGraph,\nSTART\nclass\nSubgraphState\n(\nTypedDict\n):\nbar:\nstr\n# Subgraph\ndef\nsubgraph_node_1\n(\nstate\n: SubgraphState):\nreturn\n{\n\"bar\"\n:\n\"hi! \"\n+\nstate[\n\"bar\"\n]}\nsubgraph_builder\n=\nStateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(\nSTART\n,\n\"subgraph_node_1\"\n)\nsubgraph\n=\nsubgraph_builder.compile()\n# Parent graph\nclass\nState\n(\nTypedDict\n):\nfoo:\nstr\ndef\ncall_subgraph\n(\nstate\n: State):\n# Transform the state to the subgraph state\nsubgraph_output\n=\nsubgraph.invoke({\n\"bar\"\n: state[\n\"foo\"\n]})\n# Transform response back to the parent state\nreturn\n{\n\"foo\"\n: subgraph_output[\n\"bar\"\n]}\nbuilder\n=\nStateGraph(State)\nbuilder.add_node(\n\"node_1\"\n, call_subgraph)\nbuilder.add_edge(\nSTART\n,\n\"node_1\"\n)\ngraph\n=\nbuilder.compile()\nFull example: different state schemas\nCopy\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph.state\nimport\nStateGraph,\nSTART\n# Define subgraph\nclass\nSubgraphState\n(\nTypedDict\n):\n# note that none of these keys are shared with the parent graph state\nbar:\nstr\nbaz:\nstr\ndef\nsubgraph_node_1\n(\nstate\n: SubgraphState):\nreturn\n{\n\"baz\"\n:\n\"baz\"\n}\ndef\nsubgraph_node_2\n(\nstate\n: SubgraphState):\nreturn\n{\n\"bar\"\n: state[\n\"bar\"\n]\n+\nstate[\n\"baz\"\n]}\nsubgraph_builder\n=\nStateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(\nSTART\n,\n\"subgraph_node_1\"\n)\nsubgraph_builder.add_edge(\n\"subgraph_node_1\"\n,\n\"subgraph_node_2\"\n)\nsubgraph\n=\nsubgraph_builder.compile()\n# Define parent graph\nclass\nParentState\n(\nTypedDict\n):\nfoo:\nstr\ndef\nnode_1\n(\nstate\n: ParentState):\nreturn\n{\n\"foo\"\n:\n\"hi! \"\n+\nstate[\n\"foo\"\n]}\ndef\nnode_2\n(\nstate\n: ParentState):\n# Transform the state to the subgraph state\nresponse\n=\nsubgraph.invoke({\n\"bar\"\n: state[\n\"foo\"\n]})\n# Transform response back to the parent state\nreturn\n{\n\"foo\"\n: response[\n\"bar\"\n]}\nbuilder\n=\nStateGraph(ParentState)\nbuilder.add_node(\n\"node_1\"\n, node_1)\nbuilder.add_node(\n\"node_2\"\n, node_2)\nbuilder.add_edge(\nSTART\n,\n\"node_1\"\n)\nbuilder.add_edge(\n\"node_1\"\n,\n\"node_2\"\n)\ngraph\n=\nbuilder.compile()\nfor\nchunk\nin\ngraph.stream({\n\"foo\"\n:\n\"foo\"\n},\nsubgraphs\n=\nTrue\n):\nprint\n(chunk)\nCopy\n((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:577b710b-64ae-31fb-9455-6a4d4cc2b0b9',), {'subgraph_node_1': {'baz': 'baz'}})\n(('node_2:577b710b-64ae-31fb-9455-6a4d4cc2b0b9',), {'subgraph_node_2': {'bar': 'hi! foobaz'}})\n((), {'node_2': {'foo': 'hi! foobaz'}})\nFull example: different state schemas (two levels of subgraphs)\nThis is an example with two levels of subgraphs: parent -> child -> grandchild.\nCopy\n# Grandchild graph\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph.state\nimport\nStateGraph,\nSTART\n,\nEND\nclass\nGrandChildState\n(\nTypedDict\n):\nmy_grandchild_key:\nstr\ndef\ngrandchild_1\n(\nstate\n: GrandChildState) -> GrandChildState:\n#\nNOTE\n: child or parent keys will not be accessible here\nreturn\n{\n\"my_grandchild_key\"\n: state[\n\"my_grandchild_key\"\n]\n+\n\", how are you\"\n}\ngrandchild\n=\nStateGraph(GrandChildState)\ngrandchild.add_node(\n\"grandchild_1\"\n, grandchild_1)\ngrandchild.add_edge(\nSTART\n,\n\"grandchild_1\"\n)\ngrandchild.add_edge(\n\"grandchild_1\"\n,\nEND\n)\ngrandchild_graph\n=\ngrandchild.compile()\n# Child graph\nclass\nChildState\n(\nTypedDict\n):\nmy_child_key:\nstr\ndef\ncall_grandchild_graph\n(\nstate\n: ChildState) -> ChildState:\n#\nNOTE\n: parent or grandchild keys won't be accessible here\ngrandchild_graph_input\n=\n{\n\"my_grandchild_key\"\n: state[\n\"my_child_key\"\n]}\ngrandchild_graph_output\n=\ngrandchild_graph.invoke(grandchild_graph_input)\nreturn\n{\n\"my_child_key\"\n: grandchild_graph_output[\n\"my_grandchild_key\"\n]\n+\n\" today?\"\n}\nchild\n=\nStateGraph(ChildState)\n# We're passing a function here instead of just compiled graph (`grandchild_graph`)\nchild.add_node(\n\"child_1\"\n, call_grandchild_graph)\nchild.add_edge(\nSTART\n,\n\"child_1\"\n)\nchild.add_edge(\n\"child_1\"\n,\nEND\n)\nchild_graph\n=\nchild.compile()\n# Parent graph\nclass\nParentState\n(\nTypedDict\n):\nmy_key:\nstr\ndef\nparent_1\n(\nstate\n: ParentState) -> ParentState:\n#\nNOTE\n: child or grandchild keys won't be accessible here\nreturn\n{\n\"my_key\"\n:\n\"hi \"\n+\nstate[\n\"my_key\"\n]}\ndef\nparent_2\n(\nstate\n: ParentState) -> ParentState:\nreturn\n{\n\"my_key\"\n: state[\n\"my_key\"\n]\n+\n\" bye!\"\n}\ndef\ncall_child_graph\n(\nstate\n: ParentState) -> ParentState:\nchild_graph_input\n=\n{\n\"my_child_key\"\n: state[\n\"my_key\"\n]}\nchild_graph_output\n=\nchild_graph.invoke(child_graph_input)\nreturn\n{\n\"my_key\"\n: child_graph_output[\n\"my_child_key\"\n]}\nparent\n=\nStateGraph(ParentState)\nparent.add_node(\n\"parent_1\"\n, parent_1)\n# We're passing a function here instead of just a compiled graph (`child_graph`)\nparent.add_node(\n\"child\"\n, call_child_graph)\nparent.add_node(\n\"parent_2\"\n, parent_2)\nparent.add_edge(\nSTART\n,\n\"parent_1\"\n)\nparent.add_edge(\n\"parent_1\"\n,\n\"child\"\n)\nparent.add_edge(\n\"child\"\n,\n\"parent_2\"\n)\nparent.add_edge(\n\"parent_2\"\n,\nEND\n)\nparent_graph\n=\nparent.compile()\nfor\nchunk\nin\nparent_graph.stream({\n\"my_key\"\n:\n\"Bob\"\n},\nsubgraphs\n=\nTrue\n):\nprint\n(chunk)\nCopy\n((), {'parent_1': {'my_key': 'hi Bob'}})\n(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child_1:781bb3b1-3971-84ce-810b-acf819a03f9c'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})\n((), {'child': {'my_key': 'hi Bob, how are you today?'}})\n((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})\n\u200b\nAdd a graph as a node\nWhen the parent graph and subgraph can communicate over a shared state key (channel) in the\nschema\n, you can add a graph as a\nnode\nin another graph. For example, in\nmulti-agent\nsystems, the agents often communicate over a shared\nmessages\nkey.\nIf your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:\nDefine the subgraph workflow (\nsubgraph_builder\nin the example below) and compile it\nPass compiled subgraph to the\nadd_node\nmethod when defining the parent graph workflow\nCopy\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph.state\nimport\nStateGraph,\nSTART\nclass\nState\n(\nTypedDict\n):\nfoo:\nstr\n# Subgraph\ndef\nsubgraph_node_1\n(\nstate\n: State):\nreturn\n{\n\"foo\"\n:\n\"hi! \"\n+\nstate[\n\"foo\"\n]}\nsubgraph_builder\n=\nStateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(\nSTART\n,\n\"subgraph_node_1\"\n)\nsubgraph\n=\nsubgraph_builder.compile()\n# Parent graph\nbuilder\n=\nStateGraph(State)\nbuilder.add_node(\n\"node_1\"\n, subgraph)\nbuilder.add_edge(\nSTART\n,\n\"node_1\"\n)\ngraph\n=\nbuilder.compile()\nFull example: shared state schemas\nCopy\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph.state\nimport\nStateGraph,\nSTART\n# Define subgraph\nclass\nSubgraphState\n(\nTypedDict\n):\nfoo:\nstr\n# shared with parent graph state\nbar:\nstr\n# private to SubgraphState\ndef\nsubgraph_node_1\n(\nstate\n: SubgraphState):\nreturn\n{\n\"bar\"\n:\n\"bar\"\n}\ndef\nsubgraph_node_2\n(\nstate\n: SubgraphState):\n# note that this node is using a state key ('bar') that is only available in the subgraph\n# and is sending update on the shared state key ('foo')\nreturn\n{\n\"foo\"\n: state[\n\"foo\"\n]\n+\nstate[\n\"bar\"\n]}\nsubgraph_builder\n=\nStateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(\nSTART\n,\n\"subgraph_node_1\"\n)\nsubgraph_builder.add_edge(\n\"subgraph_node_1\"\n,\n\"subgraph_node_2\"\n)\nsubgraph\n=\nsubgraph_builder.compile()\n# Define parent graph\nclass\nParentState\n(\nTypedDict\n):\nfoo:\nstr\ndef\nnode_1\n(\nstate\n: ParentState):\nreturn\n{\n\"foo\"\n:\n\"hi! \"\n+\nstate[\n\"foo\"\n]}\nbuilder\n=\nStateGraph(ParentState)\nbuilder.add_node(\n\"node_1\"\n, node_1)\nbuilder.add_node(\n\"node_2\"\n, subgraph)\nbuilder.add_edge(\nSTART\n,\n\"node_1\"\n)\nbuilder.add_edge(\n\"node_1\"\n,\n\"node_2\"\n)\ngraph\n=\nbuilder.compile()\nfor\nchunk\nin\ngraph.stream({\n\"foo\"\n:\n\"foo\"\n}):\nprint\n(chunk)\nCopy\n{'node_1': {'foo': 'hi! foo'}}\n{'node_2': {'foo': 'hi! foobar'}}\n\u200b\nAdd persistence\nYou only need to\nprovide the checkpointer when compiling the parent graph\n. LangGraph will automatically propagate the checkpointer to the child subgraphs.\nCopy\nfrom\nlanggraph.graph\nimport\nSTART\n, StateGraph\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\nfrom\ntyping_extensions\nimport\nTypedDict\nclass\nState\n(\nTypedDict\n):\nfoo:\nstr\n# Subgraph\ndef\nsubgraph_node_1\n(\nstate\n: State):\nreturn\n{\n\"foo\"\n: state[\n\"foo\"\n]\n+\n\"bar\"\n}\nsubgraph_builder\n=\nStateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(\nSTART\n,\n\"subgraph_node_1\"\n)\nsubgraph\n=\nsubgraph_builder.compile()\n# Parent graph\nbuilder\n=\nStateGraph(State)\nbuilder.add_node(\n\"node_1\"\n, subgraph)\nbuilder.add_edge(\nSTART\n,\n\"node_1\"\n)\ncheckpointer\n=\nMemorySaver()\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nIf you want the subgraph to\nhave its own memory\n, you can compile it with the appropriate checkpointer option. This is useful in\nmulti-agent\nsystems, if you want agents to keep track of their internal message histories:\nCopy\nsubgraph_builder\n=\nStateGraph(\n...\n)\nsubgraph\n=\nsubgraph_builder.compile(\ncheckpointer\n=\nTrue\n)\n\u200b\nView subgraph state\nWhen you enable\npersistence\n, you can\ninspect the graph state\n(checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.\nYou can inspect the graph state via\ngraph.get_state(config)\n. To view the subgraph state, you can use\ngraph.get_state(config, subgraphs=True)\n.\nAvailable\nonly\nwhen interrupted\nSubgraph state can only be viewed\nwhen the subgraph is interrupted\n. Once you resume the graph, you won\u2019t be able to access the subgraph state.\nView interrupted subgraph state\nCopy\nfrom\nlanggraph.graph\nimport\nSTART\n, StateGraph\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\nfrom\nlanggraph.types\nimport\ninterrupt, Command\nfrom\ntyping_extensions\nimport\nTypedDict\nclass\nState\n(\nTypedDict\n):\nfoo:\nstr\n# Subgraph\ndef\nsubgraph_node_1\n(\nstate\n: State):\nvalue\n=\ninterrupt(\n\"Provide value:\"\n)\nreturn\n{\n\"foo\"\n: state[\n\"foo\"\n]\n+\nvalue}\nsubgraph_builder\n=\nStateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(\nSTART\n,\n\"subgraph_node_1\"\n)\nsubgraph\n=\nsubgraph_builder.compile()\n# Parent graph\nbuilder\n=\nStateGraph(State)\nbuilder.add_node(\n\"node_1\"\n, subgraph)\nbuilder.add_edge(\nSTART\n,\n\"node_1\"\n)\ncheckpointer\n=\nMemorySaver()\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\ngraph.invoke({\n\"foo\"\n:\n\"\"\n}, config)\nparent_state\n=\ngraph.get_state(config)\n# This will be available only when the subgraph is interrupted.\n# Once you resume the graph, you won't be able to access the subgraph state.\nsubgraph_state\n=\ngraph.get_state(config,\nsubgraphs\n=\nTrue\n).tasks[\n0\n].state\n# resume the subgraph\ngraph.invoke(Command(\nresume\n=\n\"bar\"\n), config)\nThis will be available only when the subgraph is interrupted. Once you resume the graph, you won\u2019t be able to access the subgraph state.\n\u200b\nStream subgraph outputs\nTo include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\nCopy\nfor\nchunk\nin\ngraph.stream(\n{\n\"foo\"\n:\n\"foo\"\n},\nsubgraphs\n=\nTrue\n,\nstream_mode\n=\n\"updates\"\n,\n):\nprint\n(chunk)\nStream from subgraphs\nCopy\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph.state\nimport\nStateGraph,\nSTART\n# Define subgraph\nclass\nSubgraphState\n(\nTypedDict\n):\nfoo:\nstr\nbar:\nstr\ndef\nsubgraph_node_1\n(\nstate\n: SubgraphState):\nreturn\n{\n\"bar\"\n:\n\"bar\"\n}\ndef\nsubgraph_node_2\n(\nstate\n: SubgraphState):\n# note that this node is using a state key ('bar') that is only available in the subgraph\n# and is sending update on the shared state key ('foo')\nreturn\n{\n\"foo\"\n: state[\n\"foo\"\n]\n+\nstate[\n\"bar\"\n]}\nsubgraph_builder\n=\nStateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(\nSTART\n,\n\"subgraph_node_1\"\n)\nsubgraph_builder.add_edge(\n\"subgraph_node_1\"\n,\n\"subgraph_node_2\"\n)\nsubgraph\n=\nsubgraph_builder.compile()\n# Define parent graph\nclass\nParentState\n(\nTypedDict\n):\nfoo:\nstr\ndef\nnode_1\n(\nstate\n: ParentState):\nreturn\n{\n\"foo\"\n:\n\"hi! \"\n+\nstate[\n\"foo\"\n]}\nbuilder\n=\nStateGraph(ParentState)\nbuilder.add_node(\n\"node_1\"\n, node_1)\nbuilder.add_node(\n\"node_2\"\n, subgraph)\nbuilder.add_edge(\nSTART\n,\n\"node_1\"\n)\nbuilder.add_edge(\n\"node_1\"\n,\n\"node_2\"\n)\ngraph\n=\nbuilder.compile()\nfor\nchunk\nin\ngraph.stream(\n{\n\"foo\"\n:\n\"foo\"\n},\nstream_mode\n=\n\"updates\"\n,\nsubgraphs\n=\nTrue\n,\n):\nprint\n(chunk)\nCopy\n((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})\n(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n((), {'node_2': {'foo': 'hi! foobar'}})\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nMemory\nPrevious\nApplication structure\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/use-subgraphs",
      "title": "Subgraphs - Docs by LangChain",
      "heading": "Subgraphs"
    }
  },
  {
    "page_content": "Application structure - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nProduction\nApplication structure\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nKey Concepts\nFile structure\nConfiguration file\nExamples\nDependencies\nGraphs\nEnvironment variables\nProduction\nApplication structure\nCopy page\nCopy page\nA LangGraph application consists of one or more graphs, a configuration file (\nlanggraph.json\n), a file that specifies dependencies, and an optional\n.env\nfile that specifies environment variables.\nThis guide shows a typical structure of an application and shows you how to provide the required configuration to deploy an application with\nLangSmith Deployment\n.\nLangSmith Deployment is a managed hosting platform for deploying and scaling LangGraph agents. It handles the infrastructure, scaling, and operational concerns so you can deploy your stateful, long-running agents directly from your repository. Learn more in the\nDeployment documentation\n.\n\u200b\nKey Concepts\nTo deploy using the LangSmith, the following information should be provided:\nA\nLangGraph configuration file\n(\nlanggraph.json\n) that specifies the dependencies, graphs, and environment variables to use for the application.\nThe\ngraphs\nthat implement the logic of the application.\nA file that specifies\ndependencies\nrequired to run the application.\nEnvironment variables\nthat are required for the application to run.\n\u200b\nFile structure\nBelow are examples of directory structures for applications:\nPython (requirements.txt)\nPython (pyproject.toml)\nCopy\nmy-app/\n\u251c\u2500\u2500 my_agent # all project code lies within here\n\u2502   \u251c\u2500\u2500 utils # utilities for your graph\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 tools.py # tools for your graph\n\u2502   \u2502   \u251c\u2500\u2500 nodes.py # node functions for your graph\n\u2502   \u2502   \u2514\u2500\u2500 state.py # state definition of your graph\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 agent.py # code for constructing your graph\n\u251c\u2500\u2500 .env # environment variables\n\u251c\u2500\u2500 requirements.txt # package dependencies\n\u2514\u2500\u2500 langgraph.json # configuration file for LangGraph\nCopy\nmy-app/\n\u251c\u2500\u2500 my_agent # all project code lies within here\n\u2502   \u251c\u2500\u2500 utils # utilities for your graph\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 tools.py # tools for your graph\n\u2502   \u2502   \u251c\u2500\u2500 nodes.py # node functions for your graph\n\u2502   \u2502   \u2514\u2500\u2500 state.py # state definition of your graph\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 agent.py # code for constructing your graph\n\u251c\u2500\u2500 .env # environment variables\n\u251c\u2500\u2500 langgraph.json  # configuration file for LangGraph\n\u2514\u2500\u2500 pyproject.toml # dependencies for your project\nThe directory structure of a LangGraph application can vary depending on the programming language and the package manager used.\n\u200b\nConfiguration file\nThe\nlanggraph.json\nfile is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.\nSee the\nLangGraph configuration file reference\nfor details on all supported keys in the JSON file.\nThe\nLangGraph CLI\ndefaults to using the configuration file\nlanggraph.json\nin the current directory.\n\u200b\nExamples\nThe dependencies involve a custom local package and the\nlangchain_openai\npackage.\nA single graph will be loaded from the file\n./your_package/your_file.py\nwith the variable\nvariable\n.\nThe environment variables are loaded from the\n.env\nfile.\nCopy\n{\n\"dependencies\"\n: [\n\"langchain_openai\"\n,\n\"./your_package\"\n],\n\"graphs\"\n: {\n\"my_agent\"\n:\n\"./your_package/your_file.py:agent\"\n},\n\"env\"\n:\n\"./.env\"\n}\n\u200b\nDependencies\nA LangGraph application may depend on other Python packages.\nYou will generally need to specify the following information for dependencies to be set up correctly:\nA file in the directory that specifies the dependencies (e.g.\nrequirements.txt\n,\npyproject.toml\n, or\npackage.json\n).\nA\ndependencies\nkey in the\nLangGraph configuration file\nthat specifies the dependencies required to run the LangGraph application.\nAny additional binaries or system libraries can be specified using\ndockerfile_lines\nkey in the\nLangGraph configuration file\n.\n\u200b\nGraphs\nUse the\ngraphs\nkey in the\nLangGraph configuration file\nto specify which graphs will be available in the deployed LangGraph application.\nYou can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.\n\u200b\nEnvironment variables\nIf you\u2019re working with a deployed LangGraph application locally, you can configure environment variables in the\nenv\nkey of the\nLangGraph configuration file\n.\nFor a production deployment, you will typically want to configure the environment variables in the deployment environment.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nSubgraphs\nPrevious\nTest\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/application-structure",
      "title": "Application structure - Docs by LangChain",
      "heading": "Application structure"
    }
  },
  {
    "page_content": "Test - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nProduction\nTest\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nPrerequisites\nGetting started\nTesting individual nodes and edges\nPartial execution\nProduction\nTest\nCopy page\nCopy page\nAfter you\u2019ve prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests.\nNote that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out\nthis section\nthat uses LangChain\u2019s built-in\ncreate_agent\ninstead.\n\u200b\nPrerequisites\nFirst, make sure you have\npytest\ninstalled:\nCopy\n$\npip\ninstall\n-U\npytest\n\u200b\nGetting started\nBecause many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.\nThe below example shows how this works with a simple, linear graph that progresses through\nnode1\nand\nnode2\n. Each node updates the single state key\nmy_key\n:\nCopy\nimport\npytest\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\ndef\ncreate_graph\n() -> StateGraph:\nclass\nMyState\n(\nTypedDict\n):\nmy_key:\nstr\ngraph\n=\nStateGraph(MyState)\ngraph.add_node(\n\"node1\"\n,\nlambda\nstate\n: {\n\"my_key\"\n:\n\"hello from node1\"\n})\ngraph.add_node(\n\"node2\"\n,\nlambda\nstate\n: {\n\"my_key\"\n:\n\"hello from node2\"\n})\ngraph.add_edge(\nSTART\n,\n\"node1\"\n)\ngraph.add_edge(\n\"node1\"\n,\n\"node2\"\n)\ngraph.add_edge(\n\"node2\"\n,\nEND\n)\nreturn\ngraph\ndef\ntest_basic_agent_execution\n() ->\nNone\n:\ncheckpointer\n=\nMemorySaver()\ngraph\n=\ncreate_graph()\ncompiled_graph\n=\ngraph.compile(\ncheckpointer\n=\ncheckpointer)\nresult\n=\ncompiled_graph.invoke(\n{\n\"my_key\"\n:\n\"initial_value\"\n},\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\n)\nassert\nresult[\n\"my_key\"\n]\n==\n\"hello from node2\"\n\u200b\nTesting individual nodes and edges\nCompiled LangGraph agents expose references to each individual node as\ngraph.nodes\n. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:\nCopy\nimport\npytest\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\ndef\ncreate_graph\n() -> StateGraph:\nclass\nMyState\n(\nTypedDict\n):\nmy_key:\nstr\ngraph\n=\nStateGraph(MyState)\ngraph.add_node(\n\"node1\"\n,\nlambda\nstate\n: {\n\"my_key\"\n:\n\"hello from node1\"\n})\ngraph.add_node(\n\"node2\"\n,\nlambda\nstate\n: {\n\"my_key\"\n:\n\"hello from node2\"\n})\ngraph.add_edge(\nSTART\n,\n\"node1\"\n)\ngraph.add_edge(\n\"node1\"\n,\n\"node2\"\n)\ngraph.add_edge(\n\"node2\"\n,\nEND\n)\nreturn\ngraph\ndef\ntest_individual_node_execution\n() ->\nNone\n:\n# Will be ignored in this example\ncheckpointer\n=\nMemorySaver()\ngraph\n=\ncreate_graph()\ncompiled_graph\n=\ngraph.compile(\ncheckpointer\n=\ncheckpointer)\n# Only invoke node 1\nresult\n=\ncompiled_graph.nodes[\n\"node1\"\n].invoke(\n{\n\"my_key\"\n:\n\"initial_value\"\n},\n)\nassert\nresult[\n\"my_key\"\n]\n==\n\"hello from node1\"\n\u200b\nPartial execution\nFor agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to\nrestructure these sections as subgraphs\n, which you can invoke in isolation as normal.\nHowever, if you do not wish to make changes to your agent graph\u2019s overall structure, you can use LangGraph\u2019s persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:\nCompile your agent with a checkpointer (the in-memory checkpointer\nInMemorySaver\nwill suffice for testing).\nCall your agent\u2019s\nupdate_state\nmethod with an\nas_node\nparameter set to the name of the node\nbefore\nthe one you want to start your test.\nInvoke your agent with the same\nthread_id\nyou used to update the state and an\ninterrupt_after\nparameter set to the name of the node you want to stop at.\nHere\u2019s an example that executes only the second and third nodes in a linear graph:\nCopy\nimport\npytest\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\ndef\ncreate_graph\n() -> StateGraph:\nclass\nMyState\n(\nTypedDict\n):\nmy_key:\nstr\ngraph\n=\nStateGraph(MyState)\ngraph.add_node(\n\"node1\"\n,\nlambda\nstate\n: {\n\"my_key\"\n:\n\"hello from node1\"\n})\ngraph.add_node(\n\"node2\"\n,\nlambda\nstate\n: {\n\"my_key\"\n:\n\"hello from node2\"\n})\ngraph.add_node(\n\"node3\"\n,\nlambda\nstate\n: {\n\"my_key\"\n:\n\"hello from node3\"\n})\ngraph.add_node(\n\"node4\"\n,\nlambda\nstate\n: {\n\"my_key\"\n:\n\"hello from node4\"\n})\ngraph.add_edge(\nSTART\n,\n\"node1\"\n)\ngraph.add_edge(\n\"node1\"\n,\n\"node2\"\n)\ngraph.add_edge(\n\"node2\"\n,\n\"node3\"\n)\ngraph.add_edge(\n\"node3\"\n,\n\"node4\"\n)\ngraph.add_edge(\n\"node4\"\n,\nEND\n)\nreturn\ngraph\ndef\ntest_partial_execution_from_node2_to_node3\n() ->\nNone\n:\ncheckpointer\n=\nMemorySaver()\ngraph\n=\ncreate_graph()\ncompiled_graph\n=\ngraph.compile(\ncheckpointer\n=\ncheckpointer)\ncompiled_graph.update_state(\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}\n},\n# The state passed into node 2 - simulating the state at\n# the end of node 1\nvalues\n=\n{\n\"my_key\"\n:\n\"initial_value\"\n},\n# Update saved state as if it came from node 1\n# Execution will resume at node 2\nas_node\n=\n\"node1\"\n,\n)\nresult\n=\ncompiled_graph.invoke(\n# Resume execution by passing None\nNone\n,\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}},\n# Stop after node 3 so that node 4 doesn't run\ninterrupt_after\n=\n\"node3\"\n,\n)\nassert\nresult[\n\"my_key\"\n]\n==\n\"hello from node3\"\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nApplication structure\nPrevious\nLangSmith Studio\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/test",
      "title": "Test - Docs by LangChain",
      "heading": "Test"
    }
  },
  {
    "page_content": "Agent Chat UI - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nProduction\nAgent Chat UI\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nQuick start\nLocal development\nConnect to your agent\nProduction\nAgent Chat UI\nCopy page\nCopy page\nAgent Chat UI\nis a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI works seamlessly with agents created using\ncreate_agent\nand provides interactive experiences for your agents with minimal setup, whether you\u2019re running locally or in a deployed context (such as\nLangSmith\n).\nAgent Chat UI is open source and can be adapted to your application needs.\nYou can use generative UI in the Agent Chat UI. For more information, see\nImplement generative user interfaces with LangGraph\n.\n\u200b\nQuick start\nThe fastest way to get started is using the hosted version:\nVisit\nAgent Chat UI\nConnect your agent\nby entering your deployment URL or local server address\nStart chatting\n- the UI will automatically detect and render tool calls and interrupts\n\u200b\nLocal development\nFor customization or local development, you can run Agent Chat UI locally:\nUse npx\nClone repository\nCopy\n# Create a new Agent Chat UI project\nnpx\ncreate-agent-chat-app\n--project-name\nmy-chat-ui\ncd\nmy-chat-ui\n# Install dependencies and start\npnpm\ninstall\npnpm\ndev\n\u200b\nConnect to your agent\nAgent Chat UI can connect to both\nlocal\nand\ndeployed agents\n.\nAfter starting Agent Chat UI, you\u2019ll need to configure it to connect to your agent:\nGraph ID\n: Enter your graph name (find this under\ngraphs\nin your\nlanggraph.json\nfile)\nDeployment URL\n: Your Agent server\u2019s endpoint (e.g.,\nhttp://localhost:2024\nfor local development, or your deployed agent\u2019s URL)\nLangSmith API key (optional)\n: Add your LangSmith API key (not required if you\u2019re using a local Agent server)\nOnce configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\nAgent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see\nHiding Messages in the Chat\n.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nLangSmith Studio\nPrevious\nLangSmith Deployment\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/ui",
      "title": "Agent Chat UI - Docs by LangChain",
      "heading": "Agent Chat UI"
    }
  },
  {
    "page_content": "Agents - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore components\nAgents\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCore components\nModel\nStatic model\nDynamic model\nTools\nDefining tools\nTool error handling\nTool use in the ReAct loop\nSystem prompt\nDynamic system prompt\nInvocation\nAdvanced concepts\nStructured output\nToolStrategy\nProviderStrategy\nMemory\nDefining state via middleware\nDefining state via state_schema\nStreaming\nMiddleware\nCore components\nAgents\nCopy page\nCopy page\nAgents combine language models with\ntools\nto create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\ncreate_agent\nprovides a production-ready agent implementation.\nAn LLM Agent runs tools in a loop to achieve a goal\n.\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\ncreate_agent\nbuilds a\ngraph\n-based agent runtime using\nLangGraph\n. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\nLearn more about the\nGraph API\n.\n\u200b\nCore components\n\u200b\nModel\nThe\nmodel\nis the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\u200b\nStatic model\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\nTo initialize a static model from a\nmodel identifier string\n:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nagent\n=\ncreate_agent(\n\"gpt-5\"\n,\ntools\n=\ntools)\nModel identifier strings support automatic inference (e.g.,\n\"gpt-5\"\nwill be inferred as\n\"openai:gpt-5\"\n). Refer to the\nreference\nto see a full list of model identifier string mappings.\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use\nChatOpenAI\n. See\nChat models\nfor other available chat model classes.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5\"\n,\ntemperature\n=\n0.1\n,\nmax_tokens\n=\n1000\n,\ntimeout\n=\n30\n# ... (other params)\n)\nagent\n=\ncreate_agent(model,\ntools\n=\ntools)\nModel instances give you complete control over configuration. Use them when you need to set specific\nparameters\nlike\ntemperature\n,\nmax_tokens\n,\ntimeouts\n,\nbase_url\n, and other provider-specific settings. Refer to the\nreference\nto see available params and methods on your model.\n\u200b\nDynamic model\nDynamic models are selected at\nruntime\nbased on the current\nstate\nand context. This enables sophisticated routing logic and cost optimization.\nTo use a dynamic model, create middleware using the\n@wrap_model_call\ndecorator that modifies the model in the request:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nbasic_model\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\nadvanced_model\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\n@wrap_model_call\ndef\ndynamic_model_selection\n(\nrequest\n: ModelRequest,\nhandler\n) -> ModelResponse:\n\"\"\"Choose model based on conversation complexity.\"\"\"\nmessage_count\n=\nlen\n(request.state[\n\"messages\"\n])\nif\nmessage_count\n>\n10\n:\n# Use an advanced model for longer conversations\nmodel\n=\nadvanced_model\nelse\n:\nmodel\n=\nbasic_model\nreturn\nhandler(request.override(\nmodel\n=\nmodel))\nagent\n=\ncreate_agent(\nmodel\n=\nbasic_model,\n# Default model\ntools\n=\ntools,\nmiddleware\n=\n[dynamic_model_selection]\n)\nPre-bound models (models with\nbind_tools\nalready called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\nFor model configuration details, see\nModels\n. For dynamic model selection patterns, see\nDynamic model in middleware\n.\n\u200b\nTools\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\nMultiple tool calls in sequence (triggered by a single prompt)\nParallel tool calls when appropriate\nDynamic tool selection based on previous results\nTool retry logic and error handling\nState persistence across tool calls\nFor more information, see\nTools\n.\n\u200b\nDefining tools\nPass a list of tools to the agent.\nTools can be specified as plain Python functions or\ncoroutines\n.\nThe\ntool decorator\ncan be used to customize tool names, descriptions, argument schemas, and other properties.\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.agents\nimport\ncreate_agent\n@tool\ndef\nsearch\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search for information.\"\"\"\nreturn\nf\n\"Results for:\n{\nquery\n}\n\"\n@tool\ndef\nget_weather\n(\nlocation\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather information for a location.\"\"\"\nreturn\nf\n\"Weather in\n{\nlocation\n}\n: Sunny, 72\u00b0F\"\nagent\n=\ncreate_agent(model,\ntools\n=\n[search, get_weather])\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\u200b\nTool error handling\nTo customize how tool errors are handled, use the\n@wrap_tool_call\ndecorator to create middleware:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_tool_call\nfrom\nlangchain.messages\nimport\nToolMessage\n@wrap_tool_call\ndef\nhandle_tool_errors\n(\nrequest\n,\nhandler\n):\n\"\"\"Handle tool execution errors with custom messages.\"\"\"\ntry\n:\nreturn\nhandler(request)\nexcept\nException\nas\ne:\n# Return a custom error message to the model\nreturn\nToolMessage(\ncontent\n=\nf\n\"Tool error: Please check your input and try again. (\n{\nstr\n(e)\n}\n)\"\n,\ntool_call_id\n=\nrequest.tool_call[\n\"id\"\n]\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search, get_weather],\nmiddleware\n=\n[handle_tool_errors]\n)\nThe agent will return a\nToolMessage\nwith the custom error message when a tool fails:\nCopy\n[\n...\nToolMessage(\ncontent\n=\n\"Tool error: Please check your input and try again. (division by zero)\"\n,\ntool_call_id\n=\n\"...\"\n),\n...\n]\n\u200b\nTool use in the ReAct loop\nAgents follow the ReAct (\u201cReasoning + Acting\u201d) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\nExample of ReAct loop\nPrompt:\nIdentify the current most popular wireless headphones and verify availability.\nCopy\n================================ Human Message =================================\nFind the most popular wireless headphones right now and check if they're in stock\nReasoning\n: \u201cPopularity is time-sensitive, I need to use the provided search tool.\u201d\nActing\n: Call\nsearch_products(\"wireless headphones\")\nCopy\n================================== Ai Message ==================================\nTool Calls:\nsearch_products (call_abc123)\nCall ID: call_abc123\nArgs:\nquery: wireless headphones\nCopy\n================================= Tool Message =================================\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\nReasoning\n: \u201cI need to confirm availability for the top-ranked item before answering.\u201d\nActing\n: Call\ncheck_inventory(\"WH-1000XM5\")\nCopy\n================================== Ai Message ==================================\nTool Calls:\ncheck_inventory (call_def456)\nCall ID: call_def456\nArgs:\nproduct_id: WH-1000XM5\nCopy\n================================= Tool Message =================================\nProduct WH-1000XM5: 10 units in stock\nReasoning\n: \u201cI have the most popular model and its stock status. I can now answer the user\u2019s question.\u201d\nActing\n: Produce final answer\nCopy\n================================== Ai Message ==================================\nI found wireless headphones (model WH-1000XM5) with 10 units in stock...\nTo learn more about tools, see\nTools\n.\n\u200b\nSystem prompt\nYou can shape how your agent approaches tasks by providing a prompt. The\nsystem_prompt\nparameter can be provided as a string:\nCopy\nagent\n=\ncreate_agent(\nmodel,\ntools,\nsystem_prompt\n=\n\"You are a helpful assistant. Be concise and accurate.\"\n)\nWhen no\nsystem_prompt\nis provided, the agent will infer its task from the messages directly.\nThe\nsystem_prompt\nparameter accepts either a\nstr\nor a\nSystemMessage\n. Using a\nSystemMessage\ngives you more control over the prompt structure, which is useful for provider-specific features like\nAnthropic\u2019s prompt caching\n:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.messages\nimport\nSystemMessage, HumanMessage\nliterary_agent\n=\ncreate_agent(\nmodel\n=\n\"anthropic:claude-sonnet-4-5\"\n,\nsystem_prompt\n=\nSystemMessage(\ncontent\n=\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"You are an AI assistant tasked with analyzing literary works.\"\n,\n},\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"<the entire contents of 'Pride and Prejudice'>\"\n,\n\"cache_control\"\n: {\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\n)\n)\nresult\n=\nliterary_agent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"Analyze the major themes in 'Pride and Prejudice'.\"\n)]}\n)\nThe\ncache_control\nfield with\n{\"type\": \"ephemeral\"}\ntells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n\u200b\nDynamic system prompt\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use\nmiddleware\n.\nThe\n@dynamic_prompt\ndecorator creates middleware that generates system prompts based on the model request:\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\nclass\nContext\n(\nTypedDict\n):\nuser_role:\nstr\n@dynamic_prompt\ndef\nuser_role_prompt\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Generate system prompt based on user role.\"\"\"\nuser_role\n=\nrequest.runtime.context.get(\n\"user_role\"\n,\n\"user\"\n)\nbase_prompt\n=\n\"You are a helpful assistant.\"\nif\nuser_role\n==\n\"expert\"\n:\nreturn\nf\n\"\n{\nbase_prompt\n}\nProvide detailed technical responses.\"\nelif\nuser_role\n==\n\"beginner\"\n:\nreturn\nf\n\"\n{\nbase_prompt\n}\nExplain concepts simply and avoid jargon.\"\nreturn\nbase_prompt\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[web_search],\nmiddleware\n=\n[user_role_prompt],\ncontext_schema\n=\nContext\n)\n# The system prompt will be set dynamically based on context\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Explain machine learning\"\n}]},\ncontext\n=\n{\n\"user_role\"\n:\n\"expert\"\n}\n)\nFor more details on message types and formatting, see\nMessages\n. For comprehensive middleware documentation, see\nMiddleware\n.\n\u200b\nInvocation\nYou can invoke an agent by passing an update to its\nState\n. All agents include a\nsequence of messages\nin their state; to invoke the agent, pass a new message:\nCopy\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the weather in San Francisco?\"\n}]}\n)\nFor streaming steps and / or tokens from the agent, refer to the\nstreaming\nguide.\nOtherwise, the agent follows the LangGraph\nGraph API\nand supports all associated methods, such as\nstream\nand\ninvoke\n.\n\u200b\nAdvanced concepts\n\u200b\nStructured output\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the\nresponse_format\nparameter.\n\u200b\nToolStrategy\nToolStrategy\nuses artificial tool calling to generate structured output. This works with any model that supports tool calling:\nCopy\nfrom\npydantic\nimport\nBaseModel\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.structured_output\nimport\nToolStrategy\nclass\nContactInfo\n(\nBaseModel\n):\nname:\nstr\nemail:\nstr\nphone:\nstr\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool],\nresponse_format\n=\nToolStrategy(ContactInfo)\n)\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Extract contact info from: John Doe,\n[email\u00a0protected]\n, (555) 123-4567\"\n}]\n})\nresult[\n\"structured_response\"\n]\n# ContactInfo(name='John Doe', email='\n[email\u00a0protected]\n', phone='(555) 123-4567')\n\u200b\nProviderStrategy\nProviderStrategy\nuses the model provider\u2019s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):\nCopy\nfrom\nlangchain.agents.structured_output\nimport\nProviderStrategy\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\nresponse_format\n=\nProviderStrategy(ContactInfo)\n)\nAs of\nlangchain 1.0\n, simply passing a schema (e.g.,\nresponse_format=ContactInfo\n) is no longer supported. You must explicitly use\nToolStrategy\nor\nProviderStrategy\n.\nTo learn about structured output, see\nStructured output\n.\n\u200b\nMemory\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\nInformation stored in the state can be thought of as the\nshort-term memory\nof the agent:\nCustom state schemas must extend\nAgentState\nas a\nTypedDict\n.\nThere are two ways to define custom state:\nVia\nmiddleware\n(preferred)\nVia\nstate_schema\non\ncreate_agent\n\u200b\nDefining state via middleware\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\nCopy\nfrom\nlangchain.agents\nimport\nAgentState\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware\nfrom\ntyping\nimport\nAny\nclass\nCustomState\n(\nAgentState\n):\nuser_preferences:\ndict\nclass\nCustomMiddleware\n(\nAgentMiddleware\n):\nstate_schema\n=\nCustomState\ntools\n=\n[tool1, tool2]\ndef\nbefore_model\n(\nself\n,\nstate\n: CustomState,\nruntime\n) -> dict[\nstr\n, Any]\n|\nNone\n:\n...\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\ntools,\nmiddleware\n=\n[CustomMiddleware()]\n)\n# The agent can now track additional state beyond messages\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"I prefer technical explanations\"\n}],\n\"user_preferences\"\n: {\n\"style\"\n:\n\"technical\"\n,\n\"verbosity\"\n:\n\"detailed\"\n},\n})\n\u200b\nDefining state via\nstate_schema\nUse the\nstate_schema\nparameter as a shortcut to define custom state that is only used in tools.\nCopy\nfrom\nlangchain.agents\nimport\nAgentState\nclass\nCustomState\n(\nAgentState\n):\nuser_preferences:\ndict\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[tool1, tool2],\nstate_schema\n=\nCustomState\n)\n# The agent can now track additional state beyond messages\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"I prefer technical explanations\"\n}],\n\"user_preferences\"\n: {\n\"style\"\n:\n\"technical\"\n,\n\"verbosity\"\n:\n\"detailed\"\n},\n})\nAs of\nlangchain 1.0\n, custom state schemas\nmust\nbe\nTypedDict\ntypes. Pydantic models and dataclasses are no longer supported. See the\nv1 migration guide\nfor more details.\nDefining custom state via middleware is preferred over defining it via\nstate_schema\non\ncreate_agent\nbecause it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\nstate_schema\nis still supported for backwards compatibility on\ncreate_agent\n.\nTo learn more about memory, see\nMemory\n. For information on implementing long-term memory that persists across sessions, see\nLong-term memory\n.\n\u200b\nStreaming\nWe\u2019ve seen how the agent can be called with\ninvoke\nto get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\nCopy\nfor\nchunk\nin\nagent.stream({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Search for AI news and summarize the findings\"\n}]\n},\nstream_mode\n=\n\"values\"\n):\n# Each chunk contains the full state at that point\nlatest_message\n=\nchunk[\n\"messages\"\n][\n-\n1\n]\nif\nlatest_message.content:\nprint\n(\nf\n\"Agent:\n{\nlatest_message.content\n}\n\"\n)\nelif\nlatest_message.tool_calls:\nprint\n(\nf\n\"Calling tools:\n{\n[tc[\n'name'\n]\nfor\ntc\nin\nlatest_message.tool_calls]\n}\n\"\n)\nFor more details on streaming, see\nStreaming\n.\n\u200b\nMiddleware\nMiddleware\nprovides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\nProcess state before the model is called (e.g., message trimming, context injection)\nModify or validate the model\u2019s response (e.g., guardrails, content filtering)\nHandle tool execution errors with custom logic\nImplement dynamic model selection based on state or context\nAdd custom logging, monitoring, or analytics\nMiddleware integrates seamlessly into the agent\u2019s execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\nFor comprehensive middleware documentation including decorators like\n@before_model\n,\n@after_model\n, and\n@wrap_tool_call\n, see\nMiddleware\n.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nPhilosophy\nPrevious\nModels\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/agents",
      "title": "Agents - Docs by LangChain",
      "heading": "Agents"
    }
  },
  {
    "page_content": "Models - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore components\nModels\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nBasic usage\nInitialize a model\nKey methods\nParameters\nInvocation\nInvoke\nStream\nBatch\nTool calling\nStructured output\nSupported models\nAdvanced topics\nModel profiles\nMultimodal\nReasoning\nLocal models\nPrompt caching\nServer-side tool use\nRate limiting\nBase URL or proxy\nLog probabilities\nToken usage\nInvocation config\nConfigurable models\nCore components\nModels\nCopy page\nCopy page\nLLMs\nare powerful AI tools that can interpret and generate text like humans. They\u2019re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\nIn addition to text generation, many models support:\nTool calling\n- calling external tools (like databases queries or API calls) and use results in their responses.\nStructured output\n- where the model\u2019s response is constrained to follow a defined format.\nMultimodality\n- process and return data other than text, such as images, audio, and video.\nReasoning\n- models perform multi-step reasoning to arrive at a conclusion.\nModels are the reasoning engine of\nagents\n. They drive the agent\u2019s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.\nThe quality and capabilities of the model you choose directly impact your agent\u2019s baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.\nLangChain\u2019s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.\nFor provider-specific integration information and capabilities, see the provider\u2019s\nchat model page\n.\n\u200b\nBasic usage\nModels can be utilized in two ways:\nWith agents\n- Models can be dynamically specified when creating an\nagent\n.\nStandalone\n- Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.\nThe same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\n\u200b\nInitialize a model\nThe easiest way to get started with a standalone model in LangChain is to use\ninit_chat_model\nto initialize one from a\nchat model provider\nof your choice (examples below):\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nCopy\nresponse\n=\nmodel.invoke(\n\"Why do parrots talk?\"\n)\nSee\ninit_chat_model\nfor more detail, including information on how to pass model\nparameters\n.\n\u200b\nKey methods\nInvoke\nThe model takes messages as input and outputs messages after generating a complete response.\nStream\nInvoke the model, but stream the output as it is generated in real-time.\nBatch\nSend multiple requests to a model in a batch for more efficient processing.\nIn addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the\nintegrations page\nfor details.\n\u200b\nParameters\nA chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:\n\u200b\nmodel\nstring\nrequired\nThe name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the \u2019\n:\n\u2019 format, for example, \u2018openai:o1\u2019.\n\u200b\napi_key\nstring\nThe key required for authenticating with the model\u2019s provider. This is usually issued when you sign up for access to the model. Often accessed by setting an\nenvironment variable\n.\n\u200b\ntemperature\nnumber\nControls the randomness of the model\u2019s output. A higher number makes responses more creative; lower ones make them more deterministic.\n\u200b\nmax_tokens\nnumber\nLimits the total number of\ntokens\nin the response, effectively controlling how long the output can be.\n\u200b\ntimeout\nnumber\nThe maximum time (in seconds) to wait for a response from the model before canceling the request.\n\u200b\nmax_retries\nnumber\nThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\nUsing\ninit_chat_model\n, pass these parameters as inline\n**kwargs\n:\nInitialize using model parameters\nCopy\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n,\n# Kwargs passed to the model:\ntemperature\n=\n0.7\n,\ntimeout\n=\n30\n,\nmax_tokens\n=\n1000\n,\n)\nEach chat model integration may have additional params used to control provider-specific functionality.\nFor example,\nChatOpenAI\nhas\nuse_responses_api\nto dictate whether to use the OpenAI Responses or Completions API.\nTo find all the parameters supported by a given chat model, head to the\nchat model integrations\npage.\n\u200b\nInvocation\nA chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.\n\u200b\nInvoke\nThe most straightforward way to call a model is to use\ninvoke()\nwith a single message or a list of messages.\nSingle message\nCopy\nresponse\n=\nmodel.invoke(\n\"Why do parrots have colorful feathers?\"\n)\nprint\n(response)\nA list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.\nSee the\nmessages\nguide for more detail on roles, types, and content.\nDictionary format\nCopy\nconversation\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant that translates English to French.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Translate: I love programming.\"\n},\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"J'adore la programmation.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Translate: I love building applications.\"\n}\n]\nresponse\n=\nmodel.invoke(conversation)\nprint\n(response)\n# AIMessage(\"J'adore cr\u00e9er des applications.\")\nMessage objects\nCopy\nfrom\nlangchain.messages\nimport\nHumanMessage, AIMessage, SystemMessage\nconversation\n=\n[\nSystemMessage(\n\"You are a helpful assistant that translates English to French.\"\n),\nHumanMessage(\n\"Translate: I love programming.\"\n),\nAIMessage(\n\"J'adore la programmation.\"\n),\nHumanMessage(\n\"Translate: I love building applications.\"\n)\n]\nresponse\n=\nmodel.invoke(conversation)\nprint\n(response)\n# AIMessage(\"J'adore cr\u00e9er des applications.\")\nIf the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with \u201cChat\u201d, e.g.,\nChatOpenAI\n(/oss/integrations/chat/openai).\n\u200b\nStream\nMost models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\nCalling\nstream()\nreturns an\niterator\nthat yields output chunks as they are produced. You can use a loop to process each chunk in real-time:\nBasic text streaming\nStream tool calls, reasoning, and other content\nCopy\nfor\nchunk\nin\nmodel.stream(\n\"Why do parrots have colorful feathers?\"\n):\nprint\n(chunk.text,\nend\n=\n\"|\"\n,\nflush\n=\nTrue\n)\nAs opposed to\ninvoke()\n, which returns a single\nAIMessage\nafter the model has finished generating its full response,\nstream()\nreturns multiple\nAIMessageChunk\nobjects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:\nConstruct an AIMessage\nCopy\nfull\n=\nNone\n# None | AIMessageChunk\nfor\nchunk\nin\nmodel.stream(\n\"What color is the sky?\"\n):\nfull\n=\nchunk\nif\nfull\nis\nNone\nelse\nfull\n+\nchunk\nprint\n(full.text)\n# The\n# The sky\n# The sky is\n# The sky is typically\n# The sky is typically blue\n# ...\nprint\n(full.content_blocks)\n# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]\nThe resulting message can be treated the same as a message that was generated with\ninvoke()\n\u2013 for example, it can be aggregated into a message history and passed back to the model as conversational context.\nStreaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn\u2019t streaming-capable would be one that needs to store the entire output in memory before it can be processed.\nAdvanced streaming topics\nStreaming events\nLangChain chat models can also stream semantic events using\nastream_events()\n.\nThis simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.\nCopy\nasync\nfor\nevent\nin\nmodel.astream_events(\n\"Hello\"\n):\nif\nevent[\n\"event\"\n]\n==\n\"on_chat_model_start\"\n:\nprint\n(\nf\n\"Input:\n{\nevent[\n'data'\n][\n'input'\n]\n}\n\"\n)\nelif\nevent[\n\"event\"\n]\n==\n\"on_chat_model_stream\"\n:\nprint\n(\nf\n\"Token:\n{\nevent[\n'data'\n][\n'chunk'\n].text\n}\n\"\n)\nelif\nevent[\n\"event\"\n]\n==\n\"on_chat_model_end\"\n:\nprint\n(\nf\n\"Full message:\n{\nevent[\n'data'\n][\n'output'\n].text\n}\n\"\n)\nelse\n:\npass\nCopy\nInput: Hello\nToken: Hi\nToken:  there\nToken: !\nToken:  How\nToken:  can\nToken:  I\n...\nFull message: Hi there! How can I help today?\nSee the\nastream_events()\nreference for event types and other details.\n\"Auto-streaming\" chat models\nLangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you\u2019re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.\nIn\nLangGraph agents\n, for example, you can call\nmodel.invoke()\nwithin nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.\n\u200b\nHow it works\nWhen you\ninvoke()\na chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking\non_llm_new_token\nevents in LangChain\u2019s callback system.\nCallback events allow LangGraph\nstream()\nand\nastream_events()\nto surface the chat model\u2019s output in real-time.\n\u200b\nBatch\nBatching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:\nBatch\nCopy\nresponses\n=\nmodel.batch([\n\"Why do parrots have colorful feathers?\"\n,\n\"How do airplanes fly?\"\n,\n\"What is quantum computing?\"\n])\nfor\nresponse\nin\nresponses:\nprint\n(response)\nThis section describes a chat model method\nbatch()\n, which parallelizes model calls client-side.\nIt is\ndistinct\nfrom batch APIs supported by inference providers, such as\nOpenAI\nor\nAnthropic\n.\nBy default,\nbatch()\nwill only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with\nbatch_as_completed()\n:\nYield batch responses upon completion\nCopy\nfor\nresponse\nin\nmodel.batch_as_completed([\n\"Why do parrots have colorful feathers?\"\n,\n\"How do airplanes fly?\"\n,\n\"What is quantum computing?\"\n]):\nprint\n(response)\nWhen using\nbatch_as_completed()\n, results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.\nWhen processing a large number of inputs using\nbatch()\nor\nbatch_as_completed()\n, you may want to control the maximum number of parallel calls. This can be done by setting the\nmax_concurrency\nattribute in the\nRunnableConfig\ndictionary.\nBatch with max concurrency\nCopy\nmodel.batch(\nlist_of_inputs,\nconfig\n=\n{\n'max_concurrency'\n:\n5\n,\n# Limit to 5 parallel calls\n}\n)\nSee the\nRunnableConfig\nreference for a full list of supported attributes.\nFor more details on batching, see the\nreference\n.\n\u200b\nTool calling\nModels can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\nA schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\nA function or\ncoroutine\nto execute.\nYou may hear the term \u201cfunction calling\u201d. We use this interchangeably with \u201ctool calling\u201d.\nHere\u2019s the basic tool calling flow between a user and a model:\nTo make tools that you have defined available for use by a model, you must bind them using\nbind_tools\n. In subsequent invocations, the model can choose to call any of the bound tools as needed.\nSome model providers offer\nbuilt-in tools\nthat can be enabled via model or invocation parameters (e.g.\nChatOpenAI\n,\nChatAnthropic\n). Check the respective\nprovider reference\nfor details.\nSee the\ntools guide\nfor details and other options for creating tools.\nBinding user tools\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\ndef\nget_weather\n(\nlocation\n:\nstr\n) ->\nstr\n:\n\"\"\"Get the weather at a location.\"\"\"\nreturn\nf\n\"It's sunny in\n{\nlocation\n}\n.\"\nmodel_with_tools\n=\nmodel.bind_tools([get_weather])\nresponse\n=\nmodel_with_tools.invoke(\n\"What's the weather like in Boston?\"\n)\nfor\ntool_call\nin\nresponse.tool_calls:\n# View tool calls made by the model\nprint\n(\nf\n\"Tool:\n{\ntool_call[\n'name'\n]\n}\n\"\n)\nprint\n(\nf\n\"Args:\n{\ntool_call[\n'args'\n]\n}\n\"\n)\nWhen binding user-defined tools, the model\u2019s response includes a\nrequest\nto execute a tool. When using a model separately from an\nagent\n, it is up to you to execute the requested tool and return the result back to the model for use in subsequent reasoning. When using an\nagent\n, the agent loop will handle the tool execution loop for you.\nBelow, we show some common ways you can use tool calling.\nTool execution loop\nWhen a model returns tool calls, you need to execute the tools and pass the results back to the model. This creates a conversation loop where the model can use tool results to generate its final response. LangChain includes\nagent\nabstractions that handle this orchestration for you.\nHere\u2019s a simple example of how to do this:\nTool execution loop\nCopy\n# Bind (potentially multiple) tools to the model\nmodel_with_tools\n=\nmodel.bind_tools([get_weather])\n# Step 1: Model generates tool calls\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the weather in Boston?\"\n}]\nai_msg\n=\nmodel_with_tools.invoke(messages)\nmessages.append(ai_msg)\n# Step 2: Execute tools and collect results\nfor\ntool_call\nin\nai_msg.tool_calls:\n# Execute the tool with the generated arguments\ntool_result\n=\nget_weather.invoke(tool_call)\nmessages.append(tool_result)\n# Step 3: Pass results back to model for final response\nfinal_response\n=\nmodel_with_tools.invoke(messages)\nprint\n(final_response.text)\n# \"The current weather in Boston is 72\u00b0F and sunny.\"\nEach\nToolMessage\nreturned by the tool includes a\ntool_call_id\nthat matches the original tool call, helping the model correlate results with requests.\nForcing tool calls\nBy default, the model has the freedom to choose which bound tool to use based on the user\u2019s input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or\nany\ntool from a given list:\nForce use of any tool\nForce use of specific tools\nCopy\nmodel_with_tools\n=\nmodel.bind_tools([tool_1],\ntool_choice\n=\n\"any\"\n)\nParallel tool calls\nMany models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.\nParallel tool calls\nCopy\nmodel_with_tools\n=\nmodel.bind_tools([get_weather])\nresponse\n=\nmodel_with_tools.invoke(\n\"What's the weather in Boston and Tokyo?\"\n)\n# The model may generate multiple tool calls\nprint\n(response.tool_calls)\n# [\n#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},\n# ]\n# Execute all tools (can be done in parallel with async)\nresults\n=\n[]\nfor\ntool_call\nin\nresponse.tool_calls:\nif\ntool_call[\n'name'\n]\n==\n'get_weather'\n:\nresult\n=\nget_weather.invoke(tool_call)\n...\nresults.append(result)\nThe model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.\nMost models supporting tool calling enable parallel tool calls by default. Some (including\nOpenAI\nand\nAnthropic\n) allow you to disable this feature. To do this, set\nparallel_tool_calls=False\n:\nCopy\nmodel.bind_tools([get_weather],\nparallel_tool_calls\n=\nFalse\n)\nStreaming tool calls\nWhen streaming responses, tool calls are progressively built through\nToolCallChunk\n. This allows you to see tool calls as they\u2019re being generated rather than waiting for the complete response.\nStreaming tool calls\nCopy\nfor\nchunk\nin\nmodel_with_tools.stream(\n\"What's the weather in Boston and Tokyo?\"\n):\n# Tool call chunks arrive progressively\nfor\ntool_chunk\nin\nchunk.tool_call_chunks:\nif\nname\n:=\ntool_chunk.get(\n\"name\"\n):\nprint\n(\nf\n\"Tool:\n{\nname\n}\n\"\n)\nif\nid_\n:=\ntool_chunk.get(\n\"id\"\n):\nprint\n(\nf\n\"ID:\n{\nid_\n}\n\"\n)\nif\nargs\n:=\ntool_chunk.get(\n\"args\"\n):\nprint\n(\nf\n\"Args:\n{\nargs\n}\n\"\n)\n# Output:\n# Tool: get_weather\n# ID: call_SvMlU1TVIZugrFLckFE2ceRE\n# Args: {\"lo\n# Args: catio\n# Args: n\": \"B\n# Args: osto\n# Args: n\"}\n# Tool: get_weather\n# ID: call_QMZdy6qInx13oWKE7KhuhOLR\n# Args: {\"lo\n# Args: catio\n# Args: n\": \"T\n# Args: okyo\n# Args: \"}\nYou can accumulate chunks to build complete tool calls:\nAccumulate tool calls\nCopy\ngathered\n=\nNone\nfor\nchunk\nin\nmodel_with_tools.stream(\n\"What's the weather in Boston?\"\n):\ngathered\n=\nchunk\nif\ngathered\nis\nNone\nelse\ngathered\n+\nchunk\nprint\n(gathered.tool_calls)\n\u200b\nStructured output\nModels can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\nPydantic\nTypedDict\nJSON Schema\nPydantic models\nprovide the richest feature set with field validation, descriptions, and nested structures.\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nclass\nMovie\n(\nBaseModel\n):\n\"\"\"A movie with details.\"\"\"\ntitle:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The title of the movie\"\n)\nyear:\nint\n=\nField(\n...\n,\ndescription\n=\n\"The year the movie was released\"\n)\ndirector:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The director of the movie\"\n)\nrating:\nfloat\n=\nField(\n...\n,\ndescription\n=\n\"The movie's rating out of 10\"\n)\nmodel_with_structure\n=\nmodel.with_structured_output(Movie)\nresponse\n=\nmodel_with_structure.invoke(\n\"Provide details about the movie Inception\"\n)\nprint\n(response)\n# Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)\nTypedDict\nprovides a simpler alternative using Python\u2019s built-in typing, ideal when you don\u2019t need runtime validation.\nCopy\nfrom\ntyping_extensions\nimport\nTypedDict, Annotated\nclass\nMovieDict\n(\nTypedDict\n):\n\"\"\"A movie with details.\"\"\"\ntitle: Annotated[\nstr\n,\n...\n,\n\"The title of the movie\"\n]\nyear: Annotated[\nint\n,\n...\n,\n\"The year the movie was released\"\n]\ndirector: Annotated[\nstr\n,\n...\n,\n\"The director of the movie\"\n]\nrating: Annotated[\nfloat\n,\n...\n,\n\"The movie's rating out of 10\"\n]\nmodel_with_structure\n=\nmodel.with_structured_output(MovieDict)\nresponse\n=\nmodel_with_structure.invoke(\n\"Provide details about the movie Inception\"\n)\nprint\n(response)\n# {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}\nFor maximum control or interoperability, you can provide a raw JSON Schema.\nCopy\nimport\njson\njson_schema\n=\n{\n\"title\"\n:\n\"Movie\"\n,\n\"description\"\n:\n\"A movie with details\"\n,\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"title\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The title of the movie\"\n},\n\"year\"\n: {\n\"type\"\n:\n\"integer\"\n,\n\"description\"\n:\n\"The year the movie was released\"\n},\n\"director\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The director of the movie\"\n},\n\"rating\"\n: {\n\"type\"\n:\n\"number\"\n,\n\"description\"\n:\n\"The movie's rating out of 10\"\n}\n},\n\"required\"\n: [\n\"title\"\n,\n\"year\"\n,\n\"director\"\n,\n\"rating\"\n]\n}\nmodel_with_structure\n=\nmodel.with_structured_output(\njson_schema,\nmethod\n=\n\"json_schema\"\n,\n)\nresponse\n=\nmodel_with_structure.invoke(\n\"Provide details about the movie Inception\"\n)\nprint\n(response)\n# {'title': 'Inception', 'year': 2010, ...}\nKey considerations for structured output:\nMethod parameter\n: Some providers support different methods (\n'json_schema'\n,\n'function_calling'\n,\n'json_mode'\n)\n'json_schema'\ntypically refers to dedicated structured output features offered by a provider\n'function_calling'\nderives structured output by forcing a\ntool call\nfollowing the given schema\n'json_mode'\nis a precursor to\n'json_schema'\noffered by some providers - it generates valid json, but the schema must be described in the prompt\nInclude raw\n: Use\ninclude_raw=True\nto get both the parsed output and the raw AI message\nValidation\n: Pydantic models provide automatic validation, while\nTypedDict\nand JSON Schema require manual validation\nExample: Message output alongside parsed structure\nIt can be useful to return the raw\nAIMessage\nobject alongside the parsed representation to access response metadata such as\ntoken counts\n. To do this, set\ninclude_raw=True\nwhen calling\nwith_structured_output\n:\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nclass\nMovie\n(\nBaseModel\n):\n\"\"\"A movie with details.\"\"\"\ntitle:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The title of the movie\"\n)\nyear:\nint\n=\nField(\n...\n,\ndescription\n=\n\"The year the movie was released\"\n)\ndirector:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The director of the movie\"\n)\nrating:\nfloat\n=\nField(\n...\n,\ndescription\n=\n\"The movie's rating out of 10\"\n)\nmodel_with_structure\n=\nmodel.with_structured_output(Movie,\ninclude_raw\n=\nTrue\n)\nresponse\n=\nmodel_with_structure.invoke(\n\"Provide details about the movie Inception\"\n)\nresponse\n# {\n#     \"raw\": AIMessage(...),\n#     \"parsed\": Movie(title=..., year=..., ...),\n#     \"parsing_error\": None,\n# }\nExample: Nested structures\nSchemas can be nested:\nPydantic BaseModel\nTypedDict\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nclass\nActor\n(\nBaseModel\n):\nname:\nstr\nrole:\nstr\nclass\nMovieDetails\n(\nBaseModel\n):\ntitle:\nstr\nyear:\nint\ncast: list[Actor]\ngenres: list[\nstr\n]\nbudget:\nfloat\n|\nNone\n=\nField(\nNone\n,\ndescription\n=\n\"Budget in millions USD\"\n)\nmodel_with_structure\n=\nmodel.with_structured_output(MovieDetails)\n\u200b\nSupported models\nLangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the\nintegrations page\n.\n\u200b\nAdvanced topics\n\u200b\nModel profiles\nThis is a beta feature. The format of model profiles is subject to change.\nModel profiles require\nlangchain>=1.1\n.\nLangChain chat models can expose a dictionary of supported features and capabilities through a\n.profile\nattribute:\nCopy\nmodel.profile\n# {\n#   \"max_input_tokens\": 400000,\n#   \"image_inputs\": True,\n#   \"reasoning_output\": True,\n#   \"tool_calling\": True,\n#   ...\n# }\nRefer to the full set of fields in the\nAPI reference\n.\nMuch of the model profile data is powered by the\nmodels.dev\nproject, an open source initiative that provides model capability data. These data are augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.\nModel profile data allow applications to work around model capabilities dynamically. For example:\nSummarization middleware\ncan trigger summarization based on a model\u2019s context window size.\nStructured output\nstrategies in\ncreate_agent\ncan be inferred automatically (e.g., by checking support for native structured output features).\nModel inputs can be gated based on supported\nmodalities\nand maximum input tokens.\nUpdating or overwriting profile data\nModel profile data can be changed if it is missing, stale, or incorrect.\nOption 1 (quick fix)\nYou can instantiate a chat model with any valid profile:\nCopy\ncustom_profile\n=\n{\n\"max_input_tokens\"\n:\n100_000\n,\n\"tool_calling\"\n:\nTrue\n,\n\"structured_output\"\n:\nTrue\n,\n# ...\n}\nmodel\n=\ninit_chat_model(\n\"...\"\n,\nprofile\n=\ncustom_profile)\nThe\nprofile\nis also a regular\ndict\nand can be updated in place. If the model instance is shared, consider using\nmodel_copy\nto avoid mutating shared state.\nCopy\nnew_profile\n=\nmodel.profile\n|\n{\n\"key\"\n:\n\"value\"\n}\nmodel.model_copy(\nupdate\n=\n{\n\"profile\"\n: new_profile})\nOption 2 (fix data upstream)\nThe primary source for the data is the\nmodels.dev\nproject. This data is merged with additional fields and overrides in LangChain\nintegration packages\nand are shipped with those packages.\nModel profile data can be updated through the following process:\n(If needed) update the source data at\nmodels.dev\nthrough a pull request to its\nrepository on GitHub\n.\n(If needed) update additional fields and overrides in\nlangchain_<package>/data/profile_augmentations.toml\nthrough a pull request to the LangChain\nintegration package\n`.\nUse the\nlangchain-model-profiles\nCLI tool to pull the latest data from\nmodels.dev\n, merge in the augmentations and update the profile data:\nCopy\npip\ninstall\nlangchain-model-profiles\nCopy\nlangchain-profiles\nrefresh\n--provider\n<\nprovide\nr\n>\n--data-dir\n<\ndata_di\nr\n>\nThis command:\nDownloads the latest data for\n<provider>\nfrom models.dev\nMerges augmentations from\nprofile_augmentations.toml\nin\n<data_dir>\nWrites merged profiles to\nprofiles.py\nin\n<data_dir>\nFor example: from\nlibs/partners/anthropic\nin the\nLangChain monorepo\n:\nCopy\nuv\nrun\n--with\nlangchain-model-profiles\n--provider\nanthropic\n--data-dir\nlangchain_anthropic/data\n\u200b\nMultimodal\nCertain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing\ncontent blocks\n.\nAll LangChain chat models with underlying multimodal capabilities support:\nData in the cross-provider standard format (see\nour messages guide\n)\nOpenAI\nchat completions\nformat\nAny format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)\nSee the\nmultimodal section\nof the messages guide for details.\nSome models\ncan return multimodal data as part of their response. If invoked to do so, the resulting\nAIMessage\nwill have content blocks with multimodal types.\nMultimodal output\nCopy\nresponse\n=\nmodel.invoke(\n\"Create a picture of a cat\"\n)\nprint\n(response.content_blocks)\n# [\n#     {\"type\": \"text\", \"text\": \"Here's a picture of a cat\"},\n#     {\"type\": \"image\", \"base64\": \"...\", \"mime_type\": \"image/jpeg\"},\n# ]\nSee the\nintegrations page\nfor details on specific providers.\n\u200b\nReasoning\nMany models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.\nIf supported by the underlying model,\nyou can surface this reasoning process to better understand how the model arrived at its final answer.\nStream reasoning output\nComplete reasoning output\nCopy\nfor\nchunk\nin\nmodel.stream(\n\"Why do parrots have colorful feathers?\"\n):\nreasoning_steps\n=\n[r\nfor\nr\nin\nchunk.content_blocks\nif\nr[\n\"type\"\n]\n==\n\"reasoning\"\n]\nprint\n(reasoning_steps\nif\nreasoning_steps\nelse\nchunk.text)\nDepending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical \u201ctiers\u201d of reasoning (e.g.,\n'low'\nor\n'high'\n) or integer token budgets.\nFor details, see the\nintegrations page\nor\nreference\nfor your respective chat model.\n\u200b\nLocal models\nLangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\nOllama\nis one of the easiest ways to run models locally. See the full list of local integrations on the\nintegrations page\n.\n\u200b\nPrompt caching\nMany providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be\nimplicit\nor\nexplicit\n:\nImplicit prompt caching:\nproviders will automatically pass on cost savings if a request hits a cache. Examples:\nOpenAI\nand\nGemini\n.\nExplicit caching:\nproviders allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:\nChatOpenAI\n(via\nprompt_cache_key\n)\nAnthropic\u2019s\nAnthropicPromptCachingMiddleware\nGemini\n.\nAWS Bedrock\nPrompt caching is often only engaged above a minimum input token threshold. See\nprovider pages\nfor details.\nCache usage will be reflected in the\nusage metadata\nof the model response.\n\u200b\nServer-side tool use\nSome providers support server-side\ntool-calling\nloops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.\nIf a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the\ncontent blocks\nof the response will return the server-side tool calls and results in a provider-agnostic format:\nInvoke with server-side tool use\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\ntool\n=\n{\n\"type\"\n:\n\"web_search\"\n}\nmodel_with_tools\n=\nmodel.bind_tools([tool])\nresponse\n=\nmodel_with_tools.invoke(\n\"What was a positive news story from today?\"\n)\nresponse.content_blocks\nResult\nCopy\n[\n{\n\"type\"\n:\n\"server_tool_call\"\n,\n\"name\"\n:\n\"web_search\"\n,\n\"args\"\n: {\n\"query\"\n:\n\"positive news stories today\"\n,\n\"type\"\n:\n\"search\"\n},\n\"id\"\n:\n\"ws_abc123\"\n},\n{\n\"type\"\n:\n\"server_tool_result\"\n,\n\"tool_call_id\"\n:\n\"ws_abc123\"\n,\n\"status\"\n:\n\"success\"\n},\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Here are some positive news stories from today...\"\n,\n\"annotations\"\n: [\n{\n\"end_index\"\n:\n410\n,\n\"start_index\"\n:\n337\n,\n\"title\"\n:\n\"article title\"\n,\n\"type\"\n:\n\"citation\"\n,\n\"url\"\n:\n\"...\"\n}\n]\n}\n]\nSee all 29 lines\nThis represents a single conversational turn; there are no associated\nToolMessage\nobjects that need to be passed in as in client-side\ntool-calling\n.\nSee the\nintegration page\nfor your given provider for available tools and usage details.\n\u200b\nRate limiting\nMany chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\nTo help manage rate limits, chat model integrations accept a\nrate_limiter\nparameter that can be provided during initialization to control the rate at which requests are made.\nInitialize and use a rate limiter\nLangChain in comes with (an optional) built-in\nInMemoryRateLimiter\n. This limiter is thread safe and can be shared by multiple threads in the same process.\nDefine a rate limiter\nCopy\nfrom\nlangchain_core.rate_limiters\nimport\nInMemoryRateLimiter\nrate_limiter\n=\nInMemoryRateLimiter(\nrequests_per_second\n=\n0.1\n,\n# 1 request every 10s\ncheck_every_n_seconds\n=\n0.1\n,\n# Check every 100ms whether allowed to make a request\nmax_bucket_size\n=\n10\n,\n# Controls the maximum burst size.\n)\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5\"\n,\nmodel_provider\n=\n\"openai\"\n,\nrate_limiter\n=\nrate_limiter\n)\nThe provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.\n\u200b\nBase URL or proxy\nFor many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.\nBase URL\nMany model providers offer OpenAI-compatible APIs (e.g.,\nTogether AI\n,\nvLLM\n). You can use\ninit_chat_model\nwith these providers by specifying the appropriate\nbase_url\nparameter:\nCopy\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"MODEL_NAME\"\n,\nmodel_provider\n=\n\"openai\"\n,\nbase_url\n=\n\"BASE_URL\"\n,\napi_key\n=\n\"YOUR_API_KEY\"\n,\n)\nWhen using direct chat model class instantiation, the parameter name may vary by provider. Check the respective\nreference\nfor details.\nProxy configuration\nFor deployments requiring HTTP proxies, some model integrations support proxy configuration:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\nopenai_proxy\n=\n\"http://proxy.example.com:8080\"\n)\nProxy support varies by integration. Check the specific model provider\u2019s\nreference\nfor proxy configuration options.\n\u200b\nLog probabilities\nCertain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the\nlogprobs\nparameter when initializing the model:\nCopy\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n,\nmodel_provider\n=\n\"openai\"\n).bind(\nlogprobs\n=\nTrue\n)\nresponse\n=\nmodel.invoke(\n\"Why do parrots talk?\"\n)\nprint\n(response.response_metadata[\n\"logprobs\"\n])\n\u200b\nToken usage\nA number of model providers return token usage information as part of the invocation response. When available, this information will be included on the\nAIMessage\nobjects produced by the corresponding model. For more details, see the\nmessages\nguide.\nSome provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the\nstreaming usage metadata\nsection of the integration guide for details.\nYou can track aggregate token counts across models in an application using either a callback or context manager, as shown below:\nCallback handler\nContext manager\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain_core.callbacks\nimport\nUsageMetadataCallbackHandler\nmodel_1\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n)\nmodel_2\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\ncallback\n=\nUsageMetadataCallbackHandler()\nresult_1\n=\nmodel_1.invoke(\n\"Hello\"\n,\nconfig\n=\n{\n\"callbacks\"\n: [callback]})\nresult_2\n=\nmodel_2.invoke(\n\"Hello\"\n,\nconfig\n=\n{\n\"callbacks\"\n: [callback]})\ncallback.usage_metadata\nCopy\n{\n'gpt-5-mini-2024-07-18'\n: {\n'input_tokens'\n:\n8\n,\n'output_tokens'\n:\n10\n,\n'total_tokens'\n:\n18\n,\n'input_token_details'\n: {\n'audio'\n:\n0\n,\n'cache_read'\n:\n0\n},\n'output_token_details'\n: {\n'audio'\n:\n0\n,\n'reasoning'\n:\n0\n}\n},\n'claude-haiku-4-5-20251001'\n: {\n'input_tokens'\n:\n8\n,\n'output_tokens'\n:\n21\n,\n'total_tokens'\n:\n29\n,\n'input_token_details'\n: {\n'cache_read'\n:\n0\n,\n'cache_creation'\n:\n0\n}\n}\n}\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain_core.callbacks\nimport\nget_usage_metadata_callback\nmodel_1\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n)\nmodel_2\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nwith\nget_usage_metadata_callback()\nas\ncb:\nmodel_1.invoke(\n\"Hello\"\n)\nmodel_2.invoke(\n\"Hello\"\n)\nprint\n(cb.usage_metadata)\nCopy\n{\n'gpt-5-mini-2024-07-18'\n: {\n'input_tokens'\n:\n8\n,\n'output_tokens'\n:\n10\n,\n'total_tokens'\n:\n18\n,\n'input_token_details'\n: {\n'audio'\n:\n0\n,\n'cache_read'\n:\n0\n},\n'output_token_details'\n: {\n'audio'\n:\n0\n,\n'reasoning'\n:\n0\n}\n},\n'claude-haiku-4-5-20251001'\n: {\n'input_tokens'\n:\n8\n,\n'output_tokens'\n:\n21\n,\n'total_tokens'\n:\n29\n,\n'input_token_details'\n: {\n'cache_read'\n:\n0\n,\n'cache_creation'\n:\n0\n}\n}\n}\n\u200b\nInvocation config\nWhen invoking a model, you can pass additional configuration through the\nconfig\nparameter using a\nRunnableConfig\ndictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking.\nCommon configuration options include:\nInvocation with config\nCopy\nresponse\n=\nmodel.invoke(\n\"Tell me a joke\"\n,\nconfig\n=\n{\n\"run_name\"\n:\n\"joke_generation\"\n,\n# Custom name for this run\n\"tags\"\n: [\n\"humor\"\n,\n\"demo\"\n],\n# Tags for categorization\n\"metadata\"\n: {\n\"user_id\"\n:\n\"123\"\n},\n# Custom metadata\n\"callbacks\"\n: [my_callback_handler],\n# Callback handlers\n}\n)\nThese configuration values are particularly useful when:\nDebugging with\nLangSmith\ntracing\nImplementing custom logging or monitoring\nControlling resource usage in production\nTracking invocations across complex pipelines\nKey configuration attributes\n\u200b\nrun_name\nstring\nIdentifies this specific invocation in logs and traces. Not inherited by sub-calls.\n\u200b\ntags\nstring[]\nLabels inherited by all sub-calls for filtering and organization in debugging tools.\n\u200b\nmetadata\nobject\nCustom key-value pairs for tracking additional context, inherited by all sub-calls.\n\u200b\nmax_concurrency\nnumber\nControls the maximum number of parallel calls when using\nbatch()\nor\nbatch_as_completed()\n.\n\u200b\ncallbacks\narray\nHandlers for monitoring and responding to events during execution.\n\u200b\nrecursion_limit\nnumber\nMaximum recursion depth for chains to prevent infinite loops in complex pipelines.\nSee full\nRunnableConfig\nreference for all supported attributes.\n\u200b\nConfigurable models\nYou can also create a runtime-configurable model by specifying\nconfigurable_fields\n. If you don\u2019t specify a model value, then\n'model'\nand\n'model_provider'\nwill be configurable by default.\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nconfigurable_model\n=\ninit_chat_model(\ntemperature\n=\n0\n)\nconfigurable_model.invoke(\n\"what's your name\"\n,\nconfig\n=\n{\n\"configurable\"\n: {\n\"model\"\n:\n\"gpt-5-mini\"\n}},\n# Run with gpt-5-mini\n)\nconfigurable_model.invoke(\n\"what's your name\"\n,\nconfig\n=\n{\n\"configurable\"\n: {\n\"model\"\n:\n\"claude-sonnet-4-5-20250929\"\n}},\n# Run with Claude\n)\nConfigurable model with default values\nWe can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:\nCopy\nfirst_model\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5-mini\"\n,\ntemperature\n=\n0\n,\nconfigurable_fields\n=\n(\n\"model\"\n,\n\"model_provider\"\n,\n\"temperature\"\n,\n\"max_tokens\"\n),\nconfig_prefix\n=\n\"first\"\n,\n# Useful when you have a chain with multiple models\n)\nfirst_model.invoke(\n\"what's your name\"\n)\nCopy\nfirst_model.invoke(\n\"what's your name\"\n,\nconfig\n=\n{\n\"configurable\"\n: {\n\"first_model\"\n:\n\"claude-sonnet-4-5-20250929\"\n,\n\"first_temperature\"\n:\n0.5\n,\n\"first_max_tokens\"\n:\n100\n,\n}\n},\n)\nSee the\ninit_chat_model\nreference for more details on\nconfigurable_fields\nand\nconfig_prefix\n.\nUsing a configurable model declaratively\nWe can call declarative operations like\nbind_tools\n,\nwith_structured_output\n,\nwith_configurable\n, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nclass\nGetWeather\n(\nBaseModel\n):\n\"\"\"Get the current weather in a given location\"\"\"\nlocation:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The city and state, e.g. San Francisco, CA\"\n)\nclass\nGetPopulation\n(\nBaseModel\n):\n\"\"\"Get the current population in a given location\"\"\"\nlocation:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The city and state, e.g. San Francisco, CA\"\n)\nmodel\n=\ninit_chat_model(\ntemperature\n=\n0\n)\nmodel_with_tools\n=\nmodel.bind_tools([GetWeather, GetPopulation])\nmodel_with_tools.invoke(\n\"what's bigger in 2024 LA or NYC\"\n,\nconfig\n=\n{\n\"configurable\"\n: {\n\"model\"\n:\n\"gpt-5-mini\"\n}}\n).tool_calls\nCopy\n[\n{\n'name': 'GetPopulation',\n'args': {'location': 'Los Angeles, CA'},\n'id': 'call_Ga9m8FAArIyEjItHmztPYA22',\n'type': 'tool_call'\n},\n{\n'name': 'GetPopulation',\n'args': {'location': 'New York, NY'},\n'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',\n'type': 'tool_call'\n}\n]\nCopy\nmodel_with_tools.invoke(\n\"what's bigger in 2024 LA or NYC\"\n,\nconfig\n=\n{\n\"configurable\"\n: {\n\"model\"\n:\n\"claude-sonnet-4-5-20250929\"\n}},\n).tool_calls\nCopy\n[\n{\n'name': 'GetPopulation',\n'args': {'location': 'Los Angeles, CA'},\n'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',\n'type': 'tool_call'\n},\n{\n'name': 'GetPopulation',\n'args': {'location': 'New York City, NY'},\n'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',\n'type': 'tool_call'\n}\n]\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nAgents\nPrevious\nMessages\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/models",
      "title": "Models - Docs by LangChain",
      "heading": "Models"
    }
  },
  {
    "page_content": "Messages - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore components\nMessages\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nBasic usage\nText prompts\nMessage prompts\nDictionary format\nMessage types\nSystem Message\nHuman Message\nText content\nMessage metadata\nAI Message\nTool calls\nToken usage\nStreaming and chunks\nTool Message\nMessage content\nStandard content blocks\nMultimodal\nContent block reference\nUse with chat models\nCore components\nMessages\nCopy page\nCopy page\nMessages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\nMessages are objects that contain:\nRole\n- Identifies the message type (e.g.\nsystem\n,\nuser\n)\nContent\n- Represents the actual content of the message (like text, images, audio, documents, etc.)\nMetadata\n- Optional fields such as response information, message IDs, and token usage\nLangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.\n\u200b\nBasic usage\nThe simplest way to use messages is to create message objects and pass them to a model when\ninvoking\n.\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain.messages\nimport\nHumanMessage, AIMessage, SystemMessage\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\nsystem_msg\n=\nSystemMessage(\n\"You are a helpful assistant.\"\n)\nhuman_msg\n=\nHumanMessage(\n\"Hello, how are you?\"\n)\n# Use with chat models\nmessages\n=\n[system_msg, human_msg]\nresponse\n=\nmodel.invoke(messages)\n# Returns AIMessage\n\u200b\nText prompts\nText prompts are strings - ideal for straightforward generation tasks where you don\u2019t need to retain conversation history.\nCopy\nresponse\n=\nmodel.invoke(\n\"Write a haiku about spring\"\n)\nUse text prompts when:\nYou have a single, standalone request\nYou don\u2019t need conversation history\nYou want minimal code complexity\n\u200b\nMessage prompts\nAlternatively, you can pass in a list of messages to the model by providing a list of message objects.\nCopy\nfrom\nlangchain.messages\nimport\nSystemMessage, HumanMessage, AIMessage\nmessages\n=\n[\nSystemMessage(\n\"You are a poetry expert\"\n),\nHumanMessage(\n\"Write a haiku about spring\"\n),\nAIMessage(\n\"Cherry blossoms bloom...\"\n)\n]\nresponse\n=\nmodel.invoke(messages)\nUse message prompts when:\nManaging multi-turn conversations\nWorking with multimodal content (images, audio, files)\nIncluding system instructions\n\u200b\nDictionary format\nYou can also specify messages directly in OpenAI chat completions format.\nCopy\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a poetry expert\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Write a haiku about spring\"\n},\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"Cherry blossoms bloom...\"\n}\n]\nresponse\n=\nmodel.invoke(messages)\n\u200b\nMessage types\nSystem message\n- Tells the model how to behave and provide context for interactions\nHuman message\n- Represents user input and interactions with the model\nAI message\n- Responses generated by the model, including text content, tool calls, and metadata\nTool message\n- Represents the outputs of\ntool calls\n\u200b\nSystem Message\nA\nSystemMessage\nrepresent an initial set of instructions that primes the model\u2019s behavior. You can use a system message to set the tone, define the model\u2019s role, and establish guidelines for responses.\nBasic instructions\nCopy\nsystem_msg\n=\nSystemMessage(\n\"You are a helpful coding assistant.\"\n)\nmessages\n=\n[\nsystem_msg,\nHumanMessage(\n\"How do I create a REST API?\"\n)\n]\nresponse\n=\nmodel.invoke(messages)\nDetailed persona\nCopy\nfrom\nlangchain.messages\nimport\nSystemMessage, HumanMessage\nsystem_msg\n=\nSystemMessage(\n\"\"\"\nYou are a senior Python developer with expertise in web frameworks.\nAlways provide code examples and explain your reasoning.\nBe concise but thorough in your explanations.\n\"\"\"\n)\nmessages\n=\n[\nsystem_msg,\nHumanMessage(\n\"How do I create a REST API?\"\n)\n]\nresponse\n=\nmodel.invoke(messages)\n\u200b\nHuman Message\nA\nHumanMessage\nrepresents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal\ncontent\n.\n\u200b\nText content\nMessage object\nString shortcut\nCopy\nresponse\n=\nmodel.invoke([\nHumanMessage(\n\"What is machine learning?\"\n)\n])\n\u200b\nMessage metadata\nAdd metadata\nCopy\nhuman_msg\n=\nHumanMessage(\ncontent\n=\n\"Hello!\"\n,\nname\n=\n\"alice\"\n,\n# Optional: identify different users\nid\n=\n\"msg_123\"\n,\n# Optional: unique identifier for tracing\n)\nThe\nname\nfield behavior varies by provider \u2013 some use it for user identification, others ignore it. To check, refer to the model provider\u2019s\nreference\n.\n\u200b\nAI Message\nAn\nAIMessage\nrepresents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\nCopy\nresponse\n=\nmodel.invoke(\n\"Explain AI\"\n)\nprint\n(\ntype\n(response))\n# <class 'langchain.messages.AIMessage'>\nAIMessage\nobjects are returned by the model when calling it, which contains all of the associated metadata in the response.\nProviders weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new\nAIMessage\nobject and insert it into the message history as if it came from the model.\nCopy\nfrom\nlangchain.messages\nimport\nAIMessage, SystemMessage, HumanMessage\n# Create an AI message manually (e.g., for conversation history)\nai_msg\n=\nAIMessage(\n\"I'd be happy to help you with that question!\"\n)\n# Add to conversation history\nmessages\n=\n[\nSystemMessage(\n\"You are a helpful assistant\"\n),\nHumanMessage(\n\"Can you help me?\"\n),\nai_msg,\n# Insert as if it came from the model\nHumanMessage(\n\"Great! What's 2+2?\"\n)\n]\nresponse\n=\nmodel.invoke(messages)\nAttributes\n\u200b\ntext\nstring\nThe text content of the message.\n\u200b\ncontent\nstring | dict[]\nThe raw content of the message.\n\u200b\ncontent_blocks\nContentBlock[]\nThe standardized\ncontent blocks\nof the message.\n\u200b\ntool_calls\ndict[] | None\nThe tool calls made by the model.\nEmpty if no tools are called.\n\u200b\nid\nstring\nA unique identifier for the message (either automatically generated by LangChain or returned in the provider response)\n\u200b\nusage_metadata\ndict | None\nThe usage metadata of the message, which can contain token counts when available.\n\u200b\nresponse_metadata\nResponseMetadata | None\nThe response metadata of the message.\n\u200b\nTool calls\nWhen models make\ntool calls\n, they\u2019re included in the\nAIMessage\n:\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\ndef\nget_weather\n(\nlocation\n:\nstr\n) ->\nstr\n:\n\"\"\"Get the weather at a location.\"\"\"\n...\nmodel_with_tools\n=\nmodel.bind_tools([get_weather])\nresponse\n=\nmodel_with_tools.invoke(\n\"What's the weather in Paris?\"\n)\nfor\ntool_call\nin\nresponse.tool_calls:\nprint\n(\nf\n\"Tool:\n{\ntool_call[\n'name'\n]\n}\n\"\n)\nprint\n(\nf\n\"Args:\n{\ntool_call[\n'args'\n]\n}\n\"\n)\nprint\n(\nf\n\"ID:\n{\ntool_call[\n'id'\n]\n}\n\"\n)\nOther structured data, such as reasoning or citations, can also appear in message\ncontent\n.\n\u200b\nToken usage\nAn\nAIMessage\ncan hold token counts and other usage metadata in its\nusage_metadata\nfield:\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\nresponse\n=\nmodel.invoke(\n\"Hello!\"\n)\nresponse.usage_metadata\nCopy\n{'input_tokens': 8,\n'output_tokens': 304,\n'total_tokens': 312,\n'input_token_details': {'audio': 0, 'cache_read': 0},\n'output_token_details': {'audio': 0, 'reasoning': 256}}\nSee\nUsageMetadata\nfor details.\n\u200b\nStreaming and chunks\nDuring streaming, you\u2019ll receive\nAIMessageChunk\nobjects that can be combined into a full message object:\nCopy\nchunks\n=\n[]\nfull_message\n=\nNone\nfor\nchunk\nin\nmodel.stream(\n\"Hi\"\n):\nchunks.append(chunk)\nprint\n(chunk.text)\nfull_message\n=\nchunk\nif\nfull_message\nis\nNone\nelse\nfull_message\n+\nchunk\nLearn more:\nStreaming tokens from chat models\nStreaming tokens and/or steps from agents\n\u200b\nTool Message\nFor models that support\ntool calling\n, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.\nTools\ncan generate\nToolMessage\nobjects directly. Below, we show a simple example. Read more in the\ntools guide\n.\nCopy\nfrom\nlangchain.messages\nimport\nAIMessage\nfrom\nlangchain.messages\nimport\nToolMessage\n# After a model makes a tool call\n# (Here, we demonstrate manually creating the messages for brevity)\nai_message\n=\nAIMessage(\ncontent\n=\n[],\ntool_calls\n=\n[{\n\"name\"\n:\n\"get_weather\"\n,\n\"args\"\n: {\n\"location\"\n:\n\"San Francisco\"\n},\n\"id\"\n:\n\"call_123\"\n}]\n)\n# Execute tool and create result message\nweather_result\n=\n\"Sunny, 72\u00b0F\"\ntool_message\n=\nToolMessage(\ncontent\n=\nweather_result,\ntool_call_id\n=\n\"call_123\"\n# Must match the call ID\n)\n# Continue conversation\nmessages\n=\n[\nHumanMessage(\n\"What's the weather in San Francisco?\"\n),\nai_message,\n# Model's tool call\ntool_message,\n# Tool execution result\n]\nresponse\n=\nmodel.invoke(messages)\n# Model processes the result\nAttributes\n\u200b\ncontent\nstring\nrequired\nThe stringified output of the tool call.\n\u200b\ntool_call_id\nstring\nrequired\nThe ID of the tool call that this message is responding to. Must match the ID of the tool call in the\nAIMessage\n.\n\u200b\nname\nstring\nrequired\nThe name of the tool that was called.\n\u200b\nartifact\ndict\nAdditional data not sent to the model but can be accessed programmatically.\nThe\nartifact\nfield stores supplementary data that won\u2019t be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model\u2019s context.\nExample: Using artifact for retrieval metadata\nFor example, a\nretrieval\ntool could retrieve a passage from a document for reference by a model. Where message\ncontent\ncontains text that the model will reference, an\nartifact\ncan contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:\nCopy\nfrom\nlangchain.messages\nimport\nToolMessage\n# Sent to model\nmessage_content\n=\n\"It was the best of times, it was the worst of times.\"\n# Artifact available downstream\nartifact\n=\n{\n\"document_id\"\n:\n\"doc_123\"\n,\n\"page\"\n:\n0\n}\ntool_message\n=\nToolMessage(\ncontent\n=\nmessage_content,\ntool_call_id\n=\n\"call_123\"\n,\nname\n=\n\"search_books\"\n,\nartifact\n=\nartifact,\n)\nSee the\nRAG tutorial\nfor an end-to-end example of building retrieval\nagents\nwith LangChain.\n\u200b\nMessage content\nYou can think of a message\u2019s content as the payload of data that gets sent to the model. Messages have a\ncontent\nattribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as\nmultimodal\ncontent and other data.\nSeparately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See\ncontent blocks\nbelow.\nLangChain chat models accept message content in the\ncontent\nattribute.\nThis may contain either:\nA string\nA list of content blocks in a provider-native format\nA list of\nLangChain\u2019s standard content blocks\nSee below for an example using\nmultimodal\ninputs:\nCopy\nfrom\nlangchain.messages\nimport\nHumanMessage\n# String content\nhuman_message\n=\nHumanMessage(\n\"Hello, how are you?\"\n)\n# Provider-native format (e.g., OpenAI)\nhuman_message\n=\nHumanMessage(\ncontent\n=\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Hello, how are you?\"\n},\n{\n\"type\"\n:\n\"image_url\"\n,\n\"image_url\"\n: {\n\"url\"\n:\n\"https://example.com/image.jpg\"\n}}\n])\n# List of standard content blocks\nhuman_message\n=\nHumanMessage(\ncontent_blocks\n=\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Hello, how are you?\"\n},\n{\n\"type\"\n:\n\"image\"\n,\n\"url\"\n:\n\"https://example.com/image.jpg\"\n},\n])\nSpecifying\ncontent_blocks\nwhen initializing a message will still populate message\ncontent\n, but provides a type-safe interface for doing so.\n\u200b\nStandard content blocks\nLangChain provides a standard representation for message content that works across providers.\nMessage objects implement a\ncontent_blocks\nproperty that will lazily parse the\ncontent\nattribute into a standard, type-safe representation. For example, messages generated from\nChatAnthropic\nor\nChatOpenAI\nwill include\nthinking\nor\nreasoning\nblocks in the format of the respective provider, but can be lazily parsed into a consistent\nReasoningContentBlock\nrepresentation:\nAnthropic\nOpenAI\nCopy\nfrom\nlangchain.messages\nimport\nAIMessage\nmessage\n=\nAIMessage(\ncontent\n=\n[\n{\n\"type\"\n:\n\"thinking\"\n,\n\"thinking\"\n:\n\"...\"\n,\n\"signature\"\n:\n\"WaUjzkyp...\"\n},\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"...\"\n},\n],\nresponse_metadata\n=\n{\n\"model_provider\"\n:\n\"anthropic\"\n}\n)\nmessage.content_blocks\nCopy\n[{'type': 'reasoning',\n'reasoning': '...',\n'extras': {'signature': 'WaUjzkyp...'}},\n{'type': 'text', 'text': '...'}]\nCopy\nfrom\nlangchain.messages\nimport\nAIMessage\nmessage\n=\nAIMessage(\ncontent\n=\n[\n{\n\"type\"\n:\n\"reasoning\"\n,\n\"id\"\n:\n\"rs_abc123\"\n,\n\"summary\"\n: [\n{\n\"type\"\n:\n\"summary_text\"\n,\n\"text\"\n:\n\"summary 1\"\n},\n{\n\"type\"\n:\n\"summary_text\"\n,\n\"text\"\n:\n\"summary 2\"\n},\n],\n},\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"...\"\n,\n\"id\"\n:\n\"msg_abc123\"\n},\n],\nresponse_metadata\n=\n{\n\"model_provider\"\n:\n\"openai\"\n}\n)\nmessage.content_blocks\nCopy\n[{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 1'},\n{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 2'},\n{'type': 'text', 'text': '...', 'id': 'msg_abc123'}]\nSee the\nintegrations guides\nto get started with the\ninference provider of your choice.\nSerializing standard content\nIf an application outside of LangChain needs access to the standard content block\nrepresentation, you can opt-in to storing content blocks in message content.\nTo do this, you can set the\nLC_OUTPUT_VERSION\nenvironment variable to\nv1\n. Or,\ninitialize any chat model with\noutput_version=\"v1\"\n:\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n,\noutput_version\n=\n\"v1\"\n)\n\u200b\nMultimodal\nMultimodality\nrefers to the ability to work with data that comes in different\nforms, such as text, audio, images, and video. LangChain includes standard types\nfor these data that can be used across providers.\nChat models\ncan accept multimodal data as input and generate\nit as output. Below we show short examples of input messages featuring multimodal data.\nExtra keys can be included top-level in the content block or nested in\n\"extras\": {\"key\": value}\n.\nOpenAI\nand\nAWS Bedrock Converse\n,\nfor example, require a filename for PDFs. See the\nprovider page\nfor your chosen model for specifics.\nImage input\nPDF document input\nAudio input\nVideo input\nCopy\n# From URL\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: [\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the content of this image.\"\n},\n{\n\"type\"\n:\n\"image\"\n,\n\"url\"\n:\n\"https://example.com/path/to/image.jpg\"\n},\n]\n}\n# From base64 data\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: [\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the content of this image.\"\n},\n{\n\"type\"\n:\n\"image\"\n,\n\"base64\"\n:\n\"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\"\n,\n\"mime_type\"\n:\n\"image/jpeg\"\n,\n},\n]\n}\n# From provider-managed File ID\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: [\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Describe the content of this image.\"\n},\n{\n\"type\"\n:\n\"image\"\n,\n\"file_id\"\n:\n\"file-abc123\"\n},\n]\n}\nNot all models support all file types. Check the model provider\u2019s\nreference\nfor supported formats and size limits.\n\u200b\nContent block reference\nContent blocks are represented (either when creating a message or accessing the\ncontent_blocks\nproperty) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:\nCore\nTextContentBlock\nPurpose:\nStandard text output\n\u200b\ntype\nstring\nrequired\nAlways\n\"text\"\n\u200b\ntext\nstring\nrequired\nThe text content\n\u200b\nannotations\nobject[]\nList of annotations for the text\n\u200b\nextras\nobject\nAdditional provider-specific data\nExample:\nCopy\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Hello world\"\n,\n\"annotations\"\n: []\n}\nReasoningContentBlock\nPurpose:\nModel reasoning steps\n\u200b\ntype\nstring\nrequired\nAlways\n\"reasoning\"\n\u200b\nreasoning\nstring\nThe reasoning content\n\u200b\nextras\nobject\nAdditional provider-specific data\nExample:\nCopy\n{\n\"type\"\n:\n\"reasoning\"\n,\n\"reasoning\"\n:\n\"The user is asking about...\"\n,\n\"extras\"\n: {\n\"signature\"\n:\n\"abc123\"\n},\n}\nMultimodal\nImageContentBlock\nPurpose:\nImage data\n\u200b\ntype\nstring\nrequired\nAlways\n\"image\"\n\u200b\nurl\nstring\nURL pointing to the image location.\n\u200b\nbase64\nstring\nBase64-encoded image data.\n\u200b\nid\nstring\nReference ID to an externally stored image (e.g., in a provider\u2019s file system or in a bucket).\n\u200b\nmime_type\nstring\nImage\nMIME type\n(e.g.,\nimage/jpeg\n,\nimage/png\n)\nAudioContentBlock\nPurpose:\nAudio data\n\u200b\ntype\nstring\nrequired\nAlways\n\"audio\"\n\u200b\nurl\nstring\nURL pointing to the audio location.\n\u200b\nbase64\nstring\nBase64-encoded audio data.\n\u200b\nid\nstring\nReference ID to an externally stored audio file (e.g., in a provider\u2019s file system or in a bucket).\n\u200b\nmime_type\nstring\nAudio\nMIME type\n(e.g.,\naudio/mpeg\n,\naudio/wav\n)\nVideoContentBlock\nPurpose:\nVideo data\n\u200b\ntype\nstring\nrequired\nAlways\n\"video\"\n\u200b\nurl\nstring\nURL pointing to the video location.\n\u200b\nbase64\nstring\nBase64-encoded video data.\n\u200b\nid\nstring\nReference ID to an externally stored video file (e.g., in a provider\u2019s file system or in a bucket).\n\u200b\nmime_type\nstring\nVideo\nMIME type\n(e.g.,\nvideo/mp4\n,\nvideo/webm\n)\nFileContentBlock\nPurpose:\nGeneric files (PDF, etc)\n\u200b\ntype\nstring\nrequired\nAlways\n\"file\"\n\u200b\nurl\nstring\nURL pointing to the file location.\n\u200b\nbase64\nstring\nBase64-encoded file data.\n\u200b\nid\nstring\nReference ID to an externally stored file (e.g., in a provider\u2019s file system or in a bucket).\n\u200b\nmime_type\nstring\nFile\nMIME type\n(e.g.,\napplication/pdf\n)\nPlainTextContentBlock\nPurpose:\nDocument text (\n.txt\n,\n.md\n)\n\u200b\ntype\nstring\nrequired\nAlways\n\"text-plain\"\n\u200b\ntext\nstring\nThe text content\n\u200b\nmime_type\nstring\nMIME type\nof the text (e.g.,\ntext/plain\n,\ntext/markdown\n)\nTool Calling\nToolCall\nPurpose:\nFunction calls\n\u200b\ntype\nstring\nrequired\nAlways\n\"tool_call\"\n\u200b\nname\nstring\nrequired\nName of the tool to call\n\u200b\nargs\nobject\nrequired\nArguments to pass to the tool\n\u200b\nid\nstring\nrequired\nUnique identifier for this tool call\nExample:\nCopy\n{\n\"type\"\n:\n\"tool_call\"\n,\n\"name\"\n:\n\"search\"\n,\n\"args\"\n: {\n\"query\"\n:\n\"weather\"\n},\n\"id\"\n:\n\"call_123\"\n}\nToolCallChunk\nPurpose:\nStreaming tool call fragments\n\u200b\ntype\nstring\nrequired\nAlways\n\"tool_call_chunk\"\n\u200b\nname\nstring\nName of the tool being called\n\u200b\nargs\nstring\nPartial tool arguments (may be incomplete JSON)\n\u200b\nid\nstring\nTool call identifier\n\u200b\nindex\nnumber | string\nPosition of this chunk in the stream\nInvalidToolCall\nPurpose:\nMalformed calls, intended to catch JSON parsing errors.\n\u200b\ntype\nstring\nrequired\nAlways\n\"invalid_tool_call\"\n\u200b\nname\nstring\nName of the tool that failed to be called\n\u200b\nargs\nobject\nArguments to pass to the tool\n\u200b\nerror\nstring\nDescription of what went wrong\nServer-Side Tool Execution\nServerToolCall\nPurpose:\nTool call that is executed server-side.\n\u200b\ntype\nstring\nrequired\nAlways\n\"server_tool_call\"\n\u200b\nid\nstring\nrequired\nAn identifier associated with the tool call.\n\u200b\nname\nstring\nrequired\nThe name of the tool to be called.\n\u200b\nargs\nstring\nrequired\nPartial tool arguments (may be incomplete JSON)\nServerToolCallChunk\nPurpose:\nStreaming server-side tool call fragments\n\u200b\ntype\nstring\nrequired\nAlways\n\"server_tool_call_chunk\"\n\u200b\nid\nstring\nAn identifier associated with the tool call.\n\u200b\nname\nstring\nName of the tool being called\n\u200b\nargs\nstring\nPartial tool arguments (may be incomplete JSON)\n\u200b\nindex\nnumber | string\nPosition of this chunk in the stream\nServerToolResult\nPurpose:\nSearch results\n\u200b\ntype\nstring\nrequired\nAlways\n\"server_tool_result\"\n\u200b\ntool_call_id\nstring\nrequired\nIdentifier of the corresponding server tool call.\n\u200b\nid\nstring\nIdentifier associated with the server tool result.\n\u200b\nstatus\nstring\nrequired\nExecution status of the server-side tool.\n\"success\"\nor\n\"error\"\n.\n\u200b\noutput\nOutput of the executed tool.\nProvider-Specific Blocks\nNonStandardContentBlock\nPurpose:\nProvider-specific escape hatch\n\u200b\ntype\nstring\nrequired\nAlways\n\"non_standard\"\n\u200b\nvalue\nobject\nrequired\nProvider-specific data structure\nUsage:\nFor experimental or provider-unique features\nAdditional provider-specific content types may be found within the\nreference documentation\nof each model provider.\nView the canonical type definitions in the\nAPI reference\n.\nContent blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.\nContent blocks are not a replacement for the\ncontent\nproperty, but rather a new property that can be used to access the content of a message in a standardized format.\n\u200b\nUse with chat models\nChat models\naccept a sequence of message objects as input and return an\nAIMessage\nas output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.\nRefer to the below guides to learn more:\nBuilt-in features for\npersisting and managing conversation histories\nStrategies for managing context windows, including\ntrimming and summarizing messages\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nModels\nPrevious\nTools\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/messages",
      "title": "Messages - Docs by LangChain",
      "heading": "Messages"
    }
  },
  {
    "page_content": "Tools - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore components\nTools\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate tools\nBasic tool definition\nCustomize tool properties\nCustom tool name\nCustom tool description\nAdvanced schema definition\nReserved argument names\nAccessing Context\nToolRuntime\nContext\nMemory (Store)\nStream Writer\nCore components\nTools\nCopy page\nCopy page\nTools extend what\nagents\ncan do\u2014letting them fetch real-time data, execute code, query external databases, and take actions in the world.\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a\nchat model\n. The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\nFor details on how models handle tool calls, see\nTool calling\n.\n\u200b\nCreate tools\n\u200b\nBasic tool definition\nThe simplest way to create a tool is with the\n@tool\ndecorator. By default, the function\u2019s docstring becomes the tool\u2019s description that helps the model understand when to use it:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\ndef\nsearch_database\n(\nquery\n:\nstr\n,\nlimit\n:\nint\n=\n10\n) ->\nstr\n:\n\"\"\"Search the customer database for records matching the query.\nArgs:\nquery: Search terms to look for\nlimit: Maximum number of results to return\n\"\"\"\nreturn\nf\n\"Found\n{\nlimit\n}\nresults for '\n{\nquery\n}\n'\"\nType hints are\nrequired\nas they define the tool\u2019s input schema. The docstring should be informative and concise to help the model understand the tool\u2019s purpose.\nServer-side tool use\nSome chat models (e.g.,\nOpenAI\n,\nAnthropic\n, and\nGemini\n) feature\nbuilt-in tools\nthat are executed server-side, such as web search and code interpreters. Refer to the\nprovider overview\nto learn how to access these tools with your specific chat model.\n\u200b\nCustomize tool properties\n\u200b\nCustom tool name\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\nCopy\n@tool\n(\n\"web_search\"\n)\n# Custom name\ndef\nsearch\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search the web for information.\"\"\"\nreturn\nf\n\"Results for:\n{\nquery\n}\n\"\nprint\n(search.name)\n# web_search\n\u200b\nCustom tool description\nOverride the auto-generated tool description for clearer model guidance:\nCopy\n@tool\n(\n\"calculator\"\n,\ndescription\n=\n\"Performs arithmetic calculations. Use this for any math problems.\"\n)\ndef\ncalc\n(\nexpression\n:\nstr\n) ->\nstr\n:\n\"\"\"Evaluate mathematical expressions.\"\"\"\nreturn\nstr\n(\neval\n(expression))\n\u200b\nAdvanced schema definition\nDefine complex inputs with Pydantic models or JSON schemas:\nPydantic model\nJSON Schema\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\ntyping\nimport\nLiteral\nclass\nWeatherInput\n(\nBaseModel\n):\n\"\"\"Input for weather queries.\"\"\"\nlocation:\nstr\n=\nField(\ndescription\n=\n\"City name or coordinates\"\n)\nunits: Literal[\n\"celsius\"\n,\n\"fahrenheit\"\n]\n=\nField(\ndefault\n=\n\"celsius\"\n,\ndescription\n=\n\"Temperature unit preference\"\n)\ninclude_forecast:\nbool\n=\nField(\ndefault\n=\nFalse\n,\ndescription\n=\n\"Include 5-day forecast\"\n)\n@tool\n(\nargs_schema\n=\nWeatherInput)\ndef\nget_weather\n(\nlocation\n:\nstr\n,\nunits\n:\nstr\n=\n\"celsius\"\n,\ninclude_forecast\n:\nbool\n=\nFalse\n) ->\nstr\n:\n\"\"\"Get current weather and optional forecast.\"\"\"\ntemp\n=\n22\nif\nunits\n==\n\"celsius\"\nelse\n72\nresult\n=\nf\n\"Current weather in\n{\nlocation\n}\n:\n{\ntemp\n}\ndegrees\n{\nunits[\n0\n].upper()\n}\n\"\nif\ninclude_forecast:\nresult\n+=\n\"\n\\n\nNext 5 days: Sunny\"\nreturn\nresult\n\u200b\nReserved argument names\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\nParameter name\nPurpose\nconfig\nReserved for passing\nRunnableConfig\nto tools internally\nruntime\nReserved for\nToolRuntime\nparameter (accessing state, context, store)\nTo access runtime information, use the\nToolRuntime\nparameter instead of naming your own arguments\nconfig\nor\nruntime\n.\n\u200b\nAccessing Context\nWhy this matters:\nTools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\nRuntime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.\nTools can access runtime information through the\nToolRuntime\nparameter, which provides:\nState\n- Mutable data that flows through execution (e.g., messages, counters, custom fields)\nContext\n- Immutable configuration like user IDs, session details, or application-specific configuration\nStore\n- Persistent long-term memory across conversations\nStream Writer\n- Stream custom updates as tools execute\nConfig\n-\nRunnableConfig\nfor the execution\nTool Call ID\n- ID of the current tool call\n\u200b\nToolRuntime\nUse\nToolRuntime\nto access all runtime information in a single parameter. Simply add\nruntime: ToolRuntime\nto your tool signature, and it will be automatically injected without being exposed to the LLM.\nToolRuntime\n: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate\nInjectedState\n,\nInjectedStore\n,\nget_runtime\n, and\nInjectedToolCallId\nannotations.\nThe runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.\nAccessing state:\nTools can access the current graph state using\nToolRuntime\n:\nCopy\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\n# Access the current conversation state\n@tool\ndef\nsummarize_conversation\n(\nruntime\n: ToolRuntime\n) ->\nstr\n:\n\"\"\"Summarize the conversation so far.\"\"\"\nmessages\n=\nruntime.state[\n\"messages\"\n]\nhuman_msgs\n=\nsum\n(\n1\nfor\nm\nin\nmessages\nif\nm.\n__class__\n.\n__name__\n==\n\"HumanMessage\"\n)\nai_msgs\n=\nsum\n(\n1\nfor\nm\nin\nmessages\nif\nm.\n__class__\n.\n__name__\n==\n\"AIMessage\"\n)\ntool_msgs\n=\nsum\n(\n1\nfor\nm\nin\nmessages\nif\nm.\n__class__\n.\n__name__\n==\n\"ToolMessage\"\n)\nreturn\nf\n\"Conversation has\n{\nhuman_msgs\n}\nuser messages,\n{\nai_msgs\n}\nAI responses, and\n{\ntool_msgs\n}\ntool results\"\n# Access custom state fields\n@tool\ndef\nget_user_preference\n(\npref_name\n:\nstr\n,\nruntime\n: ToolRuntime\n# ToolRuntime parameter is not visible to the model\n) ->\nstr\n:\n\"\"\"Get a user preference value.\"\"\"\npreferences\n=\nruntime.state.get(\n\"user_preferences\"\n, {})\nreturn\npreferences.get(pref_name,\n\"Not set\"\n)\nThe\nruntime\nparameter is hidden from the model. For the example above, the model only sees\npref_name\nin the tool schema -\nruntime\nis\nnot\nincluded in the request.\nUpdating state:\nUse\nCommand\nto update the agent\u2019s state or control the graph\u2019s execution flow:\nCopy\nfrom\nlanggraph.types\nimport\nCommand\nfrom\nlangchain.messages\nimport\nRemoveMessage\nfrom\nlanggraph.graph.message\nimport\nREMOVE_ALL_MESSAGES\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\n# Update the conversation history by removing all messages\n@tool\ndef\nclear_conversation\n() -> Command:\n\"\"\"Clear the conversation history.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [RemoveMessage(\nid\n=\nREMOVE_ALL_MESSAGES\n)],\n}\n)\n# Update the user_name in the agent state\n@tool\ndef\nupdate_user_name\n(\nnew_name\n:\nstr\n,\nruntime\n: ToolRuntime\n) -> Command:\n\"\"\"Update the user's name.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"user_name\"\n: new_name})\n\u200b\nContext\nAccess immutable configuration and contextual data like user IDs, session details, or application-specific configuration through\nruntime.context\n.\nTools can access runtime context through\nToolRuntime\n:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nUSER_DATABASE\n=\n{\n\"user123\"\n: {\n\"name\"\n:\n\"Alice Johnson\"\n,\n\"account_type\"\n:\n\"Premium\"\n,\n\"balance\"\n:\n5000\n,\n\"email\"\n:\n\"\n[email\u00a0protected]\n\"\n},\n\"user456\"\n: {\n\"name\"\n:\n\"Bob Smith\"\n,\n\"account_type\"\n:\n\"Standard\"\n,\n\"balance\"\n:\n1200\n,\n\"email\"\n:\n\"\n[email\u00a0protected]\n\"\n}\n}\n@dataclass\nclass\nUserContext\n:\nuser_id:\nstr\n@tool\ndef\nget_account_info\n(\nruntime\n: ToolRuntime[UserContext]) ->\nstr\n:\n\"\"\"Get the current user's account information.\"\"\"\nuser_id\n=\nruntime.context.user_id\nif\nuser_id\nin\nUSER_DATABASE\n:\nuser\n=\nUSER_DATABASE\n[user_id]\nreturn\nf\n\"Account holder:\n{\nuser[\n'name'\n]\n}\n\\n\nType:\n{\nuser[\n'account_type'\n]\n}\n\\n\nBalance: $\n{\nuser[\n'balance'\n]\n}\n\"\nreturn\n\"User not found\"\nmodel\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[get_account_info],\ncontext_schema\n=\nUserContext,\nsystem_prompt\n=\n\"You are a financial assistant.\"\n)\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's my current balance?\"\n}]},\ncontext\n=\nUserContext(\nuser_id\n=\n\"user123\"\n)\n)\n\u200b\nMemory (Store)\nAccess persistent data across conversations using the store. The store is accessed via\nruntime.store\nand allows you to save and retrieve user-specific or application-specific data.\nTools can access and update the store through\nToolRuntime\n:\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\n# Access memory\n@tool\ndef\nget_user_info\n(\nuser_id\n:\nstr\n,\nruntime\n: ToolRuntime) ->\nstr\n:\n\"\"\"Look up user info.\"\"\"\nstore\n=\nruntime.store\nuser_info\n=\nstore.get((\n\"users\"\n,), user_id)\nreturn\nstr\n(user_info.value)\nif\nuser_info\nelse\n\"Unknown user\"\n# Update memory\n@tool\ndef\nsave_user_info\n(\nuser_id\n:\nstr\n,\nuser_info\n: dict[\nstr\n, Any],\nruntime\n: ToolRuntime) ->\nstr\n:\n\"\"\"Save user info.\"\"\"\nstore\n=\nruntime.store\nstore.put((\n\"users\"\n,), user_id, user_info)\nreturn\n\"Successfully saved user info.\"\nstore\n=\nInMemoryStore()\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[get_user_info, save_user_info],\nstore\n=\nstore\n)\n# First session: save user info\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Save the following user: userid: abc123, name: Foo, age: 25, email:\n[email\u00a0protected]\n\"\n}]\n})\n# Second session: get user info\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Get user info for user with id 'abc123'\"\n}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email:\n[email\u00a0protected]\nSee all 42 lines\n\u200b\nStream Writer\nStream custom updates from tools as they execute using\nruntime.stream_writer\n. This is useful for providing real-time feedback to users about what a tool is doing.\nCopy\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\n@tool\ndef\nget_weather\n(\ncity\n:\nstr\n,\nruntime\n: ToolRuntime) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nwriter\n=\nruntime.stream_writer\n# Stream custom updates as the tool executes\nwriter(\nf\n\"Looking up data for city:\n{\ncity\n}\n\"\n)\nwriter(\nf\n\"Acquired data for city:\n{\ncity\n}\n\"\n)\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nIf you use\nruntime.stream_writer\ninside your tool, the tool must be invoked within a LangGraph execution context. See\nStreaming\nfor more details.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nMessages\nPrevious\nShort-term memory\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/tools",
      "title": "Tools - Docs by LangChain",
      "heading": "Tools"
    }
  },
  {
    "page_content": "Short-term memory - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore components\nShort-term memory\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nUsage\nIn production\nCustomizing agent memory\nCommon patterns\nTrim messages\nDelete messages\nSummarize messages\nAccess memory\nTools\nRead short-term memory in a tool\nWrite short-term memory from tools\nPrompt\nBefore model\nAfter model\nCore components\nShort-term memory\nCopy page\nCopy page\n\u200b\nOverview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\nShort term memory lets your application remember previous interactions within a single thread or conversation.\nA thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\nConversation history is the most common form of short-term memory. Long conversations pose a challenge to today\u2019s LLMs; a full history may not fit inside an LLM\u2019s context window, resulting in an context loss or errors.\nEven if your model supports the full context length, most LLMs still perform poorly over long contexts. They get \u201cdistracted\u201d by stale or off-topic content, all while suffering from slower response times and higher costs.\nChat models accept context using\nmessages\n, which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or \u201cforget\u201d stale information.\n\u200b\nUsage\nTo add short-term memory (thread-level persistence) to an agent, you need to specify a\ncheckpointer\nwhen creating an agent.\nLangChain\u2019s agent manages short-term memory as a part of your agent\u2019s state.\nBy storing these in the graph\u2019s state, the agent can access the full context for a given conversation while maintaining separation between different threads.\nState is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\nShort-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nagent\n=\ncreate_agent(\n\"gpt-5\"\n,\ntools\n=\n[get_user_info],\ncheckpointer\n=\nInMemorySaver(),\n)\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi! My name is Bob.\"\n}]},\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}},\n)\n\u200b\nIn production\nIn production, use a checkpointer backed by a database:\nCopy\npip\ninstall\nlanggraph-checkpoint-postgres\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.checkpoint.postgres\nimport\nPostgresSaver\nDB_URI\n=\n\"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith\nPostgresSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer:\ncheckpointer.setup()\n# auto create tables in PostgresSql\nagent\n=\ncreate_agent(\n\"gpt-5\"\n,\ntools\n=\n[get_user_info],\ncheckpointer\n=\ncheckpointer,\n)\n\u200b\nCustomizing agent memory\nBy default, agents use\nAgentState\nto manage short term memory, specifically the conversation history via a\nmessages\nkey.\nYou can extend\nAgentState\nto add additional fields. Custom state schemas are passed to\ncreate_agent\nusing the\nstate_schema\nparameter.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent, AgentState\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nclass\nCustomAgentState\n(\nAgentState\n):\nuser_id:\nstr\npreferences:\ndict\nagent\n=\ncreate_agent(\n\"gpt-5\"\n,\ntools\n=\n[get_user_info],\nstate_schema\n=\nCustomAgentState,\ncheckpointer\n=\nInMemorySaver(),\n)\n# Custom state can be passed in invoke\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hello\"\n}],\n\"user_id\"\n:\n\"user_123\"\n,\n\"preferences\"\n: {\n\"theme\"\n:\n\"dark\"\n}\n},\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}})\n\u200b\nCommon patterns\nWith\nshort-term memory\nenabled, long conversations can exceed the LLM\u2019s context window. Common solutions are:\nTrim messages\nRemove first or last N messages (before calling LLM)\nDelete messages\nDelete messages from LangGraph state permanently\nSummarize messages\nSummarize earlier messages in the history and replace them with a summary\nCustom strategies\nCustom strategies (e.g., message filtering, etc.)\nThis allows the agent to keep track of the conversation without exceeding the LLM\u2019s context window.\n\u200b\nTrim messages\nMost LLMs have a maximum supported context window (denominated in tokens).\nOne way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you\u2019re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the\nstrategy\n(e.g., keep the last\nmax_tokens\n) to use for handling the boundary.\nTo trim message history in an agent, use the\n@before_model\nmiddleware decorator:\nCopy\nfrom\nlangchain.messages\nimport\nRemoveMessage\nfrom\nlanggraph.graph.message\nimport\nREMOVE_ALL_MESSAGES\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlangchain.agents\nimport\ncreate_agent, AgentState\nfrom\nlangchain.agents.middleware\nimport\nbefore_model\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\ntyping\nimport\nAny\n@before_model\ndef\ntrim_messages\n(\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\n\"\"\"Keep only the last few messages to fit context window.\"\"\"\nmessages\n=\nstate[\n\"messages\"\n]\nif\nlen\n(messages)\n<=\n3\n:\nreturn\nNone\n# No changes needed\nfirst_msg\n=\nmessages[\n0\n]\nrecent_messages\n=\nmessages[\n-\n3\n:]\nif\nlen\n(messages)\n%\n2\n==\n0\nelse\nmessages[\n-\n4\n:]\nnew_messages\n=\n[first_msg]\n+\nrecent_messages\nreturn\n{\n\"messages\"\n: [\nRemoveMessage(\nid\n=\nREMOVE_ALL_MESSAGES\n),\n*\nnew_messages\n]\n}\nagent\n=\ncreate_agent(\nyour_model_here,\ntools\n=\nyour_tools_here,\nmiddleware\n=\n[trim_messages],\ncheckpointer\n=\nInMemorySaver(),\n)\nconfig: RunnableConfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\nagent.invoke({\n\"messages\"\n:\n\"hi, my name is bob\"\n}, config)\nagent.invoke({\n\"messages\"\n:\n\"write a short poem about cats\"\n}, config)\nagent.invoke({\n\"messages\"\n:\n\"now do the same but for dogs\"\n}, config)\nfinal_response\n=\nagent.invoke({\n\"messages\"\n:\n\"what's my name?\"\n}, config)\nfinal_response[\n\"messages\"\n][\n-\n1\n].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n\u200b\nDelete messages\nYou can delete messages from the graph state to manage the message history.\nThis is useful when you want to remove specific messages or clear the entire message history.\nTo delete messages from the graph state, you can use the\nRemoveMessage\n.\nFor\nRemoveMessage\nto work, you need to use a state key with\nadd_messages\nreducer\n.\nThe default\nAgentState\nprovides this.\nTo remove specific messages:\nCopy\nfrom\nlangchain.messages\nimport\nRemoveMessage\ndef\ndelete_messages\n(\nstate\n):\nmessages\n=\nstate[\n\"messages\"\n]\nif\nlen\n(messages)\n>\n2\n:\n# remove the earliest two messages\nreturn\n{\n\"messages\"\n: [RemoveMessage(\nid\n=\nm.id)\nfor\nm\nin\nmessages[:\n2\n]]}\nTo remove\nall\nmessages:\nCopy\nfrom\nlanggraph.graph.message\nimport\nREMOVE_ALL_MESSAGES\ndef\ndelete_messages\n(\nstate\n):\nreturn\n{\n\"messages\"\n: [RemoveMessage(\nid\n=\nREMOVE_ALL_MESSAGES\n)]}\nWhen deleting messages,\nmake sure\nthat the resulting message history is valid. Check the limitations of the LLM provider you\u2019re using. For example:\nSome providers expect message history to start with a\nuser\nmessage\nMost providers require\nassistant\nmessages with tool calls to be followed by corresponding\ntool\nresult messages.\nCopy\nfrom\nlangchain.messages\nimport\nRemoveMessage\nfrom\nlangchain.agents\nimport\ncreate_agent, AgentState\nfrom\nlangchain.agents.middleware\nimport\nafter_model\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\n@after_model\ndef\ndelete_old_messages\n(\nstate\n: AgentState,\nruntime\n: Runtime) ->\ndict\n|\nNone\n:\n\"\"\"Remove old messages to keep conversation manageable.\"\"\"\nmessages\n=\nstate[\n\"messages\"\n]\nif\nlen\n(messages)\n>\n2\n:\n# remove the earliest two messages\nreturn\n{\n\"messages\"\n: [RemoveMessage(\nid\n=\nm.id)\nfor\nm\nin\nmessages[:\n2\n]]}\nreturn\nNone\nagent\n=\ncreate_agent(\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nsystem_prompt\n=\n\"Please be concise and to the point.\"\n,\nmiddleware\n=\n[delete_old_messages],\ncheckpointer\n=\nInMemorySaver(),\n)\nconfig: RunnableConfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi! I'm bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nprint\n([(message.type, message.content)\nfor\nmessage\nin\nevent[\n\"messages\"\n]])\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nprint\n([(message.type, message.content)\nfor\nmessage\nin\nevent[\n\"messages\"\n]])\nCopy\n[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n[('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n\u200b\nSummarize messages\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.\nBecause of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\nTo summarize message history in an agent, use the built-in\nSummarizationMiddleware\n:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nSummarizationMiddleware\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\ncheckpointer\n=\nInMemorySaver()\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nSummarizationMiddleware(\nmodel\n=\n\"gpt-5-mini\"\n,\ntrigger\n=\n(\n\"tokens\"\n,\n4000\n),\nkeep\n=\n(\n\"messages\"\n,\n20\n)\n)\n],\ncheckpointer\n=\ncheckpointer,\n)\nconfig: RunnableConfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\nagent.invoke({\n\"messages\"\n:\n\"hi, my name is bob\"\n}, config)\nagent.invoke({\n\"messages\"\n:\n\"write a short poem about cats\"\n}, config)\nagent.invoke({\n\"messages\"\n:\n\"now do the same but for dogs\"\n}, config)\nfinal_response\n=\nagent.invoke({\n\"messages\"\n:\n\"what's my name?\"\n}, config)\nfinal_response[\n\"messages\"\n][\n-\n1\n].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\nYour name is Bob!\n\"\"\"\nSee\nSummarizationMiddleware\nfor more configuration options.\n\u200b\nAccess memory\nYou can access and modify the short-term memory (state) of an agent in several ways:\n\u200b\nTools\n\u200b\nRead short-term memory in a tool\nAccess short term memory (state) in a tool using the\nToolRuntime\nparameter.\nThe\ntool_runtime\nparameter is hidden from the tool signature (so the model doesn\u2019t see it), but the tool can access the state through it.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent, AgentState\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nclass\nCustomState\n(\nAgentState\n):\nuser_id:\nstr\n@tool\ndef\nget_user_info\n(\nruntime\n: ToolRuntime\n) ->\nstr\n:\n\"\"\"Look up user info.\"\"\"\nuser_id\n=\nruntime.state[\n\"user_id\"\n]\nreturn\n\"User is John Smith\"\nif\nuser_id\n==\n\"user_123\"\nelse\n\"Unknown user\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_user_info],\nstate_schema\n=\nCustomState,\n)\nresult\n=\nagent.invoke({\n\"messages\"\n:\n\"look up user information\"\n,\n\"user_id\"\n:\n\"user_123\"\n})\nprint\n(result[\n\"messages\"\n][\n-\n1\n].content)\n# > User is John Smith.\n\u200b\nWrite short-term memory from tools\nTo modify the agent\u2019s short-term memory (state) during execution, you can return state updates directly from the tools.\nThis is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\nCopy\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlangchain.messages\nimport\nToolMessage\nfrom\nlangchain.agents\nimport\ncreate_agent, AgentState\nfrom\nlanggraph.types\nimport\nCommand\nfrom\npydantic\nimport\nBaseModel\nclass\nCustomState\n(\nAgentState\n):\nuser_name:\nstr\nclass\nCustomContext\n(\nBaseModel\n):\nuser_id:\nstr\n@tool\ndef\nupdate_user_info\n(\nruntime\n: ToolRuntime[CustomContext, CustomState],\n) -> Command:\n\"\"\"Look up and update user info.\"\"\"\nuser_id\n=\nruntime.context.user_id\nname\n=\n\"John Smith\"\nif\nuser_id\n==\n\"user_123\"\nelse\n\"Unknown user\"\nreturn\nCommand(\nupdate\n=\n{\n\"user_name\"\n: name,\n# update the message history\n\"messages\"\n: [\nToolMessage(\n\"Successfully looked up user information\"\n,\ntool_call_id\n=\nruntime.tool_call_id\n)\n]\n})\n@tool\ndef\ngreet\n(\nruntime\n: ToolRuntime[CustomContext, CustomState]\n) ->\nstr\n|\nCommand:\n\"\"\"Use this to greet the user once you found their info.\"\"\"\nuser_name\n=\nruntime.state.get(\n\"user_name\"\n,\nNone\n)\nif\nuser_name\nis\nNone\n:\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\n\"Please call the 'update_user_info' tool it will get and update the user's name.\"\n,\ntool_call_id\n=\nruntime.tool_call_id\n)\n]\n})\nreturn\nf\n\"Hello\n{\nuser_name\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[update_user_info, greet],\nstate_schema\n=\nCustomState,\ncontext_schema\n=\nCustomContext,\n)\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"greet the user\"\n}]},\ncontext\n=\nCustomContext(\nuser_id\n=\n\"user_123\"\n),\n)\n\u200b\nPrompt\nAccess short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\ntyping\nimport\nTypedDict\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\nclass\nCustomContext\n(\nTypedDict\n):\nuser_name:\nstr\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get the weather in a city.\"\"\"\nreturn\nf\n\"The weather in\n{\ncity\n}\nis always sunny!\"\n@dynamic_prompt\ndef\ndynamic_system_prompt\n(\nrequest\n: ModelRequest) ->\nstr\n:\nuser_name\n=\nrequest.runtime.context[\n\"user_name\"\n]\nsystem_prompt\n=\nf\n\"You are a helpful assistant. Address the user as\n{\nuser_name\n}\n.\"\nreturn\nsystem_prompt\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_weather],\nmiddleware\n=\n[dynamic_system_prompt],\ncontext_schema\n=\nCustomContext,\n)\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in SF?\"\n}]},\ncontext\n=\nCustomContext(\nuser_name\n=\n\"John Smith\"\n),\n)\nfor\nmsg\nin\nresult[\n\"messages\"\n]:\nmsg.pretty_print()\nOutput\nCopy\n================================\nHuman\nMessage\n=================================\nWhat\nis\nthe\nweather\nin\nSF?\n==================================\nAi\nMessage\n==================================\nTool\nCalls:\nget_weather\n(call_WFQlOGn4b2yoJrv7cih342FG)\nCall\nID:\ncall_WFQlOGn4b2yoJrv7cih342FG\nArgs:\ncity:\nSan\nFrancisco\n=================================\nTool\nMessage\n=================================\nName:\nget_weather\nThe\nweather\nin\nSan\nFrancisco\nis\nalways\nsunny!\n==================================\nAi\nMessage\n==================================\nHi\nJohn\nSmith,\nthe\nweather\nin\nSan\nFrancisco\nis\nalways\nsunny!\n\u200b\nBefore model\nAccess short term memory (state) in\n@before_model\nmiddleware to process messages before model calls.\nCopy\nfrom\nlangchain.messages\nimport\nRemoveMessage\nfrom\nlanggraph.graph.message\nimport\nREMOVE_ALL_MESSAGES\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlangchain.agents\nimport\ncreate_agent, AgentState\nfrom\nlangchain.agents.middleware\nimport\nbefore_model\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\ntyping\nimport\nAny\n@before_model\ndef\ntrim_messages\n(\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\n\"\"\"Keep only the last few messages to fit context window.\"\"\"\nmessages\n=\nstate[\n\"messages\"\n]\nif\nlen\n(messages)\n<=\n3\n:\nreturn\nNone\n# No changes needed\nfirst_msg\n=\nmessages[\n0\n]\nrecent_messages\n=\nmessages[\n-\n3\n:]\nif\nlen\n(messages)\n%\n2\n==\n0\nelse\nmessages[\n-\n4\n:]\nnew_messages\n=\n[first_msg]\n+\nrecent_messages\nreturn\n{\n\"messages\"\n: [\nRemoveMessage(\nid\n=\nREMOVE_ALL_MESSAGES\n),\n*\nnew_messages\n]\n}\nagent\n=\ncreate_agent(\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[trim_messages],\ncheckpointer\n=\nInMemorySaver()\n)\nconfig: RunnableConfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\nagent.invoke({\n\"messages\"\n:\n\"hi, my name is bob\"\n}, config)\nagent.invoke({\n\"messages\"\n:\n\"write a short poem about cats\"\n}, config)\nagent.invoke({\n\"messages\"\n:\n\"now do the same but for dogs\"\n}, config)\nfinal_response\n=\nagent.invoke({\n\"messages\"\n:\n\"what's my name?\"\n}, config)\nfinal_response[\n\"messages\"\n][\n-\n1\n].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n\u200b\nAfter model\nAccess short term memory (state) in\n@after_model\nmiddleware to process messages after model calls.\nCopy\nfrom\nlangchain.messages\nimport\nRemoveMessage\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlangchain.agents\nimport\ncreate_agent, AgentState\nfrom\nlangchain.agents.middleware\nimport\nafter_model\nfrom\nlanggraph.runtime\nimport\nRuntime\n@after_model\ndef\nvalidate_response\n(\nstate\n: AgentState,\nruntime\n: Runtime) ->\ndict\n|\nNone\n:\n\"\"\"Remove messages containing sensitive words.\"\"\"\nSTOP_WORDS\n=\n[\n\"password\"\n,\n\"secret\"\n]\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nif\nany\n(word\nin\nlast_message.content\nfor\nword\nin\nSTOP_WORDS\n):\nreturn\n{\n\"messages\"\n: [RemoveMessage(\nid\n=\nlast_message.id)]}\nreturn\nNone\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[validate_response],\ncheckpointer\n=\nInMemorySaver(),\n)\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nTools\nPrevious\nStreaming\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/short-term-memory",
      "title": "Short-term memory - Docs by LangChain",
      "heading": "Short-term memory"
    }
  },
  {
    "page_content": "Streaming - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore components\nStreaming\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nSupported stream modes\nAgent progress\nLLM tokens\nCustom updates\nStream multiple modes\nCommon patterns\nStreaming tool calls\nAccessing completed messages\nStreaming with human-in-the-loop\nStreaming from sub-agents\nDisable streaming\nRelated\nCore components\nStreaming\nCopy page\nCopy page\nLangChain implements a streaming system to surface real-time updates.\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\u200b\nOverview\nLangChain\u2019s streaming system lets you surface live feedback from agent runs to your application.\nWhat\u2019s possible with LangChain streaming:\nStream agent progress\n\u2014 get state updates after each agent step.\nStream LLM tokens\n\u2014 stream language model tokens as they\u2019re generated.\nStream custom updates\n\u2014 emit user-defined signals (e.g.,\n\"Fetched 10/100 records\"\n).\nStream multiple modes\n\u2014 choose from\nupdates\n(agent progress),\nmessages\n(LLM tokens + metadata), or\ncustom\n(arbitrary user data).\nSee the\ncommon patterns\nsection below for additional end-to-end examples.\n\u200b\nSupported stream modes\nPass one or more of the following stream modes as a list to the\nstream\nor\nastream\nmethods:\nMode\nDescription\nupdates\nStreams state updates after each agent step. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.\nmessages\nStreams tuples of\n(token, metadata)\nfrom any graph nodes where an LLM is invoked.\ncustom\nStreams custom data from inside your graph nodes using the stream writer.\n\u200b\nAgent progress\nTo stream agent progress, use the\nstream\nor\nastream\nmethods with\nstream_mode=\"updates\"\n. This emits an event after every agent step.\nFor example, if you have an agent that calls a tool once, you should see the following updates:\nLLM node\n:\nAIMessage\nwith tool call requests\nTool node\n:\nToolMessage\nwith execution result\nLLM node\n: Final AI response\nStreaming agent progress\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_weather],\n)\nfor\nchunk\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in SF?\"\n}]},\nstream_mode\n=\n\"updates\"\n,\n):\nfor\nstep, data\nin\nchunk.items():\nprint\n(\nf\n\"step:\n{\nstep\n}\n\"\n)\nprint\n(\nf\n\"content:\n{\ndata[\n'messages'\n][\n-\n1\n].content_blocks\n}\n\"\n)\nOutput\nCopy\nstep:\nmodel\ncontent:\n[{\n'type'\n:\n'tool_call',\n'name':\n'get_weather',\n'args':\n{'city':\n'San Francisco'},\n'id':\n'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]\nstep:\ntools\ncontent:\n[{\n'type'\n:\n'text',\n'text':\n\"It's always sunny in San Francisco!\"}]\nstep:\nmodel\ncontent:\n[{\n'type'\n:\n'text',\n'text':\n'It's\nalways\nsunny\nin\nSan\nFrancisco!'}]\n\u200b\nLLM tokens\nTo stream tokens as they are produced by the LLM, use\nstream_mode=\"messages\"\n. Below you can see the output of the agent streaming tool calls and the final response.\nStreaming LLM tokens\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_weather],\n)\nfor\ntoken, metadata\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in SF?\"\n}]},\nstream_mode\n=\n\"messages\"\n,\n):\nprint\n(\nf\n\"node:\n{\nmetadata[\n'langgraph_node'\n]\n}\n\"\n)\nprint\n(\nf\n\"content:\n{\ntoken.content_blocks\n}\n\"\n)\nprint\n(\n\"\n\\n\n\"\n)\nOutput\nCopy\nnode:\nmodel\ncontent:\n[{\n'type'\n:\n'tool_call_chunk',\n'id':\n'call_vbCyBcP8VuneUzyYlSBZZsVa',\n'name':\n'get_weather',\n'args':\n'',\n'index':\n0\n}]\nnode:\nmodel\ncontent:\n[{\n'type'\n:\n'tool_call_chunk',\n'id':\nNone,\n'name':\nNone,\n'args':\n'{\"',\n'index':\n0\n}]\nnode:\nmodel\ncontent:\n[{\n'type'\n:\n'tool_call_chunk',\n'id':\nNone,\n'name':\nNone,\n'args':\n'city',\n'index':\n0\n}]\nnode:\nmodel\ncontent:\n[{\n'type'\n:\n'tool_call_chunk',\n'id':\nNone,\n'name':\nNone,\n'args':\n'\":\"',\n'index':\n0\n}]\nnode:\nmodel\ncontent:\n[{\n'type'\n:\n'tool_call_chunk',\n'id':\nNone,\n'name':\nNone,\n'args':\n'San',\n'index':\n0\n}]\nnode:\nmodel\ncontent:\n[{\n'type'\n:\n'tool_call_chunk',\n'id':\nNone,\n'name':\nNone,\n'args':\n' Francisco',\n'index':\n0\n}]\nnode:\nmodel\ncontent:\n[{\n'type'\n:\n'tool_call_chunk',\n'id':\nNone,\n'name':\nNone,\n'args':\n'\"}',\n'index':\n0\n}]\nnode:\nmodel\ncontent:\n[]\nnode:\ntools\ncontent:\n[{\n'type'\n:\n'text',\n'text':\n\"It's always sunny in San Francisco!\"}]\nnode:\nmodel\ncontent:\n[]\nnode:\nmodel\ncontent:\n[{\n'type'\n:\n'text',\n'text':\n'Here'}]\nnode:\nmodel\ncontent:\n[{\n'type'\n:\n'text',\n'text':\n''s'}]\nnode: model\ncontent: [{'type': 'text', 'text': '\nwhat'}]\nnode: model\ncontent: [{'type': 'text', 'text': '\nI'}]\nnode: model\ncontent: [{'type': 'text', 'text': '\ngot'}]\nnode: model\ncontent: [{'type': 'text', 'text': ':'}]\nnode: model\ncontent: [{'type': 'text', 'text': '\n\"'}]\nnode: model\ncontent: [{'type': 'text', 'text': \"It's\"}]\nnode: model\ncontent: [{'type': 'text', 'text': '\nalways'}]\nnode: model\ncontent: [{'type': 'text', 'text': '\nsunny'}]\nnode: model\ncontent: [{'type': 'text', 'text': '\nin'}]\nnode: model\ncontent: [{'type': 'text', 'text': '\nSan'}]\nnode: model\ncontent: [{'type': 'text', 'text': '\nFrancisco'}]\nnode: model\ncontent: [{'type': 'text', 'text': '!\"\\n\\n'}]\nSee all 94 lines\n\u200b\nCustom updates\nTo stream updates from tools as they are executed, you can use\nget_stream_writer\n.\nStreaming custom updates\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.config\nimport\nget_stream_writer\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nwriter\n=\nget_stream_writer()\n# stream any arbitrary data\nwriter(\nf\n\"Looking up data for city:\n{\ncity\n}\n\"\n)\nwriter(\nf\n\"Acquired data for city:\n{\ncity\n}\n\"\n)\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\n)\nfor\nchunk\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in SF?\"\n}]},\nstream_mode\n=\n\"custom\"\n):\nprint\n(chunk)\nOutput\nCopy\nLooking\nup\ndata\nfor\ncity:\nSan\nFrancisco\nAcquired\ndata\nfor\ncity:\nSan\nFrancisco\nIf you add\nget_stream_writer\ninside your tool, you won\u2019t be able to invoke the tool outside of a LangGraph execution context.\n\u200b\nStream multiple modes\nYou can specify multiple streaming modes by passing stream mode as a list:\nstream_mode=[\"updates\", \"custom\"]\n.\nThe streamed outputs will be tuples of\n(mode, chunk)\nwhere\nmode\nis the name of the stream mode and\nchunk\nis the data streamed by that mode.\nStreaming multiple modes\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.config\nimport\nget_stream_writer\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nwriter\n=\nget_stream_writer()\nwriter(\nf\n\"Looking up data for city:\n{\ncity\n}\n\"\n)\nwriter(\nf\n\"Acquired data for city:\n{\ncity\n}\n\"\n)\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_weather],\n)\nfor\nstream_mode, chunk\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in SF?\"\n}]},\nstream_mode\n=\n[\n\"updates\"\n,\n\"custom\"\n]\n):\nprint\n(\nf\n\"stream_mode:\n{\nstream_mode\n}\n\"\n)\nprint\n(\nf\n\"content:\n{\nchunk\n}\n\"\n)\nprint\n(\n\"\n\\n\n\"\n)\nOutput\nCopy\nstream_mode:\nupdates\ncontent:\n{'model':\n{'messages':\n[AIMessage(content=\n''\n,\nresponse_metadata={'token_usage':\n{'completion_tokens':\n280,\n'prompt_tokens':\n132,\n'total_tokens':\n412,\n'completion_tokens_details':\n{'accepted_prediction_tokens':\n0,\n'audio_tokens':\n0,\n'reasoning_tokens':\n256,\n'rejected_prediction_tokens':\n0\n},\n'prompt_tokens_details':\n{'audio_tokens':\n0,\n'cached_tokens':\n0\n}},\n'model_provider':\n'openai',\n'model_name':\n'gpt-5-mini-2025-08-07',\n'system_fingerprint':\nNone,\n'id':\n'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7',\n'service_tier':\n'default',\n'finish_reason':\n'tool_calls',\n'logprobs':\nNone},\nid='lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0',\ntool_calls=[{'name':\n'get_weather',\n'args':\n{'city':\n'San Francisco'},\n'id':\n'call_KTNQIftMrl9vgNwEfAJMVu7r',\n'type':\n'tool_call'}],\nusage_metadata={'input_tokens':\n132,\n'output_tokens':\n280,\n'total_tokens':\n412,\n'input_token_details':\n{'audio':\n0,\n'cache_read':\n0\n},\n'output_token_details':\n{'audio':\n0,\n'reasoning':\n256\n}}\n)]}}\nstream_mode:\ncustom\ncontent:\nLooking\nup\ndata\nfor\ncity:\nSan\nFrancisco\nstream_mode:\ncustom\ncontent:\nAcquired\ndata\nfor\ncity:\nSan\nFrancisco\nstream_mode:\nupdates\ncontent:\n{'tools':\n{'messages':\n[ToolMessage(content=\n\"It's always sunny in San Francisco!\"\n,\nname='get_weather',\ntool_call_id='call_KTNQIftMrl9vgNwEfAJMVu7r'\n)]}}\nstream_mode:\nupdates\ncontent:\n{'model':\n{'messages':\n[AIMessage(content=\n'San Francisco weather: It'\ns\nalways\nsunny\nin\nSan\nFrancisco!\n\\n\\n\n', response_metadata={'token_usage': {'completion_tokens': 764, 'prompt_tokens': 168, 'total_tokens': 932, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage_metadata={'input_tokens': 168, 'output_tokens': 764, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}}\n\u200b\nCommon patterns\nBelow are examples showing common use cases for streaming.\n\u200b\nStreaming tool calls\nYou may want to stream both:\nPartial JSON as\ntool calls\nare generated\nThe completed, parsed tool calls that are executed\nSpecifying\nstream_mode=\"messages\"\nwill stream incremental\nmessage chunks\ngenerated by all LLM calls in the agent. To access the completed messages with parsed tool calls:\nIf those messages are tracked in the\nstate\n(as in the model node of\ncreate_agent\n), use\nstream_mode=[\"messages\", \"updates\"]\nto access completed messages through\nstate updates\n(demonstrated below).\nIf those messages are not tracked in the state, use\ncustom updates\nor aggregate the chunks during the streaming loop (\nnext section\n).\nRefer to the section below on\nstreaming from sub-agents\nif your agent includes multiple LLMs.\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.messages\nimport\nAIMessage, AIMessageChunk, AnyMessage, ToolMessage\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\n\"openai:gpt-5.2\"\n,\ntools\n=\n[get_weather])\ndef\n_render_message_chunk\n(\ntoken\n: AIMessageChunk) ->\nNone\n:\nif\ntoken.text:\nprint\n(token.text,\nend\n=\n\"|\"\n)\nif\ntoken.tool_call_chunks:\nprint\n(token.tool_call_chunks)\n# N.B. all content is available through token.content_blocks\ndef\n_render_completed_message\n(\nmessage\n: AnyMessage) ->\nNone\n:\nif\nisinstance\n(message, AIMessage)\nand\nmessage.tool_calls:\nprint\n(\nf\n\"Tool calls:\n{\nmessage.tool_calls\n}\n\"\n)\nif\nisinstance\n(message, ToolMessage):\nprint\n(\nf\n\"Tool response:\n{\nmessage.content_blocks\n}\n\"\n)\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in Boston?\"\n}\nfor\nstream_mode, data\nin\nagent.stream(\n{\n\"messages\"\n: [input_message]},\nstream_mode\n=\n[\n\"messages\"\n,\n\"updates\"\n],\n):\nif\nstream_mode\n==\n\"messages\"\n:\ntoken, metadata\n=\ndata\nif\nisinstance\n(token, AIMessageChunk):\n_render_message_chunk(token)\nif\nstream_mode\n==\n\"updates\"\n:\nfor\nsource, update\nin\ndata.items():\nif\nsource\nin\n(\n\"model\"\n,\n\"tools\"\n):\n# `source` captures node name\n_render_completed_message(update[\n\"messages\"\n][\n-\n1\n])\nOutput\nCopy\n[{\n'name'\n:\n'get_weather'\n,\n'args'\n:\n''\n,\n'id'\n:\n'call_D3Orjr89KgsLTZ9hTzYv7Hpf'\n,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'{\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'city'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\":\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'Boston'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\"}'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\nTool\ncalls:\n[{\n'name'\n:\n'get_weather',\n'args':\n{'city':\n'Boston'},\n'id':\n'call_D3Orjr89KgsLTZ9hTzYv7Hpf',\n'type':\n'tool_call'}]\nTool\nresponse:\n[{\n'type'\n:\n'text',\n'text':\n\"It's always sunny in Boston!\"}]\nThe\n|\nweather\n|\nin\n|\nBoston\n|\nis\n|\n**\n|\nsun\n|\nny\n|**|\n.\n|\nSee all 9 lines\n\u200b\nAccessing completed messages\nIf completed messages are tracked in an agent\u2019s\nstate\n, you can use\nstream_mode=[\"messages\", \"updates\"]\nas demonstrated\nabove\nto access completed messages during streaming.\nIn some cases, completed messages are not reflected in\nstate updates\n. If you have access to the agent internals, you can use\ncustom updates\nto access these messages during streaming. Otherwise, you can aggregate message chunks in the streaming loop (see below).\nConsider the below example, where we incorporate a\nstream writer\ninto a simplified\nguardrail middleware\n. This middleware demonstrates tool calling to generate a structured \u201csafe / unsafe\u201d evaluation (one could also use\nstructured outputs\nfor this):\nCopy\nfrom\ntyping\nimport\nAny, Literal\nfrom\nlangchain.agents.middleware\nimport\nafter_agent, AgentState\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\nlangchain.messages\nimport\nAIMessage\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.config\nimport\nget_stream_writer\nfrom\npydantic\nimport\nBaseModel\nclass\nResponseSafety\n(\nBaseModel\n):\n\"\"\"Evaluate a response as safe or unsafe.\"\"\"\nevaluation: Literal[\n\"safe\"\n,\n\"unsafe\"\n]\nsafety_model\n=\ninit_chat_model(\n\"openai:gpt-5.2\"\n)\n@after_agent\n(\ncan_jump_to\n=\n[\n\"end\"\n])\ndef\nsafety_guardrail\n(\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\n\"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\nstream_writer\n=\nget_stream_writer()\n# Get the model response\nif\nnot\nstate[\n\"messages\"\n]:\nreturn\nNone\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nif\nnot\nisinstance\n(last_message, AIMessage):\nreturn\nNone\n# Use another model to evaluate safety\nmodel_with_tools\n=\nsafety_model.bind_tools([ResponseSafety],\ntool_choice\n=\n\"any\"\n)\nresult\n=\nmodel_with_tools.invoke(\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"Evaluate this AI response as generally safe or unsafe.\"\n,\n}\n],\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nf\n\"AI response:\n{\nlast_message.text\n}\n\"\n},\n)\nstream_writer(result)\ntool_call\n=\nresult.tool_calls[\n0\n]\nif\ntool_call[\n\"args\"\n][\n\"evaluation\"\n]\n==\n\"unsafe\"\n:\nlast_message.content\n=\n\"I cannot provide that response. Please rephrase your request.\"\nreturn\nNone\nWe can then incorporate this middleware into our agent and include its custom stream events:\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.messages\nimport\nAIMessageChunk, AIMessage, AnyMessage\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"openai:gpt-5.2\"\n,\ntools\n=\n[get_weather],\nmiddleware\n=\n[safety_guardrail],\n)\ndef\n_render_message_chunk\n(\ntoken\n: AIMessageChunk) ->\nNone\n:\nif\ntoken.text:\nprint\n(token.text,\nend\n=\n\"|\"\n)\nif\ntoken.tool_call_chunks:\nprint\n(token.tool_call_chunks)\ndef\n_render_completed_message\n(\nmessage\n: AnyMessage) ->\nNone\n:\nif\nisinstance\n(message, AIMessage)\nand\nmessage.tool_calls:\nprint\n(\nf\n\"Tool calls:\n{\nmessage.tool_calls\n}\n\"\n)\nif\nisinstance\n(message, ToolMessage):\nprint\n(\nf\n\"Tool response:\n{\nmessage.content_blocks\n}\n\"\n)\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in Boston?\"\n}\nfor\nstream_mode, data\nin\nagent.stream(\n{\n\"messages\"\n: [input_message]},\nstream_mode\n=\n[\n\"messages\"\n,\n\"updates\"\n,\n\"custom\"\n],\n):\nif\nstream_mode\n==\n\"messages\"\n:\ntoken, metadata\n=\ndata\nif\nisinstance\n(token, AIMessageChunk):\n_render_message_chunk(token)\nif\nstream_mode\n==\n\"updates\"\n:\nfor\nsource, update\nin\ndata.items():\nif\nsource\nin\n(\n\"model\"\n,\n\"tools\"\n):\n_render_completed_message(update[\n\"messages\"\n][\n-\n1\n])\nif\nstream_mode\n==\n\"custom\"\n:\n# access completed message in stream\nprint\n(\nf\n\"Tool calls:\n{\ndata.tool_calls\n}\n\"\n)\nOutput\nCopy\n[{\n'name'\n:\n'get_weather'\n,\n'args'\n:\n''\n,\n'id'\n:\n'call_je6LWgxYzuZ84mmoDalTYMJC'\n,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'{\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'city'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\":\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'Boston'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\"}'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\nTool\ncalls:\n[{\n'name'\n:\n'get_weather',\n'args':\n{'city':\n'Boston'},\n'id':\n'call_je6LWgxYzuZ84mmoDalTYMJC',\n'type':\n'tool_call'}]\nTool\nresponse:\n[{\n'type'\n:\n'text',\n'text':\n\"It's always sunny in Boston!\"}]\nThe\n|\nweather\n|\nin\n|\n**\n|\nBoston\n|**|\nis\n|\n**\n|\nsun\n|\nny\n|**|\n.\n|\n[{\n'name'\n:\n'ResponseSafety'\n,\n'args'\n:\n''\n,\n'id'\n:\n'call_O8VJIbOG4Q9nQF0T8ltVi58O'\n,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'{\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'evaluation'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\":\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'unsafe'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\"}'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\nTool\ncalls:\n[{\n'name'\n:\n'ResponseSafety',\n'args':\n{'evaluation':\n'unsafe'},\n'id':\n'call_O8VJIbOG4Q9nQF0T8ltVi58O',\n'type':\n'tool_call'}]\nSee all 15 lines\nAlternatively, if you aren\u2019t able to add custom events to the stream, you can aggregate message chunks within the streaming loop:\nCopy\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in Boston?\"\n}\nfull_message\n=\nNone\nfor\nstream_mode, data\nin\nagent.stream(\n{\n\"messages\"\n: [input_message]},\nstream_mode\n=\n[\n\"messages\"\n,\n\"updates\"\n],\n):\nif\nstream_mode\n==\n\"messages\"\n:\ntoken, metadata\n=\ndata\nif\nisinstance\n(token, AIMessageChunk):\n_render_message_chunk(token)\nfull_message\n=\ntoken\nif\nfull_message\nis\nNone\nelse\nfull_message\n+\ntoken\nif\ntoken.chunk_position\n==\n\"last\"\n:\nif\nfull_message.tool_calls:\nprint\n(\nf\n\"Tool calls:\n{\nfull_message.tool_calls\n}\n\"\n)\nfull_message\n=\nNone\nif\nstream_mode\n==\n\"updates\"\n:\nfor\nsource, update\nin\ndata.items():\nif\nsource\n==\n\"tools\"\n:\n_render_completed_message(update[\n\"messages\"\n][\n-\n1\n])\n\u200b\nStreaming with human-in-the-loop\nTo handle human-in-the-loop\ninterrupts\n, we build on the\nabove example\n:\nWe configure the agent with\nhuman-in-the-loop middleware and a checkpointer\nWe collect interrupts generated during the\n\"updates\"\nstream mode\nWe respond to those interrupts with a\ncommand\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nHumanInTheLoopMiddleware\nfrom\nlangchain.messages\nimport\nAIMessage, AIMessageChunk, AnyMessage, ToolMessage\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.types\nimport\nCommand, Interrupt\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\ncheckpointer\n=\nInMemorySaver()\nagent\n=\ncreate_agent(\n\"openai:gpt-5.2\"\n,\ntools\n=\n[get_weather],\nmiddleware\n=\n[\nHumanInTheLoopMiddleware(\ninterrupt_on\n=\n{\n\"get_weather\"\n:\nTrue\n}),\n],\ncheckpointer\n=\ncheckpointer,\n)\ndef\n_render_message_chunk\n(\ntoken\n: AIMessageChunk) ->\nNone\n:\nif\ntoken.text:\nprint\n(token.text,\nend\n=\n\"|\"\n)\nif\ntoken.tool_call_chunks:\nprint\n(token.tool_call_chunks)\ndef\n_render_completed_message\n(\nmessage\n: AnyMessage) ->\nNone\n:\nif\nisinstance\n(message, AIMessage)\nand\nmessage.tool_calls:\nprint\n(\nf\n\"Tool calls:\n{\nmessage.tool_calls\n}\n\"\n)\nif\nisinstance\n(message, ToolMessage):\nprint\n(\nf\n\"Tool response:\n{\nmessage.content_blocks\n}\n\"\n)\ndef\n_render_interrupt\n(\ninterrupt\n: Interrupt) ->\nNone\n:\ninterrupts\n=\ninterrupt.value\nfor\nrequest\nin\ninterrupts[\n\"action_requests\"\n]:\nprint\n(request[\n\"description\"\n])\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: (\n\"Can you look up the weather in Boston and San Francisco?\"\n),\n}\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"some_id\"\n}}\ninterrupts\n=\n[]\nfor\nstream_mode, data\nin\nagent.stream(\n{\n\"messages\"\n: [input_message]},\nconfig\n=\nconfig,\nstream_mode\n=\n[\n\"messages\"\n,\n\"updates\"\n],\n):\nif\nstream_mode\n==\n\"messages\"\n:\ntoken, metadata\n=\ndata\nif\nisinstance\n(token, AIMessageChunk):\n_render_message_chunk(token)\nif\nstream_mode\n==\n\"updates\"\n:\nfor\nsource, update\nin\ndata.items():\nif\nsource\nin\n(\n\"model\"\n,\n\"tools\"\n):\n_render_completed_message(update[\n\"messages\"\n][\n-\n1\n])\nif\nsource\n==\n\"__interrupt__\"\n:\ninterrupts.extend(update)\n_render_interrupt(update[\n0\n])\nOutput\nCopy\n[{\n'name'\n:\n'get_weather'\n,\n'args'\n:\n''\n,\n'id'\n:\n'call_GOwNaQHeqMixay2qy80padfE'\n,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'{\"ci'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'ty\": '\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\"Bosto'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'n\"}'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n:\n'get_weather'\n,\n'args'\n:\n''\n,\n'id'\n:\n'call_Ndb4jvWm2uMA0JDQXu37wDH6'\n,\n'index'\n: 1,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'{\"ci'\n,\n'id'\n: None,\n'index'\n: 1,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'ty\": '\n,\n'id'\n: None,\n'index'\n: 1,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\"San F'\n,\n'id'\n: None,\n'index'\n: 1,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'ranc'\n,\n'id'\n: None,\n'index'\n: 1,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'isco\"'\n,\n'id'\n: None,\n'index'\n: 1,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'}'\n,\n'id'\n: None,\n'index'\n: 1,\n'type'\n:\n'tool_call_chunk'\n}]\nTool\ncalls:\n[{\n'name'\n:\n'get_weather',\n'args':\n{'city':\n'Boston'},\n'id':\n'call_GOwNaQHeqMixay2qy80padfE',\n'type':\n'tool_call'},\n{'name':\n'get_weather',\n'args':\n{'city':\n'San Francisco'},\n'id':\n'call_Ndb4jvWm2uMA0JDQXu37wDH6',\n'type':\n'tool_call'}]\nTool\nexecution\nrequires\napproval\nTool:\nget_weather\nArgs:\n{'city':\n'Boston'}\nTool\nexecution\nrequires\napproval\nTool:\nget_weather\nArgs:\n{'city':\n'San Francisco'}\nSee all 21 lines\nWe next collect a\ndecision\nfor each interrupt. Importantly, the order of decisions must match the order of actions we collected.\nTo illustrate, we will edit one tool call and accept the other:\nCopy\ndef\n_get_interrupt_decisions\n(\ninterrupt\n: Interrupt) -> list[\ndict\n]:\nreturn\n[\n{\n\"type\"\n:\n\"edit\"\n,\n\"edited_action\"\n: {\n\"name\"\n:\n\"get_weather\"\n,\n\"args\"\n: {\n\"city\"\n:\n\"Boston, U.K.\"\n},\n},\n}\nif\n\"boston\"\nin\nrequest[\n\"description\"\n].lower()\nelse\n{\n\"type\"\n:\n\"approve\"\n}\nfor\nrequest\nin\ninterrupt.value[\n\"action_requests\"\n]\n]\ndecisions\n=\n{}\nfor\ninterrupt\nin\ninterrupts:\ndecisions[interrupt.id]\n=\n{\n\"decisions\"\n: _get_interrupt_decisions(interrupt)\n}\ndecisions\nOutput\nCopy\n{\n'a96c40474e429d661b5b32a8d86f0f3e'\n:\n{\n'decisions'\n:\n[\n{\n'type'\n:\n'edit',\n'edited_action'\n:\n{\n'name'\n:\n'get_weather',\n'args'\n:\n{'city':\n'Boston, U.K.'}\n}\n},\n{\n'type'\n:\n'approve'},\n]\n}\n}\nWe can then resume by passing a\ncommand\ninto the same streaming loop:\nCopy\ninterrupts\n=\n[]\nfor\nstream_mode, data\nin\nagent.stream(\nCommand(\nresume\n=\ndecisions),\nconfig\n=\nconfig,\nstream_mode\n=\n[\n\"messages\"\n,\n\"updates\"\n],\n):\n# Streaming loop is unchanged\nif\nstream_mode\n==\n\"messages\"\n:\ntoken, metadata\n=\ndata\nif\nisinstance\n(token, AIMessageChunk):\n_render_message_chunk(token)\nif\nstream_mode\n==\n\"updates\"\n:\nfor\nsource, update\nin\ndata.items():\nif\nsource\nin\n(\n\"model\"\n,\n\"tools\"\n):\n_render_completed_message(update[\n\"messages\"\n][\n-\n1\n])\nif\nsource\n==\n\"__interrupt__\"\n:\ninterrupts.extend(update)\n_render_interrupt(update[\n0\n])\nOutput\nCopy\nTool\nresponse:\n[{\n'type'\n:\n'text',\n'text':\n\"It's always sunny in Boston, U.K.!\"}]\nTool\nresponse:\n[{\n'type'\n:\n'text',\n'text':\n\"It's always sunny in San Francisco!\"}]\n-\n|\n**\n|\nBoston\n|**|\n:\n|\nIt\n|\n\u2019s\n|\nalways\n|\nsunny\n|\nin\n|\nBoston\n|\n,\n|\nU\n|\n.K\n|\n.\n|\n|\n-\n|\n**\n|\nSan\n|\nFrancisco\n|**|\n:\n|\nIt\n|\n\u2019s\n|\nalways\n|\nsunny\n|\nin\n|\nSan\n|\nFrancisco\n|!|\n\u200b\nStreaming from sub-agents\nWhen there are multiple LLMs at any point in an agent, it\u2019s often necessary to disambiguate the source of messages as they are generated.\nTo do this, you can initialize any model with\ntags\n. These tags are then available in metadata when streaming in\n\"messages\"\nmode.\nBelow, we update the\nstreaming tool calls\nexample:\nWe replace our tool with a\ncall_weather_agent\ntool that invokes an agent internally\nWe add a string tag to this LLM and the outer\n\u201csupervisor\u201d\nLLM\nWe specify\nsubgraphs=True\nwhen creating the stream\nOur stream processing is identical to before, but we add logic to keep track of what LLM is active\nIf streaming tokens from sub-agents is not needed, you can initialize the sub-agent with a\nname\n. This name is accessible on messages generated by the sub-agent when streaming\nupdates\n.\nFirst we construct the agent:\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain.messages\nimport\nAIMessage, AnyMessage\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nweather_model\n=\ninit_chat_model(\n\"openai:gpt-5.2\"\n,\ntags\n=\n[\n\"weather_sub_agent\"\n],\n)\nweather_agent\n=\ncreate_agent(\nmodel\n=\nweather_model,\ntools\n=\n[get_weather])\ndef\ncall_weather_agent\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Query the weather agent.\"\"\"\nresult\n=\nweather_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]\n})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].text\nsupervisor_model\n=\ninit_chat_model(\n\"openai:gpt-5.2\"\n,\ntags\n=\n[\n\"supervisor\"\n],\n)\nagent\n=\ncreate_agent(\nmodel\n=\nsupervisor_model,\ntools\n=\n[call_weather_agent])\nNext, we add logic to the streaming loop to report which agent is emitting tokens:\nCopy\ndef\n_render_message_chunk\n(\ntoken\n: AIMessageChunk) ->\nNone\n:\nif\ntoken.text:\nprint\n(token.text,\nend\n=\n\"|\"\n)\nif\ntoken.tool_call_chunks:\nprint\n(token.tool_call_chunks)\ndef\n_render_completed_message\n(\nmessage\n: AnyMessage) ->\nNone\n:\nif\nisinstance\n(message, AIMessage)\nand\nmessage.tool_calls:\nprint\n(\nf\n\"Tool calls:\n{\nmessage.tool_calls\n}\n\"\n)\nif\nisinstance\n(message, ToolMessage):\nprint\n(\nf\n\"Tool response:\n{\nmessage.content_blocks\n}\n\"\n)\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in Boston?\"\n}\ncurrent_agent\n=\nNone\nfor\n_, stream_mode, data\nin\nagent.stream(\n{\n\"messages\"\n: [input_message]},\nstream_mode\n=\n[\n\"messages\"\n,\n\"updates\"\n],\nsubgraphs\n=\nTrue\n,\n):\nif\nstream_mode\n==\n\"messages\"\n:\ntoken, metadata\n=\ndata\nif\ntags\n:=\nmetadata.get(\n\"tags\"\n, []):\nthis_agent\n=\ntags[\n0\n]\nif\nthis_agent\n!=\ncurrent_agent:\nprint\n(\nf\n\"\ud83e\udd16\n{\nthis_agent\n}\n: \"\n)\ncurrent_agent\n=\nthis_agent\nif\nisinstance\n(token, AIMessage):\n_render_message_chunk(token)\nif\nstream_mode\n==\n\"updates\"\n:\nfor\nsource, update\nin\ndata.items():\nif\nsource\nin\n(\n\"model\"\n,\n\"tools\"\n):\n_render_completed_message(update[\n\"messages\"\n][\n-\n1\n])\nOutput\nCopy\n\ud83e\udd16\nsupervisor:\n[{\n'name'\n:\n'call_weather_agent'\n,\n'args'\n:\n''\n,\n'id'\n:\n'call_asorzUf0mB6sb7MiKfgojp7I'\n,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'{\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'query'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\":\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'Boston'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n' weather'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n' right'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n' now'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n' and'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n\" today's\"\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n' forecast'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\"}'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\nTool\ncalls:\n[{\n'name'\n:\n'call_weather_agent',\n'args':\n{'query':\n\"Boston weather right now and today's forecast\"},\n'id':\n'call_asorzUf0mB6sb7MiKfgojp7I',\n'type':\n'tool_call'}]\n\ud83e\udd16\nweather_sub_agent:\n[{\n'name'\n:\n'get_weather'\n,\n'args'\n:\n''\n,\n'id'\n:\n'call_LZ89lT8fW6w8vqck5pZeaDIx'\n,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'{\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'city'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\":\"'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'Boston'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\n[{\n'name'\n: None,\n'args'\n:\n'\"}'\n,\n'id'\n: None,\n'index'\n: 0,\n'type'\n:\n'tool_call_chunk'\n}]\nTool\ncalls:\n[{\n'name'\n:\n'get_weather',\n'args':\n{'city':\n'Boston'},\n'id':\n'call_LZ89lT8fW6w8vqck5pZeaDIx',\n'type':\n'tool_call'}]\nTool\nresponse:\n[{\n'type'\n:\n'text',\n'text':\n\"It's always sunny in Boston!\"}]\nBoston\n|\nweather\n|\nright\n|\nnow\n|\n:\n|\n**\n|\nSunny\n|**|\n.\n|\nToday\n|\n\u2019s\n|\nforecast\n|\nfor\n|\nBoston\n|\n:\n|\n**\n|\nSunny\n|\nall\n|\nday\n|**|\n.\n|\nTool\nresponse:\n[{\n'type'\n:\n'text',\n'text':\n'Boston weather right now: **Sunny**.\\n\\nToday\u2019s forecast for Boston: **Sunny all day**.'}]\n\ud83e\udd16\nsupervisor:\nBoston\n|\nweather\n|\nright\n|\nnow\n|\n:\n|\n**\n|\nSunny\n|**|\n.\n|\nToday\n|\n\u2019s\n|\nforecast\n|\nfor\n|\nBoston\n|\n:\n|\n**\n|\nSunny\n|\nall\n|\nday\n|**|\n.\n|\nSee all 30 lines\n\u200b\nDisable streaming\nIn some applications you might need to disable streaming of individual tokens for a given model. This is useful when:\nWorking with\nmulti-agent\nsystems to control which agents stream their output\nMixing models that support streaming with those that do not\nDeploying to\nLangSmith\nand wanting to prevent certain model outputs from being streamed to the client\nSet\nstreaming=False\nwhen initializing the model.\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\nstreaming\n=\nFalse\n)\nWhen deploying to LangSmith, set\nstreaming=False\non any models whose output you don\u2019t want streamed to the client. This is configured in your graph code before deployment.\nNot all chat model integrations support the\nstreaming\nparameter. If your model doesn\u2019t support it, use\ndisable_streaming=True\ninstead. This parameter is available on all chat models via the base class.\nSee the\nLangGraph streaming guide\nfor more details.\n\u200b\nRelated\nStreaming with chat models\n\u2014 Stream tokens directly from a chat model without using an agent or graph\nStreaming with human-in-the-loop\n\u2014 Stream agent progress while handling interrupts for human review\nLangGraph streaming\n\u2014 Advanced streaming options including\nvalues\n,\ndebug\nmodes, and subgraph streaming\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nShort-term memory\nPrevious\nStructured output\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/streaming",
      "title": "Streaming - Docs by LangChain",
      "heading": "Streaming"
    }
  },
  {
    "page_content": "Structured output - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore components\nStructured output\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nResponse Format\nProvider strategy\nTool calling strategy\nCustom tool message content\nError handling\nMultiple structured outputs error\nSchema validation error\nError handling strategies\nCore components\nStructured output\nCopy page\nCopy page\nStructured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.\nLangChain\u2019s\ncreate_agent\nhandles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it\u2019s captured, validated, and returned in the\n'structured_response'\nkey of the agent\u2019s state.\nCopy\ndef\ncreate_agent\n(\n...\nresponse_format\n: Union[\nToolStrategy[StructuredResponseT],\nProviderStrategy[StructuredResponseT],\ntype[StructuredResponseT],\n]\n\u200b\nResponse Format\nControls how the agent returns structured data:\nToolStrategy[StructuredResponseT]\n: Uses tool calling for structured output\nProviderStrategy[StructuredResponseT]\n: Uses provider-native structured output\ntype[StructuredResponseT]\n: Schema type - automatically selects best strategy based on model capabilities\nNone\n: No structured output\nWhen a schema type is provided directly, LangChain automatically chooses:\nProviderStrategy\nfor models supporting native structured output (e.g.\nOpenAI\n,\nAnthropic\n, or\nGrok\n).\nToolStrategy\nfor all other models.\nSupport for native structured output features is read dynamically from the model\u2019s\nprofile data\nif using\nlangchain>=1.1\n. If data are not available, use another condition or specify manually:\nCopy\ncustom_profile\n=\n{\n\"structured_output\"\n:\nTrue\n,\n# ...\n}\nmodel\n=\ninit_chat_model(\n\"...\"\n,\nprofile\n=\ncustom_profile)\nIf tools are specified, the model must support simultaneous use of tools and structured output.\nThe structured response is returned in the\nstructured_response\nkey of the agent\u2019s final state.\n\u200b\nProvider strategy\nSome model providers support structured output natively through their APIs (e.g. OpenAI, Grok, Gemini). This is the most reliable method when available.\nTo use this strategy, configure a\nProviderStrategy\n:\nCopy\nclass\nProviderStrategy\n(Generic[SchemaT]):\nschema: type[SchemaT]\nstrict:\nbool\n|\nNone\n=\nNone\nThe\nstrict\nparam requires\nlangchain>=1.2\n.\n\u200b\nschema\nrequired\nThe schema defining the structured output format. Supports:\nPydantic models\n:\nBaseModel\nsubclasses with field validation\nDataclasses\n: Python dataclasses with type annotations\nTypedDict\n: Typed dictionary classes\nJSON Schema\n: Dictionary with JSON schema specification\n\u200b\nstrict\nOptional boolean parameter to enable strict schema adherence. Supported by some providers (e.g.,\nOpenAI\nand\nxAI\n). Defaults to\nNone\n(disabled).\nLangChain automatically uses\nProviderStrategy\nwhen you pass a schema type directly to\ncreate_agent.response_format\nand the model supports native structured output:\nPydantic Model\nDataclass\nTypedDict\nJSON Schema\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\nlangchain.agents\nimport\ncreate_agent\nclass\nContactInfo\n(\nBaseModel\n):\n\"\"\"Contact information for a person.\"\"\"\nname:\nstr\n=\nField(\ndescription\n=\n\"The name of the person\"\n)\nemail:\nstr\n=\nField(\ndescription\n=\n\"The email address of the person\"\n)\nphone:\nstr\n=\nField(\ndescription\n=\n\"The phone number of the person\"\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5\"\n,\nresponse_format\n=\nContactInfo\n# Auto-selects ProviderStrategy\n)\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Extract contact info from: John Doe,\n[email\u00a0protected]\n, (555) 123-4567\"\n}]\n})\nprint\n(result[\n\"structured_response\"\n])\n# ContactInfo(name='John Doe', email='\n[email\u00a0protected]\n', phone='(555) 123-4567')\nProvider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.\nIf the provider natively supports structured output for your model choice, it is functionally equivalent to write\nresponse_format=ProductReview\ninstead of\nresponse_format=ProviderStrategy(ProductReview)\n. In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.\n\u200b\nTool calling strategy\nFor models that don\u2019t support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models.\nTo use this strategy, configure a\nToolStrategy\n:\nCopy\nclass\nToolStrategy\n(Generic[SchemaT]):\nschema: type[SchemaT]\ntool_message_content:\nstr\n|\nNone\nhandle_errors: Union[\nbool\n,\nstr\n,\ntype[\nException\n],\ntuple[type[\nException\n],\n...\n],\nCallable[[\nException\n],\nstr\n],\n]\n\u200b\nschema\nrequired\nThe schema defining the structured output format. Supports:\nPydantic models\n:\nBaseModel\nsubclasses with field validation\nDataclasses\n: Python dataclasses with type annotations\nTypedDict\n: Typed dictionary classes\nJSON Schema\n: Dictionary with JSON schema specification\nUnion types\n: Multiple schema options. The model will choose the most appropriate schema based on the context.\n\u200b\ntool_message_content\nCustom content for the tool message returned when structured output is generated.\nIf not provided, defaults to a message showing the structured response data.\n\u200b\nhandle_errors\nError handling strategy for structured output validation failures. Defaults to\nTrue\n.\nTrue\n: Catch all errors with default error template\nstr\n: Catch all errors with this custom message\ntype[Exception]\n: Only catch this exception type with default message\ntuple[type[Exception], ...]\n: Only catch these exception types with default message\nCallable[[Exception], str]\n: Custom function that returns error message\nFalse\n: No retry, let exceptions propagate\nPydantic Model\nDataclass\nTypedDict\nJSON Schema\nUnion Types\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.structured_output\nimport\nToolStrategy\nclass\nProductReview\n(\nBaseModel\n):\n\"\"\"Analysis of a product review.\"\"\"\nrating:\nint\n|\nNone\n=\nField(\ndescription\n=\n\"The rating of the product\"\n,\nge\n=\n1\n,\nle\n=\n5\n)\nsentiment: Literal[\n\"positive\"\n,\n\"negative\"\n]\n=\nField(\ndescription\n=\n\"The sentiment of the review\"\n)\nkey_points: list[\nstr\n]\n=\nField(\ndescription\n=\n\"The key points of the review. Lowercase, 1-3 words each.\"\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5\"\n,\ntools\n=\ntools,\nresponse_format\n=\nToolStrategy(ProductReview)\n)\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"\n}]\n})\nresult[\n\"structured_response\"\n]\n# ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n\u200b\nCustom tool message content\nThe\ntool_message_content\nparameter allows you to customize the message that appears in the conversation history when structured output is generated:\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.structured_output\nimport\nToolStrategy\nclass\nMeetingAction\n(\nBaseModel\n):\n\"\"\"Action items extracted from a meeting transcript.\"\"\"\ntask:\nstr\n=\nField(\ndescription\n=\n\"The specific task to be completed\"\n)\nassignee:\nstr\n=\nField(\ndescription\n=\n\"Person responsible for the task\"\n)\npriority: Literal[\n\"low\"\n,\n\"medium\"\n,\n\"high\"\n]\n=\nField(\ndescription\n=\n\"Priority level\"\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5\"\n,\ntools\n=\n[],\nresponse_format\n=\nToolStrategy(\nschema\n=\nMeetingAction,\ntool_message_content\n=\n\"Action item captured and added to meeting notes!\"\n)\n)\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"From our meeting: Sarah needs to update the project timeline as soon as possible\"\n}]\n})\nCopy\n================================ Human Message =================================\nFrom our meeting: Sarah needs to update the project timeline as soon as possible\n================================== Ai Message ==================================\nTool Calls:\nMeetingAction (call_1)\nCall ID: call_1\nArgs:\ntask: Update the project timeline\nassignee: Sarah\npriority: high\n================================= Tool Message =================================\nName: MeetingAction\nAction item captured and added to meeting notes!\nWithout\ntool_message_content\n, our final\nToolMessage\nwould be:\nCopy\n================================= Tool Message =================================\nName: MeetingAction\nReturning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}\n\u200b\nError handling\nModels can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.\n\u200b\nMultiple structured outputs error\nWhen a model incorrectly calls multiple structured output tools, the agent provides error feedback in a\nToolMessage\nand prompts the model to retry:\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\ntyping\nimport\nUnion\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.structured_output\nimport\nToolStrategy\nclass\nContactInfo\n(\nBaseModel\n):\nname:\nstr\n=\nField(\ndescription\n=\n\"Person's name\"\n)\nemail:\nstr\n=\nField(\ndescription\n=\n\"Email address\"\n)\nclass\nEventDetails\n(\nBaseModel\n):\nevent_name:\nstr\n=\nField(\ndescription\n=\n\"Name of the event\"\n)\ndate:\nstr\n=\nField(\ndescription\n=\n\"Event date\"\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5\"\n,\ntools\n=\n[],\nresponse_format\n=\nToolStrategy(Union[ContactInfo, EventDetails])\n# Default: handle_errors=True\n)\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Extract info: John Doe (\n[email\u00a0protected]\n) is organizing Tech Conference on March 15th\"\n}]\n})\nCopy\n================================ Human Message =================================\nExtract info: John Doe (\n[email\u00a0protected]\n) is organizing Tech Conference on March 15th\nNone\n================================== Ai Message ==================================\nTool Calls:\nContactInfo (call_1)\nCall ID: call_1\nArgs:\nname: John Doe\nemail:\n[email\u00a0protected]\nEventDetails (call_2)\nCall ID: call_2\nArgs:\nevent_name: Tech Conference\ndate: March 15th\n================================= Tool Message =================================\nName: ContactInfo\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\nPlease fix your mistakes.\n================================= Tool Message =================================\nName: EventDetails\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\nPlease fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\nContactInfo (call_3)\nCall ID: call_3\nArgs:\nname: John Doe\nemail:\n[email\u00a0protected]\n================================= Tool Message =================================\nName: ContactInfo\nReturning structured response: {'name': 'John Doe', 'email': '\n[email\u00a0protected]\n'}\n\u200b\nSchema validation error\nWhen structured output doesn\u2019t match the expected schema, the agent provides specific error feedback:\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.structured_output\nimport\nToolStrategy\nclass\nProductRating\n(\nBaseModel\n):\nrating:\nint\n|\nNone\n=\nField(\ndescription\n=\n\"Rating from 1-5\"\n,\nge\n=\n1\n,\nle\n=\n5\n)\ncomment:\nstr\n=\nField(\ndescription\n=\n\"Review comment\"\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5\"\n,\ntools\n=\n[],\nresponse_format\n=\nToolStrategy(ProductRating),\n# Default: handle_errors=True\nsystem_prompt\n=\n\"You are a helpful assistant that parses product reviews. Do not make any field or value up.\"\n)\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Parse this: Amazing product, 10/10!\"\n}]\n})\nCopy\n================================ Human Message =================================\nParse this: Amazing product, 10/10!\n================================== Ai Message ==================================\nTool Calls:\nProductRating (call_1)\nCall ID: call_1\nArgs:\nrating: 10\ncomment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\nError: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating.rating\nInput should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\nPlease fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\nProductRating (call_2)\nCall ID: call_2\nArgs:\nrating: 5\ncomment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\nReturning structured response: {'rating': 5, 'comment': 'Amazing product'}\n\u200b\nError handling strategies\nYou can customize how errors are handled using the\nhandle_errors\nparameter:\nCustom error message:\nCopy\nToolStrategy(\nschema\n=\nProductRating,\nhandle_errors\n=\n\"Please provide a valid rating between 1-5 and include a comment.\"\n)\nIf\nhandle_errors\nis a string, the agent will\nalways\nprompt the model to re-try with a fixed tool message:\nCopy\n================================= Tool Message =================================\nName: ProductRating\nPlease provide a valid rating between 1-5 and include a comment.\nHandle specific exceptions only:\nCopy\nToolStrategy(\nschema\n=\nProductRating,\nhandle_errors\n=\nValueError\n# Only retry on ValueError, raise others\n)\nIf\nhandle_errors\nis an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.\nHandle multiple exception types:\nCopy\nToolStrategy(\nschema\n=\nProductRating,\nhandle_errors\n=\n(\nValueError\n,\nTypeError\n)\n# Retry on ValueError and TypeError\n)\nIf\nhandle_errors\nis a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised.\nCustom error handler function:\nCopy\nfrom\nlangchain.agents.structured_output\nimport\nStructuredOutputValidationError\nfrom\nlangchain.agents.structured_output\nimport\nMultipleStructuredOutputsError\ndef\ncustom_error_handler\n(\nerror\n:\nException\n) ->\nstr\n:\nif\nisinstance\n(error, StructuredOutputValidationError):\nreturn\n\"There was an issue with the format. Try again.\"\nelif\nisinstance\n(error, MultipleStructuredOutputsError):\nreturn\n\"Multiple structured outputs were returned. Pick the most relevant one.\"\nelse\n:\nreturn\nf\n\"Error:\n{\nstr\n(error)\n}\n\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5\"\n,\ntools\n=\n[],\nresponse_format\n=\nToolStrategy(\nschema\n=\nUnion[ContactInfo, EventDetails],\nhandle_errors\n=\ncustom_error_handler\n)\n# Default: handle_errors=True\n)\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Extract info: John Doe (\n[email\u00a0protected]\n) is organizing Tech Conference on March 15th\"\n}]\n})\nfor\nmsg\nin\nresult[\n'messages'\n]:\n# If message is actually a ToolMessage object (not a dict), check its class name\nif\ntype\n(msg).\n__name__\n==\n\"ToolMessage\"\n:\nprint\n(msg.content)\n# If message is a dictionary or you want a fallback\nelif\nisinstance\n(msg,\ndict\n)\nand\nmsg.get(\n'tool_call_id'\n):\nprint\n(msg[\n'content'\n])\nOn\nStructuredOutputValidationError\n:\nCopy\n================================= Tool Message =================================\nName: ToolStrategy\nThere was an issue with the format. Try again.\nOn\nMultipleStructuredOutputsError\n:\nCopy\n================================= Tool Message =================================\nName: ToolStrategy\nMultiple structured outputs were returned. Pick the most relevant one.\nOn other errors:\nCopy\n================================= Tool Message =================================\nName: ToolStrategy\nError: <error message>\nNo error handling:\nCopy\nresponse_format\n=\nToolStrategy(\nschema\n=\nProductRating,\nhandle_errors\n=\nFalse\n# All errors raised\n)\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nStreaming\nPrevious\nOverview\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/structured-output",
      "title": "Structured output - Docs by LangChain",
      "heading": "Structured output"
    }
  },
  {
    "page_content": "Overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMiddleware\nOverview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nThe agent loop\nAdditional resources\nMiddleware\nOverview\nCopy page\nControl and customize agent execution at every step\nCopy page\nMiddleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:\nTracking agent behavior with logging, analytics, and debugging.\nTransforming prompts,\ntool selection\n, and output formatting.\nAdding\nretries\n,\nfallbacks\n, and early termination logic.\nApplying\nrate limits\n, guardrails, and\nPII detection\n.\nAdd middleware by passing them to\ncreate_agent\n:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nSummarizationMiddleware, HumanInTheLoopMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[\nSummarizationMiddleware(\n...\n),\nHumanInTheLoopMiddleware(\n...\n)\n],\n)\n\u200b\nThe agent loop\nThe core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\nMiddleware exposes hooks before and after each of those steps:\n\u200b\nAdditional resources\nBuilt-in middleware\nExplore built-in middleware for common use cases.\nCustom middleware\nBuild your own middleware with hooks and decorators.\nMiddleware API reference\nComplete API reference for middleware.\nTesting agents\nTest your agents with LangSmith.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nStructured output\nPrevious\nBuilt-in middleware\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/middleware/overview",
      "title": "Overview - Docs by LangChain",
      "heading": "Overview"
    }
  },
  {
    "page_content": "Built-in middleware - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMiddleware\nBuilt-in middleware\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nProvider-agnostic middleware\nSummarization\nHuman-in-the-loop\nModel call limit\nTool call limit\nModel fallback\nPII detection\nCustom PII types\nTo-do list\nLLM tool selector\nTool retry\nModel retry\nLLM tool emulator\nContext editing\nShell tool\nFile search\nProvider-specific middleware\nMiddleware\nBuilt-in middleware\nCopy page\nPrebuilt middleware for common agent use cases\nCopy page\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\n\u200b\nProvider-agnostic middleware\nThe following middleware work with any LLM provider:\nMiddleware\nDescription\nSummarization\nAutomatically summarize conversation history when approaching token limits.\nHuman-in-the-loop\nPause execution for human approval of tool calls.\nModel call limit\nLimit the number of model calls to prevent excessive costs.\nTool call limit\nControl tool execution by limiting call counts.\nModel fallback\nAutomatically fallback to alternative models when primary fails.\nPII detection\nDetect and handle Personally Identifiable Information (PII).\nTo-do list\nEquip agents with task planning and tracking capabilities.\nLLM tool selector\nUse an LLM to select relevant tools before calling main model.\nTool retry\nAutomatically retry failed tool calls with exponential backoff.\nModel retry\nAutomatically retry failed model calls with exponential backoff.\nLLM tool emulator\nEmulate tool execution using an LLM for testing purposes.\nContext editing\nManage conversation context by trimming or clearing tool uses.\nShell tool\nExpose a persistent shell session to agents for command execution.\nFile search\nProvide Glob and Grep search tools over filesystem files.\n\u200b\nSummarization\nAutomatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following:\nLong-running conversations that exceed context windows.\nMulti-turn dialogues with extensive history.\nApplications where preserving full conversation context matters.\nAPI reference:\nSummarizationMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nSummarizationMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[your_weather_tool, your_calculator_tool],\nmiddleware\n=\n[\nSummarizationMiddleware(\nmodel\n=\n\"gpt-5-mini\"\n,\ntrigger\n=\n(\n\"tokens\"\n,\n4000\n),\nkeep\n=\n(\n\"messages\"\n,\n20\n),\n),\n],\n)\nConfiguration options\nThe\nfraction\nconditions for\ntrigger\nand\nkeep\n(shown below) rely on a chat model\u2019s\nprofile data\nif using\nlangchain>=1.1\n. If data are not available, use another condition or specify manually:\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\ncustom_profile\n=\n{\n\"max_input_tokens\"\n:\n100_000\n,\n# ...\n}\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n,\nprofile\n=\ncustom_profile)\n\u200b\nmodel\nstring | BaseChatModel\nrequired\nModel for generating summaries. Can be a model identifier string (e.g.,\n'openai:gpt-5-mini'\n) or a\nBaseChatModel\ninstance. See\ninit_chat_model\nfor more information.\n\u200b\ntrigger\nContextSize | list[ContextSize] | None\nConditions for triggering summarization. Can be:\nA single\nContextSize\ndict (all properties must be met - AND logic)\nA list of\nContextSize\ndicts (any condition must be met - OR logic)\nEach condition can include:\nfraction\n(float): Fraction of model\u2019s context size (0-1)\ntokens\n(int): Absolute token count\nmessages\n(int): Message count\nAt least one property must be specified per condition. If not provided, summarization will not trigger automatically.\nSee the API reference for\nContextSize\nfor more information.\n\u200b\nkeep\nContextSize\ndefault:\n\"{messages: 20}\"\nHow much context to preserve after summarization. Specify exactly one of:\nfraction\n(float): Fraction of model\u2019s context size to keep (0-1)\ntokens\n(int): Absolute token count to keep\nmessages\n(int): Number of recent messages to keep\nSee the API reference for\nContextSize\nfor more information.\n\u200b\ntoken_counter\nfunction\nCustom token counting function. Defaults to character-based counting.\n\u200b\nsummary_prompt\nstring\nCustom prompt template for summarization. Uses built-in template if not specified. The template should include\n{messages}\nplaceholder where conversation history will be inserted.\n\u200b\ntrim_tokens_to_summarize\nnumber\ndefault:\n\"4000\"\nMaximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.\n\u200b\nsummary_prefix\nstring\nPrefix to add to the summary message. If not provided, a default prefix is used.\n\u200b\nmax_tokens_before_summary\nnumber\ndeprecated\nDeprecated:\nUse\ntrigger: {\"tokens\": value}\ninstead. Token threshold for triggering summarization.\n\u200b\nmessages_to_keep\nnumber\ndeprecated\nDeprecated:\nUse\nkeep: {\"messages\": value}\ninstead. Recent messages to preserve.\nFull example\nThe summarization middleware monitors message token counts and automatically summarizes older messages when thresholds are reached.\nTrigger conditions\ncontrol when summarization runs:\nSingle condition object (all properties must be met - AND logic)\nArray of conditions (any condition must be met - OR logic)\nEach condition can use\nfraction\n(of model\u2019s context size),\ntokens\n(absolute count), or\nmessages\n(message count)\nKeep conditions\ncontrol how much context to preserve (specify exactly one):\nfraction\n- Fraction of model\u2019s context size to keep\ntokens\n- Absolute token count to keep\nmessages\n- Number of recent messages to keep\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nSummarizationMiddleware\n# Single condition: trigger if tokens >= 4000 AND messages >= 10\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[your_weather_tool, your_calculator_tool],\nmiddleware\n=\n[\nSummarizationMiddleware(\nmodel\n=\n\"gpt-5-mini\"\n,\ntrigger\n=\n[(\n\"tokens\"\n,\n4000\n), (\n\"messages\"\n,\n10\n)],\nkeep\n=\n(\n\"messages\"\n,\n20\n),\n),\n],\n)\n# Multiple conditions\nagent2\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[your_weather_tool, your_calculator_tool],\nmiddleware\n=\n[\nSummarizationMiddleware(\nmodel\n=\n\"gpt-5-mini\"\n,\ntrigger\n=\n[\n(\n\"tokens\"\n,\n3000\n),\n(\n\"messages\"\n,\n6\n),\n],\nkeep\n=\n(\n\"messages\"\n,\n20\n),\n),\n],\n)\n# Using fractional limits\nagent3\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[your_weather_tool, your_calculator_tool],\nmiddleware\n=\n[\nSummarizationMiddleware(\nmodel\n=\n\"gpt-5-mini\"\n,\ntrigger\n=\n(\n\"fraction\"\n,\n0.8\n),\nkeep\n=\n(\n\"fraction\"\n,\n0.3\n),\n),\n],\n)\n\u200b\nHuman-in-the-loop\nPause agent execution for human approval, editing, or rejection of tool calls before they execute.\nHuman-in-the-loop\nis useful for the following:\nHigh-stakes operations requiring human approval (e.g. database writes, financial transactions).\nCompliance workflows where human oversight is mandatory.\nLong-running conversations where human feedback guides the agent.\nAPI reference:\nHumanInTheLoopMiddleware\nHuman-in-the-loop middleware requires a\ncheckpointer\nto maintain state across interruptions.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nHumanInTheLoopMiddleware\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\ndef\nread_email_tool\n(\nemail_id\n:\nstr\n) ->\nstr\n:\n\"\"\"Mock function to read an email by its ID.\"\"\"\nreturn\nf\n\"Email content for ID:\n{\nemail_id\n}\n\"\ndef\nsend_email_tool\n(\nrecipient\n:\nstr\n,\nsubject\n:\nstr\n,\nbody\n:\nstr\n) ->\nstr\n:\n\"\"\"Mock function to send an email.\"\"\"\nreturn\nf\n\"Email sent to\n{\nrecipient\n}\nwith subject '\n{\nsubject\n}\n'\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[your_read_email_tool, your_send_email_tool],\ncheckpointer\n=\nInMemorySaver(),\nmiddleware\n=\n[\nHumanInTheLoopMiddleware(\ninterrupt_on\n=\n{\n\"your_send_email_tool\"\n: {\n\"allowed_decisions\"\n: [\n\"approve\"\n,\n\"edit\"\n,\n\"reject\"\n],\n},\n\"your_read_email_tool\"\n:\nFalse\n,\n}\n),\n],\n)\nFor complete examples, configuration options, and integration patterns, see the\nHuman-in-the-loop documentation\n.\nWatch this\nvideo guide\ndemonstrating Human-in-the-loop middleware behavior.\n\u200b\nModel call limit\nLimit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following:\nPreventing runaway agents from making too many API calls.\nEnforcing cost controls on production deployments.\nTesting agent behavior within specific call budgets.\nAPI reference:\nModelCallLimitMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nModelCallLimitMiddleware\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ncheckpointer\n=\nInMemorySaver(),\n# Required for thread limiting\ntools\n=\n[],\nmiddleware\n=\n[\nModelCallLimitMiddleware(\nthread_limit\n=\n10\n,\nrun_limit\n=\n5\n,\nexit_behavior\n=\n\"end\"\n,\n),\n],\n)\nWatch this\nvideo guide\ndemonstrating Model Call Limit middleware behavior.\nConfiguration options\n\u200b\nthread_limit\nnumber\nMaximum model calls across all runs in a thread. Defaults to no limit.\n\u200b\nrun_limit\nnumber\nMaximum model calls per single invocation. Defaults to no limit.\n\u200b\nexit_behavior\nstring\ndefault:\n\"end\"\nBehavior when limit is reached. Options:\n'end'\n(graceful termination) or\n'error'\n(raise exception)\n\u200b\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following:\nPreventing excessive calls to expensive external APIs.\nLimiting web searches or database queries.\nEnforcing rate limits on specific tool usage.\nProtecting against runaway agent loops.\nAPI reference:\nToolCallLimitMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nToolCallLimitMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool, database_tool],\nmiddleware\n=\n[\n# Global limit\nToolCallLimitMiddleware(\nthread_limit\n=\n20\n,\nrun_limit\n=\n10\n),\n# Tool-specific limit\nToolCallLimitMiddleware(\ntool_name\n=\n\"search\"\n,\nthread_limit\n=\n5\n,\nrun_limit\n=\n3\n,\n),\n],\n)\nWatch this\nvideo guide\ndemonstrating Tool Call Limit middleware behavior.\nConfiguration options\n\u200b\ntool_name\nstring\nName of specific tool to limit. If not provided, limits apply to\nall tools globally\n.\n\u200b\nthread_limit\nnumber\nMaximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state.\nNone\nmeans no thread limit.\n\u200b\nrun_limit\nnumber\nMaximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message.\nNone\nmeans no run limit.\nNote:\nAt least one of\nthread_limit\nor\nrun_limit\nmust be specified.\n\u200b\nexit_behavior\nstring\ndefault:\n\"continue\"\nBehavior when limit is reached:\n'continue'\n(default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.\n'error'\n- Raise a\nToolCallLimitExceededError\nexception, stopping execution immediately\n'end'\n- Stop execution immediately with a\nToolMessage\nand AI message for the exceeded tool call. Only works when limiting a single tool; raises\nNotImplementedError\nif other tools have pending calls.\nFull example\nSpecify limits with:\nThread limit\n- Max calls across all runs in a conversation (requires checkpointer)\nRun limit\n- Max calls per single invocation (resets each turn)\nExit behaviors:\n'continue'\n(default) - Block exceeded calls with error messages, agent continues\n'error'\n- Raise exception immediately\n'end'\n- Stop with ToolMessage + AI message (single-tool scenarios only)\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nToolCallLimitMiddleware\nglobal_limiter\n=\nToolCallLimitMiddleware(\nthread_limit\n=\n20\n,\nrun_limit\n=\n10\n)\nsearch_limiter\n=\nToolCallLimitMiddleware(\ntool_name\n=\n\"search\"\n,\nthread_limit\n=\n5\n,\nrun_limit\n=\n3\n)\ndatabase_limiter\n=\nToolCallLimitMiddleware(\ntool_name\n=\n\"query_database\"\n,\nthread_limit\n=\n10\n)\nstrict_limiter\n=\nToolCallLimitMiddleware(\ntool_name\n=\n\"scrape_webpage\"\n,\nrun_limit\n=\n2\n,\nexit_behavior\n=\n\"error\"\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool, database_tool, scraper_tool],\nmiddleware\n=\n[global_limiter, search_limiter, database_limiter, strict_limiter],\n)\n\u200b\nModel fallback\nAutomatically fallback to alternative models when the primary model fails. Model fallback is useful for the following:\nBuilding resilient agents that handle model outages.\nCost optimization by falling back to cheaper models.\nProvider redundancy across OpenAI, Anthropic, etc.\nAPI reference:\nModelFallbackMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nModelFallbackMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nModelFallbackMiddleware(\n\"gpt-5-mini\"\n,\n\"claude-3-5-sonnet-20241022\"\n,\n),\n],\n)\nWatch this\nvideo guide\ndemonstrating Model Fallback middleware behavior.\nConfiguration options\n\u200b\nfirst_model\nstring | BaseChatModel\nrequired\nFirst fallback model to try when the primary model fails. Can be a model identifier string (e.g.,\n'openai:gpt-5-mini'\n) or a\nBaseChatModel\ninstance.\n\u200b\n*additional_models\nstring | BaseChatModel\nAdditional fallback models to try in order if previous models fail\n\u200b\nPII detection\nDetect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following:\nHealthcare and financial applications with compliance requirements.\nCustomer service agents that need to sanitize logs.\nAny application handling sensitive user data.\nAPI reference:\nPIIMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nPIIMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nPIIMiddleware(\n\"email\"\n,\nstrategy\n=\n\"redact\"\n,\napply_to_input\n=\nTrue\n),\nPIIMiddleware(\n\"credit_card\"\n,\nstrategy\n=\n\"mask\"\n,\napply_to_input\n=\nTrue\n),\n],\n)\n\u200b\nCustom PII types\nYou can create custom PII types by providing a\ndetector\nparameter. This allows you to detect patterns specific to your use case beyond the built-in types.\nThree ways to create custom detectors:\nRegex pattern string\n- Simple pattern matching\nCustom function\n- Complex detection logic with validation\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nPIIMiddleware\nimport\nre\n# Method 1: Regex pattern string\nagent1\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nPIIMiddleware(\n\"api_key\"\n,\ndetector\n=\nr\n\"sk-\n[\na-zA-Z0-9\n]\n{32}\n\"\n,\nstrategy\n=\n\"block\"\n,\n),\n],\n)\n# Method 2: Compiled regex pattern\nagent2\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nPIIMiddleware(\n\"phone_number\"\n,\ndetector\n=\nre.compile(\nr\n\"\n\\+\n?\n\\d\n{1,3}\n[\n\\s\n.-\n]\n?\n\\d\n{3,4}\n[\n\\s\n.-\n]\n?\n\\d\n{4}\n\"\n),\nstrategy\n=\n\"mask\"\n,\n),\n],\n)\n# Method 3: Custom detector function\ndef\ndetect_ssn\n(\ncontent\n:\nstr\n) -> list[dict[\nstr\n,\nstr\n|\nint\n]]:\n\"\"\"Detect SSN with validation.\nReturns a list of dictionaries with 'text', 'start', and 'end' keys.\n\"\"\"\nimport\nre\nmatches\n=\n[]\npattern\n=\nr\n\"\n\\d\n{3}\n-\n\\d\n{2}\n-\n\\d\n{4}\n\"\nfor\nmatch\nin\nre.finditer(pattern, content):\nssn\n=\nmatch.group(\n0\n)\n# Validate: first 3 digits shouldn't be 000, 666, or 900-999\nfirst_three\n=\nint\n(ssn[:\n3\n])\nif\nfirst_three\nnot\nin\n[\n0\n,\n666\n]\nand\nnot\n(\n900\n<=\nfirst_three\n<=\n999\n):\nmatches.append({\n\"text\"\n: ssn,\n\"start\"\n: match.start(),\n\"end\"\n: match.end(),\n})\nreturn\nmatches\nagent3\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nPIIMiddleware(\n\"ssn\"\n,\ndetector\n=\ndetect_ssn,\nstrategy\n=\n\"hash\"\n,\n),\n],\n)\nCustom detector function signature:\nThe detector function must accept a string (content) and return matches:\nReturns a list of dictionaries with\ntext\n,\nstart\n, and\nend\nkeys:\nCopy\ndef\ndetector\n(\ncontent\n:\nstr\n) -> list[dict[\nstr\n,\nstr\n|\nint\n]]:\nreturn\n[\n{\n\"text\"\n:\n\"matched_text\"\n,\n\"start\"\n:\n0\n,\n\"end\"\n:\n12\n},\n# ... more matches\n]\nFor custom detectors:\nUse regex strings for simple patterns\nUse RegExp objects when you need flags (e.g., case-insensitive matching)\nUse custom functions when you need validation logic beyond pattern matching\nCustom functions give you full control over detection logic and can implement complex validation rules\nConfiguration options\n\u200b\npii_type\nstring\nrequired\nType of PII to detect. Can be a built-in type (\nemail\n,\ncredit_card\n,\nip\n,\nmac_address\n,\nurl\n) or a custom type name.\n\u200b\nstrategy\nstring\ndefault:\n\"redact\"\nHow to handle detected PII. Options:\n'block'\n- Raise exception when detected\n'redact'\n- Replace with\n[REDACTED_{PII_TYPE}]\n'mask'\n- Partially mask (e.g.,\n****-****-****-1234\n)\n'hash'\n- Replace with deterministic hash\n\u200b\ndetector\nfunction | regex\nCustom detector function or regex pattern. If not provided, uses built-in detector for the PII type.\n\u200b\napply_to_input\nboolean\ndefault:\n\"True\"\nCheck user messages before model call\n\u200b\napply_to_output\nboolean\ndefault:\n\"False\"\nCheck AI messages after model call\n\u200b\napply_to_tool_results\nboolean\ndefault:\n\"False\"\nCheck tool result messages after execution\n\u200b\nTo-do list\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:\nComplex multi-step tasks requiring coordination across multiple tools.\nLong-running operations where progress visibility is important.\nThis middleware automatically provides agents with a\nwrite_todos\ntool and system prompts to guide effective task planning.\nAPI reference:\nTodoListMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nTodoListMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[read_file, write_file, run_tests],\nmiddleware\n=\n[TodoListMiddleware()],\n)\nWatch this\nvideo guide\ndemonstrating To-do List middleware behavior.\nConfiguration options\n\u200b\nsystem_prompt\nstring\nCustom system prompt for guiding todo usage. Uses built-in prompt if not specified.\n\u200b\ntool_description\nstring\nCustom description for the\nwrite_todos\ntool. Uses built-in description if not specified.\n\u200b\nLLM tool selector\nUse an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following:\nAgents with many tools (10+) where most aren\u2019t relevant per query.\nReducing token usage by filtering irrelevant tools.\nImproving model focus and accuracy.\nThis middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines the available tool names and descriptions. Model providers often add this structured output information to the system prompt behind the scenes.\nAPI reference:\nLLMToolSelectorMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nLLMToolSelectorMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[tool1, tool2, tool3, tool4, tool5,\n...\n],\nmiddleware\n=\n[\nLLMToolSelectorMiddleware(\nmodel\n=\n\"gpt-5-mini\"\n,\nmax_tools\n=\n3\n,\nalways_include\n=\n[\n\"search\"\n],\n),\n],\n)\nConfiguration options\n\u200b\nmodel\nstring | BaseChatModel\nModel for tool selection. Can be a model identifier string (e.g.,\n'openai:gpt-5-mini'\n) or a\nBaseChatModel\ninstance. See\ninit_chat_model\nfor more information.\nDefaults to the agent\u2019s main model.\n\u200b\nsystem_prompt\nstring\nInstructions for the selection model. Uses built-in prompt if not specified.\n\u200b\nmax_tools\nnumber\nMaximum number of tools to select. If the model selects more, only the first max_tools will be used. No limit if not specified.\n\u200b\nalways_include\nlist[string]\nTool names to always include regardless of selection. These do not count against the max_tools limit.\n\u200b\nTool retry\nAutomatically retry failed tool calls with configurable exponential backoff. Tool retry is useful for the following:\nHandling transient failures in external API calls.\nImproving reliability of network-dependent tools.\nBuilding resilient agents that gracefully handle temporary errors.\nAPI reference:\nToolRetryMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nToolRetryMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool, database_tool],\nmiddleware\n=\n[\nToolRetryMiddleware(\nmax_retries\n=\n3\n,\nbackoff_factor\n=\n2.0\n,\ninitial_delay\n=\n1.0\n,\n),\n],\n)\nConfiguration options\n\u200b\nmax_retries\nnumber\ndefault:\n\"2\"\nMaximum number of retry attempts after the initial call (3 total attempts with default)\n\u200b\ntools\nlist[BaseTool | str]\nOptional list of tools or tool names to apply retry logic to. If\nNone\n, applies to all tools.\n\u200b\nretry_on\ntuple[type[Exception], ...] | callable\ndefault:\n\"(Exception,)\"\nEither a tuple of exception types to retry on, or a callable that takes an exception and returns\nTrue\nif it should be retried.\n\u200b\non_failure\nstring | callable\ndefault:\n\"return_message\"\nBehavior when all retries are exhausted. Options:\n'return_message'\n- Return a\nToolMessage\nwith error details (allows LLM to handle failure)\n'raise'\n- Re-raise the exception (stops agent execution)\nCustom callable - Function that takes the exception and returns a string for the\nToolMessage\ncontent\n\u200b\nbackoff_factor\nnumber\ndefault:\n\"2.0\"\nMultiplier for exponential backoff. Each retry waits\ninitial_delay * (backoff_factor ** retry_number)\nseconds. Set to\n0.0\nfor constant delay.\n\u200b\ninitial_delay\nnumber\ndefault:\n\"1.0\"\nInitial delay in seconds before first retry\n\u200b\nmax_delay\nnumber\ndefault:\n\"60.0\"\nMaximum delay in seconds between retries (caps exponential backoff growth)\n\u200b\njitter\nboolean\ndefault:\n\"true\"\nWhether to add random jitter (\n\u00b125%\n) to delay to avoid thundering herd\nFull example\nThe middleware automatically retries failed tool calls with exponential backoff.\nKey configuration:\nmax_retries\n- Number of retry attempts (default: 2)\nbackoff_factor\n- Multiplier for exponential backoff (default: 2.0)\ninitial_delay\n- Starting delay in seconds (default: 1.0)\nmax_delay\n- Cap on delay growth (default: 60.0)\njitter\n- Add random variation (default: True)\nFailure handling:\non_failure='return_message'\n- Return error message\non_failure='raise'\n- Re-raise exception\nCustom function - Function returning error message\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nToolRetryMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool, database_tool, api_tool],\nmiddleware\n=\n[\nToolRetryMiddleware(\nmax_retries\n=\n3\n,\nbackoff_factor\n=\n2.0\n,\ninitial_delay\n=\n1.0\n,\nmax_delay\n=\n60.0\n,\njitter\n=\nTrue\n,\ntools\n=\n[\n\"api_tool\"\n],\nretry_on\n=\n(\nConnectionError\n,\nTimeoutError\n),\non_failure\n=\n\"continue\"\n,\n),\n],\n)\n\u200b\nModel retry\nAutomatically retry failed model calls with configurable exponential backoff. Model retry is useful for the following:\nHandling transient failures in model API calls.\nImproving reliability of network-dependent model requests.\nBuilding resilient agents that gracefully handle temporary model errors.\nAPI reference:\nModelRetryMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nModelRetryMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool, database_tool],\nmiddleware\n=\n[\nModelRetryMiddleware(\nmax_retries\n=\n3\n,\nbackoff_factor\n=\n2.0\n,\ninitial_delay\n=\n1.0\n,\n),\n],\n)\nConfiguration options\n\u200b\nmax_retries\nnumber\ndefault:\n\"2\"\nMaximum number of retry attempts after the initial call (3 total attempts with default)\n\u200b\nretry_on\ntuple[type[Exception], ...] | callable\ndefault:\n\"(Exception,)\"\nEither a tuple of exception types to retry on, or a callable that takes an exception and returns\nTrue\nif it should be retried.\n\u200b\non_failure\nstring | callable\ndefault:\n\"continue\"\nBehavior when all retries are exhausted. Options:\n'continue'\n(default) - Return an\nAIMessage\nwith error details, allowing the agent to potentially handle the failure gracefully\n'error'\n- Re-raise the exception (stops agent execution)\nCustom callable - Function that takes the exception and returns a string for the\nAIMessage\ncontent\n\u200b\nbackoff_factor\nnumber\ndefault:\n\"2.0\"\nMultiplier for exponential backoff. Each retry waits\ninitial_delay * (backoff_factor ** retry_number)\nseconds. Set to\n0.0\nfor constant delay.\n\u200b\ninitial_delay\nnumber\ndefault:\n\"1.0\"\nInitial delay in seconds before first retry\n\u200b\nmax_delay\nnumber\ndefault:\n\"60.0\"\nMaximum delay in seconds between retries (caps exponential backoff growth)\n\u200b\njitter\nboolean\ndefault:\n\"true\"\nWhether to add random jitter (\n\u00b125%\n) to delay to avoid thundering herd\nFull example\nThe middleware automatically retries failed model calls with exponential backoff.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nModelRetryMiddleware\n# Basic usage with default settings (2 retries, exponential backoff)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool],\nmiddleware\n=\n[ModelRetryMiddleware()],\n)\n# Custom exception filtering\nclass\nTimeoutError\n(\nException\n):\n\"\"\"Custom exception for timeout errors.\"\"\"\npass\nclass\nConnectionError\n(\nException\n):\n\"\"\"Custom exception for connection errors.\"\"\"\npass\n# Retry specific exceptions only\nretry\n=\nModelRetryMiddleware(\nmax_retries\n=\n4\n,\nretry_on\n=\n(\nTimeoutError\n,\nConnectionError\n),\nbackoff_factor\n=\n1.5\n,\n)\ndef\nshould_retry\n(\nerror\n:\nException\n) ->\nbool\n:\n# Only retry on rate limit errors\nif\nisinstance\n(error,\nTimeoutError\n):\nreturn\nTrue\n# Or check for specific HTTP status codes\nif\nhasattr\n(error,\n\"status_code\"\n):\nreturn\nerror.status_code\nin\n(\n429\n,\n503\n)\nreturn\nFalse\nretry_with_filter\n=\nModelRetryMiddleware(\nmax_retries\n=\n3\n,\nretry_on\n=\nshould_retry,\n)\n# Return error message instead of raising\nretry_continue\n=\nModelRetryMiddleware(\nmax_retries\n=\n4\n,\non_failure\n=\n\"continue\"\n,\n# Return AIMessage with error instead of raising\n)\n# Custom error message formatting\ndef\nformat_error\n(\nerror\n:\nException\n) ->\nstr\n:\nreturn\nf\n\"Model call failed:\n{\nerror\n}\n. Please try again later.\"\nretry_with_formatter\n=\nModelRetryMiddleware(\nmax_retries\n=\n4\n,\non_failure\n=\nformat_error,\n)\n# Constant backoff (no exponential growth)\nconstant_backoff\n=\nModelRetryMiddleware(\nmax_retries\n=\n5\n,\nbackoff_factor\n=\n0.0\n,\n# No exponential growth\ninitial_delay\n=\n2.0\n,\n# Always wait 2 seconds\n)\n# Raise exception on failure\nstrict_retry\n=\nModelRetryMiddleware(\nmax_retries\n=\n2\n,\non_failure\n=\n\"error\"\n,\n# Re-raise exception instead of returning message\n)\n\u200b\nLLM tool emulator\nEmulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses. LLM tool emulators are useful for the following:\nTesting agent behavior without executing real tools.\nDeveloping agents when external tools are unavailable or expensive.\nPrototyping agent workflows before implementing actual tools.\nAPI reference:\nLLMToolEmulator\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nLLMToolEmulator\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_weather, search_database, send_email],\nmiddleware\n=\n[\nLLMToolEmulator(),\n# Emulate all tools\n],\n)\nConfiguration options\n\u200b\ntools\nlist[str | BaseTool]\nList of tool names (str) or BaseTool instances to emulate. If\nNone\n(default), ALL tools will be emulated. If empty list\n[]\n, no tools will be emulated. If array with tool names/instances, only those tools will be emulated.\n\u200b\nmodel\nstring | BaseChatModel\nModel to use for generating emulated tool responses. Can be a model identifier string (e.g.,\n'anthropic:claude-sonnet-4-5-20250929'\n) or a\nBaseChatModel\ninstance. Defaults to the agent\u2019s model if not specified. See\ninit_chat_model\nfor more information.\nFull example\nThe middleware uses an LLM to generate plausible responses for tool calls instead of executing the actual tools.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nLLMToolEmulator\nfrom\nlangchain.tools\nimport\ntool\n@tool\ndef\nget_weather\n(\nlocation\n:\nstr\n) ->\nstr\n:\n\"\"\"Get the current weather for a location.\"\"\"\nreturn\nf\n\"Weather in\n{\nlocation\n}\n\"\n@tool\ndef\nsend_email\n(\nto\n:\nstr\n,\nsubject\n:\nstr\n,\nbody\n:\nstr\n) ->\nstr\n:\n\"\"\"Send an email.\"\"\"\nreturn\n\"Email sent\"\n# Emulate all tools (default behavior)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_weather, send_email],\nmiddleware\n=\n[LLMToolEmulator()],\n)\n# Emulate specific tools only\nagent2\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_weather, send_email],\nmiddleware\n=\n[LLMToolEmulator(\ntools\n=\n[\n\"get_weather\"\n])],\n)\n# Use custom model for emulation\nagent4\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_weather, send_email],\nmiddleware\n=\n[LLMToolEmulator(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n)],\n)\n\u200b\nContext editing\nManage conversation context by clearing older tool call outputs when token limits are reached, while preserving recent results. This helps keep context windows manageable in long conversations with many tool calls. Context editing is useful for the following:\nLong conversations with many tool calls that exceed token limits\nReducing token costs by removing older tool outputs that are no longer relevant\nMaintaining only the most recent N tool results in context\nAPI reference:\nContextEditingMiddleware\n,\nClearToolUsesEdit\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nContextEditingMiddleware, ClearToolUsesEdit\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nContextEditingMiddleware(\nedits\n=\n[\nClearToolUsesEdit(\ntrigger\n=\n100000\n,\nkeep\n=\n3\n,\n),\n],\n),\n],\n)\nConfiguration options\n\u200b\nedits\nlist[ContextEdit]\ndefault:\n\"[ClearToolUsesEdit()]\"\nList of\nContextEdit\nstrategies to apply\n\u200b\ntoken_count_method\nstring\ndefault:\n\"approximate\"\nToken counting method. Options:\n'approximate'\nor\n'model'\nClearToolUsesEdit\noptions:\n\u200b\ntrigger\nnumber\ndefault:\n\"100000\"\nToken count that triggers the edit. When the conversation exceeds this token count, older tool outputs will be cleared.\n\u200b\nclear_at_least\nnumber\ndefault:\n\"0\"\nMinimum number of tokens to reclaim when the edit runs. If set to 0, clears as much as needed.\n\u200b\nkeep\nnumber\ndefault:\n\"3\"\nNumber of most recent tool results that must be preserved. These will never be cleared.\n\u200b\nclear_tool_inputs\nboolean\ndefault:\n\"False\"\nWhether to clear the originating tool call parameters on the AI message. When\nTrue\n, tool call arguments are replaced with empty objects.\n\u200b\nexclude_tools\nlist[string]\ndefault:\n\"()\"\nList of tool names to exclude from clearing. These tools will never have their outputs cleared.\n\u200b\nplaceholder\nstring\ndefault:\n\"[cleared]\"\nPlaceholder text inserted for cleared tool outputs. This replaces the original tool message content.\nFull example\nThe middleware applies context editing strategies when token limits are reached. The most common strategy is\nClearToolUsesEdit\n, which clears older tool results while preserving recent ones.\nHow it works:\nMonitor token count in conversation\nWhen threshold is reached, clear older tool outputs\nKeep most recent N tool results\nOptionally preserve tool call arguments for context\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nContextEditingMiddleware, ClearToolUsesEdit\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool, your_calculator_tool, database_tool],\nmiddleware\n=\n[\nContextEditingMiddleware(\nedits\n=\n[\nClearToolUsesEdit(\ntrigger\n=\n2000\n,\nkeep\n=\n3\n,\nclear_tool_inputs\n=\nFalse\n,\nexclude_tools\n=\n[],\nplaceholder\n=\n\"[cleared]\"\n,\n),\n],\n),\n],\n)\n\u200b\nShell tool\nExpose a persistent shell session to agents for command execution. Shell tool middleware is useful for the following:\nAgents that need to execute system commands\nDevelopment and deployment automation tasks\nTesting and validation workflows\nFile system operations and script execution\nSecurity consideration\n: Use appropriate execution policies (\nHostExecutionPolicy\n,\nDockerExecutionPolicy\n, or\nCodexSandboxExecutionPolicy\n) to match your deployment\u2019s security requirements.\nLimitation\n: Persistent shell sessions do not currently work with interrupts (human-in-the-loop). We anticipate adding support for this in the future.\nAPI reference:\nShellToolMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\n(\nShellToolMiddleware,\nHostExecutionPolicy,\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool],\nmiddleware\n=\n[\nShellToolMiddleware(\nworkspace_root\n=\n\"/workspace\"\n,\nexecution_policy\n=\nHostExecutionPolicy(),\n),\n],\n)\nConfiguration options\n\u200b\nworkspace_root\nstr | Path | None\nBase directory for the shell session. If omitted, a temporary directory is created when the agent starts and removed when it ends.\n\u200b\nstartup_commands\ntuple[str, ...] | list[str] | str | None\nOptional commands executed sequentially after the session starts\n\u200b\nshutdown_commands\ntuple[str, ...] | list[str] | str | None\nOptional commands executed before the session shuts down\n\u200b\nexecution_policy\nBaseExecutionPolicy | None\nExecution policy controlling timeouts, output limits, and resource configuration. Options:\nHostExecutionPolicy\n- Full host access (default); best for trusted environments where the agent already runs inside a container or VM\nDockerExecutionPolicy\n- Launches a separate Docker container for each agent run, providing harder isolation\nCodexSandboxExecutionPolicy\n- Reuses the Codex CLI sandbox for additional syscall/filesystem restrictions\n\u200b\nredaction_rules\ntuple[RedactionRule, ...] | list[RedactionRule] | None\nOptional redaction rules to sanitize command output before returning it to the model\n\u200b\ntool_description\nstr | None\nOptional override for the registered shell tool description\n\u200b\nshell_command\nSequence[str] | str | None\nOptional shell executable (string) or argument sequence used to launch the persistent session. Defaults to\n/bin/bash\n.\n\u200b\nenv\nMapping[str, Any] | None\nOptional environment variables to supply to the shell session. Values are coerced to strings before command execution.\nFull example\nThe middleware provides a single persistent shell session that agents can use to execute commands sequentially.\nExecution policies:\nHostExecutionPolicy\n(default) - Native execution with full host access\nDockerExecutionPolicy\n- Isolated Docker container execution\nCodexSandboxExecutionPolicy\n- Sandboxed execution via Codex CLI\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\n(\nShellToolMiddleware,\nHostExecutionPolicy,\nDockerExecutionPolicy,\nRedactionRule,\n)\n# Basic shell tool with host execution\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool],\nmiddleware\n=\n[\nShellToolMiddleware(\nworkspace_root\n=\n\"/workspace\"\n,\nexecution_policy\n=\nHostExecutionPolicy(),\n),\n],\n)\n# Docker isolation with startup commands\nagent_docker\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nShellToolMiddleware(\nworkspace_root\n=\n\"/workspace\"\n,\nstartup_commands\n=\n[\n\"pip install requests\"\n,\n\"export PYTHONPATH=/workspace\"\n],\nexecution_policy\n=\nDockerExecutionPolicy(\nimage\n=\n\"python:3.11-slim\"\n,\ncommand_timeout\n=\n60.0\n,\n),\n),\n],\n)\n# With output redaction\nagent_redacted\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nShellToolMiddleware(\nworkspace_root\n=\n\"/workspace\"\n,\nredaction_rules\n=\n[\nRedactionRule(\npii_type\n=\n\"api_key\"\n,\ndetector\n=\nr\n\"sk-\n[\na-zA-Z0-9\n]\n{32}\n\"\n),\n],\n),\n],\n)\n\u200b\nFile search\nProvide Glob and Grep search tools over a filesystem. File search middleware is useful for the following:\nCode exploration and analysis\nFinding files by name patterns\nSearching code content with regex\nLarge codebases where file discovery is needed\nAPI reference:\nFilesystemFileSearchMiddleware\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nFilesystemFileSearchMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nFilesystemFileSearchMiddleware(\nroot_path\n=\n\"/workspace\"\n,\nuse_ripgrep\n=\nTrue\n,\n),\n],\n)\nConfiguration options\n\u200b\nroot_path\nstr\nrequired\nRoot directory to search. All file operations are relative to this path.\n\u200b\nuse_ripgrep\nbool\ndefault:\n\"True\"\nWhether to use ripgrep for search. Falls back to Python regex if ripgrep is unavailable.\n\u200b\nmax_file_size_mb\nint\ndefault:\n\"10\"\nMaximum file size to search in MB. Files larger than this are skipped.\nFull example\nThe middleware adds two search tools to agents:\nGlob tool\n- Fast file pattern matching:\nSupports patterns like\n**/*.py\n,\nsrc/**/*.ts\nReturns matching file paths sorted by modification time\nGrep tool\n- Content search with regex:\nFull regex syntax support\nFilter by file patterns with\ninclude\nparameter\nThree output modes:\nfiles_with_matches\n,\ncontent\n,\ncount\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nFilesystemFileSearchMiddleware\nfrom\nlangchain.messages\nimport\nHumanMessage\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[],\nmiddleware\n=\n[\nFilesystemFileSearchMiddleware(\nroot_path\n=\n\"/workspace\"\n,\nuse_ripgrep\n=\nTrue\n,\nmax_file_size_mb\n=\n10\n,\n),\n],\n)\n# Agent can now use glob_search and grep_search tools\nresult\n=\nagent.invoke({\n\"messages\"\n: [HumanMessage(\n\"Find all Python files containing 'async def'\"\n)]\n})\n# The agent will use:\n# 1. glob_search(pattern=\"**/*.py\") to find Python files\n# 2. grep_search(pattern=\"async def\", include=\"*.py\") to find async functions\n\u200b\nProvider-specific middleware\nThese middleware are optimized for specific LLM providers. See each provider\u2019s documentation for full details and examples.\nAnthropic\nPrompt caching, bash tool, text editor, memory, and file search middleware for Claude models.\nOpenAI\nContent moderation middleware for OpenAI models.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nOverview\nPrevious\nCustom middleware\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/middleware/built-in",
      "title": "Built-in middleware - Docs by LangChain",
      "heading": "Built-in middleware"
    }
  },
  {
    "page_content": "Custom middleware - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMiddleware\nCustom middleware\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nHooks\nNode-style hooks\nWrap-style hooks\nCreate middleware\nDecorator-based middleware\nClass-based middleware\nCustom state schema\nExecution order\nAgent jumps\nBest practices\nExamples\nDynamic model selection\nTool call monitoring\nDynamically selecting tools\nWorking with system messages\nAdditional resources\nMiddleware\nCustom middleware\nCopy page\nCopy page\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\n\u200b\nHooks\nMiddleware provides two styles of hooks to intercept agent execution:\nNode-style hooks\nRun sequentially at specific execution points.\nWrap-style hooks\nRun around each model or tool call.\n\u200b\nNode-style hooks\nRun sequentially at specific execution points. Use for logging, validation, and state updates.\nAvailable hooks:\nbefore_agent\n- Before agent starts (once per invocation)\nbefore_model\n- Before each model call\nafter_model\n- After each model response\nafter_agent\n- After agent completes (once per invocation)\nExample:\nDecorator\nClass\nCopy\nfrom\nlangchain.agents.middleware\nimport\nbefore_model, after_model, AgentState\nfrom\nlangchain.messages\nimport\nAIMessage\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\ntyping\nimport\nAny\n@before_model\n(\ncan_jump_to\n=\n[\n\"end\"\n])\ndef\ncheck_message_limit\n(\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nif\nlen\n(state[\n\"messages\"\n])\n>=\n50\n:\nreturn\n{\n\"messages\"\n: [AIMessage(\n\"Conversation limit reached.\"\n)],\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\n@after_model\ndef\nlog_response\n(\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nprint\n(\nf\n\"Model returned:\n{\nstate[\n'messages'\n][\n-\n1\n].content\n}\n\"\n)\nreturn\nNone\nCopy\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState, hook_config\nfrom\nlangchain.messages\nimport\nAIMessage\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\ntyping\nimport\nAny\nclass\nMessageLimitMiddleware\n(\nAgentMiddleware\n):\ndef\n__init__\n(\nself\n,\nmax_messages\n:\nint\n=\n50\n):\nsuper\n().\n__init__\n()\nself\n.max_messages\n=\nmax_messages\n@hook_config\n(\ncan_jump_to\n=\n[\n\"end\"\n])\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nif\nlen\n(state[\n\"messages\"\n])\n==\nself\n.max_messages:\nreturn\n{\n\"messages\"\n: [AIMessage(\n\"Conversation limit reached.\"\n)],\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\ndef\nafter_model\n(\nself\n,\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nprint\n(\nf\n\"Model returned:\n{\nstate[\n'messages'\n][\n-\n1\n].content\n}\n\"\n)\nreturn\nNone\n\u200b\nWrap-style hooks\nIntercept execution and control when the handler is called. Use for retries, caching, and transformation.\nYou decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\nAvailable hooks:\nwrap_model_call\n- Around each model call\nwrap_tool_call\n- Around each tool call\nExample:\nDecorator\nClass\nCopy\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\n@wrap_model_call\ndef\nretry_model\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\nfor\nattempt\nin\nrange\n(\n3\n):\ntry\n:\nreturn\nhandler(request)\nexcept\nException\nas\ne:\nif\nattempt\n==\n2\n:\nraise\nprint\n(\nf\n\"Retry\n{\nattempt\n+\n1\n}\n/3 after error:\n{\ne\n}\n\"\n)\nCopy\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\nclass\nRetryMiddleware\n(\nAgentMiddleware\n):\ndef\n__init__\n(\nself\n,\nmax_retries\n:\nint\n=\n3\n):\nsuper\n().\n__init__\n()\nself\n.max_retries\n=\nmax_retries\ndef\nwrap_model_call\n(\nself\n,\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\nfor\nattempt\nin\nrange\n(\nself\n.max_retries):\ntry\n:\nreturn\nhandler(request)\nexcept\nException\nas\ne:\nif\nattempt\n==\nself\n.max_retries\n-\n1\n:\nraise\nprint\n(\nf\n\"Retry\n{\nattempt\n+\n1\n}\n/\n{\nself\n.max_retries\n}\nafter error:\n{\ne\n}\n\"\n)\n\u200b\nCreate middleware\nYou can create middleware in two ways:\nDecorator-based middleware\nQuick and simple for single-hook middleware. Use decorators to wrap individual functions.\nClass-based middleware\nMore powerful for complex middleware with multiple hooks or configuration.\n\u200b\nDecorator-based middleware\nQuick and simple for single-hook middleware. Use decorators to wrap individual functions.\nAvailable decorators:\nNode-style:\n@before_agent\n- Runs before agent starts (once per invocation)\n@before_model\n- Runs before each model call\n@after_model\n- Runs after each model response\n@after_agent\n- Runs after agent completes (once per invocation)\nWrap-style:\n@wrap_model_call\n- Wraps each model call with custom logic\n@wrap_tool_call\n- Wraps each tool call with custom logic\nConvenience:\n@dynamic_prompt\n- Generates dynamic system prompts\nExample:\nCopy\nfrom\nlangchain.agents.middleware\nimport\n(\nbefore_model,\nwrap_model_call,\nAgentState,\nModelRequest,\nModelResponse,\n)\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\ntyping\nimport\nAny, Callable\n@before_model\ndef\nlog_before_model\n(\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nprint\n(\nf\n\"About to call model with\n{\nlen\n(state[\n'messages'\n])\n}\nmessages\"\n)\nreturn\nNone\n@wrap_model_call\ndef\nretry_model\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\nfor\nattempt\nin\nrange\n(\n3\n):\ntry\n:\nreturn\nhandler(request)\nexcept\nException\nas\ne:\nif\nattempt\n==\n2\n:\nraise\nprint\n(\nf\n\"Retry\n{\nattempt\n+\n1\n}\n/3 after error:\n{\ne\n}\n\"\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\nmiddleware\n=\n[log_before_model, retry_model],\ntools\n=\n[\n...\n],\n)\nWhen to use decorators:\nSingle hook needed\nNo complex configuration\nQuick prototyping\n\u200b\nClass-based middleware\nMore powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.\nExample:\nCopy\nfrom\nlangchain.agents.middleware\nimport\n(\nAgentMiddleware,\nAgentState,\nModelRequest,\nModelResponse,\n)\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\ntyping\nimport\nAny, Callable\nclass\nLoggingMiddleware\n(\nAgentMiddleware\n):\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nprint\n(\nf\n\"About to call model with\n{\nlen\n(state[\n'messages'\n])\n}\nmessages\"\n)\nreturn\nNone\ndef\nafter_model\n(\nself\n,\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nprint\n(\nf\n\"Model returned:\n{\nstate[\n'messages'\n][\n-\n1\n].content\n}\n\"\n)\nreturn\nNone\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\nmiddleware\n=\n[LoggingMiddleware()],\ntools\n=\n[\n...\n],\n)\nWhen to use classes:\nDefining both sync and async implementations for the same hook\nMultiple hooks needed in a single middleware\nComplex configuration required (e.g., configurable thresholds, custom models)\nReuse across projects with init-time configuration\n\u200b\nCustom state schema\nMiddleware can extend the agent\u2019s state with custom properties. This enables middleware to:\nTrack state across execution\n: Maintain counters, flags, or other values that persist throughout the agent\u2019s execution lifecycle\nShare data between hooks\n: Pass information from\nbefore_model\nto\nafter_model\nor between different middleware instances\nImplement cross-cutting concerns\n: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic\nMake conditional decisions\n: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically\nDecorator\nClass\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.messages\nimport\nHumanMessage\nfrom\nlangchain.agents.middleware\nimport\nAgentState, before_model, after_model\nfrom\ntyping_extensions\nimport\nNotRequired\nfrom\ntyping\nimport\nAny\nfrom\nlanggraph.runtime\nimport\nRuntime\nclass\nCustomState\n(\nAgentState\n):\nmodel_call_count: NotRequired[\nint\n]\nuser_id: NotRequired[\nstr\n]\n@before_model\n(\nstate_schema\n=\nCustomState,\ncan_jump_to\n=\n[\n\"end\"\n])\ndef\ncheck_call_limit\n(\nstate\n: CustomState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\ncount\n=\nstate.get(\n\"model_call_count\"\n,\n0\n)\nif\ncount\n>\n10\n:\nreturn\n{\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\n@after_model\n(\nstate_schema\n=\nCustomState)\ndef\nincrement_counter\n(\nstate\n: CustomState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nreturn\n{\n\"model_call_count\"\n: state.get(\n\"model_call_count\"\n,\n0\n)\n+\n1\n}\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\nmiddleware\n=\n[check_call_limit, increment_counter],\ntools\n=\n[],\n)\n# Invoke with custom state\nresult\n=\nagent.invoke({\n\"messages\"\n: [HumanMessage(\n\"Hello\"\n)],\n\"model_call_count\"\n:\n0\n,\n\"user_id\"\n:\n\"user-123\"\n,\n})\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.messages\nimport\nHumanMessage\nfrom\nlangchain.agents.middleware\nimport\nAgentState, AgentMiddleware\nfrom\ntyping_extensions\nimport\nNotRequired\nfrom\ntyping\nimport\nAny\nclass\nCustomState\n(\nAgentState\n):\nmodel_call_count: NotRequired[\nint\n]\nuser_id: NotRequired[\nstr\n]\nclass\nCallCounterMiddleware\n(AgentMiddleware[CustomState]):\nstate_schema\n=\nCustomState\ndef\nbefore_model\n(\nself\n,\nstate\n: CustomState,\nruntime\n) -> dict[\nstr\n, Any]\n|\nNone\n:\ncount\n=\nstate.get(\n\"model_call_count\"\n,\n0\n)\nif\ncount\n>\n10\n:\nreturn\n{\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\ndef\nafter_model\n(\nself\n,\nstate\n: CustomState,\nruntime\n) -> dict[\nstr\n, Any]\n|\nNone\n:\nreturn\n{\n\"model_call_count\"\n: state.get(\n\"model_call_count\"\n,\n0\n)\n+\n1\n}\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\nmiddleware\n=\n[CallCounterMiddleware()],\ntools\n=\n[],\n)\n# Invoke with custom state\nresult\n=\nagent.invoke({\n\"messages\"\n: [HumanMessage(\n\"Hello\"\n)],\n\"model_call_count\"\n:\n0\n,\n\"user_id\"\n:\n\"user-123\"\n,\n})\n\u200b\nExecution order\nWhen using multiple middleware, understand how they execute:\nCopy\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\nmiddleware\n=\n[middleware1, middleware2, middleware3],\ntools\n=\n[\n...\n],\n)\nExecution flow\nBefore hooks run in order:\nmiddleware1.before_agent()\nmiddleware2.before_agent()\nmiddleware3.before_agent()\nAgent loop starts\nmiddleware1.before_model()\nmiddleware2.before_model()\nmiddleware3.before_model()\nWrap hooks nest like function calls:\nmiddleware1.wrap_model_call()\n\u2192\nmiddleware2.wrap_model_call()\n\u2192\nmiddleware3.wrap_model_call()\n\u2192 model\nAfter hooks run in reverse order:\nmiddleware3.after_model()\nmiddleware2.after_model()\nmiddleware1.after_model()\nAgent loop ends\nmiddleware3.after_agent()\nmiddleware2.after_agent()\nmiddleware1.after_agent()\nKey rules:\nbefore_*\nhooks: First to last\nafter_*\nhooks: Last to first (reverse)\nwrap_*\nhooks: Nested (first middleware wraps all others)\n\u200b\nAgent jumps\nTo exit early from middleware, return a dictionary with\njump_to\n:\nAvailable jump targets:\n'end'\n: Jump to the end of the agent execution (or the first\nafter_agent\nhook)\n'tools'\n: Jump to the tools node\n'model'\n: Jump to the model node (or the first\nbefore_model\nhook)\nDecorator\nClass\nCopy\nfrom\nlangchain.agents.middleware\nimport\nafter_model, hook_config, AgentState\nfrom\nlangchain.messages\nimport\nAIMessage\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\ntyping\nimport\nAny\n@after_model\n@hook_config\n(\ncan_jump_to\n=\n[\n\"end\"\n])\ndef\ncheck_for_blocked\n(\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nif\n\"BLOCKED\"\nin\nlast_message.content:\nreturn\n{\n\"messages\"\n: [AIMessage(\n\"I cannot respond to that request.\"\n)],\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\nCopy\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, hook_config, AgentState\nfrom\nlangchain.messages\nimport\nAIMessage\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\ntyping\nimport\nAny\nclass\nBlockedContentMiddleware\n(\nAgentMiddleware\n):\n@hook_config\n(\ncan_jump_to\n=\n[\n\"end\"\n])\ndef\nafter_model\n(\nself\n,\nstate\n: AgentState,\nruntime\n: Runtime) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nif\n\"BLOCKED\"\nin\nlast_message.content:\nreturn\n{\n\"messages\"\n: [AIMessage(\n\"I cannot respond to that request.\"\n)],\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\n\u200b\nBest practices\nKeep middleware focused - each should do one thing well\nHandle errors gracefully - don\u2019t let middleware errors crash the agent\nUse appropriate hook types\n:\nNode-style for sequential logic (logging, validation)\nWrap-style for control flow (retry, fallback, caching)\nClearly document any custom state properties\nUnit test middleware independently before integrating\nConsider execution order - place critical middleware first in the list\nUse built-in middleware when possible\n\u200b\nExamples\n\u200b\nDynamic model selection\nDecorator\nClass\nCopy\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\ntyping\nimport\nCallable\ncomplex_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\nsimple_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n@wrap_model_call\ndef\ndynamic_model\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Use different model based on conversation length\nif\nlen\n(request.messages)\n>\n10\n:\nmodel\n=\ncomplex_model\nelse\n:\nmodel\n=\nsimple_model\nreturn\nhandler(request.override(\nmodel\n=\nmodel))\nCopy\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, ModelRequest, ModelResponse\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\ntyping\nimport\nCallable\ncomplex_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\nsimple_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\nclass\nDynamicModelMiddleware\n(\nAgentMiddleware\n):\ndef\nwrap_model_call\n(\nself\n,\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Use different model based on conversation length\nif\nlen\n(request.messages)\n>\n10\n:\nmodel\n=\ncomplex_model\nelse\n:\nmodel\n=\nsimple_model\nreturn\nhandler(request.override(\nmodel\n=\nmodel))\n\u200b\nTool call monitoring\nDecorator\nClass\nCopy\nfrom\nlangchain.agents.middleware\nimport\nwrap_tool_call\nfrom\nlangchain.tools.tool_node\nimport\nToolCallRequest\nfrom\nlangchain.messages\nimport\nToolMessage\nfrom\nlanggraph.types\nimport\nCommand\nfrom\ntyping\nimport\nCallable\n@wrap_tool_call\ndef\nmonitor_tool\n(\nrequest\n: ToolCallRequest,\nhandler\n: Callable[[ToolCallRequest], ToolMessage\n|\nCommand],\n) -> ToolMessage\n|\nCommand:\nprint\n(\nf\n\"Executing tool:\n{\nrequest.tool_call[\n'name'\n]\n}\n\"\n)\nprint\n(\nf\n\"Arguments:\n{\nrequest.tool_call[\n'args'\n]\n}\n\"\n)\ntry\n:\nresult\n=\nhandler(request)\nprint\n(\nf\n\"Tool completed successfully\"\n)\nreturn\nresult\nexcept\nException\nas\ne:\nprint\n(\nf\n\"Tool failed:\n{\ne\n}\n\"\n)\nraise\nCopy\nfrom\nlangchain.tools.tool_node\nimport\nToolCallRequest\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware\nfrom\nlangchain.messages\nimport\nToolMessage\nfrom\nlanggraph.types\nimport\nCommand\nfrom\ntyping\nimport\nCallable\nclass\nToolMonitoringMiddleware\n(\nAgentMiddleware\n):\ndef\nwrap_tool_call\n(\nself\n,\nrequest\n: ToolCallRequest,\nhandler\n: Callable[[ToolCallRequest], ToolMessage\n|\nCommand],\n) -> ToolMessage\n|\nCommand:\nprint\n(\nf\n\"Executing tool:\n{\nrequest.tool_call[\n'name'\n]\n}\n\"\n)\nprint\n(\nf\n\"Arguments:\n{\nrequest.tool_call[\n'args'\n]\n}\n\"\n)\ntry\n:\nresult\n=\nhandler(request)\nprint\n(\nf\n\"Tool completed successfully\"\n)\nreturn\nresult\nexcept\nException\nas\ne:\nprint\n(\nf\n\"Tool failed:\n{\ne\n}\n\"\n)\nraise\n\u200b\nDynamically selecting tools\nSelect relevant tools at runtime to improve performance and accuracy.\nBenefits:\nShorter prompts\n- Reduce complexity by exposing only relevant tools\nBetter accuracy\n- Models choose correctly from fewer options\nPermission control\n- Dynamically filter tools based on user access\nDecorator\nClass\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\n@wrap_model_call\ndef\nselect_tools\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n\"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n# Select a small, relevant subset of tools based on state/context\nrelevant_tools\n=\nselect_relevant_tools(request.state, request.runtime)\nreturn\nhandler(request.override(\ntools\n=\nrelevant_tools))\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\nall_tools,\n# All available tools need to be registered upfront\nmiddleware\n=\n[select_tools],\n)\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\nclass\nToolSelectorMiddleware\n(\nAgentMiddleware\n):\ndef\nwrap_model_call\n(\nself\n,\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n\"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n# Select a small, relevant subset of tools based on state/context\nrelevant_tools\n=\nselect_relevant_tools(request.state, request.runtime)\nreturn\nhandler(request.override(\ntools\n=\nrelevant_tools))\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\nall_tools,\n# All available tools need to be registered upfront\nmiddleware\n=\n[ToolSelectorMiddleware()],\n)\n\u200b\nWorking with system messages\nModify system messages in middleware using the\nsystem_message\nfield on\nModelRequest\n. The\nsystem_message\nfield contains a\nSystemMessage\nobject (even if the agent was created with a string\nsystem_prompt\n).\nExample: Adding context to system message\nDecorator\nClass\nCopy\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\nlangchain.messages\nimport\nSystemMessage\nfrom\ntyping\nimport\nCallable\n@wrap_model_call\ndef\nadd_context\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Always work with content blocks\nnew_content\n=\nlist\n(request.system_message.content_blocks)\n+\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Additional context.\"\n}\n]\nnew_system_message\n=\nSystemMessage(\ncontent\n=\nnew_content)\nreturn\nhandler(request.override(\nsystem_message\n=\nnew_system_message))\nCopy\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, ModelRequest, ModelResponse\nfrom\nlangchain.messages\nimport\nSystemMessage\nfrom\ntyping\nimport\nCallable\nclass\nContextMiddleware\n(\nAgentMiddleware\n):\ndef\nwrap_model_call\n(\nself\n,\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Always work with content blocks\nnew_content\n=\nlist\n(request.system_message.content_blocks)\n+\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Additional context.\"\n}\n]\nnew_system_message\n=\nSystemMessage(\ncontent\n=\nnew_content)\nreturn\nhandler(request.override(\nsystem_message\n=\nnew_system_message))\nExample: Working with cache control (Anthropic)\nWhen working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:\nDecorator\nClass\nCopy\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\nlangchain.messages\nimport\nSystemMessage\nfrom\ntyping\nimport\nCallable\n@wrap_model_call\ndef\nadd_cached_context\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Always work with content blocks\nnew_content\n=\nlist\n(request.system_message.content_blocks)\n+\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Here is a large document to analyze:\n\\n\\n\n<document>...</document>\"\n,\n# content up until this point is cached\n\"cache_control\"\n: {\n\"type\"\n:\n\"ephemeral\"\n}\n}\n]\nnew_system_message\n=\nSystemMessage(\ncontent\n=\nnew_content)\nreturn\nhandler(request.override(\nsystem_message\n=\nnew_system_message))\nCopy\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, ModelRequest, ModelResponse\nfrom\nlangchain.messages\nimport\nSystemMessage\nfrom\ntyping\nimport\nCallable\nclass\nCachedContextMiddleware\n(\nAgentMiddleware\n):\ndef\nwrap_model_call\n(\nself\n,\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Always work with content blocks\nnew_content\n=\nlist\n(request.system_message.content_blocks)\n+\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Here is a large document to analyze:\n\\n\\n\n<document>...</document>\"\n,\n\"cache_control\"\n: {\n\"type\"\n:\n\"ephemeral\"\n}\n# This content will be cached\n}\n]\nnew_system_message\n=\nSystemMessage(\ncontent\n=\nnew_content)\nreturn\nhandler(request.override(\nsystem_message\n=\nnew_system_message))\nNotes:\nModelRequest.system_message\nis always a\nSystemMessage\nobject, even if the agent was created with\nsystem_prompt=\"string\"\nUse\nSystemMessage.content_blocks\nto access content as a list of blocks, regardless of whether the original content was a string or list\nWhen modifying system messages, use\ncontent_blocks\nand append new blocks to preserve existing structure\nYou can pass\nSystemMessage\nobjects directly to\ncreate_agent\n\u2019s\nsystem_prompt\nparameter for advanced use cases like cache control\n\u200b\nAdditional resources\nMiddleware API reference\nBuilt-in middleware\nTesting agents\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuilt-in middleware\nPrevious\nGuardrails\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/middleware/custom",
      "title": "Custom middleware - Docs by LangChain",
      "heading": "Custom middleware"
    }
  },
  {
    "page_content": "Runtime - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nAdvanced usage\nRuntime\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nAccess\nInside tools\nInside middleware\nAdvanced usage\nRuntime\nCopy page\nCopy page\n\u200b\nOverview\nLangChain\u2019s\ncreate_agent\nruns on LangGraph\u2019s runtime under the hood.\nLangGraph exposes a\nRuntime\nobject with the following information:\nContext\n: static information like user id, db connections, or other dependencies for an agent invocation\nStore\n: a\nBaseStore\ninstance used for\nlong-term memory\nStream writer\n: an object used for streaming information via the\n\"custom\"\nstream mode\nRuntime context provides\ndependency injection\nfor your tools and middleware. Instead of hardcoding values or using global state, you can inject runtime dependencies (like database connections, user IDs, or configuration) when invoking your agent. This makes your tools more testable, reusable, and flexible.\nYou can access the runtime information within\ntools\nand\nmiddleware\n.\n\u200b\nAccess\nWhen creating an agent with\ncreate_agent\n, you can specify a\ncontext_schema\nto define the structure of the\ncontext\nstored in the agent\nRuntime\n.\nWhen invoking the agent, pass the\ncontext\nargument with the relevant configuration for the run:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\n@dataclass\nclass\nContext\n:\nuser_name:\nstr\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\ncontext_schema\n=\nContext\n)\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's my name?\"\n}]},\ncontext\n=\nContext(\nuser_name\n=\n\"John Smith\"\n)\n)\n\u200b\nInside tools\nYou can access the runtime information inside tools to:\nAccess the context\nRead or write long-term memory\nWrite to the\ncustom stream\n(ex, tool progress / updates)\nUse the\nToolRuntime\nparameter to access the\nRuntime\nobject inside a tool.\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\n@tool\ndef\nfetch_user_email_preferences\n(\nruntime\n: ToolRuntime[Context]) ->\nstr\n:\n\"\"\"Fetch the user's email preferences from the store.\"\"\"\nuser_id\n=\nruntime.context.user_id\npreferences:\nstr\n=\n\"The user prefers you to write a brief and polite email.\"\nif\nruntime.store:\nif\nmemory\n:=\nruntime.store.get((\n\"users\"\n,), user_id):\npreferences\n=\nmemory.value[\n\"preferences\"\n]\nreturn\npreferences\n\u200b\nInside middleware\nYou can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.\nUse\nrequest.runtime\nto access the\nRuntime\nobject inside middleware decorators. The runtime object is available in the\nModelRequest\nparameter passed to middleware functions.\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.messages\nimport\nAnyMessage\nfrom\nlangchain.agents\nimport\ncreate_agent, AgentState\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest, before_model, after_model\nfrom\nlanggraph.runtime\nimport\nRuntime\n@dataclass\nclass\nContext\n:\nuser_name:\nstr\n# Dynamic prompts\n@dynamic_prompt\ndef\ndynamic_system_prompt\n(\nrequest\n: ModelRequest) ->\nstr\n:\nuser_name\n=\nrequest.runtime.context.user_name\nsystem_prompt\n=\nf\n\"You are a helpful assistant. Address the user as\n{\nuser_name\n}\n.\"\nreturn\nsystem_prompt\n# Before model hook\n@before_model\ndef\nlog_before_model\n(\nstate\n: AgentState,\nruntime\n: Runtime[Context]) ->\ndict\n|\nNone\n:\nprint\n(\nf\n\"Processing request for user:\n{\nruntime.context.user_name\n}\n\"\n)\nreturn\nNone\n# After model hook\n@after_model\ndef\nlog_after_model\n(\nstate\n: AgentState,\nruntime\n: Runtime[Context]) ->\ndict\n|\nNone\n:\nprint\n(\nf\n\"Completed request for user:\n{\nruntime.context.user_name\n}\n\"\n)\nreturn\nNone\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[dynamic_system_prompt, log_before_model, log_after_model],\ncontext_schema\n=\nContext\n)\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's my name?\"\n}]},\ncontext\n=\nContext(\nuser_name\n=\n\"John Smith\"\n)\n)\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nGuardrails\nPrevious\nContext engineering in agents\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/runtime",
      "title": "Runtime - Docs by LangChain",
      "heading": "Runtime"
    }
  },
  {
    "page_content": "Context engineering in agents - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nAdvanced usage\nContext engineering in agents\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nWhy do agents fail?\nThe agent loop\nWhat you can control\nData sources\nHow it works\nModel Context\nSystem Prompt\nMessages\nTools\nDefining tools\nSelecting tools\nModel\nResponse Format\nDefining formats\nSelecting formats\nTool Context\nReads\nWrites\nLife-cycle Context\nExample: Summarization\nBest practices\nRelated resources\nAdvanced usage\nContext engineering in agents\nCopy page\nCopy page\n\u200b\nOverview\nThe hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.\n\u200b\nWhy do agents fail?\nWhen agents fail, it\u2019s usually because the LLM call inside the agent took the wrong action / didn\u2019t do what we expected. LLMs fail for one of two reasons:\nThe underlying LLM is not capable enough\nThe \u201cright\u201d context was not passed to the LLM\nMore often than not - it\u2019s actually the second reason that causes agents to not be reliable.\nContext engineering\nis providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of \u201cright\u201d context is the number one blocker for more reliable agents, and LangChain\u2019s agent abstractions are uniquely designed to facilitate context engineering.\nNew to context engineering? Start with the\nconceptual overview\nto understand the different types of context and when to use them.\n\u200b\nThe agent loop\nA typical agent loop consists of two main steps:\nModel call\n- calls the LLM with a prompt and available tools, returns either a response or a request to execute tools\nTool execution\n- executes the tools that the LLM requested, returns tool results\nThis loop continues until the LLM decides to finish.\n\u200b\nWhat you can control\nTo build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.\nContext Type\nWhat You Control\nTransient or Persistent\nModel Context\nWhat goes into model calls (instructions, message history, tools, response format)\nTransient\nTool Context\nWhat tools can access and produce (reads/writes to state, store, runtime context)\nPersistent\nLife-cycle Context\nWhat happens between model and tool calls (summarization, guardrails, logging, etc.)\nPersistent\nTransient context\nWhat the LLM sees for a single call. You can modify messages, tools, or prompts without changing what\u2019s saved in state.\nPersistent context\nWhat gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.\n\u200b\nData sources\nThroughout this process, your agent accesses (reads / writes) different sources of data:\nData Source\nAlso Known As\nScope\nExamples\nRuntime Context\nStatic configuration\nConversation-scoped\nUser ID, API keys, database connections, permissions, environment settings\nState\nShort-term memory\nConversation-scoped\nCurrent messages, uploaded files, authentication status, tool results\nStore\nLong-term memory\nCross-conversation\nUser preferences, extracted insights, memories, historical data\n\u200b\nHow it works\nLangChain\nmiddleware\nis the mechanism under the hood that makes context engineering practical for developers using LangChain.\nMiddleware allows you to hook into any step in the agent lifecycle and:\nUpdate context\nJump to a different step in the agent lifecycle\nThroughout this guide, you\u2019ll see frequent use of the middleware API as a means to the context engineering end.\n\u200b\nModel Context\nControl what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.\nSystem Prompt\nBase instructions from the developer to the LLM.\nMessages\nThe full list of messages (conversation history) sent to the LLM.\nTools\nUtilities the agent has access to to take actions.\nModel\nThe actual model (including configuration) to be called.\nResponse Format\nSchema specification for the model\u2019s final response.\nAll of these types of model context can draw from\nstate\n(short-term memory),\nstore\n(long-term memory), or\nruntime context\n(static configuration).\n\u200b\nSystem Prompt\nThe system prompt sets the LLM\u2019s behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.\nState\nStore\nRuntime Context\nAccess message count or conversation context from state:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nstate_aware_prompt\n(\nrequest\n: ModelRequest) ->\nstr\n:\n# request.messages is a shortcut for request.state[\"messages\"]\nmessage_count\n=\nlen\n(request.messages)\nbase\n=\n\"You are a helpful assistant.\"\nif\nmessage_count\n>\n10\n:\nbase\n+=\n\"\n\\n\nThis is a long conversation - be extra concise.\"\nreturn\nbase\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[state_aware_prompt]\n)\nAccess user preferences from long-term memory:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\n@dynamic_prompt\ndef\nstore_aware_prompt\n(\nrequest\n: ModelRequest) ->\nstr\n:\nuser_id\n=\nrequest.runtime.context.user_id\n# Read from Store: get user preferences\nstore\n=\nrequest.runtime.store\nuser_prefs\n=\nstore.get((\n\"preferences\"\n,), user_id)\nbase\n=\n\"You are a helpful assistant.\"\nif\nuser_prefs:\nstyle\n=\nuser_prefs.value.get(\n\"communication_style\"\n,\n\"balanced\"\n)\nbase\n+=\nf\n\"\n\\n\nUser prefers\n{\nstyle\n}\nresponses.\"\nreturn\nbase\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[store_aware_prompt],\ncontext_schema\n=\nContext,\nstore\n=\nInMemoryStore()\n)\nAccess user ID or configuration from Runtime Context:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dataclass\nclass\nContext\n:\nuser_role:\nstr\ndeployment_env:\nstr\n@dynamic_prompt\ndef\ncontext_aware_prompt\n(\nrequest\n: ModelRequest) ->\nstr\n:\n# Read from Runtime Context: user role and environment\nuser_role\n=\nrequest.runtime.context.user_role\nenv\n=\nrequest.runtime.context.deployment_env\nbase\n=\n\"You are a helpful assistant.\"\nif\nuser_role\n==\n\"admin\"\n:\nbase\n+=\n\"\n\\n\nYou have admin access. You can perform all operations.\"\nelif\nuser_role\n==\n\"viewer\"\n:\nbase\n+=\n\"\n\\n\nYou have read-only access. Guide users to read operations only.\"\nif\nenv\n==\n\"production\"\n:\nbase\n+=\n\"\n\\n\nBe extra careful with any data modifications.\"\nreturn\nbase\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[context_aware_prompt],\ncontext_schema\n=\nContext\n)\n\u200b\nMessages\nMessages make up the prompt that is sent to the LLM.\nIt\u2019s critical to manage the content of messages to ensure that the LLM has the right information to respond well.\nState\nStore\nRuntime Context\nInject uploaded file context from State when relevant to current query:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\n@wrap_model_call\ndef\ninject_file_context\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Inject context about files user has uploaded this session.\"\"\"\n# Read from State: get uploaded files metadata\nuploaded_files\n=\nrequest.state.get(\n\"uploaded_files\"\n, [])\nif\nuploaded_files:\n# Build context about available files\nfile_descriptions\n=\n[]\nfor\nfile\nin\nuploaded_files:\nfile_descriptions.append(\nf\n\"-\n{\nfile\n[\n'name'\n]\n}\n(\n{\nfile\n[\n'type'\n]\n}\n):\n{\nfile\n[\n'summary'\n]\n}\n\"\n)\nfile_context\n=\nf\n\"\"\"Files you have access to in this conversation:\n{\nchr\n(\n10\n).join(file_descriptions)\n}\nReference these files when answering questions.\"\"\"\n# Inject file context before recent messages\nmessages\n=\n[\n*\nrequest.messages,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: file_context},\n]\nrequest\n=\nrequest.override(\nmessages\n=\nmessages)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[inject_file_context]\n)\nInject user\u2019s email writing style from Store to guide drafting:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\n@wrap_model_call\ndef\ninject_writing_style\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Inject user's email writing style from Store.\"\"\"\nuser_id\n=\nrequest.runtime.context.user_id\n# Read from Store: get user's writing style examples\nstore\n=\nrequest.runtime.store\nwriting_style\n=\nstore.get((\n\"writing_style\"\n,), user_id)\nif\nwriting_style:\nstyle\n=\nwriting_style.value\n# Build style guide from stored examples\nstyle_context\n=\nf\n\"\"\"Your writing style:\n- Tone:\n{\nstyle.get(\n'tone'\n,\n'professional'\n)\n}\n- Typical greeting: \"\n{\nstyle.get(\n'greeting'\n,\n'Hi'\n)\n}\n\"\n- Typical sign-off: \"\n{\nstyle.get(\n'sign_off'\n,\n'Best'\n)\n}\n\"\n- Example email you've written:\n{\nstyle.get(\n'example_email'\n,\n''\n)\n}\n\"\"\"\n# Append at end - models pay more attention to final messages\nmessages\n=\n[\n*\nrequest.messages,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: style_context}\n]\nrequest\n=\nrequest.override(\nmessages\n=\nmessages)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[inject_writing_style],\ncontext_schema\n=\nContext,\nstore\n=\nInMemoryStore()\n)\nInject compliance rules from Runtime Context based on user\u2019s jurisdiction:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\n@dataclass\nclass\nContext\n:\nuser_jurisdiction:\nstr\nindustry:\nstr\ncompliance_frameworks: list[\nstr\n]\n@wrap_model_call\ndef\ninject_compliance_rules\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Inject compliance constraints from Runtime Context.\"\"\"\n# Read from Runtime Context: get compliance requirements\njurisdiction\n=\nrequest.runtime.context.user_jurisdiction\nindustry\n=\nrequest.runtime.context.industry\nframeworks\n=\nrequest.runtime.context.compliance_frameworks\n# Build compliance constraints\nrules\n=\n[]\nif\n\"GDPR\"\nin\nframeworks:\nrules.append(\n\"- Must obtain explicit consent before processing personal data\"\n)\nrules.append(\n\"- Users have right to data deletion\"\n)\nif\n\"HIPAA\"\nin\nframeworks:\nrules.append(\n\"- Cannot share patient health information without authorization\"\n)\nrules.append(\n\"- Must use secure, encrypted communication\"\n)\nif\nindustry\n==\n\"finance\"\n:\nrules.append(\n\"- Cannot provide financial advice without proper disclaimers\"\n)\nif\nrules:\ncompliance_context\n=\nf\n\"\"\"Compliance requirements for\n{\njurisdiction\n}\n:\n{\nchr\n(\n10\n).join(rules)\n}\n\"\"\"\n# Append at end - models pay more attention to final messages\nmessages\n=\n[\n*\nrequest.messages,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: compliance_context}\n]\nrequest\n=\nrequest.override(\nmessages\n=\nmessages)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[inject_compliance_rules],\ncontext_schema\n=\nContext\n)\nTransient vs Persistent Message Updates:\nThe examples above use\nwrap_model_call\nto make\ntransient\nupdates - modifying what messages are sent to the model for a single call without changing what\u2019s saved in state.\nFor\npersistent\nupdates that modify state (like the summarization example in\nLife-cycle Context\n), use life-cycle hooks like\nbefore_model\nor\nafter_model\nto permanently update the conversation history. See the\nmiddleware documentation\nfor more details.\n\u200b\nTools\nTools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\n\u200b\nDefining tools\nEach tool needs a clear name, description, argument names, and argument descriptions. These aren\u2019t just metadata\u2014they guide the model\u2019s reasoning about when and how to use the tool.\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nparse_docstring\n=\nTrue\n)\ndef\nsearch_orders\n(\nuser_id\n:\nstr\n,\nstatus\n:\nstr\n,\nlimit\n:\nint\n=\n10\n) ->\nstr\n:\n\"\"\"Search for user orders by status.\nUse this when the user asks about order history or wants to check\norder status. Always filter by the provided status.\nArgs:\nuser_id: Unique identifier for the user\nstatus: Order status: 'pending', 'shipped', or 'delivered'\nlimit: Maximum number of results to return\n\"\"\"\n# Implementation here\npass\n\u200b\nSelecting tools\nNot every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.\nState\nStore\nRuntime Context\nEnable advanced tools only after certain conversation milestones:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\n@wrap_model_call\ndef\nstate_based_tools\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Filter tools based on conversation State.\"\"\"\n# Read from State: check if user has authenticated\nstate\n=\nrequest.state\nis_authenticated\n=\nstate.get(\n\"authenticated\"\n,\nFalse\n)\nmessage_count\n=\nlen\n(state[\n\"messages\"\n])\n# Only enable sensitive tools after authentication\nif\nnot\nis_authenticated:\ntools\n=\n[t\nfor\nt\nin\nrequest.tools\nif\nt.name.startswith(\n\"public_\"\n)]\nrequest\n=\nrequest.override(\ntools\n=\ntools)\nelif\nmessage_count\n<\n5\n:\n# Limit tools early in conversation\ntools\n=\n[t\nfor\nt\nin\nrequest.tools\nif\nt.name\n!=\n\"advanced_search\"\n]\nrequest\n=\nrequest.override(\ntools\n=\ntools)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[public_search, private_search, advanced_search],\nmiddleware\n=\n[state_based_tools]\n)\nFilter tools based on user preferences or feature flags in Store:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\n@wrap_model_call\ndef\nstore_based_tools\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Filter tools based on Store preferences.\"\"\"\nuser_id\n=\nrequest.runtime.context.user_id\n# Read from Store: get user's enabled features\nstore\n=\nrequest.runtime.store\nfeature_flags\n=\nstore.get((\n\"features\"\n,), user_id)\nif\nfeature_flags:\nenabled_features\n=\nfeature_flags.value.get(\n\"enabled_tools\"\n, [])\n# Only include tools that are enabled for this user\ntools\n=\n[t\nfor\nt\nin\nrequest.tools\nif\nt.name\nin\nenabled_features]\nrequest\n=\nrequest.override(\ntools\n=\ntools)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[search_tool, analysis_tool, export_tool],\nmiddleware\n=\n[store_based_tools],\ncontext_schema\n=\nContext,\nstore\n=\nInMemoryStore()\n)\nFilter tools based on user permissions from Runtime Context:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\n@dataclass\nclass\nContext\n:\nuser_role:\nstr\n@wrap_model_call\ndef\ncontext_based_tools\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Filter tools based on Runtime Context permissions.\"\"\"\n# Read from Runtime Context: get user role\nuser_role\n=\nrequest.runtime.context.user_role\nif\nuser_role\n==\n\"admin\"\n:\n# Admins get all tools\npass\nelif\nuser_role\n==\n\"editor\"\n:\n# Editors can't delete\ntools\n=\n[t\nfor\nt\nin\nrequest.tools\nif\nt.name\n!=\n\"delete_data\"\n]\nrequest\n=\nrequest.override(\ntools\n=\ntools)\nelse\n:\n# Viewers get read-only tools\ntools\n=\n[t\nfor\nt\nin\nrequest.tools\nif\nt.name.startswith(\n\"read_\"\n)]\nrequest\n=\nrequest.override(\ntools\n=\ntools)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[read_data, write_data, delete_data],\nmiddleware\n=\n[context_based_tools],\ncontext_schema\n=\nContext\n)\nSee\nDynamically selecting tools\nfor more examples.\n\u200b\nModel\nDifferent models have different strengths, costs, and context windows. Select the right model for the task at hand, which\nmight change during an agent run.\nState\nStore\nRuntime Context\nUse different models based on conversation length from State:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\ntyping\nimport\nCallable\n# Initialize models once outside the middleware\nlarge_model\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\nstandard_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\nefficient_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n@wrap_model_call\ndef\nstate_based_model\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Select model based on State conversation length.\"\"\"\n# request.messages is a shortcut for request.state[\"messages\"]\nmessage_count\n=\nlen\n(request.messages)\nif\nmessage_count\n>\n20\n:\n# Long conversation - use model with larger context window\nmodel\n=\nlarge_model\nelif\nmessage_count\n>\n10\n:\n# Medium conversation\nmodel\n=\nstandard_model\nelse\n:\n# Short conversation - use efficient model\nmodel\n=\nefficient_model\nrequest\n=\nrequest.override(\nmodel\n=\nmodel)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[state_based_model]\n)\nUse user\u2019s preferred model from Store:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\ntyping\nimport\nCallable\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\n# Initialize available models once\nMODEL_MAP\n=\n{\n\"gpt-5-mini\"\n: init_chat_model(\n\"gpt-5-mini\"\n),\n\"gpt-5-mini\"\n: init_chat_model(\n\"gpt-5-mini\"\n),\n\"claude-sonnet\"\n: init_chat_model(\n\"claude-sonnet-4-5-20250929\"\n),\n}\n@wrap_model_call\ndef\nstore_based_model\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Select model based on Store preferences.\"\"\"\nuser_id\n=\nrequest.runtime.context.user_id\n# Read from Store: get user's preferred model\nstore\n=\nrequest.runtime.store\nuser_prefs\n=\nstore.get((\n\"preferences\"\n,), user_id)\nif\nuser_prefs:\npreferred_model\n=\nuser_prefs.value.get(\n\"preferred_model\"\n)\nif\npreferred_model\nand\npreferred_model\nin\nMODEL_MAP\n:\nrequest\n=\nrequest.override(\nmodel\n=\nMODEL_MAP\n[preferred_model])\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[store_based_model],\ncontext_schema\n=\nContext,\nstore\n=\nInMemoryStore()\n)\nSelect model based on cost limits or environment from Runtime Context:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\ntyping\nimport\nCallable\n@dataclass\nclass\nContext\n:\ncost_tier:\nstr\nenvironment:\nstr\n# Initialize models once outside the middleware\npremium_model\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\nstandard_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\nbudget_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n@wrap_model_call\ndef\ncontext_based_model\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Select model based on Runtime Context.\"\"\"\n# Read from Runtime Context: cost tier and environment\ncost_tier\n=\nrequest.runtime.context.cost_tier\nenvironment\n=\nrequest.runtime.context.environment\nif\nenvironment\n==\n\"production\"\nand\ncost_tier\n==\n\"premium\"\n:\n# Production premium users get best model\nmodel\n=\npremium_model\nelif\ncost_tier\n==\n\"budget\"\n:\n# Budget tier gets efficient model\nmodel\n=\nbudget_model\nelse\n:\n# Standard tier\nmodel\n=\nstandard_model\nrequest\n=\nrequest.override(\nmodel\n=\nmodel)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[context_based_model],\ncontext_schema\n=\nContext\n)\nSee\nDynamic model\nfor more examples.\n\u200b\nResponse Format\nStructured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn\u2019t sufficient.\nHow it works:\nWhen you provide a schema as the response format, the model\u2019s final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.\n\u200b\nDefining formats\nSchema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nclass\nCustomerSupportTicket\n(\nBaseModel\n):\n\"\"\"Structured ticket information extracted from customer message.\"\"\"\ncategory:\nstr\n=\nField(\ndescription\n=\n\"Issue category: 'billing', 'technical', 'account', or 'product'\"\n)\npriority:\nstr\n=\nField(\ndescription\n=\n\"Urgency level: 'low', 'medium', 'high', or 'critical'\"\n)\nsummary:\nstr\n=\nField(\ndescription\n=\n\"One-sentence summary of the customer's issue\"\n)\ncustomer_sentiment:\nstr\n=\nField(\ndescription\n=\n\"Customer's emotional tone: 'frustrated', 'neutral', or 'satisfied'\"\n)\n\u200b\nSelecting formats\nDynamic response format selection adapts schemas based on user preferences, conversation stage, or role\u2014returning simple formats early and detailed formats as complexity increases.\nState\nStore\nRuntime Context\nConfigure structured output based on conversation state:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\ntyping\nimport\nCallable\nclass\nSimpleResponse\n(\nBaseModel\n):\n\"\"\"Simple response for early conversation.\"\"\"\nanswer:\nstr\n=\nField(\ndescription\n=\n\"A brief answer\"\n)\nclass\nDetailedResponse\n(\nBaseModel\n):\n\"\"\"Detailed response for established conversation.\"\"\"\nanswer:\nstr\n=\nField(\ndescription\n=\n\"A detailed answer\"\n)\nreasoning:\nstr\n=\nField(\ndescription\n=\n\"Explanation of reasoning\"\n)\nconfidence:\nfloat\n=\nField(\ndescription\n=\n\"Confidence score 0-1\"\n)\n@wrap_model_call\ndef\nstate_based_output\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Select output format based on State.\"\"\"\n# request.messages is a shortcut for request.state[\"messages\"]\nmessage_count\n=\nlen\n(request.messages)\nif\nmessage_count\n<\n3\n:\n# Early conversation - use simple format\nrequest\n=\nrequest.override(\nresponse_format\n=\nSimpleResponse)\nelse\n:\n# Established conversation - use detailed format\nrequest\n=\nrequest.override(\nresponse_format\n=\nDetailedResponse)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[state_based_output]\n)\nConfigure output format based on user preferences in Store:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\ntyping\nimport\nCallable\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\nclass\nVerboseResponse\n(\nBaseModel\n):\n\"\"\"Verbose response with details.\"\"\"\nanswer:\nstr\n=\nField(\ndescription\n=\n\"Detailed answer\"\n)\nsources: list[\nstr\n]\n=\nField(\ndescription\n=\n\"Sources used\"\n)\nclass\nConciseResponse\n(\nBaseModel\n):\n\"\"\"Concise response.\"\"\"\nanswer:\nstr\n=\nField(\ndescription\n=\n\"Brief answer\"\n)\n@wrap_model_call\ndef\nstore_based_output\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Select output format based on Store preferences.\"\"\"\nuser_id\n=\nrequest.runtime.context.user_id\n# Read from Store: get user's preferred response style\nstore\n=\nrequest.runtime.store\nuser_prefs\n=\nstore.get((\n\"preferences\"\n,), user_id)\nif\nuser_prefs:\nstyle\n=\nuser_prefs.value.get(\n\"response_style\"\n,\n\"concise\"\n)\nif\nstyle\n==\n\"verbose\"\n:\nrequest\n=\nrequest.override(\nresponse_format\n=\nVerboseResponse)\nelse\n:\nrequest\n=\nrequest.override(\nresponse_format\n=\nConciseResponse)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[store_based_output],\ncontext_schema\n=\nContext,\nstore\n=\nInMemoryStore()\n)\nConfigure output format based on Runtime Context like user role or environment:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\ntyping\nimport\nCallable\n@dataclass\nclass\nContext\n:\nuser_role:\nstr\nenvironment:\nstr\nclass\nAdminResponse\n(\nBaseModel\n):\n\"\"\"Response with technical details for admins.\"\"\"\nanswer:\nstr\n=\nField(\ndescription\n=\n\"Answer\"\n)\ndebug_info:\ndict\n=\nField(\ndescription\n=\n\"Debug information\"\n)\nsystem_status:\nstr\n=\nField(\ndescription\n=\n\"System status\"\n)\nclass\nUserResponse\n(\nBaseModel\n):\n\"\"\"Simple response for regular users.\"\"\"\nanswer:\nstr\n=\nField(\ndescription\n=\n\"Answer\"\n)\n@wrap_model_call\ndef\ncontext_based_output\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Select output format based on Runtime Context.\"\"\"\n# Read from Runtime Context: user role and environment\nuser_role\n=\nrequest.runtime.context.user_role\nenvironment\n=\nrequest.runtime.context.environment\nif\nuser_role\n==\n\"admin\"\nand\nenvironment\n==\n\"production\"\n:\n# Admins in production get detailed output\nrequest\n=\nrequest.override(\nresponse_format\n=\nAdminResponse)\nelse\n:\n# Regular users get simple output\nrequest\n=\nrequest.override(\nresponse_format\n=\nUserResponse)\nreturn\nhandler(request)\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[context_based_output],\ncontext_schema\n=\nContext\n)\n\u200b\nTool Context\nTools are special in that they both read and write context.\nIn the most basic case, when a tool executes, it receives the LLM\u2019s request parameters and returns a tool message back. The tool does its work and produces a result.\nTools can also fetch important information for the model that allows it to perform and complete tasks.\n\u200b\nReads\nMost real-world tools need more than just the LLM\u2019s parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.\nState\nStore\nRuntime Context\nRead from State to check current session information:\nCopy\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlangchain.agents\nimport\ncreate_agent\n@tool\ndef\ncheck_authentication\n(\nruntime\n: ToolRuntime\n) ->\nstr\n:\n\"\"\"Check if user is authenticated.\"\"\"\n# Read from State: check current auth status\ncurrent_state\n=\nruntime.state\nis_authenticated\n=\ncurrent_state.get(\n\"authenticated\"\n,\nFalse\n)\nif\nis_authenticated:\nreturn\n\"User is authenticated\"\nelse\n:\nreturn\n\"User is not authenticated\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[check_authentication]\n)\nRead from Store to access persisted user preferences:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\n@tool\ndef\nget_preference\n(\npreference_key\n:\nstr\n,\nruntime\n: ToolRuntime[Context]\n) ->\nstr\n:\n\"\"\"Get user preference from Store.\"\"\"\nuser_id\n=\nruntime.context.user_id\n# Read from Store: get existing preferences\nstore\n=\nruntime.store\nexisting_prefs\n=\nstore.get((\n\"preferences\"\n,), user_id)\nif\nexisting_prefs:\nvalue\n=\nexisting_prefs.value.get(preference_key)\nreturn\nf\n\"\n{\npreference_key\n}\n:\n{\nvalue\n}\n\"\nif\nvalue\nelse\nf\n\"No preference set for\n{\npreference_key\n}\n\"\nelse\n:\nreturn\n\"No preferences found\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[get_preference],\ncontext_schema\n=\nContext,\nstore\n=\nInMemoryStore()\n)\nRead from Runtime Context for configuration like API keys and user IDs:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlangchain.agents\nimport\ncreate_agent\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\napi_key:\nstr\ndb_connection:\nstr\n@tool\ndef\nfetch_user_data\n(\nquery\n:\nstr\n,\nruntime\n: ToolRuntime[Context]\n) ->\nstr\n:\n\"\"\"Fetch data using Runtime Context configuration.\"\"\"\n# Read from Runtime Context: get API key and DB connection\nuser_id\n=\nruntime.context.user_id\napi_key\n=\nruntime.context.api_key\ndb_connection\n=\nruntime.context.db_connection\n# Use configuration to fetch data\nresults\n=\nperform_database_query(db_connection, query, api_key)\nreturn\nf\n\"Found\n{\nlen\n(results)\n}\nresults for user\n{\nuser_id\n}\n\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[fetch_user_data],\ncontext_schema\n=\nContext\n)\n# Invoke with runtime context\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Get my data\"\n}]},\ncontext\n=\nContext(\nuser_id\n=\n\"user_123\"\n,\napi_key\n=\n\"sk-...\"\n,\ndb_connection\n=\n\"postgresql://...\"\n)\n)\n\u200b\nWrites\nTool results can be used to help an agent complete a given task. Tools can both return results directly to the model\nand update the memory of the agent to make important context available to future steps.\nState\nStore\nWrite to State to track session-specific information using Command:\nCopy\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.types\nimport\nCommand\n@tool\ndef\nauthenticate_user\n(\npassword\n:\nstr\n,\nruntime\n: ToolRuntime\n) -> Command:\n\"\"\"Authenticate user and update State.\"\"\"\n# Perform authentication (simplified)\nif\npassword\n==\n\"correct\"\n:\n# Write to State: mark as authenticated using Command\nreturn\nCommand(\nupdate\n=\n{\n\"authenticated\"\n:\nTrue\n},\n)\nelse\n:\nreturn\nCommand(\nupdate\n=\n{\n\"authenticated\"\n:\nFalse\n})\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[authenticate_user]\n)\nWrite to Store to persist data across sessions:\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\n@tool\ndef\nsave_preference\n(\npreference_key\n:\nstr\n,\npreference_value\n:\nstr\n,\nruntime\n: ToolRuntime[Context]\n) ->\nstr\n:\n\"\"\"Save user preference to Store.\"\"\"\nuser_id\n=\nruntime.context.user_id\n# Read existing preferences\nstore\n=\nruntime.store\nexisting_prefs\n=\nstore.get((\n\"preferences\"\n,), user_id)\n# Merge with new preference\nprefs\n=\nexisting_prefs.value\nif\nexisting_prefs\nelse\n{}\nprefs[preference_key]\n=\npreference_value\n# Write to Store: save updated preferences\nstore.put((\n\"preferences\"\n,), user_id, prefs)\nreturn\nf\n\"Saved preference:\n{\npreference_key\n}\n=\n{\npreference_value\n}\n\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[save_preference],\ncontext_schema\n=\nContext,\nstore\n=\nInMemoryStore()\n)\nSee\nTools\nfor comprehensive examples of accessing state, store, and runtime context in tools.\n\u200b\nLife-cycle Context\nControl what happens\nbetween\nthe core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.\nAs you\u2019ve seen in\nModel Context\nand\nTool Context\n,\nmiddleware\nis the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:\nUpdate context\n- Modify state and store to persist changes, update conversation history, or save insights\nJump in the lifecycle\n- Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)\n\u200b\nExample: Summarization\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in\nModel Context\n, summarization\npersistently updates state\n- permanently replacing old messages with a summary that\u2019s saved for all future turns.\nLangChain offers built-in middleware for this:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nSummarizationMiddleware\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[\nSummarizationMiddleware(\nmodel\n=\n\"gpt-5-mini\"\n,\ntrigger\n=\n{\n\"tokens\"\n:\n4000\n},\nkeep\n=\n{\n\"messages\"\n:\n20\n},\n),\n],\n)\nWhen the conversation exceeds the token limit,\nSummarizationMiddleware\nautomatically:\nSummarizes older messages using a separate LLM call\nReplaces them with a summary message in State (permanently)\nKeeps recent messages intact for context\nThe summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.\nFor a complete list of built-in middleware, available hooks, and how to create custom middleware, see the\nMiddleware documentation\n.\n\u200b\nBest practices\nStart simple\n- Begin with static prompts and tools, add dynamics only when needed\nTest incrementally\n- Add one context engineering feature at a time\nMonitor performance\n- Track model calls, token usage, and latency\nUse built-in middleware\n- Leverage\nSummarizationMiddleware\n,\nLLMToolSelectorMiddleware\n, etc.\nDocument your context strategy\n- Make it clear what context is being passed and why\nUnderstand transient vs persistent\n: Model context changes are transient (per-call), while life-cycle context changes persist to state\n\u200b\nRelated resources\nContext conceptual overview\n- Understand context types and when to use them\nMiddleware\n- Complete middleware guide\nTools\n- Tool creation and context access\nMemory\n- Short-term and long-term memory patterns\nAgents\n- Core agent concepts\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nRuntime\nPrevious\nModel Context Protocol (MCP)\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/context-engineering",
      "title": "Context engineering in agents - Docs by LangChain",
      "heading": "Context engineering in agents"
    }
  },
  {
    "page_content": "Model Context Protocol (MCP) - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nAdvanced usage\nModel Context Protocol (MCP)\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nQuickstart\nCustom servers\nTransports\nHTTP\nPassing headers\nAuthentication\nstdio\nStateful sessions\nCore features\nTools\nLoading tools\nStructured content\nMultimodal tool content\nResources\nLoading resources\nPrompts\nLoading prompts\nAdvanced features\nTool Interceptors\nAccessing runtime context\nState updates and commands\nCustom interceptors\nProgress notifications\nLogging\nElicitation\nServer setup\nClient setup\nResponse actions\nAdditional resources\nAdvanced usage\nModel Context Protocol (MCP)\nCopy page\nCopy page\nModel Context Protocol (MCP)\nis an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the\nlangchain-mcp-adapters\nlibrary.\n\u200b\nQuickstart\nInstall the\nlangchain-mcp-adapters\nlibrary:\npip\nuv\nCopy\npip\ninstall\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nenables agents to use tools defined across one or more MCP servers.\nMultiServerMCPClient\nis\nstateless by default\n. Each tool invocation creates a fresh MCP\nClientSession\n, executes the tool, and then cleans up. See the\nstateful sessions\nsection for more details.\nAccessing multiple MCP servers\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain.agents\nimport\ncreate_agent\nclient\n=\nMultiServerMCPClient(\n{\n\"math\"\n: {\n\"transport\"\n:\n\"stdio\"\n,\n# Local subprocess communication\n\"command\"\n:\n\"python\"\n,\n# Absolute path to your math_server.py file\n\"args\"\n: [\n\"/path/to/math_server.py\"\n],\n},\n\"weather\"\n: {\n\"transport\"\n:\n\"http\"\n,\n# HTTP-based remote server\n# Ensure you start your weather server on port 8000\n\"url\"\n:\n\"http://localhost:8000/mcp\"\n,\n}\n}\n)\ntools\n=\nawait\nclient.get_tools()\nagent\n=\ncreate_agent(\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n)\nmath_response\n=\nawait\nagent.ainvoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's (3 + 5) x 12?\"\n}]}\n)\nweather_response\n=\nawait\nagent.ainvoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in nyc?\"\n}]}\n)\n\u200b\nCustom servers\nTo create a custom MCP server, use the\nFastMCP\nlibrary:\npip\nuv\nCopy\npip\ninstall\nfastmcp\nTo test your agent with MCP tool servers, use the following examples:\nMath server (stdio transport)\nWeather server (streamable HTTP transport)\nCopy\nfrom\nfastmcp\nimport\nFastMCP\nmcp\n=\nFastMCP(\n\"Math\"\n)\n@mcp.tool\n()\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n) ->\nint\n:\n\"\"\"Add two numbers\"\"\"\nreturn\na\n+\nb\n@mcp.tool\n()\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n) ->\nint\n:\n\"\"\"Multiply two numbers\"\"\"\nreturn\na\n*\nb\nif\n__name__\n==\n\"__main__\"\n:\nmcp.run(\ntransport\n=\n\"stdio\"\n)\n\u200b\nTransports\nMCP supports different transport mechanisms for client-server communication.\n\u200b\nHTTP\nThe\nhttp\ntransport (also referred to as\nstreamable-http\n) uses HTTP requests for client-server communication. See the\nMCP HTTP transport specification\nfor more details.\nCopy\nclient\n=\nMultiServerMCPClient(\n{\n\"weather\"\n: {\n\"transport\"\n:\n\"http\"\n,\n\"url\"\n:\n\"http://localhost:8000/mcp\"\n,\n}\n}\n)\n\u200b\nPassing headers\nWhen connecting to MCP servers over HTTP, you can include custom headers (e.g., for authentication or tracing) using the\nheaders\nfield in the connection configuration. This is supported for\nsse\n(deprecated by MCP spec) and\nstreamable_http\ntransports.\nPassing headers with MultiServerMCPClient\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain.agents\nimport\ncreate_agent\nclient\n=\nMultiServerMCPClient(\n{\n\"weather\"\n: {\n\"transport\"\n:\n\"http\"\n,\n\"url\"\n:\n\"http://localhost:8000/mcp\"\n,\n\"headers\"\n: {\n\"Authorization\"\n:\n\"Bearer YOUR_TOKEN\"\n,\n\"X-Custom-Header\"\n:\n\"custom-value\"\n},\n}\n}\n)\ntools\n=\nawait\nclient.get_tools()\nagent\n=\ncreate_agent(\n\"openai:gpt-5-mini\"\n, tools)\nresponse\n=\nawait\nagent.ainvoke({\n\"messages\"\n:\n\"what is the weather in nyc?\"\n})\n\u200b\nAuthentication\nThe\nlangchain-mcp-adapters\nlibrary uses the official\nMCP SDK\nunder the hood, which allows you to provide a custom authentication mechanism by implementing the\nhttpx.Auth\ninterface.\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nclient\n=\nMultiServerMCPClient(\n{\n\"weather\"\n: {\n\"transport\"\n:\n\"http\"\n,\n\"url\"\n:\n\"http://localhost:8000/mcp\"\n,\n\"auth\"\n: auth,\n}\n}\n)\nExample custom auth implementation\nBuilt-in OAuth flow\n\u200b\nstdio\nClient launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\nUnlike HTTP transports,\nstdio\nconnections are inherently\nstateful\n\u2014the subprocess persists for the lifetime of the client connection. However, when using\nMultiServerMCPClient\nwithout explicit session management, each tool call still creates a new session. See\nstateful sessions\nfor managing persistent connections.\nCopy\nclient\n=\nMultiServerMCPClient(\n{\n\"math\"\n: {\n\"transport\"\n:\n\"stdio\"\n,\n\"command\"\n:\n\"python\"\n,\n\"args\"\n: [\n\"/path/to/math_server.py\"\n],\n}\n}\n)\n\u200b\nStateful sessions\nBy default,\nMultiServerMCPClient\nis\nstateless\n\u2014each tool invocation creates a fresh MCP session, executes the tool, and then cleans up.\nIf you need to control the\nlifecycle\nof an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent\nClientSession\nusing\nclient.session()\n.\nUsing MCP ClientSession for stateful tool usage\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.tools\nimport\nload_mcp_tools\nfrom\nlangchain.agents\nimport\ncreate_agent\nclient\n=\nMultiServerMCPClient({\n...\n})\n# Create a session explicitly\nasync\nwith\nclient.session(\n\"server_name\"\n)\nas\nsession:\n# Pass the session to load tools, resources, or prompts\ntools\n=\nawait\nload_mcp_tools(session)\nagent\n=\ncreate_agent(\n\"anthropic:claude-3-7-sonnet-latest\"\n,\ntools\n)\n\u200b\nCore features\n\u200b\nTools\nTools\nallow MCP servers to expose executable functions that LLMs can invoke to perform actions\u2014such as querying databases, calling APIs, or interacting with external systems. LangChain converts MCP tools into LangChain\ntools\n, making them directly usable in any LangChain agent or workflow.\n\u200b\nLoading tools\nUse\nclient.get_tools()\nto retrieve tools from MCP servers and pass them to your agent:\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain.agents\nimport\ncreate_agent\nclient\n=\nMultiServerMCPClient({\n...\n})\ntools\n=\nawait\nclient.get_tools()\nagent\n=\ncreate_agent(\n\"claude-sonnet-4-5-20250929\"\n, tools)\n\u200b\nStructured content\nMCP tools can return\nstructured content\nalongside the human-readable text response. This is useful when a tool needs to return machine-parseable data (like JSON) in addition to text that gets shown to the model.\nWhen an MCP tool returns\nstructuredContent\n, the adapter wraps it in an\nMCPToolArtifact\nand returns it as the tool\u2019s artifact. You can access this using the\nartifact\nfield on the\nToolMessage\n. You can also use\ninterceptors\nto process or transform structured content automatically.\nExtracting structured content from artifact\nAfter invoking your agent, you can access the structured content from tool messages in the response:\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.messages\nimport\nToolMessage\nclient\n=\nMultiServerMCPClient({\n...\n})\ntools\n=\nawait\nclient.get_tools()\nagent\n=\ncreate_agent(\n\"claude-sonnet-4-5-20250929\"\n, tools)\nresult\n=\nawait\nagent.ainvoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Get data from the server\"\n}]}\n)\n# Extract structured content from tool messages\nfor\nmessage\nin\nresult[\n\"messages\"\n]:\nif\nisinstance\n(message, ToolMessage)\nand\nmessage.artifact:\nstructured_content\n=\nmessage.artifact[\n\"structured_content\"\n]\nAppending structured content via interceptor\nIf you want structured content to be visible in the conversation history (visible to the model), you can use an\ninterceptor\nto automatically append structured content to the tool result:\nCopy\nimport\njson\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.interceptors\nimport\nMCPToolCallRequest\nfrom\nmcp.types\nimport\nTextContent\nasync\ndef\nappend_structured_content\n(\nrequest\n: MCPToolCallRequest,\nhandler\n):\n\"\"\"Append structured content from artifact to tool message.\"\"\"\nresult\n=\nawait\nhandler(request)\nif\nresult.structuredContent:\nresult.content\n+=\n[\nTextContent(\ntype\n=\n\"text\"\n,\ntext\n=\njson.dumps(result.structuredContent)),\n]\nreturn\nresult\nclient\n=\nMultiServerMCPClient({\n...\n},\ntool_interceptors\n=\n[append_structured_content])\n\u200b\nMultimodal tool content\nMCP tools can return\nmultimodal content\n(images, text, etc.) in their responses. When an MCP server returns content with multiple parts (e.g., text and images), the adapter converts them to LangChain\u2019s\nstandard content blocks\n. You can access the standardized representation via the\ncontent_blocks\nproperty on the\nToolMessage\n:\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain.agents\nimport\ncreate_agent\nclient\n=\nMultiServerMCPClient({\n...\n})\ntools\n=\nawait\nclient.get_tools()\nagent\n=\ncreate_agent(\n\"claude-sonnet-4-5-20250929\"\n, tools)\nresult\n=\nawait\nagent.ainvoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Take a screenshot of the current page\"\n}]}\n)\n# Access multimodal content from tool messages\nfor\nmessage\nin\nresult[\n\"messages\"\n]:\nif\nmessage.type\n==\n\"tool\"\n:\n# Raw content in provider-native format\nprint\n(\nf\n\"Raw content:\n{\nmessage.content\n}\n\"\n)\n# Standardized content blocks  #\nfor\nblock\nin\nmessage.content_blocks:\nif\nblock[\n\"type\"\n]\n==\n\"text\"\n:\nprint\n(\nf\n\"Text:\n{\nblock[\n'text'\n]\n}\n\"\n)\nelif\nblock[\n\"type\"\n]\n==\n\"image\"\n:\nprint\n(\nf\n\"Image URL:\n{\nblock.get(\n'url'\n)\n}\n\"\n)\nprint\n(\nf\n\"Image base64:\n{\nblock.get(\n'base64'\n,\n''\n)[:\n50\n]\n}\n...\"\n)\nThis allows you to handle multimodal tool responses in a provider-agnostic way, regardless of how the underlying MCP server formats its content.\n\u200b\nResources\nResources\nallow MCP servers to expose data\u2014such as files, database records, or API responses\u2014that can be read by clients. LangChain converts MCP resources into\nBlob\nobjects, which provide a unified interface for handling both text and binary content.\n\u200b\nLoading resources\nUse\nclient.get_resources()\nto load resources from an MCP server:\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nclient\n=\nMultiServerMCPClient({\n...\n})\n# Load all resources from a server\nblobs\n=\nawait\nclient.get_resources(\n\"server_name\"\n)\n# Or load specific resources by URI\nblobs\n=\nawait\nclient.get_resources(\n\"server_name\"\n,\nuris\n=\n[\n\"file:///path/to/file.txt\"\n])\nfor\nblob\nin\nblobs:\nprint\n(\nf\n\"URI:\n{\nblob.metadata[\n'uri'\n]\n}\n, MIME type:\n{\nblob.mimetype\n}\n\"\n)\nprint\n(blob.as_string())\n# For text content\nYou can also use\nload_mcp_resources\ndirectly with a session for more control:\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.resources\nimport\nload_mcp_resources\nclient\n=\nMultiServerMCPClient({\n...\n})\nasync\nwith\nclient.session(\n\"server_name\"\n)\nas\nsession:\n# Load all resources\nblobs\n=\nawait\nload_mcp_resources(session)\n# Or load specific resources by URI\nblobs\n=\nawait\nload_mcp_resources(session,\nuris\n=\n[\n\"file:///path/to/file.txt\"\n])\n\u200b\nPrompts\nPrompts\nallow MCP servers to expose reusable prompt templates that can be retrieved and used by clients. LangChain converts MCP prompts into\nmessages\n, making them easy to integrate into chat-based workflows.\n\u200b\nLoading prompts\nUse\nclient.get_prompt()\nto load a prompt from an MCP server:\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nclient\n=\nMultiServerMCPClient({\n...\n})\n# Load a prompt by name\nmessages\n=\nawait\nclient.get_prompt(\n\"server_name\"\n,\n\"summarize\"\n)\n# Load a prompt with arguments\nmessages\n=\nawait\nclient.get_prompt(\n\"server_name\"\n,\n\"code_review\"\n,\narguments\n=\n{\n\"language\"\n:\n\"python\"\n,\n\"focus\"\n:\n\"security\"\n}\n)\n# Use the messages in your workflow\nfor\nmessage\nin\nmessages:\nprint\n(\nf\n\"\n{\nmessage.type\n}\n:\n{\nmessage.content\n}\n\"\n)\nYou can also use\nload_mcp_prompt\ndirectly with a session for more control:\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.prompts\nimport\nload_mcp_prompt\nclient\n=\nMultiServerMCPClient({\n...\n})\nasync\nwith\nclient.session(\n\"server_name\"\n)\nas\nsession:\n# Load a prompt by name\nmessages\n=\nawait\nload_mcp_prompt(session,\n\"summarize\"\n)\n# Load a prompt with arguments\nmessages\n=\nawait\nload_mcp_prompt(\nsession,\n\"code_review\"\n,\narguments\n=\n{\n\"language\"\n:\n\"python\"\n,\n\"focus\"\n:\n\"security\"\n}\n)\n\u200b\nAdvanced features\n\u200b\nTool Interceptors\nMCP servers run as separate processes\u2014they can\u2019t access LangGraph runtime information like the\nstore\n,\ncontext\n, or agent state.\nInterceptors bridge this gap\nby giving you access to this runtime context during MCP tool execution.\nInterceptors also provide middleware-like control over tool calls: you can modify requests, implement retries, add headers dynamically, or short-circuit execution entirely.\nSection\nDescription\nAccessing runtime context\nRead user IDs, API keys, store data, and agent state\nState updates and commands\nUpdate agent state or control graph flow with\nCommand\nWriting interceptors\nPatterns for modifying requests, composing interceptors, and error handling\n\u200b\nAccessing runtime context\nWhen MCP tools are used within a LangChain agent (via\ncreate_agent\n), interceptors receive access to the\nToolRuntime\ncontext. This provides access to the tool call ID, state, config, and store\u2014enabling powerful patterns for accessing user data, persisting information, and controlling agent behavior.\nRuntime context\nStore\nState\nTool call ID\nAccess user-specific configuration like user IDs, API keys, or permissions that are passed at invocation time:\nInject user context into MCP tool calls\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.interceptors\nimport\nMCPToolCallRequest\nfrom\nlangchain.agents\nimport\ncreate_agent\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\napi_key:\nstr\nasync\ndef\ninject_user_context\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"Inject user credentials into MCP tool calls.\"\"\"\nruntime\n=\nrequest.runtime\nuser_id\n=\nruntime.context.user_id\napi_key\n=\nruntime.context.api_key\n# Add user context to tool arguments\nmodified_request\n=\nrequest.override(\nargs\n=\n{\n**\nrequest.args,\n\"user_id\"\n: user_id}\n)\nreturn\nawait\nhandler(modified_request)\nclient\n=\nMultiServerMCPClient(\n{\n...\n},\ntool_interceptors\n=\n[inject_user_context],\n)\ntools\n=\nawait\nclient.get_tools()\nagent\n=\ncreate_agent(\n\"gpt-5-mini\"\n, tools,\ncontext_schema\n=\nContext)\n# Invoke with user context\nresult\n=\nawait\nagent.ainvoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Search my orders\"\n}]},\ncontext\n=\n{\n\"user_id\"\n:\n\"user_123\"\n,\n\"api_key\"\n:\n\"sk-...\"\n}\n)\nAccess long-term memory to retrieve user preferences or persist data across conversations:\nAccess user preferences from store\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.interceptors\nimport\nMCPToolCallRequest\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\nasync\ndef\npersonalize_search\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"Personalize MCP tool calls using stored preferences.\"\"\"\nruntime\n=\nrequest.runtime\nuser_id\n=\nruntime.context.user_id\nstore\n=\nruntime.store\n# Read user preferences from store\nprefs\n=\nstore.get((\n\"preferences\"\n,), user_id)\nif\nprefs\nand\nrequest.name\n==\n\"search\"\n:\n# Apply user's preferred language and result limit\nmodified_args\n=\n{\n**\nrequest.args,\n\"language\"\n: prefs.value.get(\n\"language\"\n,\n\"en\"\n),\n\"limit\"\n: prefs.value.get(\n\"result_limit\"\n,\n10\n),\n}\nrequest\n=\nrequest.override(\nargs\n=\nmodified_args)\nreturn\nawait\nhandler(request)\nclient\n=\nMultiServerMCPClient(\n{\n...\n},\ntool_interceptors\n=\n[personalize_search],\n)\ntools\n=\nawait\nclient.get_tools()\nagent\n=\ncreate_agent(\n\"gpt-5-mini\"\n,\ntools,\ncontext_schema\n=\nContext,\nstore\n=\nInMemoryStore()\n)\nAccess conversation state to make decisions based on the current session:\nFilter tools based on authentication state\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.interceptors\nimport\nMCPToolCallRequest\nfrom\nlangchain.messages\nimport\nToolMessage\nasync\ndef\nrequire_authentication\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"Block sensitive MCP tools if user is not authenticated.\"\"\"\nruntime\n=\nrequest.runtime\nstate\n=\nruntime.state\nis_authenticated\n=\nstate.get(\n\"authenticated\"\n,\nFalse\n)\nsensitive_tools\n=\n[\n\"delete_file\"\n,\n\"update_settings\"\n,\n\"export_data\"\n]\nif\nrequest.name\nin\nsensitive_tools\nand\nnot\nis_authenticated:\n# Return error instead of calling tool\nreturn\nToolMessage(\ncontent\n=\n\"Authentication required. Please log in first.\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\nreturn\nawait\nhandler(request)\nclient\n=\nMultiServerMCPClient(\n{\n...\n},\ntool_interceptors\n=\n[require_authentication],\n)\nAccess the tool call ID to return properly formatted responses or track tool executions:\nReturn custom responses with tool call ID\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.interceptors\nimport\nMCPToolCallRequest\nfrom\nlangchain.messages\nimport\nToolMessage\nasync\ndef\nrate_limit_interceptor\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"Rate limit expensive MCP tool calls.\"\"\"\nruntime\n=\nrequest.runtime\ntool_call_id\n=\nruntime.tool_call_id\n# Check rate limit (simplified example)\nif\nis_rate_limited(request.name):\nreturn\nToolMessage(\ncontent\n=\n\"Rate limit exceeded. Please try again later.\"\n,\ntool_call_id\n=\ntool_call_id,\n)\nresult\n=\nawait\nhandler(request)\n# Log successful tool call\nlog_tool_execution(tool_call_id, request.name,\nsuccess\n=\nTrue\n)\nreturn\nresult\nclient\n=\nMultiServerMCPClient(\n{\n...\n},\ntool_interceptors\n=\n[rate_limit_interceptor],\n)\nFor more context engineering patterns, see\nContext engineering\nand\nTools\n.\n\u200b\nState updates and commands\nInterceptors can return\nCommand\nobjects to update agent state or control graph execution flow. This is useful for tracking task progress, switching between agents, or ending execution early.\nMark task complete and switch agents\nCopy\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_mcp_adapters.interceptors\nimport\nMCPToolCallRequest\nfrom\nlangchain.messages\nimport\nToolMessage\nfrom\nlanggraph.types\nimport\nCommand\nasync\ndef\nhandle_task_completion\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"Mark task complete and hand off to summary agent.\"\"\"\nresult\n=\nawait\nhandler(request)\nif\nrequest.name\n==\n\"submit_order\"\n:\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [result]\nif\nisinstance\n(result, ToolMessage)\nelse\n[],\n\"task_status\"\n:\n\"completed\"\n,\n},\ngoto\n=\n\"summary_agent\"\n,\n)\nreturn\nresult\nUse\nCommand\nwith\ngoto=\"__end__\"\nto end execution early:\nEnd agent run on completion\nCopy\nasync\ndef\nend_on_success\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"End agent run when task is marked complete.\"\"\"\nresult\n=\nawait\nhandler(request)\nif\nrequest.name\n==\n\"mark_complete\"\n:\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [result],\n\"status\"\n:\n\"done\"\n},\ngoto\n=\n\"__end__\"\n,\n)\nreturn\nresult\n\u200b\nCustom interceptors\nInterceptors are async functions that wrap tool execution, enabling request/response modification, retry logic, and other cross-cutting concerns. They follow an \u201conion\u201d pattern where the first interceptor in the list is the outermost layer.\nBasic pattern\nAn interceptor is an async function that receives a request and a handler. You can modify the request before calling the handler, modify the response after, or skip the handler entirely.\nBasic interceptor pattern\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.interceptors\nimport\nMCPToolCallRequest\nasync\ndef\nlogging_interceptor\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"Log tool calls before and after execution.\"\"\"\nprint\n(\nf\n\"Calling tool:\n{\nrequest.name\n}\nwith args:\n{\nrequest.args\n}\n\"\n)\nresult\n=\nawait\nhandler(request)\nprint\n(\nf\n\"Tool\n{\nrequest.name\n}\nreturned:\n{\nresult\n}\n\"\n)\nreturn\nresult\nclient\n=\nMultiServerMCPClient(\n{\n\"math\"\n: {\n\"transport\"\n:\n\"stdio\"\n,\n\"command\"\n:\n\"python\"\n,\n\"args\"\n: [\n\"/path/to/server.py\"\n]}},\ntool_interceptors\n=\n[logging_interceptor],\n)\nModifying requests\nUse\nrequest.override()\nto create a modified request. This follows an immutable pattern, leaving the original request unchanged.\nModifying tool arguments\nCopy\nasync\ndef\ndouble_args_interceptor\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"Double all numeric arguments before execution.\"\"\"\nmodified_args\n=\n{k: v\n*\n2\nfor\nk, v\nin\nrequest.args.items()}\nmodified_request\n=\nrequest.override(\nargs\n=\nmodified_args)\nreturn\nawait\nhandler(modified_request)\n# Original call: add(a=2, b=3) becomes add(a=4, b=6)\nModifying headers at runtime\nInterceptors can modify HTTP headers dynamically based on the request context:\nDynamic header modification\nCopy\nasync\ndef\nauth_header_interceptor\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"Add authentication headers based on the tool being called.\"\"\"\ntoken\n=\nget_token_for_tool(request.name)\nmodified_request\n=\nrequest.override(\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Bearer\n{\ntoken\n}\n\"\n}\n)\nreturn\nawait\nhandler(modified_request)\nComposing interceptors\nMultiple interceptors compose in \u201conion\u201d order \u2014 the first interceptor in the list is the outermost layer:\nComposing multiple interceptors\nCopy\nasync\ndef\nouter_interceptor\n(\nrequest\n,\nhandler\n):\nprint\n(\n\"outer: before\"\n)\nresult\n=\nawait\nhandler(request)\nprint\n(\n\"outer: after\"\n)\nreturn\nresult\nasync\ndef\ninner_interceptor\n(\nrequest\n,\nhandler\n):\nprint\n(\n\"inner: before\"\n)\nresult\n=\nawait\nhandler(request)\nprint\n(\n\"inner: after\"\n)\nreturn\nresult\nclient\n=\nMultiServerMCPClient(\n{\n...\n},\ntool_interceptors\n=\n[outer_interceptor, inner_interceptor],\n)\n# Execution order:\n# outer: before -> inner: before -> tool execution -> inner: after -> outer: after\nError handling\nUse interceptors to catch tool execution errors and implement retry logic:\nRetry on error\nCopy\nimport\nasyncio\nasync\ndef\nretry_interceptor\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\nmax_retries\n:\nint\n=\n3\n,\ndelay\n:\nfloat\n=\n1.0\n,\n):\n\"\"\"Retry failed tool calls with exponential backoff.\"\"\"\nlast_error\n=\nNone\nfor\nattempt\nin\nrange\n(max_retries):\ntry\n:\nreturn\nawait\nhandler(request)\nexcept\nException\nas\ne:\nlast_error\n=\ne\nif\nattempt\n<\nmax_retries\n-\n1\n:\nwait_time\n=\ndelay\n*\n(\n2\n**\nattempt)\n# Exponential backoff\nprint\n(\nf\n\"Tool\n{\nrequest.name\n}\nfailed (attempt\n{\nattempt\n+\n1\n}\n), retrying in\n{\nwait_time\n}\ns...\"\n)\nawait\nasyncio.sleep(wait_time)\nraise\nlast_error\nclient\n=\nMultiServerMCPClient(\n{\n...\n},\ntool_interceptors\n=\n[retry_interceptor],\n)\nYou can also catch specific error types and return fallback values:\nError handling with fallback\nCopy\nasync\ndef\nfallback_interceptor\n(\nrequest\n: MCPToolCallRequest,\nhandler\n,\n):\n\"\"\"Return a fallback value if tool execution fails.\"\"\"\ntry\n:\nreturn\nawait\nhandler(request)\nexcept\nTimeoutError\n:\nreturn\nf\n\"Tool\n{\nrequest.name\n}\ntimed out. Please try again later.\"\nexcept\nConnectionError\n:\nreturn\nf\n\"Could not connect to\n{\nrequest.name\n}\nservice. Using cached data.\"\n\u200b\nProgress notifications\nSubscribe to progress updates for long-running tool executions:\nProgress callback\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.callbacks\nimport\nCallbacks, CallbackContext\nasync\ndef\non_progress\n(\nprogress\n:\nfloat\n,\ntotal\n:\nfloat\n|\nNone\n,\nmessage\n:\nstr\n|\nNone\n,\ncontext\n: CallbackContext,\n):\n\"\"\"Handle progress updates from MCP servers.\"\"\"\npercent\n=\n(progress\n/\ntotal\n*\n100\n)\nif\ntotal\nelse\nprogress\ntool_info\n=\nf\n\" (\n{\ncontext.tool_name\n}\n)\"\nif\ncontext.tool_name\nelse\n\"\"\nprint\n(\nf\n\"[\n{\ncontext.server_name\n}{\ntool_info\n}\n] Progress:\n{\npercent\n:.1f}\n% -\n{\nmessage\n}\n\"\n)\nclient\n=\nMultiServerMCPClient(\n{\n...\n},\ncallbacks\n=\nCallbacks(\non_progress\n=\non_progress),\n)\nThe\nCallbackContext\nprovides:\nserver_name\n: Name of the MCP server\ntool_name\n: Name of the tool being executed (available during tool calls)\n\u200b\nLogging\nThe MCP protocol supports\nlogging\nnotifications from servers. Use the\nCallbacks\nclass to subscribe to these events.\nLogging callback\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.callbacks\nimport\nCallbacks, CallbackContext\nfrom\nmcp.types\nimport\nLoggingMessageNotificationParams\nasync\ndef\non_logging_message\n(\nparams\n: LoggingMessageNotificationParams,\ncontext\n: CallbackContext,\n):\n\"\"\"Handle log messages from MCP servers.\"\"\"\nprint\n(\nf\n\"[\n{\ncontext.server_name\n}\n]\n{\nparams.level\n}\n:\n{\nparams.data\n}\n\"\n)\nclient\n=\nMultiServerMCPClient(\n{\n...\n},\ncallbacks\n=\nCallbacks(\non_logging_message\n=\non_logging_message),\n)\n\u200b\nElicitation\nElicitation\nallows MCP servers to request additional input from users during tool execution. Instead of requiring all inputs upfront, servers can interactively ask for information as needed.\n\u200b\nServer setup\nDefine a tool that uses\nctx.elicit()\nto request user input with a schema:\nMCP server with elicitation\nCopy\nfrom\npydantic\nimport\nBaseModel\nfrom\nmcp.server.fastmcp\nimport\nContext, FastMCP\nserver\n=\nFastMCP(\n\"Profile\"\n)\nclass\nUserDetails\n(\nBaseModel\n):\nemail:\nstr\nage:\nint\n@server.tool\n()\nasync\ndef\ncreate_profile\n(\nname\n:\nstr\n,\nctx\n: Context) ->\nstr\n:\n\"\"\"Create a user profile, requesting details via elicitation.\"\"\"\nresult\n=\nawait\nctx.elicit(\nmessage\n=\nf\n\"Please provide details for\n{\nname\n}\n's profile:\"\n,\nschema\n=\nUserDetails,\n)\nif\nresult.action\n==\n\"accept\"\nand\nresult.data:\nreturn\nf\n\"Created profile for\n{\nname\n}\n: email=\n{\nresult.data.email\n}\n, age=\n{\nresult.data.age\n}\n\"\nif\nresult.action\n==\n\"decline\"\n:\nreturn\nf\n\"User declined. Created minimal profile for\n{\nname\n}\n.\"\nreturn\n\"Profile creation cancelled.\"\nif\n__name__\n==\n\"__main__\"\n:\nserver.run(\ntransport\n=\n\"http\"\n)\n\u200b\nClient setup\nHandle elicitation requests by providing a callback to\nMultiServerMCPClient\n:\nHandling elicitation requests\nCopy\nfrom\nlangchain_mcp_adapters.client\nimport\nMultiServerMCPClient\nfrom\nlangchain_mcp_adapters.callbacks\nimport\nCallbacks, CallbackContext\nfrom\nmcp.shared.context\nimport\nRequestContext\nfrom\nmcp.types\nimport\nElicitRequestParams, ElicitResult\nasync\ndef\non_elicitation\n(\nmcp_context\n: RequestContext,\nparams\n: ElicitRequestParams,\ncontext\n: CallbackContext,\n) -> ElicitResult:\n\"\"\"Handle elicitation requests from MCP servers.\"\"\"\n# In a real application, you would prompt the user for input\n# based on params.message and params.requestedSchema\nreturn\nElicitResult(\naction\n=\n\"accept\"\n,\ncontent\n=\n{\n\"email\"\n:\n\"\n[email\u00a0protected]\n\"\n,\n\"age\"\n:\n25\n},\n)\nclient\n=\nMultiServerMCPClient(\n{\n\"profile\"\n: {\n\"url\"\n:\n\"http://localhost:8000/mcp\"\n,\n\"transport\"\n:\n\"http\"\n,\n}\n},\ncallbacks\n=\nCallbacks(\non_elicitation\n=\non_elicitation),\n)\n\u200b\nResponse actions\nThe elicitation callback can return one of three actions:\nAction\nDescription\naccept\nUser provided valid input. Include the data in the\ncontent\nfield.\ndecline\nUser chose not to provide the requested information.\ncancel\nUser cancelled the operation entirely.\nResponse action examples\nCopy\n# Accept with data\nElicitResult(\naction\n=\n\"accept\"\n,\ncontent\n=\n{\n\"email\"\n:\n\"\n[email\u00a0protected]\n\"\n,\n\"age\"\n:\n25\n})\n# Decline (user doesn't want to provide info)\nElicitResult(\naction\n=\n\"decline\"\n)\n# Cancel (abort the operation)\nElicitResult(\naction\n=\n\"cancel\"\n)\n\u200b\nAdditional resources\nMCP documentation\nMCP Transport documentation\nlangchain-mcp-adapters\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nContext engineering in agents\nPrevious\nHuman-in-the-loop\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/mcp",
      "title": "Model Context Protocol (MCP) - Docs by LangChain",
      "heading": "Model Context Protocol (MCP)"
    }
  },
  {
    "page_content": "Human-in-the-loop - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nAdvanced usage\nHuman-in-the-loop\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nInterrupt decision types\nConfiguring interrupts\nResponding to interrupts\nDecision types\nStreaming with human-in-the-loop\nExecution lifecycle\nCustom HITL logic\nAdvanced usage\nHuman-in-the-loop\nCopy page\nCopy page\nThe Human-in-the-Loop (HITL)\nmiddleware\nlets you add human oversight to agent tool calls.\nWhen a model proposes an action that might require review \u2014 for example, writing to a file or executing SQL \u2014 the middleware can pause execution and wait for a decision.\nIt does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an\ninterrupt\nthat halts execution. The graph state is saved using LangGraph\u2019s\npersistence layer\n, so execution can pause safely and resume later.\nA human decision then determines what happens next: the action can be approved as-is (\napprove\n), modified before running (\nedit\n), or rejected with feedback (\nreject\n).\n\u200b\nInterrupt decision types\nThe\nmiddleware\ndefines three built-in ways a human can respond to an interrupt:\nDecision Type\nDescription\nExample Use Case\n\u2705\napprove\nThe action is approved as-is and executed without changes.\nSend an email draft exactly as written\n\u270f\ufe0f\nedit\nThe tool call is executed with modifications.\nChange the recipient before sending an email\n\u274c\nreject\nThe tool call is rejected, with an explanation added to the conversation.\nReject an email draft and explain how to rewrite it\nThe available decision types for each tool depend on the policy you configure in\ninterrupt_on\n.\nWhen multiple tool calls are paused at the same time, each action requires a separate decision.\nDecisions must be provided in the same order as the actions appear in the interrupt request.\nWhen\nediting\ntool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n\u200b\nConfiguring interrupts\nTo use HITL, add the\nmiddleware\nto the agent\u2019s\nmiddleware\nlist when creating the agent.\nYou configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nHumanInTheLoopMiddleware\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[write_file_tool, execute_sql_tool, read_data_tool],\nmiddleware\n=\n[\nHumanInTheLoopMiddleware(\ninterrupt_on\n=\n{\n\"write_file\"\n:\nTrue\n,\n# All decisions (approve, edit, reject) allowed\n\"execute_sql\"\n: {\n\"allowed_decisions\"\n: [\n\"approve\"\n,\n\"reject\"\n]},\n# No editing allowed\n# Safe operation, no approval needed\n\"read_data\"\n:\nFalse\n,\n},\n# Prefix for interrupt messages - combined with tool name and args to form the full message\n# e.g., \"Tool execution pending approval: execute_sql with query='DELETE FROM...'\"\n# Individual tools can override this by specifying a \"description\" in their interrupt config\ndescription_prefix\n=\n\"Tool execution pending approval\"\n,\n),\n],\n# Human-in-the-loop requires checkpointing to handle interrupts.\n# In production, use a persistent checkpointer like AsyncPostgresSaver.\ncheckpointer\n=\nInMemorySaver(),\n)\nYou must configure a checkpointer to persist the graph state across interrupts.\nIn production, use a persistent checkpointer like\nAsyncPostgresSaver\n. For testing or prototyping, use\nInMemorySaver\n.\nWhen invoking the agent, pass a\nconfig\nthat includes the\nthread ID\nto associate execution with a conversation thread.\nSee the\nLangGraph interrupts documentation\nfor details.\nConfiguration options\n\u200b\ninterrupt_on\ndict\nrequired\nMapping of tool names to approval configs. Values can be\nTrue\n(interrupt with default config),\nFalse\n(auto-approve), or an\nInterruptOnConfig\nobject.\n\u200b\ndescription_prefix\nstring\ndefault:\n\"Tool execution requires approval\"\nPrefix for action request descriptions\nInterruptOnConfig\noptions:\n\u200b\nallowed_decisions\nlist[string]\nList of allowed decisions:\n'approve'\n,\n'edit'\n, or\n'reject'\n\u200b\ndescription\nstring | callable\nStatic string or callable function for custom description\n\u200b\nResponding to interrupts\nWhen you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in\ninterrupt_on\n. In that case, the invocation result will include an\n__interrupt__\nfield with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.\nCopy\nfrom\nlanggraph.types\nimport\nCommand\n# Human-in-the-loop leverages LangGraph's persistence layer.\n# You must provide a thread ID to associate the execution with a conversation thread,\n# so the conversation can be paused and resumed (as is needed for human review).\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"some_id\"\n}}\n# Run the graph until the interrupt is hit.\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Delete old records from the database\"\n,\n}\n]\n},\nconfig\n=\nconfig\n)\n# The interrupt contains the full HITL request with action_requests and review_configs\nprint\n(result[\n'__interrupt__'\n])\n# > [\n# >    Interrupt(\n# >       value={\n# >          'action_requests': [\n# >             {\n# >                'name': 'execute_sql',\n# >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \\'30 days\\';'},\n# >                'description': 'Tool execution pending approval\\n\\nTool: execute_sql\\nArgs: {...}'\n# >             }\n# >          ],\n# >          'review_configs': [\n# >             {\n# >                'action_name': 'execute_sql',\n# >                'allowed_decisions': ['approve', 'reject']\n# >             }\n# >          ]\n# >       }\n# >    )\n# > ]\n# Resume with approval decision\nagent.invoke(\nCommand(\nresume\n=\n{\n\"decisions\"\n: [{\n\"type\"\n:\n\"approve\"\n}]}\n# or \"reject\"\n),\nconfig\n=\nconfig\n# Same thread ID to resume the paused conversation\n)\n\u200b\nDecision types\n\u2705 approve\n\u270f\ufe0f edit\n\u274c reject\nUse\napprove\nto approve the tool call as-is and execute it without changes.\nCopy\nagent.invoke(\nCommand(\n# Decisions are provided as a list, one per action under review.\n# The order of decisions must match the order of actions\n# listed in the `__interrupt__` request.\nresume\n=\n{\n\"decisions\"\n: [\n{\n\"type\"\n:\n\"approve\"\n,\n}\n]\n}\n),\nconfig\n=\nconfig\n# Same thread ID to resume the paused conversation\n)\nUse\nedit\nto modify the tool call before execution.\nProvide the edited action with the new tool name and arguments.\nCopy\nagent.invoke(\nCommand(\n# Decisions are provided as a list, one per action under review.\n# The order of decisions must match the order of actions\n# listed in the `__interrupt__` request.\nresume\n=\n{\n\"decisions\"\n: [\n{\n\"type\"\n:\n\"edit\"\n,\n# Edited action with tool name and args\n\"edited_action\"\n: {\n# Tool name to call.\n# Will usually be the same as the original action.\n\"name\"\n:\n\"new_tool_name\"\n,\n# Arguments to pass to the tool.\n\"args\"\n: {\n\"key1\"\n:\n\"new_value\"\n,\n\"key2\"\n:\n\"original_value\"\n},\n}\n}\n]\n}\n),\nconfig\n=\nconfig\n# Same thread ID to resume the paused conversation\n)\nWhen\nediting\ntool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\nUse\nreject\nto reject the tool call and provide feedback instead of execution.\nCopy\nagent.invoke(\nCommand(\n# Decisions are provided as a list, one per action under review.\n# The order of decisions must match the order of actions\n# listed in the `__interrupt__` request.\nresume\n=\n{\n\"decisions\"\n: [\n{\n\"type\"\n:\n\"reject\"\n,\n# An explanation about why the action was rejected\n\"message\"\n:\n\"No, this is wrong because ..., instead do this ...\"\n,\n}\n]\n}\n),\nconfig\n=\nconfig\n# Same thread ID to resume the paused conversation\n)\nThe\nmessage\nis added to the conversation as feedback to help the agent understand why the action was rejected and what it should do instead.\n\u200b\nMultiple decisions\nWhen multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:\nCopy\n{\n\"decisions\"\n: [\n{\n\"type\"\n:\n\"approve\"\n},\n{\n\"type\"\n:\n\"edit\"\n,\n\"edited_action\"\n: {\n\"name\"\n:\n\"tool_name\"\n,\n\"args\"\n: {\n\"param\"\n:\n\"new_value\"\n}\n}\n},\n{\n\"type\"\n:\n\"reject\"\n,\n\"message\"\n:\n\"This action is not allowed\"\n}\n]\n}\n\u200b\nStreaming with human-in-the-loop\nYou can use\nstream()\ninstead of\ninvoke()\nto get real-time updates while the agent runs and handles interrupts. Use\nstream_mode=['updates', 'messages']\nto stream both agent progress and LLM tokens.\nCopy\nfrom\nlanggraph.types\nimport\nCommand\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"some_id\"\n}}\n# Stream agent progress and LLM tokens until interrupt\nfor\nmode, chunk\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Delete old records from the database\"\n}]},\nconfig\n=\nconfig,\nstream_mode\n=\n[\n\"updates\"\n,\n\"messages\"\n],\n):\nif\nmode\n==\n\"messages\"\n:\n# LLM token\ntoken, metadata\n=\nchunk\nif\ntoken.content:\nprint\n(token.content,\nend\n=\n\"\"\n,\nflush\n=\nTrue\n)\nelif\nmode\n==\n\"updates\"\n:\n# Check for interrupt\nif\n\"__interrupt__\"\nin\nchunk:\nprint\n(\nf\n\"\n\\n\\n\nInterrupt:\n{\nchunk[\n'__interrupt__'\n]\n}\n\"\n)\n# Resume with streaming after human decision\nfor\nmode, chunk\nin\nagent.stream(\nCommand(\nresume\n=\n{\n\"decisions\"\n: [{\n\"type\"\n:\n\"approve\"\n}]}),\nconfig\n=\nconfig,\nstream_mode\n=\n[\n\"updates\"\n,\n\"messages\"\n],\n):\nif\nmode\n==\n\"messages\"\n:\ntoken, metadata\n=\nchunk\nif\ntoken.content:\nprint\n(token.content,\nend\n=\n\"\"\n,\nflush\n=\nTrue\n)\nSee the\nStreaming\nguide for more details on stream modes.\n\u200b\nExecution lifecycle\nThe middleware defines an\nafter_model\nhook that runs after the model generates a response but before any tool calls are executed:\nThe agent invokes the model to generate a response.\nThe middleware inspects the response for tool calls.\nIf any calls require human input, the middleware builds a\nHITLRequest\nwith\naction_requests\nand\nreview_configs\nand calls\ninterrupt\n.\nThe agent waits for human decisions.\nBased on the\nHITLResponse\ndecisions, the middleware executes approved or edited calls, synthesizes\nToolMessage\n\u2019s for rejected calls, and resumes execution.\n\u200b\nCustom HITL logic\nFor more specialized workflows, you can build custom HITL logic directly using the\ninterrupt\nprimitive and\nmiddleware\nabstraction.\nReview the\nexecution lifecycle\nabove to understand how to integrate interrupts into the agent\u2019s operation.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nModel Context Protocol (MCP)\nPrevious\nMulti-agent\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/human-in-the-loop",
      "title": "Human-in-the-loop - Docs by LangChain",
      "heading": "Human-in-the-loop"
    }
  },
  {
    "page_content": "Retrieval - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nAdvanced usage\nRetrieval\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nBuilding a knowledge base\nFrom retrieval to RAG\nRetrieval Pipeline\nBuilding Blocks\nRAG Architectures\n2-step RAG\nAgentic RAG\nHybrid RAG\nAdvanced usage\nRetrieval\nCopy page\nCopy page\nLarge Language Models (LLMs) are powerful, but they have two key limitations:\nFinite context\n\u2014 they can\u2019t ingest entire corpora at once.\nStatic knowledge\n\u2014 their training data is frozen at a point in time.\nRetrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of\nRetrieval-Augmented Generation (RAG)\n: enhancing an LLM\u2019s answers with context-specific information.\n\u200b\nBuilding a knowledge base\nA\nknowledge base\nis a repository of documents or structured data used during retrieval.\nIf you need a custom knowledge base, you can use LangChain\u2019s document loaders and vector stores to build one from your own data.\nIf you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do\nnot\nneed to rebuild it. You can:\nConnect it as a\ntool\nfor an agent in Agentic RAG.\nQuery it and supply the retrieved content as context to the LLM\n(2-Step RAG)\n.\nSee the following tutorial to build a searchable knowledge base and minimal RAG workflow:\nTutorial: Semantic search\nLearn how to create a searchable knowledge base from your own data using LangChain\u2019s document loaders, embeddings, and vector stores.\nIn this tutorial, you\u2019ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You\u2019ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\nLearn more\n\u200b\nFrom retrieval to RAG\nRetrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they\nintegrate retrieval with generation\nto produce grounded, context-aware answers.\nThis is the core idea behind\nRetrieval-Augmented Generation (RAG)\n. The retrieval pipeline becomes a foundation for a broader system that combines search with generation.\n\u200b\nRetrieval Pipeline\nA typical retrieval workflow looks like this:\nEach component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app\u2019s logic.\n\u200b\nBuilding Blocks\nDocument loaders\nIngest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized\nDocument\nobjects.\nLearn more\nText splitters\nBreak large docs into smaller chunks that will be retrievable individually and fit within a model\u2019s context window.\nLearn more\nEmbedding models\nAn embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\nLearn more\nVector stores\nSpecialized databases for storing and searching embeddings.\nLearn more\nRetrievers\nA retriever is an interface that returns documents given an unstructured query.\nLearn more\n\u200b\nRAG Architectures\nRAG can be implemented in multiple ways, depending on your system\u2019s needs. We outline each type in the sections below.\nArchitecture\nDescription\nControl\nFlexibility\nLatency\nExample Use Case\n2-Step RAG\nRetrieval always happens before generation. Simple and predictable\n\u2705 High\n\u274c Low\n\u26a1 Fast\nFAQs, documentation bots\nAgentic RAG\nAn LLM-powered agent decides\nwhen\nand\nhow\nto retrieve during reasoning\n\u274c Low\n\u2705 High\n\u23f3 Variable\nResearch assistants with access to multiple tools\nHybrid\nCombines characteristics of both approaches with validation steps\n\u2696\ufe0f Medium\n\u2696\ufe0f Medium\n\u23f3 Variable\nDomain-specific Q&A with quality validation\nLatency\n: Latency is generally more\npredictable\nin\n2-Step RAG\n, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps\u2014such as API response times, network delays, or database queries\u2014which can vary based on the tools and infrastructure in use.\n\u200b\n2-step RAG\nIn\n2-Step RAG\n, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.\nTutorial: Retrieval-Augmented Generation (RAG)\nSee how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\nThis tutorial walks through two approaches:\nA\nRAG agent\nthat runs searches with a flexible tool\u2014great for general-purpose use.\nA\n2-step RAG\nchain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.\nLearn more\n\u200b\nAgentic RAG\nAgentic Retrieval-Augmented Generation (RAG)\ncombines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides\nwhen\nand\nhow\nto retrieve information during the interaction.\nThe only thing an agent needs to enable RAG behavior is access to one or more\ntools\nthat can fetch external knowledge \u2014 such as documentation loaders, web APIs, or database queries.\nCopy\nimport\nrequests\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain.agents\nimport\ncreate_agent\n@tool\ndef\nfetch_url\n(\nurl\n:\nstr\n) ->\nstr\n:\n\"\"\"Fetch text content from a URL\"\"\"\nresponse\n=\nrequests.get(url,\ntimeout\n=\n10.0\n)\nresponse.raise_for_status()\nreturn\nresponse.text\nsystem_prompt\n=\n\"\"\"\n\\\nUse fetch_url when you need to fetch information from a web-page; quote relevant snippets.\n\"\"\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[fetch_url],\n# A tool for retrieval\nsystem_prompt\n=\nsystem_prompt,\n)\nShow\nExtended example: Agentic RAG for LangGraph's llms.txt\nThis example implements an\nAgentic RAG system\nto assist users in querying LangGraph documentation. The agent begins by loading\nllms.txt\n, which lists available documentation URLs, and can then dynamically use a\nfetch_documentation\ntool to retrieve and process the relevant content based on the user\u2019s question.\nCopy\nimport\nrequests\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.messages\nimport\nHumanMessage\nfrom\nlangchain.tools\nimport\ntool\nfrom\nmarkdownify\nimport\nmarkdownify\nALLOWED_DOMAINS\n=\n[\n\"https://langchain-ai.github.io/\"\n]\nLLMS_TXT\n=\n'https://langchain-ai.github.io/langgraph/llms.txt'\n@tool\ndef\nfetch_documentation\n(\nurl\n:\nstr\n) ->\nstr\n:\n\"\"\"Fetch and convert documentation from a URL\"\"\"\nif\nnot\nany\n(url.startswith(domain)\nfor\ndomain\nin\nALLOWED_DOMAINS\n):\nreturn\n(\n\"Error: URL not allowed. \"\nf\n\"Must start with one of:\n{\n', '\n.join(\nALLOWED_DOMAINS\n)\n}\n\"\n)\nresponse\n=\nrequests.get(url,\ntimeout\n=\n10.0\n)\nresponse.raise_for_status()\nreturn\nmarkdownify(response.text)\n# We will fetch the content of llms.txt, so this can\n# be done ahead of time without requiring an LLM request.\nllms_txt_content\n=\nrequests.get(\nLLMS_TXT\n).text\n# System prompt for the agent\nsystem_prompt\n=\nf\n\"\"\"\nYou are an expert Python developer and technical assistant.\nYour primary role is to help users with questions about LangGraph and related tools.\nInstructions:\n1. If a user asks a question you're unsure about \u2014 or one that likely involves API usage,\nbehavior, or configuration \u2014 you MUST use the `fetch_documentation` tool to consult the relevant docs.\n2. When citing documentation, summarize clearly and include relevant context from the content.\n3. Do not use any URLs outside of the allowed domain.\n4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.\nYou can access official documentation from the following approved sources:\n{\nllms_txt_content\n}\nYou MUST consult the documentation to get up to date documentation\nbefore answering a user's question about LangGraph.\nYour answers should be clear, concise, and technically accurate.\n\"\"\"\ntools\n=\n[fetch_documentation]\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-0\"\n,\nmax_tokens\n=\n32_000\n)\nagent\n=\ncreate_agent(\nmodel\n=\nmodel,\ntools\n=\ntools,\nsystem_prompt\n=\nsystem_prompt,\nname\n=\n\"Agentic RAG\"\n,\n)\nresponse\n=\nagent.invoke({\n'messages'\n: [\nHumanMessage(\ncontent\n=\n(\n\"Write a short example of a langgraph agent using the \"\n\"prebuilt create react agent. the agent should be able \"\n\"to look up stock pricing information.\"\n))\n]\n})\nprint\n(response[\n'messages'\n][\n-\n1\n].content)\nTutorial: Retrieval-Augmented Generation (RAG)\nSee how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\nThis tutorial walks through two approaches:\nA\nRAG agent\nthat runs searches with a flexible tool\u2014great for general-purpose use.\nA\n2-step RAG\nchain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.\nLearn more\n\u200b\nHybrid RAG\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\nTypical components include:\nQuery enhancement\n: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\nRetrieval validation\n: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\nAnswer validation\n: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\nThe architecture often supports multiple iterations between these steps:\nThis architecture is suitable for:\nApplications with ambiguous or underspecified queries\nSystems that require validation or quality control steps\nWorkflows involving multiple sources or iterative refinement\nTutorial: Agentic RAG with Self-Correction\nAn example of\nHybrid RAG\nthat combines agentic reasoning with retrieval and self-correction.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nCustom workflow\nPrevious\nLong-term memory\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/retrieval",
      "title": "Retrieval - Docs by LangChain",
      "heading": "Retrieval"
    }
  },
  {
    "page_content": "Long-term memory - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nAdvanced usage\nLong-term memory\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nMemory storage\nRead long-term memory in tools\nWrite long-term memory from tools\nAdvanced usage\nLong-term memory\nCopy page\nCopy page\n\u200b\nOverview\nLangChain agents use\nLangGraph persistence\nto enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.\n\u200b\nMemory storage\nLangGraph stores long-term memories as JSON documents in a\nstore\n.\nEach memory is organized under a custom\nnamespace\n(similar to a folder) and a distinct\nkey\n(like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.\nThis structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\nCopy\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\ndef\nembed\n(\ntexts\n: list[\nstr\n]) -> list[list[\nfloat\n]]:\n# Replace with an actual embedding function or LangChain embeddings object\nreturn\n[[\n1.0\n,\n2.0\n]\n*\nlen\n(texts)]\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore\n=\nInMemoryStore(\nindex\n=\n{\n\"embed\"\n: embed,\n\"dims\"\n:\n2\n})\nuser_id\n=\n\"my-user\"\napplication_context\n=\n\"chitchat\"\nnamespace\n=\n(user_id, application_context)\nstore.put(\nnamespace,\n\"a-memory\"\n,\n{\n\"rules\"\n: [\n\"User likes short, direct language\"\n,\n\"User only speaks English & python\"\n,\n],\n\"my-key\"\n:\n\"my-value\"\n,\n},\n)\n# get the \"memory\" by ID\nitem\n=\nstore.get(namespace,\n\"a-memory\"\n)\n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems\n=\nstore.search(\nnamespace,\nfilter\n=\n{\n\"my-key\"\n:\n\"my-value\"\n},\nquery\n=\n\"language preferences\"\n)\nFor more information about the memory store, see the\nPersistence\nguide.\n\u200b\nRead long-term memory in tools\nA tool the agent can use to look up user information\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore\n=\nInMemoryStore()\n# Write sample data to the store using the put method\nstore.put(\n(\n\"users\"\n,),\n# Namespace to group related data together (users namespace for user data)\n\"user_123\"\n,\n# Key within the namespace (user ID as key)\n{\n\"name\"\n:\n\"John Smith\"\n,\n\"language\"\n:\n\"English\"\n,\n}\n# Data to store for the given user\n)\n@tool\ndef\nget_user_info\n(\nruntime\n: ToolRuntime[Context]) ->\nstr\n:\n\"\"\"Look up user info.\"\"\"\n# Access the store - same as that provided to `create_agent`\nstore\n=\nruntime.store\nuser_id\n=\nruntime.context.user_id\n# Retrieve data from store - returns StoreValue object with value and metadata\nuser_info\n=\nstore.get((\n\"users\"\n,), user_id)\nreturn\nstr\n(user_info.value)\nif\nuser_info\nelse\n\"Unknown user\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_user_info],\n# Pass store to agent - enables agent to access store when running tools\nstore\n=\nstore,\ncontext_schema\n=\nContext\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"look up user information\"\n}]},\ncontext\n=\nContext(\nuser_id\n=\n\"user_123\"\n)\n)\n\u200b\nWrite long-term memory from tools\nExample of a tool that updates user information\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore\n=\nInMemoryStore()\n@dataclass\nclass\nContext\n:\nuser_id:\nstr\n# TypedDict defines the structure of user information for the LLM\nclass\nUserInfo\n(\nTypedDict\n):\nname:\nstr\n# Tool that allows agent to update user information (useful for chat applications)\n@tool\ndef\nsave_user_info\n(\nuser_info\n: UserInfo,\nruntime\n: ToolRuntime[Context]) ->\nstr\n:\n\"\"\"Save user info.\"\"\"\n# Access the store - same as that provided to `create_agent`\nstore\n=\nruntime.store\nuser_id\n=\nruntime.context.user_id\n# Store data in the store (namespace, key, data)\nstore.put((\n\"users\"\n,), user_id, user_info)\nreturn\n\"Successfully saved user info.\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[save_user_info],\nstore\n=\nstore,\ncontext_schema\n=\nContext\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"My name is John Smith\"\n}]},\n# user_id passed in context to identify whose information is being updated\ncontext\n=\nContext(\nuser_id\n=\n\"user_123\"\n)\n)\n# You can access the store directly to get the value\nstore.get((\n\"users\"\n,),\n\"user_123\"\n).value\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nRetrieval\nPrevious\nLangSmith Studio\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/long-term-memory",
      "title": "Long-term memory - Docs by LangChain",
      "heading": "Long-term memory"
    }
  },
  {
    "page_content": "Quickstart - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nGet started\nQuickstart\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nQuickstart\nCustomization\nCore capabilities\nAgent harness\nBackends\nSubagents\nHuman-in-the-loop\nLong-term memory\nMiddleware\nCommand line interface\nUse the CLI\nOn this page\nPrerequisites\nStep 1: Install dependencies\nStep 2: Set up your API keys\nStep 3: Create a search tool\nStep 4: Create a deep agent\nStep 5: Run the agent\nWhat happened?\nNext steps\nGet started\nQuickstart\nCopy page\nBuild your first deep agent in minutes\nCopy page\nThis guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You\u2019ll build a research agent that can conduct research and write reports.\n\u200b\nPrerequisites\nBefore you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).\n\u200b\nStep 1: Install dependencies\npip\nuv\npoetry\nCopy\npip\ninstall\ndeepagents\ntavily-python\n\u200b\nStep 2: Set up your API keys\nCopy\nexport\nANTHROPIC_API_KEY\n=\n\"your-api-key\"\nexport\nTAVILY_API_KEY\n=\n\"your-tavily-api-key\"\n\u200b\nStep 3: Create a search tool\nCopy\nimport\nos\nfrom\ntyping\nimport\nLiteral\nfrom\ntavily\nimport\nTavilyClient\nfrom\ndeepagents\nimport\ncreate_deep_agent\ntavily_client\n=\nTavilyClient(\napi_key\n=\nos.environ[\n\"TAVILY_API_KEY\"\n])\ndef\ninternet_search\n(\nquery\n:\nstr\n,\nmax_results\n:\nint\n=\n5\n,\ntopic\n: Literal[\n\"general\"\n,\n\"news\"\n,\n\"finance\"\n]\n=\n\"general\"\n,\ninclude_raw_content\n:\nbool\n=\nFalse\n,\n):\n\"\"\"Run a web search\"\"\"\nreturn\ntavily_client.search(\nquery,\nmax_results\n=\nmax_results,\ninclude_raw_content\n=\ninclude_raw_content,\ntopic\n=\ntopic,\n)\n\u200b\nStep 4: Create a deep agent\nCopy\n# System prompt to steer the agent to be an expert researcher\nresearch_instructions\n=\n\"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\nYou have access to an internet search tool as your primary means of gathering information.\n## `internet_search`\nUse this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n\"\"\"\nagent\n=\ncreate_deep_agent(\ntools\n=\n[internet_search],\nsystem_prompt\n=\nresearch_instructions\n)\n\u200b\nStep 5: Run the agent\nCopy\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is langgraph?\"\n}]})\n# Print the agent's response\nprint\n(result[\n\"messages\"\n][\n-\n1\n].content)\n\u200b\nWhat happened?\nYour deep agent automatically:\nPlanned its approach\n: Used the built-in\nwrite_todos\ntool to break down the research task\nConducted research\n: Called the\ninternet_search\ntool to gather information\nManaged context\n: Used file system tools (\nwrite_file\n,\nread_file\n) to offload large search results\nSpawned subagents\n(if needed): Delegated complex subtasks to specialized subagents\nSynthesized a report\n: Compiled findings into a coherent response\n\u200b\nNext steps\nNow that you\u2019ve built your first deep agent:\nCustomize your agent\n: Learn about\ncustomization options\n, including custom system prompts, tools, and subagents.\nUnderstand middleware\n: Dive into the\nmiddleware architecture\nthat powers deep agents.\nAdd long-term memory\n: Enable\npersistent memory\nacross conversations.\nDeploy to production\n: Learn about\ndeployment options\nfor LangGraph applications.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nDeep Agents overview\nPrevious\nCustomize Deep Agents\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/deepagents/quickstart",
      "title": "Quickstart - Docs by LangChain",
      "heading": "Quickstart"
    }
  },
  {
    "page_content": "Customize Deep Agents - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nGet started\nCustomize Deep Agents\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nQuickstart\nCustomization\nCore capabilities\nAgent harness\nBackends\nSubagents\nHuman-in-the-loop\nLong-term memory\nMiddleware\nCommand line interface\nUse the CLI\nOn this page\nModel\nSystem prompt\nTools\nGet started\nCustomize Deep Agents\nCopy page\nLearn how to customize deep agents with system prompts, tools, subagents, and more\nCopy page\n\u200b\nModel\nBy default,\ndeepagents\nuses\nclaude-sonnet-4-5-20250929\n. You can customize the model used by passing any supported\nmodel identifier string\nor\nLangChain model object\n.\nModel string\nLangChain model object\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\ndeepagents\nimport\ncreate_deep_agent\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"gpt-5\"\n)\nagent\n=\ncreate_deep_agent(\nmodel\n=\nmodel)\n\u200b\nSystem prompt\nDeep agents come with a built-in system prompt inspired by Claude Code\u2019s system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents.\nEach deep agent tailored to a use case should include a custom system prompt specific to that use case.\nCopy\nfrom\ndeepagents\nimport\ncreate_deep_agent\nresearch_instructions\n=\n\"\"\"\n\\\nYou are an expert researcher. Your job is to conduct\n\\\nthorough research, and then write a polished report.\n\\\n\"\"\"\nagent\n=\ncreate_deep_agent(\nsystem_prompt\n=\nresearch_instructions,\n)\n\u200b\nTools\nJust like tool-calling agents, a deep agent gets a set of top level tools that it has access to.\nCopy\nimport\nos\nfrom\ntyping\nimport\nLiteral\nfrom\ntavily\nimport\nTavilyClient\nfrom\ndeepagents\nimport\ncreate_deep_agent\ntavily_client\n=\nTavilyClient(\napi_key\n=\nos.environ[\n\"TAVILY_API_KEY\"\n])\ndef\ninternet_search\n(\nquery\n:\nstr\n,\nmax_results\n:\nint\n=\n5\n,\ntopic\n: Literal[\n\"general\"\n,\n\"news\"\n,\n\"finance\"\n]\n=\n\"general\"\n,\ninclude_raw_content\n:\nbool\n=\nFalse\n,\n):\n\"\"\"Run a web search\"\"\"\nreturn\ntavily_client.search(\nquery,\nmax_results\n=\nmax_results,\ninclude_raw_content\n=\ninclude_raw_content,\ntopic\n=\ntopic,\n)\nagent\n=\ncreate_deep_agent(\ntools\n=\n[internet_search]\n)\nIn addition to any tools that you provide, deep agents also get access to a number of default tools:\nwrite_todos\n\u2013 Update the agent\u2019s to-do list\nls\n\u2013 List all files in the agent\u2019s filesystem\nread_file\n\u2013 Read a file from the agent\u2019s filesystem\nwrite_file\n\u2013 Write a new file in the agent\u2019s filesystem\nedit_file\n\u2013 Edit an existing file in the agent\u2019s filesystem\ntask\n\u2013 Spawn a subagent to handle a specific task\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nQuickstart\nPrevious\nAgent harness capabilities\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/deepagents/customization",
      "title": "Customize Deep Agents - Docs by LangChain",
      "heading": "Customize Deep Agents"
    }
  },
  {
    "page_content": "Agent harness capabilities - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore capabilities\nAgent harness capabilities\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nQuickstart\nCustomization\nCore capabilities\nAgent harness\nBackends\nSubagents\nHuman-in-the-loop\nLong-term memory\nMiddleware\nCommand line interface\nUse the CLI\nOn this page\nFile system access\nLarge tool result eviction\nPluggable storage backends\nTask delegation (subagents)\nConversation history summarization\nDangling tool call repair\nTo-do list tracking\nHuman-in-the-Loop\nPrompt caching (Anthropic)\nCore capabilities\nAgent harness capabilities\nCopy page\nCopy page\nWe think of\ndeepagents\nas an\n\u201cagent harness\u201d\n. It is the same core tool calling loop as other agent frameworks, but with built-in tools and capabilities.\nThis page lists out the components that make up the agent harness.\n\u200b\nFile system access\nThe harness provides six tools for file system operations, making files first-class citizens in the agent\u2019s environment:\nTool\nDescription\nls\nList files in a directory with metadata (size, modified time)\nread_file\nRead file contents with line numbers, supports offset/limit for large files\nwrite_file\nCreate new files\nedit_file\nPerform exact string replacements in files (with global replace mode)\nglob\nFind files matching patterns (e.g.,\n**/*.py\n)\ngrep\nSearch file contents with multiple output modes (files only, content with context, or counts)\n\u200b\nLarge tool result eviction\nThe harness automatically dumps large tool results to the file system when they exceed a token threshold, preventing context window saturation.\nHow it works:\nMonitors tool call results for size (default threshold: 20,000 tokens)\nWhen exceeded, writes the result to a file instead\nReplaces the tool result with a concise reference to the file\nAgent can later read the file if needed\n\u200b\nPluggable storage backends\nThe harness abstracts file system operations behind a protocol, allowing different storage strategies for different use cases.\nAvailable backends:\nStateBackend\n- Ephemeral in-memory storage\nFiles live in the agent\u2019s state (checkpointed with conversation)\nPersists within a thread but not across threads\nUseful for temporary working files\nFilesystemBackend\n- Real filesystem access\nRead/write from actual disk\nSupports virtual mode (sandboxed to a root directory)\nIntegrates with system tools (ripgrep for grep)\nSecurity features: path validation, size limits, symlink prevention\nStoreBackend\n- Persistent cross-conversation storage\nUses LangGraph\u2019s BaseStore for durability\nNamespaced per assistant_id\nFiles persist across conversations\nUseful for long-term memory or knowledge bases\nCompositeBackend\n- Route different paths to different backends\nExample:\n/\n\u2192 StateBackend,\n/memories/\n\u2192 StoreBackend\nLongest-prefix matching for routing\nEnables hybrid storage strategies\n\u200b\nTask delegation (subagents)\nThe harness allows the main agent to create ephemeral \u201csubagents\u201d for isolated multi-step tasks.\nWhy it\u2019s useful:\nContext isolation\n- Subagent\u2019s work doesn\u2019t clutter main agent\u2019s context\nParallel execution\n- Multiple subagents can run concurrently\nSpecialization\n- Subagents can have different tools/configurations\nToken efficiency\n- Large subtask context is compressed into a single result\nHow it works:\nMain agent has a\ntask\ntool\nWhen invoked, creates a fresh agent instance with its own context\nSubagent executes autonomously until completion\nReturns a single final report to the main agent\nSubagents are stateless (can\u2019t send multiple messages back)\nDefault subagent:\n\u201cgeneral-purpose\u201d subagent automatically available\nHas filesystem tools by default\nCan be customized with additional tools/middleware\nCustom subagents:\nDefine specialized subagents with specific tools\nExample: code-reviewer, web-researcher, test-runner\nConfigure via\nsubagents\nparameter\n\u200b\nConversation history summarization\nThe harness automatically compresses old conversation history when token usage becomes excessive.\nConfiguration:\nTriggers at 170,000 tokens\nKeeps the most recent 6 messages intact\nOlder messages are summarized by the model\nWhy it\u2019s useful:\nEnables very long conversations without hitting context limits\nPreserves recent context while compressing ancient history\nTransparent to the agent (appears as a special system message)\n\u200b\nDangling tool call repair\nThe harness fixes message history when tool calls are interrupted or cancelled before receiving results.\nThe problem:\nAgent requests tool call: \u201cPlease run X\u201d\nTool call is interrupted (user cancels, error, etc.)\nAgent sees tool_call in AIMessage but no corresponding ToolMessage\nThis creates an invalid message sequence\nThe solution:\nDetects AIMessages with tool_calls that have no results\nCreates synthetic ToolMessage responses indicating the call was cancelled\nRepairs the message history before agent execution\nWhy it\u2019s useful:\nPrevents agent confusion from incomplete message chains\nGracefully handles interruptions and errors\nMaintains conversation coherence\n\u200b\nTo-do list tracking\nThe harness provides a\nwrite_todos\ntool that agents can use to maintain a structured task list.\nFeatures:\nTrack multiple tasks with statuses (pending, in_progress, completed)\nPersisted in agent state\nHelps agent organize complex multi-step work\nUseful for long-running tasks and planning\n\u200b\nHuman-in-the-Loop\nThe harness pauses agent execution at specified tool calls to allow human approval/modification.\nConfiguration:\nMap tool names to interrupt configurations\nExample:\n{\"edit_file\": True}\n- pause before every edit\nCan provide approval messages or modify tool inputs\nWhy it\u2019s useful:\nSafety gates for destructive operations\nUser verification before expensive API calls\nInteractive debugging and guidance\n\u200b\nPrompt caching (Anthropic)\nThe harness enables Anthropic\u2019s prompt caching feature to reduce redundant token processing.\nHow it works:\nCaches portions of the prompt that repeat across turns\nSignificantly reduces latency and cost for long system prompts\nAutomatically skips for non-Anthropic models\nWhy it\u2019s useful:\nSystem prompts (especially with filesystem docs) can be 5k+ tokens\nThese repeat every turn without caching\nCaching provides ~10x speedup and cost reduction\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nCustomize Deep Agents\nPrevious\nBackends\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/deepagents/harness",
      "title": "Agent harness capabilities - Docs by LangChain",
      "heading": "Agent harness capabilities"
    }
  },
  {
    "page_content": "Backends - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore capabilities\nBackends\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nQuickstart\nCustomization\nCore capabilities\nAgent harness\nBackends\nSubagents\nHuman-in-the-loop\nLong-term memory\nMiddleware\nCommand line interface\nUse the CLI\nOn this page\nQuickstart\nBuilt-in backends\nStateBackend (ephemeral)\nFilesystemBackend (local disk)\nStoreBackend (LangGraph Store)\nCompositeBackend (router)\nSpecify a backend\nRoute to different backends\nUse a virtual filesystem\nAdd policy hooks\nProtocol reference\nCore capabilities\nBackends\nCopy page\nChoose and configure filesystem backends for deep agents. You can specify routes to different backends, implement virtual filesystems, and enforce policies.\nCopy page\nDeep agents expose a filesystem surface to the agent via tools like\nls\n,\nread_file\n,\nwrite_file\n,\nedit_file\n,\nglob\n, and\ngrep\n. These tools operate through a pluggable backend.\nThis page explains how to\nchoose a backend\n,\nroute different paths to different backends\n,\nimplement your own virtual filesystem\n(e.g., S3 or Postgres),\nadd policy hooks\n, and\ncomply with the backend protocol\n.\n\u200b\nQuickstart\nHere are a few pre-built filesystem backends that you can quickly use with your deep agent:\nBuilt-in backend\nDescription\nDefault\nagent = create_deep_agent()\nEphemeral in state. The default filesystem backend for an agent is stored in\nlanggraph\nstate. Note that this filesystem only persists\nfor a single thread\n.\nLocal filesystem persistence\nagent = create_deep_agent(backend=FilesystemBackend(root_dir=\"/Users/nh/Desktop/\"))\nThis gives the deep agent access to your local machine\u2019s filesystem. You can specify the root directory that the agent has access to. Note that any provided\nroot_dir\nmust be an absolute path.\nDurable store (LangGraph store)\nagent = create_deep_agent(backend=lambda rt: StoreBackend(rt))\nThis gives the agent access to long-term storage that is\npersisted across threads\n. This is great for storing longer term memories or instructions that are applicable to the agent over multiple executions.\nComposite\nEphemeral by default,\n/memories/\npersisted. The Composite backend is maximally flexible. You can specify different routes in the filesystem to point towards different backends. See Composite routing below for a ready-to-paste example.\n\u200b\nBuilt-in backends\n\u200b\nStateBackend (ephemeral)\nCopy\n# By default we provide a StateBackend\nagent\n=\ncreate_deep_agent()\n# Under the hood, it looks like\nfrom\ndeepagents.backends\nimport\nStateBackend\nagent\n=\ncreate_deep_agent(\nbackend\n=\n(\nlambda\nrt\n: StateBackend(rt))\n# Note that the tools access State through the runtime.state\n)\nHow it works:\nStores files in LangGraph agent state for the current thread.\nPersists across multiple agent turns on the same thread via checkpoints.\nBest for:\nA scratch pad for the agent to write intermediate results.\nAutomatic eviction of large tool outputs which the agent can then read back in piece by piece.\n\u200b\nFilesystemBackend (local disk)\nCopy\nfrom\ndeepagents.backends\nimport\nFilesystemBackend\nagent\n=\ncreate_deep_agent(\nbackend\n=\nFilesystemBackend(\nroot_dir\n=\n\".\"\n,\nvirtual_mode\n=\nTrue\n)\n)\nHow it works:\nReads/writes real files under a configurable\nroot_dir\n.\nYou can optionally set\nvirtual_mode=True\nto sandbox and normalize paths under\nroot_dir\n.\nUses secure path resolution, prevents unsafe symlink traversal when possible, can use ripgrep for fast\ngrep\n.\nBest for:\nLocal projects on your machine\nCI sandboxes\nMounted persistent volumes\n\u200b\nStoreBackend (LangGraph Store)\nCopy\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nfrom\ndeepagents.backends\nimport\nStoreBackend\nagent\n=\ncreate_deep_agent(\nbackend\n=\n(\nlambda\nrt\n: StoreBackend(rt)),\n# Note that the tools access Store through the runtime.store\nstore\n=\nInMemoryStore()\n)\nHow it works:\nStores files in a LangGraph\nBaseStore\nprovided by the runtime, enabling cross\u2011thread durable storage.\nBest for:\nWhen you already run with a configured LangGraph store (for example, Redis, Postgres, or cloud implementations behind\nBaseStore\n).\nWhen you\u2019re deploying your agent through LangSmith Deployment (a store is automatically provisioned for your agent).\n\u200b\nCompositeBackend (router)\nCopy\nfrom\ndeepagents\nimport\ncreate_deep_agent\nfrom\ndeepagents.backends\nimport\nCompositeBackend, StateBackend, StoreBackend\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\ncomposite_backend\n=\nlambda\nrt\n: CompositeBackend(\ndefault\n=\nStateBackend(rt),\nroutes\n=\n{\n\"/memories/\"\n: StoreBackend(rt),\n}\n)\nagent\n=\ncreate_deep_agent(\nbackend\n=\ncomposite_backend,\nstore\n=\nInMemoryStore()\n# Store passed to create_deep_agent, not backend\n)\nHow it works:\nRoutes file operations to different backends based on path prefix.\nPreserves the original path prefixes in listings and search results.\nBest for:\nWhen you want to give your agent both ephemeral and cross-thread storage, a CompositeBackend allows you provide both a StateBackend and StoreBackend\nWhen you have multiple sources of information that you want to provide to your agent as part of a single filesystem.\ne.g. You have long-term memories stored under /memories/ in one Store and you also have a custom backend that has documentation accessible at /docs/.\n\u200b\nSpecify a backend\nPass a backend to\ncreate_deep_agent(backend=...)\n. The filesystem middleware uses it for all tooling.\nYou can pass either:\nAn instance implementing\nBackendProtocol\n(for example,\nFilesystemBackend(root_dir=\".\")\n), or\nA factory\nBackendFactory = Callable[[ToolRuntime], BackendProtocol]\n(for backends that need runtime like\nStateBackend\nor\nStoreBackend\n).\nIf omitted, the default is\nlambda rt: StateBackend(rt)\n.\n\u200b\nRoute to different backends\nRoute parts of the namespace to different backends. Commonly used to persist\n/memories/*\nand keep everything else ephemeral.\nCopy\nfrom\ndeepagents\nimport\ncreate_deep_agent\nfrom\ndeepagents.backends\nimport\nCompositeBackend, StateBackend, FilesystemBackend\ncomposite_backend\n=\nlambda\nrt\n: CompositeBackend(\ndefault\n=\nStateBackend(rt),\nroutes\n=\n{\n\"/memories/\"\n: FilesystemBackend(\nroot_dir\n=\n\"/deepagents/myagent\"\n,\nvirtual_mode\n=\nTrue\n),\n},\n)\nagent\n=\ncreate_deep_agent(\nbackend\n=\ncomposite_backend)\nBehavior:\n/workspace/plan.md\n\u2192 StateBackend (ephemeral)\n/memories/agent.md\n\u2192 FilesystemBackend under\n/deepagents/myagent\nls\n,\nglob\n,\ngrep\naggregate results and show original path prefixes.\nNotes:\nLonger prefixes win (for example, route\n\"/memories/projects/\"\ncan override\n\"/memories/\"\n).\nFor StoreBackend routing, ensure the agent runtime provides a store (\nruntime.store\n).\n\u200b\nUse a virtual filesystem\nBuild a custom backend to project a remote or database filesystem (e.g., S3 or Postgres) into the tools namespace.\nDesign guidelines:\nPaths are absolute (\n/x/y.txt\n). Decide how to map them to your storage keys/rows.\nImplement\nls_info\nand\nglob_info\nefficiently (server-side listing where available, otherwise local filter).\nReturn user-readable error strings for missing files or invalid regex patterns.\nFor external persistence, set\nfiles_update=None\nin results; only in-state backends should return a\nfiles_update\ndict.\nS3-style outline:\nCopy\nfrom\ndeepagents.backends.protocol\nimport\nBackendProtocol, WriteResult, EditResult\nfrom\ndeepagents.backends.utils\nimport\nFileInfo, GrepMatch\nclass\nS3Backend\n(\nBackendProtocol\n):\ndef\n__init__\n(\nself\n,\nbucket\n:\nstr\n,\nprefix\n:\nstr\n=\n\"\"\n):\nself\n.bucket\n=\nbucket\nself\n.prefix\n=\nprefix.rstrip(\n\"/\"\n)\ndef\n_key\n(\nself\n,\npath\n:\nstr\n) ->\nstr\n:\nreturn\nf\n\"\n{\nself\n.prefix\n}{\npath\n}\n\"\ndef\nls_info\n(\nself\n,\npath\n:\nstr\n) -> list[FileInfo]:\n# List objects under _key(path); build FileInfo entries (path, size, modified_at)\n...\ndef\nread\n(\nself\n,\nfile_path\n:\nstr\n,\noffset\n:\nint\n=\n0\n,\nlimit\n:\nint\n=\n2000\n) ->\nstr\n:\n# Fetch object; return numbered content or an error string\n...\ndef\ngrep_raw\n(\nself\n,\npattern\n:\nstr\n,\npath\n:\nstr\n|\nNone\n=\nNone\n,\nglob\n:\nstr\n|\nNone\n=\nNone\n) -> list[GrepMatch]\n|\nstr\n:\n# Optionally filter server\u2011side; else list and scan content\n...\ndef\nglob_info\n(\nself\n,\npattern\n:\nstr\n,\npath\n:\nstr\n=\n\"/\"\n) -> list[FileInfo]:\n# Apply glob relative to path across keys\n...\ndef\nwrite\n(\nself\n,\nfile_path\n:\nstr\n,\ncontent\n:\nstr\n) -> WriteResult:\n# Enforce create\u2011only semantics; return WriteResult(path=file_path, files_update=None)\n...\ndef\nedit\n(\nself\n,\nfile_path\n:\nstr\n,\nold_string\n:\nstr\n,\nnew_string\n:\nstr\n,\nreplace_all\n:\nbool\n=\nFalse\n) -> EditResult:\n# Read \u2192 replace (respect uniqueness vs replace_all) \u2192 write \u2192 return occurrences\n...\nPostgres-style outline:\nTable\nfiles(path text primary key, content text, created_at timestamptz, modified_at timestamptz)\nMap tool operations onto SQL:\nls_info\nuses\nWHERE path LIKE $1 || '%'\nglob_info\nfilter in SQL or fetch then apply glob in Python\ngrep_raw\ncan fetch candidate rows by extension or last modified time, then scan lines\n\u200b\nAdd policy hooks\nEnforce enterprise rules by subclassing or wrapping a backend.\nBlock writes/edits under selected prefixes (subclass):\nCopy\nfrom\ndeepagents.backends.filesystem\nimport\nFilesystemBackend\nfrom\ndeepagents.backends.protocol\nimport\nWriteResult, EditResult\nclass\nGuardedBackend\n(\nFilesystemBackend\n):\ndef\n__init__\n(\nself\n,\n*\n,\ndeny_prefixes\n: list[\nstr\n],\n**\nkwargs\n):\nsuper\n().\n__init__\n(\n**\nkwargs)\nself\n.deny_prefixes\n=\n[p\nif\np.endswith(\n\"/\"\n)\nelse\np\n+\n\"/\"\nfor\np\nin\ndeny_prefixes]\ndef\nwrite\n(\nself\n,\nfile_path\n:\nstr\n,\ncontent\n:\nstr\n) -> WriteResult:\nif\nany\n(file_path.startswith(p)\nfor\np\nin\nself\n.deny_prefixes):\nreturn\nWriteResult(\nerror\n=\nf\n\"Writes are not allowed under\n{\nfile_path\n}\n\"\n)\nreturn\nsuper\n().write(file_path, content)\ndef\nedit\n(\nself\n,\nfile_path\n:\nstr\n,\nold_string\n:\nstr\n,\nnew_string\n:\nstr\n,\nreplace_all\n:\nbool\n=\nFalse\n) -> EditResult:\nif\nany\n(file_path.startswith(p)\nfor\np\nin\nself\n.deny_prefixes):\nreturn\nEditResult(\nerror\n=\nf\n\"Edits are not allowed under\n{\nfile_path\n}\n\"\n)\nreturn\nsuper\n().edit(file_path, old_string, new_string, replace_all)\nGeneric wrapper (works with any backend):\nCopy\nfrom\ndeepagents.backends.protocol\nimport\nBackendProtocol, WriteResult, EditResult\nfrom\ndeepagents.backends.utils\nimport\nFileInfo, GrepMatch\nclass\nPolicyWrapper\n(\nBackendProtocol\n):\ndef\n__init__\n(\nself\n,\ninner\n: BackendProtocol,\ndeny_prefixes\n: list[\nstr\n]\n|\nNone\n=\nNone\n):\nself\n.inner\n=\ninner\nself\n.deny_prefixes\n=\n[p\nif\np.endswith(\n\"/\"\n)\nelse\np\n+\n\"/\"\nfor\np\nin\n(deny_prefixes\nor\n[])]\ndef\n_deny\n(\nself\n,\npath\n:\nstr\n) ->\nbool\n:\nreturn\nany\n(path.startswith(p)\nfor\np\nin\nself\n.deny_prefixes)\ndef\nls_info\n(\nself\n,\npath\n:\nstr\n) -> list[FileInfo]:\nreturn\nself\n.inner.ls_info(path)\ndef\nread\n(\nself\n,\nfile_path\n:\nstr\n,\noffset\n:\nint\n=\n0\n,\nlimit\n:\nint\n=\n2000\n) ->\nstr\n:\nreturn\nself\n.inner.read(file_path,\noffset\n=\noffset,\nlimit\n=\nlimit)\ndef\ngrep_raw\n(\nself\n,\npattern\n:\nstr\n,\npath\n:\nstr\n|\nNone\n=\nNone\n,\nglob\n:\nstr\n|\nNone\n=\nNone\n) -> list[GrepMatch]\n|\nstr\n:\nreturn\nself\n.inner.grep_raw(pattern, path, glob)\ndef\nglob_info\n(\nself\n,\npattern\n:\nstr\n,\npath\n:\nstr\n=\n\"/\"\n) -> list[FileInfo]:\nreturn\nself\n.inner.glob_info(pattern, path)\ndef\nwrite\n(\nself\n,\nfile_path\n:\nstr\n,\ncontent\n:\nstr\n) -> WriteResult:\nif\nself\n._deny(file_path):\nreturn\nWriteResult(\nerror\n=\nf\n\"Writes are not allowed under\n{\nfile_path\n}\n\"\n)\nreturn\nself\n.inner.write(file_path, content)\ndef\nedit\n(\nself\n,\nfile_path\n:\nstr\n,\nold_string\n:\nstr\n,\nnew_string\n:\nstr\n,\nreplace_all\n:\nbool\n=\nFalse\n) -> EditResult:\nif\nself\n._deny(file_path):\nreturn\nEditResult(\nerror\n=\nf\n\"Edits are not allowed under\n{\nfile_path\n}\n\"\n)\nreturn\nself\n.inner.edit(file_path, old_string, new_string, replace_all)\n\u200b\nProtocol reference\nBackends must implement the\nBackendProtocol\n.\nRequired endpoints:\nls_info(path: str) -> list[FileInfo]\nReturn entries with at least\npath\n. Include\nis_dir\n,\nsize\n,\nmodified_at\nwhen available. Sort by\npath\nfor deterministic output.\nread(file_path: str, offset: int = 0, limit: int = 2000) -> str\nReturn numbered content. On missing file, return\n\"Error: File '/x' not found\"\n.\ngrep_raw(pattern: str, path: Optional[str] = None, glob: Optional[str] = None) -> list[GrepMatch] | str\nReturn structured matches. For an invalid regex, return a string like\n\"Invalid regex pattern: ...\"\n(do not raise).\nglob_info(pattern: str, path: str = \"/\") -> list[FileInfo]\nReturn matched files as\nFileInfo\nentries (empty list if none).\nwrite(file_path: str, content: str) -> WriteResult\nCreate-only. On conflict, return\nWriteResult(error=...)\n. On success, set\npath\nand for state backends set\nfiles_update={...}\n; external backends should use\nfiles_update=None\n.\nedit(file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult\nEnforce uniqueness of\nold_string\nunless\nreplace_all=True\n. If not found, return error. Include\noccurrences\non success.\nSupporting types:\nWriteResult(error, path, files_update)\nEditResult(error, path, files_update, occurrences)\nFileInfo\nwith fields:\npath\n(required), optionally\nis_dir\n,\nsize\n,\nmodified_at\n.\nGrepMatch\nwith fields:\npath\n,\nline\n,\ntext\n.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nAgent harness capabilities\nPrevious\nSubagents\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/deepagents/backends",
      "title": "Backends - Docs by LangChain",
      "heading": "Backends"
    }
  },
  {
    "page_content": "Subagents - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore capabilities\nSubagents\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nQuickstart\nCustomization\nCore capabilities\nAgent harness\nBackends\nSubagents\nHuman-in-the-loop\nLong-term memory\nMiddleware\nCommand line interface\nUse the CLI\nOn this page\nWhy use subagents?\nConfiguration\nSubAgent (Dictionary-based)\nCompiledSubAgent\nUsing SubAgent\nUsing CompiledSubAgent\nThe general-purpose subagent\nWhen to use it\nBest practices\nWrite clear descriptions\nKeep system prompts detailed\nMinimize tool sets\nChoose models by task\nReturn concise results\nCommon patterns\nMultiple specialized subagents\nTroubleshooting\nSubagent not being called\nContext still getting bloated\nWrong subagent being selected\nCore capabilities\nSubagents\nCopy page\nLearn how to use subagents to delegate work and keep context clean\nCopy page\nDeep agents can create subagents to delegate work. You can specify custom subagents in the\nsubagents\nparameter. Subagents are useful for\ncontext quarantine\n(keeping the main agent\u2019s context clean) and for providing specialized instructions.\n\u200b\nWhy use subagents?\nSubagents solve the\ncontext bloat problem\n. When agents use tools with large outputs (web search, file reads, database queries), the context window fills up quickly with intermediate results. Subagents isolate this detailed work\u2014the main agent receives only the final result, not the dozens of tool calls that produced it.\nWhen to use subagents:\n\u2705 Multi-step tasks that would clutter the main agent\u2019s context\n\u2705 Specialized domains that need custom instructions or tools\n\u2705 Tasks requiring different model capabilities\n\u2705 When you want to keep the main agent focused on high-level coordination\nWhen NOT to use subagents:\n\u274c Simple, single-step tasks\n\u274c When you need to maintain intermediate context\n\u274c When the overhead outweighs benefits\n\u200b\nConfiguration\nsubagents\nshould be a list of dictionaries or\nCompiledSubAgent\nobjects. There are two types:\n\u200b\nSubAgent (Dictionary-based)\nFor most use cases, define subagents as dictionaries:\nRequired fields:\nname\n(\nstr\n): Unique identifier for the subagent. The main agent uses this name when calling the\ntask()\ntool.\ndescription\n(\nstr\n): What this subagent does. Be specific and action-oriented. The main agent uses this to decide when to delegate.\nsystem_prompt\n(\nstr\n): Instructions for the subagent. Include tool usage guidance and output format requirements.\ntools\n(\nList[Callable]\n): Tools the subagent can use. Keep this minimal and include only what\u2019s needed.\nOptional fields:\nmodel\n(\nstr | BaseChatModel\n): Override the main agent\u2019s model. Use the format\n\"provider:model-name\"\n(for example,\n\"openai:gpt-5-mini\"\n).\nmiddleware\n(\nList[Middleware]\n): Additional middleware for custom behavior, logging, or rate limiting.\ninterrupt_on\n(\nDict[str, bool]\n): Configure human-in-the-loop for specific tools. Requires a checkpointer.\n\u200b\nCompiledSubAgent\nFor complex workflows, use a pre-built LangGraph graph:\nFields:\nname\n(\nstr\n): Unique identifier\ndescription\n(\nstr\n): What this subagent does\nrunnable\n(\nRunnable\n): A compiled LangGraph graph (must call\n.compile()\nfirst)\n\u200b\nUsing SubAgent\nCopy\nimport\nos\nfrom\ntyping\nimport\nLiteral\nfrom\ntavily\nimport\nTavilyClient\nfrom\ndeepagents\nimport\ncreate_deep_agent\ntavily_client\n=\nTavilyClient(\napi_key\n=\nos.environ[\n\"TAVILY_API_KEY\"\n])\ndef\ninternet_search\n(\nquery\n:\nstr\n,\nmax_results\n:\nint\n=\n5\n,\ntopic\n: Literal[\n\"general\"\n,\n\"news\"\n,\n\"finance\"\n]\n=\n\"general\"\n,\ninclude_raw_content\n:\nbool\n=\nFalse\n,\n):\n\"\"\"Run a web search\"\"\"\nreturn\ntavily_client.search(\nquery,\nmax_results\n=\nmax_results,\ninclude_raw_content\n=\ninclude_raw_content,\ntopic\n=\ntopic,\n)\nresearch_subagent\n=\n{\n\"name\"\n:\n\"research-agent\"\n,\n\"description\"\n:\n\"Used to research more in depth questions\"\n,\n\"system_prompt\"\n:\n\"You are a great researcher\"\n,\n\"tools\"\n: [internet_search],\n\"model\"\n:\n\"openai:gpt-5-mini\"\n,\n# Optional override, defaults to main agent model\n}\nsubagents\n=\n[research_subagent]\nagent\n=\ncreate_deep_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\nsubagents\n=\nsubagents\n)\n\u200b\nUsing CompiledSubAgent\nFor more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:\nCopy\nfrom\ndeepagents\nimport\ncreate_deep_agent, CompiledSubAgent\nfrom\nlangchain.agents\nimport\ncreate_agent\n# Create a custom agent graph\ncustom_graph\n=\ncreate_agent(\nmodel\n=\nyour_model,\ntools\n=\nspecialized_tools,\nprompt\n=\n\"You are a specialized agent for data analysis...\"\n)\n# Use it as a custom subagent\ncustom_subagent\n=\nCompiledSubAgent(\nname\n=\n\"data-analyzer\"\n,\ndescription\n=\n\"Specialized agent for complex data analysis tasks\"\n,\nrunnable\n=\ncustom_graph\n)\nsubagents\n=\n[custom_subagent]\nagent\n=\ncreate_deep_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[internet_search],\nsystem_prompt\n=\nresearch_instructions,\nsubagents\n=\nsubagents\n)\n\u200b\nThe general-purpose subagent\nIn addition to any user-defined subagents, deep agents have access to a\ngeneral-purpose\nsubagent at all times. This subagent:\nHas the same system prompt as the main agent\nHas access to all the same tools\nUses the same model (unless overridden)\n\u200b\nWhen to use it\nThe general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls.\nExample\nInstead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent:\ntask(name=\"general-purpose\", task=\"Research quantum computing trends\")\n. The subagent performs all the searches internally and returns only a summary.\n\u200b\nBest practices\n\u200b\nWrite clear descriptions\nThe main agent uses descriptions to decide which subagent to call. Be specific:\n\u2705\nGood:\n\"Analyzes financial data and generates investment insights with confidence scores\"\n\u274c\nBad:\n\"Does finance stuff\"\n\u200b\nKeep system prompts detailed\nInclude specific guidance on how to use tools and format outputs:\nCopy\nresearch_subagent\n=\n{\n\"name\"\n:\n\"research-agent\"\n,\n\"description\"\n:\n\"Conducts in-depth research using web search and synthesizes findings\"\n,\n\"system_prompt\"\n:\n\"\"\"You are a thorough researcher. Your job is to:\n1. Break down the research question into searchable queries\n2. Use internet_search to find relevant information\n3. Synthesize findings into a comprehensive but concise summary\n4. Cite sources when making claims\nOutput format:\n- Summary (2-3 paragraphs)\n- Key findings (bullet points)\n- Sources (with URLs)\nKeep your response under 500 words to maintain clean context.\"\"\"\n,\n\"tools\"\n: [internet_search],\n}\n\u200b\nMinimize tool sets\nOnly give subagents the tools they need. This improves focus and security:\nCopy\n# \u2705 Good: Focused tool set\nemail_agent\n=\n{\n\"name\"\n:\n\"email-sender\"\n,\n\"tools\"\n: [send_email, validate_email],\n# Only email-related\n}\n# \u274c Bad: Too many tools\nemail_agent\n=\n{\n\"name\"\n:\n\"email-sender\"\n,\n\"tools\"\n: [send_email, web_search, database_query, file_upload],\n# Unfocused\n}\n\u200b\nChoose models by task\nDifferent models excel at different tasks:\nCopy\nsubagents\n=\n[\n{\n\"name\"\n:\n\"contract-reviewer\"\n,\n\"description\"\n:\n\"Reviews legal documents and contracts\"\n,\n\"system_prompt\"\n:\n\"You are an expert legal reviewer...\"\n,\n\"tools\"\n: [read_document, analyze_contract],\n\"model\"\n:\n\"claude-sonnet-4-5-20250929\"\n,\n# Large context for long documents\n},\n{\n\"name\"\n:\n\"financial-analyst\"\n,\n\"description\"\n:\n\"Analyzes financial data and market trends\"\n,\n\"system_prompt\"\n:\n\"You are an expert financial analyst...\"\n,\n\"tools\"\n: [get_stock_price, analyze_fundamentals],\n\"model\"\n:\n\"openai:gpt-5\"\n,\n# Better for numerical analysis\n},\n]\n\u200b\nReturn concise results\nInstruct subagents to return summaries, not raw data:\nCopy\ndata_analyst\n=\n{\n\"system_prompt\"\n:\n\"\"\"Analyze the data and return:\n1. Key insights (3-5 bullet points)\n2. Overall confidence score\n3. Recommended next actions\nDo NOT include:\n- Raw data\n- Intermediate calculations\n- Detailed tool outputs\nKeep response under 300 words.\"\"\"\n}\n\u200b\nCommon patterns\n\u200b\nMultiple specialized subagents\nCreate specialized subagents for different domains:\nCopy\nfrom\ndeepagents\nimport\ncreate_deep_agent\nsubagents\n=\n[\n{\n\"name\"\n:\n\"data-collector\"\n,\n\"description\"\n:\n\"Gathers raw data from various sources\"\n,\n\"system_prompt\"\n:\n\"Collect comprehensive data on the topic\"\n,\n\"tools\"\n: [web_search, api_call, database_query],\n},\n{\n\"name\"\n:\n\"data-analyzer\"\n,\n\"description\"\n:\n\"Analyzes collected data for insights\"\n,\n\"system_prompt\"\n:\n\"Analyze data and extract key insights\"\n,\n\"tools\"\n: [statistical_analysis],\n},\n{\n\"name\"\n:\n\"report-writer\"\n,\n\"description\"\n:\n\"Writes polished reports from analysis\"\n,\n\"system_prompt\"\n:\n\"Create professional reports from insights\"\n,\n\"tools\"\n: [format_document],\n},\n]\nagent\n=\ncreate_deep_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\nsystem_prompt\n=\n\"You coordinate data analysis and reporting. Use subagents for specialized tasks.\"\n,\nsubagents\n=\nsubagents\n)\nWorkflow:\nMain agent creates high-level plan\nDelegates data collection to data-collector\nPasses results to data-analyzer\nSends insights to report-writer\nCompiles final output\nEach subagent works with clean context focused only on its task.\n\u200b\nTroubleshooting\n\u200b\nSubagent not being called\nProblem\n: Main agent tries to do work itself instead of delegating.\nSolutions\n:\nMake descriptions more specific:\nCopy\n# \u2705 Good\n{\n\"name\"\n:\n\"research-specialist\"\n,\n\"description\"\n:\n\"Conducts in-depth research on specific topics using web search. Use when you need detailed information that requires multiple searches.\"\n}\n# \u274c Bad\n{\n\"name\"\n:\n\"helper\"\n,\n\"description\"\n:\n\"helps with stuff\"\n}\nInstruct main agent to delegate:\nCopy\nagent\n=\ncreate_deep_agent(\nsystem_prompt\n=\n\"\"\"...your instructions...\nIMPORTANT: For complex tasks, delegate to your subagents using the task() tool.\nThis keeps your context clean and improves results.\"\"\"\n,\nsubagents\n=\n[\n...\n]\n)\n\u200b\nContext still getting bloated\nProblem\n: Context fills up despite using subagents.\nSolutions\n:\nInstruct subagent to return concise results:\nCopy\nsystem_prompt\n=\n\"\"\"...\nIMPORTANT: Return only the essential summary.\nDo NOT include raw data, intermediate search results, or detailed tool outputs.\nYour response should be under 500 words.\"\"\"\nUse filesystem for large data:\nCopy\nsystem_prompt\n=\n\"\"\"When you gather large amounts of data:\n1. Save raw data to /data/raw_results.txt\n2. Process and analyze the data\n3. Return only the analysis summary\nThis keeps context clean.\"\"\"\n\u200b\nWrong subagent being selected\nProblem\n: Main agent calls inappropriate subagent for the task.\nSolution\n: Differentiate subagents clearly in descriptions:\nCopy\nsubagents\n=\n[\n{\n\"name\"\n:\n\"quick-researcher\"\n,\n\"description\"\n:\n\"For simple, quick research questions that need 1-2 searches. Use when you need basic facts or definitions.\"\n,\n},\n{\n\"name\"\n:\n\"deep-researcher\"\n,\n\"description\"\n:\n\"For complex, in-depth research requiring multiple searches, synthesis, and analysis. Use for comprehensive reports.\"\n,\n}\n]\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBackends\nPrevious\nHuman-in-the-loop\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/deepagents/subagents",
      "title": "Subagents - Docs by LangChain",
      "heading": "Subagents"
    }
  },
  {
    "page_content": "Human-in-the-loop - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore capabilities\nHuman-in-the-loop\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nQuickstart\nCustomization\nCore capabilities\nAgent harness\nBackends\nSubagents\nHuman-in-the-loop\nLong-term memory\nMiddleware\nCommand line interface\nUse the CLI\nOn this page\nBasic configuration\nDecision types\nHandle interrupts\nMultiple tool calls\nEdit tool arguments\nSubagent interrupts\nBest practices\nAlways use a checkpointer\nUse the same thread ID\nMatch decision order to actions\nTailor configurations by risk\nCore capabilities\nHuman-in-the-loop\nCopy page\nLearn how to configure human approval for sensitive tool operations\nCopy page\nSome tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph\u2019s interrupt capabilities. You can configure which tools require approval using the\ninterrupt_on\nparameter.\n\u200b\nBasic configuration\nThe\ninterrupt_on\nparameter accepts a dictionary mapping tool names to interrupt configurations. Each tool can be configured with:\nTrue\n: Enable interrupts with default behavior (approve, edit, reject allowed)\nFalse\n: Disable interrupts for this tool\n{\"allowed_decisions\": [...]}\n: Custom configuration with specific allowed decisions\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\ndeepagents\nimport\ncreate_deep_agent\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\n@tool\ndef\ndelete_file\n(\npath\n:\nstr\n) ->\nstr\n:\n\"\"\"Delete a file from the filesystem.\"\"\"\nreturn\nf\n\"Deleted\n{\npath\n}\n\"\n@tool\ndef\nread_file\n(\npath\n:\nstr\n) ->\nstr\n:\n\"\"\"Read a file from the filesystem.\"\"\"\nreturn\nf\n\"Contents of\n{\npath\n}\n\"\n@tool\ndef\nsend_email\n(\nto\n:\nstr\n,\nsubject\n:\nstr\n,\nbody\n:\nstr\n) ->\nstr\n:\n\"\"\"Send an email.\"\"\"\nreturn\nf\n\"Sent email to\n{\nto\n}\n\"\n# Checkpointer is REQUIRED for human-in-the-loop\ncheckpointer\n=\nMemorySaver()\nagent\n=\ncreate_deep_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[delete_file, read_file, send_email],\ninterrupt_on\n=\n{\n\"delete_file\"\n:\nTrue\n,\n# Default: approve, edit, reject\n\"read_file\"\n:\nFalse\n,\n# No interrupts needed\n\"send_email\"\n: {\n\"allowed_decisions\"\n: [\n\"approve\"\n,\n\"reject\"\n]},\n# No editing\n},\ncheckpointer\n=\ncheckpointer\n# Required!\n)\n\u200b\nDecision types\nThe\nallowed_decisions\nlist controls what actions a human can take when reviewing a tool call:\n\"approve\"\n: Execute the tool with the original arguments as proposed by the agent\n\"edit\"\n: Modify the tool arguments before execution\n\"reject\"\n: Skip executing this tool call entirely\nYou can customize which decisions are available for each tool:\nCopy\ninterrupt_on\n=\n{\n# Sensitive operations: allow all options\n\"delete_file\"\n: {\n\"allowed_decisions\"\n: [\n\"approve\"\n,\n\"edit\"\n,\n\"reject\"\n]},\n# Moderate risk: approval or rejection only\n\"write_file\"\n: {\n\"allowed_decisions\"\n: [\n\"approve\"\n,\n\"reject\"\n]},\n# Must approve (no rejection allowed)\n\"critical_operation\"\n: {\n\"allowed_decisions\"\n: [\n\"approve\"\n]},\n}\n\u200b\nHandle interrupts\nWhen an interrupt is triggered, the agent pauses execution and returns control. Check for interrupts in the result and handle them accordingly.\nCopy\nimport\nuuid\nfrom\nlanggraph.types\nimport\nCommand\n# Create config with thread_id for state persistence\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\nstr\n(uuid.uuid4())}}\n# Invoke the agent\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Delete the file temp.txt\"\n}]\n},\nconfig\n=\nconfig)\n# Check if execution was interrupted\nif\nresult.get(\n\"__interrupt__\"\n):\n# Extract interrupt information\ninterrupts\n=\nresult[\n\"__interrupt__\"\n][\n0\n].value\naction_requests\n=\ninterrupts[\n\"action_requests\"\n]\nreview_configs\n=\ninterrupts[\n\"review_configs\"\n]\n# Create a lookup map from tool name to review config\nconfig_map\n=\n{cfg[\n\"action_name\"\n]: cfg\nfor\ncfg\nin\nreview_configs}\n# Display the pending actions to the user\nfor\naction\nin\naction_requests:\nreview_config\n=\nconfig_map[action[\n\"name\"\n]]\nprint\n(\nf\n\"Tool:\n{\naction[\n'name'\n]\n}\n\"\n)\nprint\n(\nf\n\"Arguments:\n{\naction[\n'args'\n]\n}\n\"\n)\nprint\n(\nf\n\"Allowed decisions:\n{\nreview_config[\n'allowed_decisions'\n]\n}\n\"\n)\n# Get user decisions (one per action_request, in order)\ndecisions\n=\n[\n{\n\"type\"\n:\n\"approve\"\n}\n# User approved the deletion\n]\n# Resume execution with decisions\nresult\n=\nagent.invoke(\nCommand(\nresume\n=\n{\n\"decisions\"\n: decisions}),\nconfig\n=\nconfig\n# Must use the same config!\n)\n# Process final result\nprint\n(result[\n\"messages\"\n][\n-\n1\n].content)\n\u200b\nMultiple tool calls\nWhen the agent calls multiple tools that require approval, all interrupts are batched together in a single interrupt. You must provide decisions for each one in order.\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\nstr\n(uuid.uuid4())}}\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Delete temp.txt and send an email to\n[email\u00a0protected]\n\"\n}]\n},\nconfig\n=\nconfig)\nif\nresult.get(\n\"__interrupt__\"\n):\ninterrupts\n=\nresult[\n\"__interrupt__\"\n][\n0\n].value\naction_requests\n=\ninterrupts[\n\"action_requests\"\n]\n# Two tools need approval\nassert\nlen\n(action_requests)\n==\n2\n# Provide decisions in the same order as action_requests\ndecisions\n=\n[\n{\n\"type\"\n:\n\"approve\"\n},\n# First tool: delete_file\n{\n\"type\"\n:\n\"reject\"\n}\n# Second tool: send_email\n]\nresult\n=\nagent.invoke(\nCommand(\nresume\n=\n{\n\"decisions\"\n: decisions}),\nconfig\n=\nconfig\n)\n\u200b\nEdit tool arguments\nWhen\n\"edit\"\nis in the allowed decisions, you can modify the tool arguments before execution:\nCopy\nif\nresult.get(\n\"__interrupt__\"\n):\ninterrupts\n=\nresult[\n\"__interrupt__\"\n][\n0\n].value\naction_request\n=\ninterrupts[\n\"action_requests\"\n][\n0\n]\n# Original args from the agent\nprint\n(action_request[\n\"args\"\n])\n# {\"to\": \"\n[email\u00a0protected]\n\", ...}\n# User decides to edit the recipient\ndecisions\n=\n[{\n\"type\"\n:\n\"edit\"\n,\n\"edited_action\"\n: {\n\"name\"\n: action_request[\n\"name\"\n],\n# Must include the tool name\n\"args\"\n: {\n\"to\"\n:\n\"\n[email\u00a0protected]\n\"\n,\n\"subject\"\n:\n\"...\"\n,\n\"body\"\n:\n\"...\"\n}\n}\n}]\nresult\n=\nagent.invoke(\nCommand(\nresume\n=\n{\n\"decisions\"\n: decisions}),\nconfig\n=\nconfig\n)\n\u200b\nSubagent interrupts\nEach subagent can have its own\ninterrupt_on\nconfiguration that overrides the main agent\u2019s settings:\nCopy\nagent\n=\ncreate_deep_agent(\ntools\n=\n[delete_file, read_file],\ninterrupt_on\n=\n{\n\"delete_file\"\n:\nTrue\n,\n\"read_file\"\n:\nFalse\n,\n},\nsubagents\n=\n[{\n\"name\"\n:\n\"file-manager\"\n,\n\"description\"\n:\n\"Manages file operations\"\n,\n\"system_prompt\"\n:\n\"You are a file management assistant.\"\n,\n\"tools\"\n: [delete_file, read_file],\n\"interrupt_on\"\n: {\n# Override: require approval for reads in this subagent\n\"delete_file\"\n:\nTrue\n,\n\"read_file\"\n:\nTrue\n,\n# Different from main agent!\n}\n}],\ncheckpointer\n=\ncheckpointer\n)\nWhen a subagent triggers an interrupt, the handling is the same \u2013 check for\n__interrupt__\nand resume with\nCommand\n.\n\u200b\nBest practices\n\u200b\nAlways use a checkpointer\nHuman-in-the-loop requires a checkpointer to persist agent state between the interrupt and resume:\nCopy\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\ncheckpointer\n=\nMemorySaver()\nagent\n=\ncreate_deep_agent(\ntools\n=\n[\n...\n],\ninterrupt_on\n=\n{\n...\n},\ncheckpointer\n=\ncheckpointer\n# Required for HITL\n)\n\u200b\nUse the same thread ID\nWhen resuming, you must use the same config with the same\nthread_id\n:\nCopy\n# First call\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"my-thread\"\n}}\nresult\n=\nagent.invoke(\ninput\n,\nconfig\n=\nconfig)\n# Resume (use same config)\nresult\n=\nagent.invoke(Command(\nresume\n=\n{\n...\n}),\nconfig\n=\nconfig)\n\u200b\nMatch decision order to actions\nThe decisions list must match the order of\naction_requests\n:\nCopy\nif\nresult.get(\n\"__interrupt__\"\n):\ninterrupts\n=\nresult[\n\"__interrupt__\"\n][\n0\n].value\naction_requests\n=\ninterrupts[\n\"action_requests\"\n]\n# Create one decision per action, in order\ndecisions\n=\n[]\nfor\naction\nin\naction_requests:\ndecision\n=\nget_user_decision(action)\n# Your logic\ndecisions.append(decision)\nresult\n=\nagent.invoke(\nCommand(\nresume\n=\n{\n\"decisions\"\n: decisions}),\nconfig\n=\nconfig\n)\n\u200b\nTailor configurations by risk\nConfigure different tools based on their risk level:\nCopy\ninterrupt_on\n=\n{\n# High risk: full control (approve, edit, reject)\n\"delete_file\"\n: {\n\"allowed_decisions\"\n: [\n\"approve\"\n,\n\"edit\"\n,\n\"reject\"\n]},\n\"send_email\"\n: {\n\"allowed_decisions\"\n: [\n\"approve\"\n,\n\"edit\"\n,\n\"reject\"\n]},\n# Medium risk: no editing allowed\n\"write_file\"\n: {\n\"allowed_decisions\"\n: [\n\"approve\"\n,\n\"reject\"\n]},\n# Low risk: no interrupts\n\"read_file\"\n:\nFalse\n,\n\"list_files\"\n:\nFalse\n,\n}\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nSubagents\nPrevious\nLong-term memory\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/deepagents/human-in-the-loop",
      "title": "Human-in-the-loop - Docs by LangChain",
      "heading": "Human-in-the-loop"
    }
  },
  {
    "page_content": "Deep Agents Middleware - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore capabilities\nDeep Agents Middleware\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nQuickstart\nCustomization\nCore capabilities\nAgent harness\nBackends\nSubagents\nHuman-in-the-loop\nLong-term memory\nMiddleware\nCommand line interface\nUse the CLI\nOn this page\nTo-do list middleware\nFilesystem middleware\nShort-term vs. long-term filesystem\nSubagent middleware\nCore capabilities\nDeep Agents Middleware\nCopy page\nUnderstand the middleware that powers deep agents\nCopy page\nDeep agents are built with a modular middleware architecture. Deep agents have access to:\nA planning tool\nA filesystem for storing context and long-term memories\nThe ability to spawn subagents\nEach feature is implemented as separate middleware. When you create a deep agent with\ncreate_deep_agent\n, we automatically attach\nTodoListMiddleware\n,\nFilesystemMiddleware\n, and\nSubAgentMiddleware\nto your agent.\nMiddleware is composable\u2014you can add as many or as few middleware to an agent as needed. You can use any middleware independently.\nThe following sections explain what each middleware provides.\n\u200b\nTo-do list middleware\nPlanning is integral to solving complex problems. If you\u2019ve used Claude Code recently, you\u2019ll notice how it writes out a to-do list before tackling complex, multi-part tasks. You\u2019ll also notice how it can adapt and update this to-do list on the fly as more information comes in.\nTodoListMiddleware\nprovides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the\nwrite_todos\ntool to keep track of what it\u2019s doing and what still needs to be done.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nTodoListMiddleware\n# TodoListMiddleware is included by default in create_deep_agent\n# You can customize it if building a custom agent\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\n# Custom planning instructions can be added via middleware\nmiddleware\n=\n[\nTodoListMiddleware(\nsystem_prompt\n=\n\"Use the write_todos tool to...\"\n# Optional: Custom addition to the system prompt\n),\n],\n)\n\u200b\nFilesystem middleware\nContext engineering is a main challenge in building effective agents. This is particularly difficult when using tools that return variable-length results (for example, web_search and rag), as long tool results can quickly fill your context window.\nFilesystemMiddleware\nprovides four tools for interacting with both short-term and long-term memory:\nls\n: List the files in the filesystem\nread_file\n: Read an entire file or a certain number of lines from a file\nwrite_file\n: Write a new file to the filesystem\nedit_file\n: Edit an existing file in the filesystem\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\ndeepagents.middleware.filesystem\nimport\nFilesystemMiddleware\n# FilesystemMiddleware is included by default in create_deep_agent\n# You can customize it if building a custom agent\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\nmiddleware\n=\n[\nFilesystemMiddleware(\nbackend\n=\nNone\n,\n# Optional: custom backend (defaults to StateBackend)\nsystem_prompt\n=\n\"Write to the filesystem when...\"\n,\n# Optional custom addition to the system prompt\ncustom_tool_descriptions\n=\n{\n\"ls\"\n:\n\"Use the ls tool when...\"\n,\n\"read_file\"\n:\n\"Use the read_file tool to...\"\n}\n# Optional: Custom descriptions for filesystem tools\n),\n],\n)\n\u200b\nShort-term vs. long-term filesystem\nBy default, these tools write to a local \u201cfilesystem\u201d in your graph state. To enable persistent storage across threads, configure a\nCompositeBackend\nthat routes specific paths (like\n/memories/\n) to a\nStoreBackend\n.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\ndeepagents.middleware\nimport\nFilesystemMiddleware\nfrom\ndeepagents.backends\nimport\nCompositeBackend, StateBackend, StoreBackend\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nstore\n=\nInMemoryStore()\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\nstore\n=\nstore,\nmiddleware\n=\n[\nFilesystemMiddleware(\nbackend\n=\nlambda\nrt\n: CompositeBackend(\ndefault\n=\nStateBackend(rt),\nroutes\n=\n{\n\"/memories/\"\n: StoreBackend(rt)}\n),\ncustom_tool_descriptions\n=\n{\n\"ls\"\n:\n\"Use the ls tool when...\"\n,\n\"read_file\"\n:\n\"Use the read_file tool to...\"\n}\n# Optional: Custom descriptions for filesystem tools\n),\n],\n)\nWhen you configure a\nCompositeBackend\nwith a\nStoreBackend\nfor\n/memories/\n, any files prefixed with\n/memories/\nare saved to persistent storage and survive across different threads. Files without this prefix remain in ephemeral state storage.\n\u200b\nSubagent middleware\nHanding off tasks to subagents isolates context, keeping the main (supervisor) agent\u2019s context window clean while still going deep on a task.\nThe subagents middleware allows you to supply subagents through a\ntask\ntool.\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\ndeepagents.middleware.subagents\nimport\nSubAgentMiddleware\n@tool\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get the weather in a city.\"\"\"\nreturn\nf\n\"The weather in\n{\ncity\n}\nis sunny.\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\nmiddleware\n=\n[\nSubAgentMiddleware(\ndefault_model\n=\n\"claude-sonnet-4-5-20250929\"\n,\ndefault_tools\n=\n[],\nsubagents\n=\n[\n{\n\"name\"\n:\n\"weather\"\n,\n\"description\"\n:\n\"This subagent can get weather in cities.\"\n,\n\"system_prompt\"\n:\n\"Use the get_weather tool to get the weather in a city.\"\n,\n\"tools\"\n: [get_weather],\n\"model\"\n:\n\"gpt-5-mini\"\n,\n\"middleware\"\n: [],\n}\n],\n)\n],\n)\nA subagent is defined with a\nname\n,\ndescription\n,\nsystem prompt\n, and\ntools\n. You can also provide a subagent with a custom\nmodel\n, or with additional\nmiddleware\n. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.\nFor more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\ndeepagents.middleware.subagents\nimport\nSubAgentMiddleware\nfrom\ndeepagents\nimport\nCompiledSubAgent\nfrom\nlanggraph.graph\nimport\nStateGraph\n# Create a custom LangGraph graph\ndef\ncreate_weather_graph\n():\nworkflow\n=\nStateGraph(\n...\n)\n# Build your custom graph\nreturn\nworkflow.compile()\nweather_graph\n=\ncreate_weather_graph()\n# Wrap it in a CompiledSubAgent\nweather_subagent\n=\nCompiledSubAgent(\nname\n=\n\"weather\"\n,\ndescription\n=\n\"This subagent can get weather in cities.\"\n,\nrunnable\n=\nweather_graph\n)\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\nmiddleware\n=\n[\nSubAgentMiddleware(\ndefault_model\n=\n\"claude-sonnet-4-5-20250929\"\n,\ndefault_tools\n=\n[],\nsubagents\n=\n[weather_subagent],\n)\n],\n)\nIn addition to any user-defined subagents, the main agent has access to a\ngeneral-purpose\nsubagent at all times. This subagent has the same instructions as the main agent and all the tools it has access to. The primary purpose of the\ngeneral-purpose\nsubagent is context isolation\u2014the main agent can delegate a complex task to this subagent and get a concise answer back without bloat from intermediate tool calls.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nLong-term memory\nPrevious\nDeep Agents CLI\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/deepagents/middleware",
      "title": "Deep Agents Middleware - Docs by LangChain",
      "heading": "Deep Agents Middleware"
    }
  },
  {
    "page_content": "Deep Agents CLI - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCommand line interface\nDeep Agents CLI\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nQuickstart\nCustomization\nCore capabilities\nAgent harness\nBackends\nSubagents\nHuman-in-the-loop\nLong-term memory\nMiddleware\nCommand line interface\nUse the CLI\nOn this page\nQuick start\nConfiguration\nInteractive mode\nSet project conventions with memories\nUse remote sandboxes\nCommand line interface\nDeep Agents CLI\nCopy page\nInteractive command-line interface for building with Deep Agents\nCopy page\nA terminal interface for building agents with persistent memory. Agents maintain context across sessions, learn project conventions, and execute code with approval controls.\nThe Deep Agents CLI has the following built-in capabilities:\nFile operations\n- read, write, and edit files in your project with tools that enable agents to manage and modify code and documentation.\nShell command execution\n- execute shell commands to run tests, build projects, manage dependencies, and interact with version control systems.\nWeb search\n- search the web for up-to-date information and documentation (requires Tavily API key).\nHTTP requests\n- make HTTP requests to APIs and external services for data fetching and integration tasks.\nTask planning and tracking\n- break down complex tasks into discrete steps and track progress through the built-in todo system.\nMemory storage and retrieval\n- store and retrieve information across sessions, enabling agents to remember project conventions and learned patterns.\nHuman-in-the-loop\n- require human approval for sensitive tool operations.\nWatch the demo video\nto see how the Deep Agents CLI works.\n\u200b\nQuick start\nSet your API key\nExport as an environment variable:\nCopy\nexport\nANTHROPIC_API_KEY\n=\n\"your-api-key\"\nOr create a\n.env\nfile in your project root:\nCopy\nANTHROPIC_API_KEY\n=\nyour-api-key\nRun the CLI\nCopy\nuvx\ndeepagents-cli\nGive the agent a task\nCopy\n>\nCreate a Python script that prints\n\"Hello, World!\"\nThe agent proposes changes with diffs for your approval before modifying files.\nAdditional installation and configuration options\nInstall locally if needed:\npip\nuv\nCopy\npip\ninstall\ndeepagents-cli\nThe CLI uses Anthropic Claude Sonnet 4 by default. To use OpenAI:\nCopy\nexport\nOPENAI_API_KEY\n=\n\"your-key\"\nEnable web search (optional):\nCopy\nexport\nTAVILY_API_KEY\n=\n\"your-key\"\nAPI keys can be set as environment variables or in a\n.env\nfile.\n\u200b\nConfiguration\nCommand-line options\nOption\nDescription\n--agent NAME\nUse named agent with separate memory\n--auto-approve\nSkip tool confirmation prompts (toggle with\nCtrl+T\n)\n--sandbox TYPE\nExecute in remote sandbox:\nmodal\n,\ndaytona\n, or\nrunloop\n--sandbox-id ID\nReuse existing sandbox\n--sandbox-setup PATH\nRun setup script in sandbox\nCLI commands\nCommand\nDescription\ndeepagents list\nList all agents\ndeepagents help\nShow help\ndeepagents reset --agent NAME\nClear agent memory and reset to default\ndeepagents reset --agent NAME --target SOURCE\nCopy memory from another agent\n\u200b\nInteractive mode\nSlash commands\nUse these commands within the CLI session:\n/tokens\n- Display token usage\n/clear\n- Clear conversation history\n/exit\n- Exit the CLI\nBash commands\nExecute shell commands directly by prefixing with\n!\n:\nCopy\n!\ngit\nstatus\n!\nnpm\ntest\n!\nls\n-la\nKeyboard shortcuts\nShortcut\nAction\nEnter\nSubmit\nAlt+Enter\nNewline\nCtrl+E\nExternal editor\nCtrl+T\nToggle auto-approve\nCtrl+C\nInterrupt\nCtrl+D\nExit\n\u200b\nSet project conventions with memories\nAgents store information in\n~/.deepagents/AGENT_NAME/memories/\nas markdown files using a memory-first protocol:\nResearch\n: Searches memory for relevant context before starting tasks\nResponse\n: Checks memory when uncertain during execution\nLearning\n: Automatically saves new information for future sessions\nOrganize memories by topic with descriptive filenames:\nCopy\n~/.deepagents/backend-dev/memories/\n\u251c\u2500\u2500 api-conventions.md\n\u251c\u2500\u2500 database-schema.md\n\u2514\u2500\u2500 deployment-process.md\nTeach the agent conventions once:\nCopy\nuvx\ndeepagents-cli\n--agent\nbackend-dev\n>\nOur API uses snake_case and includes created_at/updated_at timestamps\nIt remembers for future sessions:\nCopy\n>\nCreate a /users endpoint\n# Applies conventions without prompting\n\u200b\nUse remote sandboxes\nExecute code in isolated remote environments for safety and flexibility. Remote sandboxes provide the following benefits:\nSafety\n: Protect your local machine from potentially harmful code execution\nClean environments\n: Use specific dependencies or OS configurations without local setup\nParallel execution\n: Run multiple agents simultaneously in isolated environments\nLong-running tasks\n: Execute time-intensive operations without blocking your machine\nReproducibility\n: Ensure consistent execution environments across teams\nTo use a remote sandbox, follow these steps:\nConfigure your sandbox provider (\nRunloop\n,\nDaytona\n, or\nModal\n):\nCopy\n# Runloop\nexport\nRUNLOOP_API_KEY\n=\n\"your-key\"\n# Daytona\nexport\nDAYTONA_API_KEY\n=\n\"your-key\"\n# Modal\nmodal\nsetup\nRun the CLI with a sandbox:\nCopy\nuvx\ndeepagents-cli\n--sandbox\nrunloop\n--sandbox-setup\n./setup.sh\nThe agent runs locally but executes all code operations in the remote sandbox. Optional setup scripts can configure environment variables, clone repositories, and prepare dependencies.\n(Optional) Create a\nsetup.sh\nfile to configure your sandbox environment:\nCopy\n#!/bin/bash\nset\n-e\n# Clone repository using GitHub token\ngit\nclone\nhttps://x-access-token:\n${\nGITHUB_TOKEN\n}\n@github.com/username/repo.git\n$HOME\n/workspace\ncd\n$HOME\n/workspace\n# Make environment variables persistent\ncat\n>>\n~/.bashrc\n<<\n'EOF'\nexport GITHUB_TOKEN=\"${GITHUB_TOKEN}\"\nexport OPENAI_API_KEY=\"${OPENAI_API_KEY}\"\ncd $HOME/workspace\nEOF\nsource\n~/.bashrc\nStore secrets in a local\n.env\nfile for the setup script to access.\nSandboxes isolate code execution, but agents remain vulnerable to prompt injection with untrusted inputs. Use human-in-the-loop approval, short-lived secrets, and trusted setup scripts only. Note that sandbox APIs are evolving rapidly, and we expect more providers to support proxies that help mitigate prompt injection and secrets management concerns.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nDeep Agents Middleware\nPrevious\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/deepagents/cli",
      "title": "Deep Agents CLI - Docs by LangChain",
      "heading": "Deep Agents CLI"
    }
  },
  {
    "page_content": "ChatOpenAI - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nChatOpenAI\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic (Claude)\nGoogle\nAWS (Amazon)\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nOverview\nIntegration details\nModel features\nSetup\nInstallation\nCredentials\nInstantiation\nInvocation\nStreaming usage metadata\nUsing with Azure OpenAI\nTool calling\nBind tools\nStrict mode\nTool calls\nStructured output and tool calls\nCustom tools\nResponses API\nWeb search\nImage generation\nFile search\nComputer use\nCode interpreter\nRemote MCP\nManaging conversation state\nManually manage state\nPassing previous_response_id\nReasoning output\nFine-tuning\nMultimodal Inputs (images, PDFs, audio)\nPredicted output\nAudio Generation (Preview)\nPrompt caching\nManual caching\nCache key strategies\nModel-level caching\nFlex processing\nAPI reference\nChatOpenAI\nCopy page\nGet started using OpenAI\nchat models\nin LangChain.\nCopy page\nYou can find information about OpenAI\u2019s latest models, their costs, context windows, and supported input types in the\nOpenAI Platform\ndocs.\nAPI Reference\nFor detailed documentation of all features and configuration options, head to the\nChatOpenAI\nAPI reference.\nChat Completions API compatibility\nChatOpenAI\nis fully compatible with OpenAI\u2019s\nChat Completions API\n. If you are looking to connect to other model providers that support the Chat Completions API, you can do so \u2013 see\ninstructions\n.\n\u200b\nOverview\n\u200b\nIntegration details\nClass\nPackage\nLocal\nSerializable\nJS/TS Support\nDownloads\nLatest Version\nChatOpenAI\nlangchain-openai\n\u274c\nbeta\n\u2705\n(npm)\n\u200b\nModel features\nTool calling\nStructured output\nImage input\nAudio input\nVideo input\nToken-level streaming\nNative async\nToken usage\nLogprobs\n\u2705\n\u2705\n\u2705\n\u2705\n\u274c\n\u2705\n\u2705\n\u2705\n\u2705\n\u200b\nSetup\nTo access OpenAI models you\u2019ll need to install the\nlangchain-openai\nintegration package and acquire an\nOpenAI Platform\nAPI key.\n\u200b\nInstallation\npip\nuv\nCopy\npip\ninstall\n-U\nlangchain-openai\n\u200b\nCredentials\nHead to the\nOpenAI Platform\nto sign up and generate an API key. Once you\u2019ve done this set the\nOPENAI_API_KEY\nenvironment variable in your environment:\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter your OpenAI API key: \"\n)\nIf you want to get automated tracing of your model calls you can also set your\nLangSmith\nAPI key:\nCopy\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter your LangSmith API key: \"\n)\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\n\u200b\nInstantiation\nNow we can instantiate our model object and generate responses:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\n# stream_usage=True,\n# temperature=None,\n# max_tokens=None,\n# timeout=None,\n# reasoning_effort=\"low\",\n# max_retries=2,\n# api_key=\"...\",  # If you prefer to pass api key in directly\n# base_url=\"...\",\n# organization=\"...\",\n# other params...\n)\nSee the\nChatOpenAI\nAPI Reference for the full set of available model parameters.\nToken parameter deprecation\nOpenAI deprecated\nmax_tokens\nin favor of\nmax_completion_tokens\nin September 2024. While\nmax_tokens\nis still supported for backwards compatibility, it\u2019s automatically converted to\nmax_completion_tokens\ninternally.\n\u200b\nInvocation\nCopy\nmessages\n=\n[\n(\n\"system\"\n,\n\"You are a helpful assistant that translates English to French. Translate the user sentence.\"\n,\n),\n(\n\"human\"\n,\n\"I love programming.\"\n),\n]\nai_msg\n=\nllm.invoke(messages)\nai_msg\nCopy\nAIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 31, 'total_tokens': 36}, 'model_name': 'gpt-5-mini-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'stop', 'logprobs': None}, id='run-63219b22-03e3-4561-8cc4-78b7c7c3a3ca-0', usage_metadata={'input_tokens': 31, 'output_tokens': 5, 'total_tokens': 36})\nCopy\nprint\n(ai_msg.text)\nCopy\nJ'adore la programmation.\n\u200b\nStreaming usage metadata\nOpenAI\u2019s Chat Completions API does not stream token usage statistics by default (see API reference\nhere\n).\nTo recover token counts when streaming with\nChatOpenAI\nor\nAzureChatOpenAI\n, set\nstream_usage=True\nas an initialization parameter or on invocation:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\nstream_usage\n=\nTrue\n)\n\u200b\nUsing with Azure OpenAI\nAzure OpenAI v1 API support\nAs of\nlangchain-openai>=1.0.1\n,\nChatOpenAI\ncan be used directly with Azure OpenAI endpoints using the new\nv1 API\n. This provides a unified way to use OpenAI models whether hosted on OpenAI or Azure.\nFor the traditional Azure-specific implementation, continue to use\nAzureChatOpenAI\n.\nUsing Azure OpenAI v1 API with API Key\nTo use\nChatOpenAI\nwith Azure OpenAI, set the\nbase_url\nto your Azure endpoint with\n/openai/v1/\nappended:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\n# Your Azure deployment name\nbase_url\n=\n\"https://{your-resource-name}.openai.azure.com/openai/v1/\"\n,\napi_key\n=\n\"your-azure-api-key\"\n)\nresponse\n=\nllm.invoke(\n\"Hello, how are you?\"\n)\nprint\n(response.content)\nUsing Azure OpenAI with Microsoft Entra ID\nThe v1 API adds native support for\nMicrosoft Entra ID\n(formerly Azure AD) authentication with automatic token refresh. Pass a token provider callable to the\napi_key\nparameter:\nCopy\nfrom\nazure.identity\nimport\nDefaultAzureCredential, get_bearer_token_provider\nfrom\nlangchain_openai\nimport\nChatOpenAI\n# Create a token provider that handles automatic refresh\ntoken_provider\n=\nget_bearer_token_provider(\nDefaultAzureCredential(),\n\"https://cognitiveservices.azure.com/.default\"\n)\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\n# Your Azure deployment name\nbase_url\n=\n\"https://{your-resource-name}.openai.azure.com/openai/v1/\"\n,\napi_key\n=\ntoken_provider\n# Callable that handles token refresh\n)\n# Use the model as normal\nmessages\n=\n[\n(\n\"system\"\n,\n\"You are a helpful assistant.\"\n),\n(\n\"human\"\n,\n\"Translate 'I love programming' to French.\"\n)\n]\nresponse\n=\nllm.invoke(messages)\nprint\n(response.content)\nThe token provider is a callable that automatically retrieves and refreshes authentication tokens, eliminating the need to manually manage token expiration.\nInstallation requirements\nTo use Microsoft Entra ID authentication, install the Azure Identity library:\nCopy\npip\ninstall\nazure-identity\nYou can also pass a token provider callable to the\napi_key\nparameter when using\nasynchronous functions. You must import DefaultAzureCredential from\nazure.identity.aio\n:\nCopy\nfrom\nazure.identity.aio\nimport\nDefaultAzureCredential\nfrom\nlangchain_openai\nimport\nChatOpenAI\ncredential\n=\nDefaultAzureCredential()\nllm_async\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\napi_key\n=\ncredential\n)\n# Use async methods when using async callable\nresponse\n=\nawait\nllm_async.ainvoke(\n\"Hello!\"\n)\nWhen using an async callable for the API key, you must use async methods (\nainvoke\n,\nastream\n, etc.). Sync methods will raise an error.\n\u200b\nTool calling\nOpenAI has a\ntool calling\n(we use \u201ctool calling\u201d and \u201cfunction calling\u201d interchangeably here) API that lets you describe tools and their arguments, and have the model return a JSON object with a tool to invoke and the inputs to that tool. tool-calling is extremely useful for building tool-using chains and agents, and for getting structured outputs from models more generally.\n\u200b\nBind tools\nWith\nChatOpenAI.bind_tools\n, we can easily pass in Pydantic classes, dict schemas, LangChain tools, or even functions as tools to the model. Under the hood these are converted to an OpenAI tool schemas, which looks like:\nCopy\n{\n\"name\": \"...\",\n\"description\": \"...\",\n\"parameters\": {...}  # JSONSchema\n}\n\u2026and are passed in every model invocation.\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nclass\nGetWeather\n(\nBaseModel\n):\n\"\"\"Get the current weather in a given location\"\"\"\nlocation:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The city and state, e.g. San Francisco, CA\"\n)\nllm_with_tools\n=\nllm.bind_tools([GetWeather])\nCopy\nai_msg\n=\nllm_with_tools.invoke(\n\"what is the weather like in San Francisco\"\n,\n)\nai_msg\nCopy\nAIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85}, 'model_name': 'gpt-5-mini-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1617c9b2-dda5-4120-996b-0333ed5992e2-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_o9udf3EVOWiV4Iupktpbpofk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85})\n\u200b\nStrict mode\nRequires\nlangchain-openai>=0.1.21\nAs of Aug 6, 2024, OpenAI supports a\nstrict\nargument when calling tools that will enforce that the tool argument schema is respected by the model.\nSee more\n.\nIf\nstrict=True\nthe tool definition will also be validated, and a subset of JSON schema are accepted. Crucially, schema cannot have optional args (those with default values).\nRead\nthe full docs\non what types of schema are supported.\nCopy\nllm_with_tools\n=\nllm.bind_tools([GetWeather],\nstrict\n=\nTrue\n)\nai_msg\n=\nllm_with_tools.invoke(\n\"what is the weather like in San Francisco\"\n,\n)\nai_msg\nCopy\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jUqhd8wzAIzInTJl72Rla8ht', 'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'GetWeather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85}, 'model_name': 'gpt-5-mini-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5e3356a9-132d-4623-8e73-dd5a898cf4a6-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_jUqhd8wzAIzInTJl72Rla8ht', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85})\n\u200b\nTool calls\nNotice that the AIMessage has a\ntool_calls\nattribute. This contains in a standardized ToolCall format that is model-provider agnostic.\nCopy\nai_msg.tool_calls\nCopy\n[{'name': 'GetWeather',\n'args': {'location': 'San Francisco, CA'},\n'id': 'call_jUqhd8wzAIzInTJl72Rla8ht',\n'type': 'tool_call'}]\nFor more on binding tools and tool call outputs, head to the\ntool calling\ndocs.\n\u200b\nStructured output and tool calls\nOpenAI\u2019s\nstructured output\nfeature can be used simultaneously with tool-calling. The model will either generate tool calls or a response adhering to a desired schema. See example below:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\npydantic\nimport\nBaseModel\ndef\nget_weather\n(\nlocation\n:\nstr\n) ->\nNone\n:\n\"\"\"Get weather at a location.\"\"\"\nreturn\n\"It's sunny.\"\nclass\nOutputSchema\n(\nBaseModel\n):\n\"\"\"Schema for response.\"\"\"\nanswer:\nstr\njustification:\nstr\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\nstructured_llm\n=\nllm.bind_tools(\n[get_weather],\nresponse_format\n=\nOutputSchema,\nstrict\n=\nTrue\n,\n)\n# Response contains tool calls:\ntool_call_response\n=\nstructured_llm.invoke(\n\"What is the weather in SF?\"\n)\n# structured_response.additional_kwargs[\"parsed\"] contains parsed output\nstructured_response\n=\nstructured_llm.invoke(\n\"What weighs more, a pound of feathers or a pound of gold?\"\n)\n\u200b\nCustom tools\nRequires\nlangchain-openai>=0.3.29\nCustom tools\nsupport tools with arbitrary string inputs. They can be particularly useful when you expect your string arguments to be long or complex.\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI, custom_tool\nfrom\nlangchain.agents\nimport\ncreate_agent\n@custom_tool\ndef\nexecute_code\n(\ncode\n:\nstr\n) ->\nstr\n:\n\"\"\"Execute python code.\"\"\"\nreturn\n\"27\"\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5\"\n,\nuse_responses_api\n=\nTrue\n)\nagent\n=\ncreate_agent(llm, [execute_code])\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Use the tool to calculate 3^3.\"\n}\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [input_message]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nUse the tool to calculate 3^3.\n================================== Ai Message ==================================\n[{'id': 'rs_68b7336cb72081a080da70bf5e980e4e0d6082d28f91357a', 'summary': [], 'type': 'reasoning'}, {'call_id': 'call_qyKsJ4XlGRudbIJDrXVA2nQa', 'input': 'print(3**3)', 'name': 'execute_code', 'type': 'custom_tool_call', 'id': 'ctc_68b7336f718481a0b39584cd35fbaa5d0d6082d28f91357a', 'status': 'completed'}]\nTool Calls:\nexecute_code (call_qyKsJ4XlGRudbIJDrXVA2nQa)\nCall ID: call_qyKsJ4XlGRudbIJDrXVA2nQa\nArgs:\n__arg1: print(3**3)\n================================= Tool Message =================================\nName: execute_code\n[{'type': 'custom_tool_call_output', 'output': '27'}]\n================================== Ai Message ==================================\n[{'type': 'text', 'text': '27', 'annotations': [], 'id': 'msg_68b73371e9e081a0927f54f88f2cd7a20d6082d28f91357a'}]\nContext-free grammars\nOpenAI supports the specification of a\ncontext-free grammar\nfor custom tool inputs in\nlark\nor\nregex\nformat. See\nOpenAI docs\nfor details. The\nformat\nparameter can be passed into\n@custom_tool\nas shown below:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI, custom_tool\nfrom\nlangchain.agents\nimport\ncreate_agent\ngrammar\n=\n\"\"\"\nstart: expr\nexpr: term (SP ADD SP term)* -> add\n| term\nterm: factor (SP MUL SP factor)* -> mul\n| factor\nfactor: INT\nSP: \" \"\nADD: \"+\"\nMUL: \"*\"\n%i\nmport common.INT\n\"\"\"\nformat_\n=\n{\n\"type\"\n:\n\"grammar\"\n,\n\"syntax\"\n:\n\"lark\"\n,\n\"definition\"\n: grammar}\n@custom_tool\n(\nformat\n=\nformat_)\ndef\ndo_math\n(\ninput_string\n:\nstr\n) ->\nstr\n:\n\"\"\"Do a mathematical operation.\"\"\"\nreturn\n\"27\"\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5\"\n,\nuse_responses_api\n=\nTrue\n)\nagent\n=\ncreate_agent(llm, [do_math])\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Use the tool to calculate 3^3.\"\n}\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [input_message]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nUse the tool to calculate 3^3.\n================================== Ai Message ==================================\n[{'id': 'rs_68b733f066a48194a41001c0cc1081760811f11b6f4bae47', 'summary': [], 'type': 'reasoning'}, {'call_id': 'call_7hTYtlTj9NgWyw8AQGqETtV9', 'input': '3 * 3 * 3', 'name': 'do_math', 'type': 'custom_tool_call', 'id': 'ctc_68b733f3a0a08194968b8338d33ad89f0811f11b6f4bae47', 'status': 'completed'}]\nTool Calls:\ndo_math (call_7hTYtlTj9NgWyw8AQGqETtV9)\nCall ID: call_7hTYtlTj9NgWyw8AQGqETtV9\nArgs:\n__arg1: 3 * 3 * 3\n================================= Tool Message =================================\nName: do_math\n[{'type': 'custom_tool_call_output', 'output': '27'}]\n================================== Ai Message ==================================\n[{'type': 'text', 'text': '27', 'annotations': [], 'id': 'msg_68b733f4bb008194937130796372bd0f0811f11b6f4bae47'}]\n\u200b\nResponses API\nRequires\nlangchain-openai>=0.3.9\nOpenAI supports a\nResponses\nAPI that is oriented toward building\nagentic\napplications. It includes a suite of\nbuilt-in tools\n, including web and file search. It also supports management of\nconversation state\n, allowing you to continue a conversational thread without explicitly passing in previous messages, as well as the output from\nreasoning processes\n.\nChatOpenAI\nwill route to the Responses API if one of these features is used. You can also specify\nuse_responses_api=True\nwhen instantiating\nChatOpenAI\n.\n\u200b\nWeb search\nTo trigger a web search, pass\n{\"type\": \"web_search_preview\"}\nto the model as you would another tool.\nYou can also pass built-in tools as invocation params:\nCopy\nllm.invoke(\n\"...\"\n,\ntools\n=\n[{\n\"type\"\n:\n\"web_search_preview\"\n}])\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\ntool\n=\n{\n\"type\"\n:\n\"web_search_preview\"\n}\nllm_with_tools\n=\nllm.bind_tools([tool])\nresponse\n=\nllm_with_tools.invoke(\n\"What was a positive news story from today?\"\n)\nNote that the response includes structured\ncontent blocks\nthat include both the text of the response and OpenAI\nannotations\nciting its sources. The output message will also contain information from any tool invocations:\nCopy\nresponse.content_blocks\nCopy\n[{'type': 'server_tool_call',\n'name': 'web_search',\n'args': {'query': 'positive news stories today', 'type': 'search'},\n'id': 'ws_68cd6f8d72e4819591dab080f4b0c340080067ad5ea8144a'},\n{'type': 'server_tool_result',\n'tool_call_id': 'ws_68cd6f8d72e4819591dab080f4b0c340080067ad5ea8144a',\n'status': 'success'},\n{'type': 'text',\n'text': 'Here are some positive news stories from today...',\n'annotations': [{'end_index': 410,\n'start_index': 337,\n'title': 'Positive News | Real Stories. Real Positive Impact',\n'type': 'citation',\n'url': 'https://www.positivenews.press/?utm_source=openai'},\n{'end_index': 969,\n'start_index': 798,\n'title': \"From Green Innovation to Community Triumphs: Uplifting US Stories Lighting Up September 2025 | That's Great News\",\n'type': 'citation',\n'url': 'https://info.thatsgreatnews.com/from-green-innovation-to-community-triumphs-uplifting-us-stories-lighting-up-september-2025/?utm_source=openai'},\n'id': 'msg_68cd6f8e8d448195a807b89f483a1277080067ad5ea8144a'}]\nYou can recover just the text content of the response as a string by using\nresponse.text\n. For example, to stream response text:\nCopy\nfor\ntoken\nin\nllm_with_tools.stream(\n\"...\"\n):\nprint\n(token.text,\nend\n=\n\"|\"\n)\nSee the\nstreaming guide\nfor more detail.\n\u200b\nImage generation\nRequires\nlangchain-openai>=0.3.19\nTo trigger an image generation, pass\n{\"type\": \"image_generation\"}\nto the model as you would another tool.\nYou can also pass built-in tools as invocation params:\nCopy\nllm.invoke(\n\"...\"\n,\ntools\n=\n[{\n\"type\"\n:\n\"image_generation\"\n}])\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\ntool\n=\n{\n\"type\"\n:\n\"image_generation\"\n,\n\"quality\"\n:\n\"low\"\n}\nllm_with_tools\n=\nllm.bind_tools([tool])\nai_message\n=\nllm_with_tools.invoke(\n\"Draw a picture of a cute fuzzy cat with an umbrella\"\n)\nCopy\nimport\nbase64\nfrom\nIPython.display\nimport\nImage\nimage\n=\nnext\n(\nitem\nfor\nitem\nin\nai_message.content_blocks\nif\nitem[\n\"type\"\n]\n==\n\"image\"\n)\nImage(base64.b64decode(image[\n\"base64\"\n]),\nwidth\n=\n200\n)\n\u200b\nFile search\nTo trigger a file search, pass a\nfile search tool\nto the model as you would another tool. You will need to populate an OpenAI-managed vector store and include the vector store ID in the tool definition. See\nOpenAI documentation\nfor more detail.\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\ninclude\n=\n[\n\"file_search_call.results\"\n],\n# optionally include search results\n)\nopenai_vector_store_ids\n=\n[\n\"vs_...\"\n,\n# your IDs here\n]\ntool\n=\n{\n\"type\"\n:\n\"file_search\"\n,\n\"vector_store_ids\"\n: openai_vector_store_ids,\n}\nllm_with_tools\n=\nllm.bind_tools([tool])\nresponse\n=\nllm_with_tools.invoke(\n\"What is deep research by OpenAI?\"\n)\nprint\n(response.text)\nCopy\nDeep Research by OpenAI is...\nAs with\nweb search\n, the response will include content blocks with citations:\nCopy\n[block[\n\"type\"\n]\nfor\nblock\nin\nresponse.content_blocks]\nCopy\n[\n'server_tool_call'\n,\n'server_tool_result'\n,\n'text'\n]\nCopy\ntext_block\n=\nnext\n(block\nfor\nblock\nin\nresponse.content_blocks\nif\nblock[\n\"type\"\n]\n==\n\"text\"\n)\ntext_block[\n\"annotations\"\n][:\n2\n]\nCopy\n[{'type': 'citation',\n'title': 'deep_research_blog.pdf',\n'extras': {'file_id': 'file-3UzgX7jcC8Dt9ZAFzywg5k', 'index': 2712}},\n{'type': 'citation',\n'title': 'deep_research_blog.pdf',\n'extras': {'file_id': 'file-3UzgX7jcC8Dt9ZAFzywg5k', 'index': 2712}}]\nIt will also include information from the built-in tool invocations:\nCopy\nresponse.content_blocks[\n0\n]\nCopy\n{'type': 'server_tool_call',\n'name': 'file_search',\n'id': 'fs_68cd704c191c81959281b3b2ec6b139908f8f7fb31b1123c',\n'args': {'queries': ['deep research by OpenAI']}}\n\u200b\nComputer use\nChatOpenAI\nsupports the\n\"computer-use-preview\"\nmodel, which is a specialized model for the built-in computer use tool. To enable, pass a\ncomputer use tool\nas you would pass another tool.\nCurrently, tool outputs for computer use are present in the message\ncontent\nfield. To reply to the computer use tool call, construct a\nToolMessage\nwith\n{\"type\": \"computer_call_output\"}\nin its\nadditional_kwargs\n. The content of the message will be a screenshot. Below, we demonstrate a simple example.\nFirst, load two screenshots:\nCopy\nimport\nbase64\ndef\nload_png_as_base64\n(\nfile_path\n):\nwith\nopen\n(file_path,\n\"rb\"\n)\nas\nimage_file:\nencoded_string\n=\nbase64.b64encode(image_file.read())\nreturn\nencoded_string.decode(\n\"utf-8\"\n)\nscreenshot_1_base64\n=\nload_png_as_base64(\n\"/path/to/screenshot_1.png\"\n)\n# perhaps a screenshot of an application\nscreenshot_2_base64\n=\nload_png_as_base64(\n\"/path/to/screenshot_2.png\"\n)\n# perhaps a screenshot of the Desktop\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\n# Initialize model\nllm\n=\nChatOpenAI(\nmodel\n=\n\"computer-use-preview\"\n,\ntruncation\n=\n\"auto\"\n)\n# Bind computer-use tool\ntool\n=\n{\n\"type\"\n:\n\"computer_use_preview\"\n,\n\"display_width\"\n:\n1024\n,\n\"display_height\"\n:\n768\n,\n\"environment\"\n:\n\"browser\"\n,\n}\nllm_with_tools\n=\nllm.bind_tools([tool])\n# Construct input message\ninput_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: [\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n: (\n\"Click the red X to close and reveal my Desktop. \"\n\"Proceed, no confirmation needed.\"\n),\n},\n{\n\"type\"\n:\n\"input_image\"\n,\n\"image_url\"\n:\nf\n\"data:image/png;base64,\n{\nscreenshot_1_base64\n}\n\"\n,\n},\n],\n}\n# Invoke model\nresponse\n=\nllm_with_tools.invoke(\n[input_message],\nreasoning\n=\n{\n\"generate_summary\"\n:\n\"concise\"\n,\n},\n)\nThe response will include a call to the computer-use tool in its\ncontent\n:\nCopy\nresponse.content\nCopy\n[{'id': 'rs_685da051742c81a1bb35ce46a9f3f53406b50b8696b0f590',\n'summary': [{'text': \"Clicking red 'X' to show desktop\",\n'type': 'summary_text'}],\n'type': 'reasoning'},\n{'id': 'cu_685da054302481a1b2cc43b56e0b381706b50b8696b0f590',\n'action': {'button': 'left', 'type': 'click', 'x': 14, 'y': 38},\n'call_id': 'call_zmQerFBh4PbBE8mQoQHkfkwy',\n'pending_safety_checks': [],\n'status': 'completed',\n'type': 'computer_call'}]\nWe next construct a\nToolMessage\nwith these properties:\nIt has a\ntool_call_id\nmatching the\ncall_id\nfrom the computer-call.\nIt has\n{\"type\": \"computer_call_output\"}\nin its\nadditional_kwargs\n.\nIts content is either an\nimage_url\nor an\ninput_image\noutput block (see\nOpenAI docs\nfor formatting).\nCopy\nfrom\nlangchain.messages\nimport\nToolMessage\ntool_call_id\n=\nnext\n(\nitem[\n\"call_id\"\n]\nfor\nitem\nin\nresponse.content\nif\nitem[\n\"type\"\n]\n==\n\"computer_call\"\n)\ntool_message\n=\nToolMessage(\ncontent\n=\n[\n{\n\"type\"\n:\n\"input_image\"\n,\n\"image_url\"\n:\nf\n\"data:image/png;base64,\n{\nscreenshot_2_base64\n}\n\"\n,\n}\n],\n# content=f\"data:image/png;base64,{screenshot_2_base64}\",  # <-- also acceptable\ntool_call_id\n=\ntool_call_id,\nadditional_kwargs\n=\n{\n\"type\"\n:\n\"computer_call_output\"\n},\n)\nWe can now invoke the model again using the message history:\nCopy\nmessages\n=\n[\ninput_message,\nresponse,\ntool_message,\n]\nresponse_2\n=\nllm_with_tools.invoke(\nmessages,\nreasoning\n=\n{\n\"generate_summary\"\n:\n\"concise\"\n,\n},\n)\nCopy\nresponse_2.text\nCopy\n'VS Code has been closed, and the desktop is now visible.'\nInstead of passing back the entire sequence, we can also use the\nprevious_response_id\n:\nCopy\nprevious_response_id\n=\nresponse.response_metadata[\n\"id\"\n]\nresponse_2\n=\nllm_with_tools.invoke(\n[tool_message],\nprevious_response_id\n=\nprevious_response_id,\nreasoning\n=\n{\n\"generate_summary\"\n:\n\"concise\"\n,\n},\n)\nCopy\nresponse_2.text\nCopy\n'The VS Code window is closed, and the desktop is now visible. Let me know if you need any further assistance.'\n\u200b\nCode interpreter\nOpenAI implements a\ncode interpreter\ntool to support the sandboxed generation and execution of code.\nExample use\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\ninclude\n=\n[\n\"code_interpreter_call.outputs\"\n],\n# optionally include outputs\n)\nllm_with_tools\n=\nllm.bind_tools(\n[\n{\n\"type\"\n:\n\"code_interpreter\"\n,\n# Create a new container\n\"container\"\n: {\n\"type\"\n:\n\"auto\"\n},\n}\n]\n)\nresponse\n=\nllm_with_tools.invoke(\n\"Write and run code to answer the question: what is 3^3?\"\n)\nNote that the above command created a new container. We can also specify an existing container ID:\nCopy\ncode_interpreter_calls\n=\n[\nitem\nfor\nitem\nin\nresponse.content\nif\nitem[\n\"type\"\n]\n==\n\"code_interpreter_call\"\n]\nassert\nlen\n(code_interpreter_calls)\n==\n1\ncontainer_id\n=\ncode_interpreter_calls[\n0\n][\n\"extras\"\n][\n\"container_id\"\n]\nllm_with_tools\n=\nllm.bind_tools(\n[\n{\n\"type\"\n:\n\"code_interpreter\"\n,\n# Use an existing container\n\"container\"\n: container_id,\n}\n]\n)\n\u200b\nRemote MCP\nOpenAI implements a\nremote MCP\ntool that allows for model-generated calls to MCP servers.\nExample use\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\nllm_with_tools\n=\nllm.bind_tools(\n[\n{\n\"type\"\n:\n\"mcp\"\n,\n\"server_label\"\n:\n\"deepwiki\"\n,\n\"server_url\"\n:\n\"https://mcp.deepwiki.com/mcp\"\n,\n\"require_approval\"\n:\n\"never\"\n,\n}\n]\n)\nresponse\n=\nllm_with_tools.invoke(\n\"What transport protocols does the 2025-03-26 version of the MCP \"\n\"spec (modelcontextprotocol/modelcontextprotocol) support?\"\n)\nMCP Approvals\nOpenAI will at times request approval before sharing data with a remote MCP server.\nIn the above command, we instructed the model to never require approval. We can also configure the model to always request approval, or to always request approval for specific tools:\nCopy\nllm_with_tools\n=\nllm.bind_tools(\n[\n{\n\"type\"\n:\n\"mcp\"\n,\n\"server_label\"\n:\n\"deepwiki\"\n,\n\"server_url\"\n:\n\"https://mcp.deepwiki.com/mcp\"\n,\n\"require_approval\"\n: {\n\"always\"\n: {\n\"tool_names\"\n: [\n\"read_wiki_structure\"\n]\n}\n}\n}\n]\n)\nresponse\n=\nllm_with_tools.invoke(\n\"What transport protocols does the 2025-03-26 version of the MCP \"\n\"spec (modelcontextprotocol/modelcontextprotocol) support?\"\n)\nResponses may then include blocks with type\n\"mcp_approval_request\"\n.\nTo submit approvals for an approval request, structure it into a content block in an input message:\nCopy\napproval_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: [\n{\n\"type\"\n:\n\"mcp_approval_response\"\n,\n\"approve\"\n:\nTrue\n,\n\"approval_request_id\"\n: block[\n\"id\"\n],\n}\nfor\nblock\nin\nresponse.content\nif\nblock[\n\"type\"\n]\n==\n\"mcp_approval_request\"\n]\n}\nnext_response\n=\nllm_with_tools.invoke(\n[approval_message],\n# continue existing thread\nprevious_response_id\n=\nresponse.response_metadata[\n\"id\"\n]\n)\n\u200b\nManaging conversation state\nThe Responses API supports management of\nconversation state\n.\n\u200b\nManually manage state\nYou can manage the state manually or using\nLangGraph\n, as with other chat models:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\nuse_responses_api\n=\nTrue\n)\nfirst_query\n=\n\"Hi, I'm Bob.\"\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: first_query}]\nresponse\n=\nllm.invoke(messages)\nprint\n(response.text)\nCopy\nHi Bob! Nice to meet you. How can I assist you today?\nCopy\nsecond_query\n=\n\"What is my name?\"\nmessages.extend(\n[\nresponse,\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: second_query},\n]\n)\nsecond_response\n=\nllm.invoke(messages)\nprint\n(second_response.text)\nCopy\nYou mentioned that your name is Bob. How can I assist you further, Bob?\nYou can use\nLangGraph\nto manage conversational threads for you in a variety of backends, including in-memory and Postgres. See\nthis tutorial\nto get started.\n\u200b\nPassing\nprevious_response_id\nWhen using the Responses API, LangChain messages will include an\n\"id\"\nfield in its metadata. Passing this ID to subsequent invocations will continue the conversation. Note that this is\nequivalent\nto manually passing in messages from a billing perspective.\nCopy\nsecond_response\n=\nllm.invoke(\n\"What is my name?\"\n,\nprevious_response_id\n=\nresponse.id,\n)\nprint\n(second_response.text)\nCopy\nYour name is Bob. How can I help you today, Bob?\nChatOpenAI can also automatically specify\nprevious_response_id\nusing the last response in a message sequence:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\nuse_previous_response_id\n=\nTrue\n,\n)\nIf we set\nuse_previous_response_id=True\n, input messages up to the most recent response will be dropped from request payloads, and\nprevious_response_id\nwill be set using the ID of the most recent response.\nThat is,\nCopy\nllm.invoke(\n[\nHumanMessage(\n\"Hello\"\n),\nAIMessage(\n\"Hi there!\"\n,\nid\n=\n\"resp_123\"\n),\nHumanMessage(\n\"How are you?\"\n),\n]\n)\n\u2026is equivalent to:\nCopy\nllm.invoke([HumanMessage(\n\"How are you?\"\n)],\nprevious_response_id\n=\n\"resp_123\"\n)\n\u200b\nReasoning output\nSome OpenAI models will generate separate text content illustrating their reasoning process. See OpenAI\u2019s\nreasoning documentation\nfor details.\nOpenAI can return a summary of the model\u2019s reasoning (although it doesn\u2019t expose the raw reasoning tokens). To configure\nChatOpenAI\nto return this summary, specify the\nreasoning\nparameter.\nChatOpenAI\nwill automatically route to the Responses API if this parameter is set.\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nreasoning\n=\n{\n\"effort\"\n:\n\"medium\"\n,\n# 'low', 'medium', or 'high'\n\"summary\"\n:\n\"auto\"\n,\n# 'detailed', 'auto', or None\n}\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\nreasoning\n=\nreasoning)\nresponse\n=\nllm.invoke(\n\"What is 3^3?\"\n)\n# Output\nresponse.text\nCopy\n'3\u00b3 = 3 \u00d7 3 \u00d7 3 = 27.'\nCopy\n# Reasoning\nfor\nblock\nin\nresponse.content_blocks:\nif\nblock[\n\"type\"\n]\n==\n\"reasoning\"\n:\nprint\n(block[\n\"reasoning\"\n])\nCopy\n**Calculating the power of three**\nThe user is asking about 3 raised to the power of 3. That's a pretty simple calculation! I know that 3^3 equals 27, so I can say, \"3 to the power of 3 equals 27.\" I might also include a quick explanation that it's 3 multiplied by itself three times: 3 \u00d7 3 \u00d7 3 = 27. So, the answer is definitely 27.\nTroubleshooting: Empty responses from reasoning models\nIf you\u2019re getting empty responses from reasoning models like\ngpt-5-mini\n, this is likely due to restrictive token limits. The model uses tokens for internal reasoning and may not have any left for the final output.\nEnsure\nmax_tokens\nis set to\nNone\nor increase the token limit to allow sufficient tokens for both reasoning and output generation.\n\u200b\nFine-tuning\nYou can call fine-tuned OpenAI models by passing in your corresponding\nmodelName\nparameter.\nThis generally takes the form of\nft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}\n. For example:\nCopy\nfine_tuned_model\n=\nChatOpenAI(\ntemperature\n=\n0\n,\nmodel_name\n=\n\"ft:gpt-5-mini-turbo-0613:langchain::7qTVM5AR\"\n)\nfine_tuned_model.invoke(messages)\nCopy\nAIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 31, 'total_tokens': 39}, 'model_name': 'ft:gpt-5-mini-turbo-0613:langchain::7qTVM5AR', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0f39b30e-c56e-4f3b-af99-5c948c984146-0', usage_metadata={'input_tokens': 31, 'output_tokens': 8, 'total_tokens': 39})\n\u200b\nMultimodal Inputs (images, PDFs, audio)\nOpenAI has models that support multimodal inputs. You can pass in images, PDFs, or audio to these models. For more information on how to do this in LangChain, head to the\nmultimodal inputs\ndocs.\nYou can see the list of models that support different modalities in\nOpenAI\u2019s documentation\n.\nFor all modalities, LangChain supports both its cross-provider standard as well as OpenAI\u2019s native content-block format.\nTo pass multimodal data into\nChatOpenAI\n, create a\ncontent block\ncontaining the data and incorporate it into a message, e.g., as below:\nCopy\nmessage\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: [\n{\n\"type\"\n:\n\"text\"\n,\n# Update prompt as desired\n\"text\"\n:\n\"Describe the (image / PDF / audio...)\"\n,\n},\ncontent_block,\n],\n}\nSee below for examples of content blocks.\nImages\nRefer to examples in the how-to guide\nhere\n.\nURLs\nCopy\n# LangChain format\ncontent_block\n=\n{\n\"type\"\n:\n\"image\"\n,\n\"url\"\n: url_string,\n}\n# OpenAI Chat Completions format\ncontent_block\n=\n{\n\"type\"\n:\n\"image_url\"\n,\n\"image_url\"\n: {\n\"url\"\n: url_string},\n}\nIn-line base64 data\nCopy\n# LangChain format\ncontent_block\n=\n{\n\"type\"\n:\n\"image\"\n,\n\"base64\"\n: base64_string,\n\"mime_type\"\n:\n\"image/jpeg\"\n,\n}\n# OpenAI Chat Completions format\ncontent_block\n=\n{\n\"type\"\n:\n\"image_url\"\n,\n\"image_url\"\n: {\n\"url\"\n:\nf\n\"data:image/jpeg;base64,\n{\nbase64_string\n}\n\"\n,\n},\n}\nPDFs\nNote: OpenAI requires file-names be specified for PDF inputs. When using LangChain\u2019s format, include the\nfilename\nkey.\nRead more\nhere\n.\nRefer to examples in the how-to guide\nhere\n.\nIn-line base64 data\nCopy\n# LangChain format\ncontent_block\n=\n{\n\"type\"\n:\n\"file\"\n,\n\"base64\"\n: base64_string,\n\"mime_type\"\n:\n\"application/pdf\"\n,\n\"filename\"\n:\n\"my-file.pdf\"\n,\n}\n# OpenAI Chat Completions format\ncontent_block\n=\n{\n\"type\"\n:\n\"file\"\n,\n\"file\"\n: {\n\"filename\"\n:\n\"my-file.pdf\"\n,\n\"file_data\"\n:\nf\n\"data:application/pdf;base64,\n{\nbase64_string\n}\n\"\n,\n}\n}\nAudio\nSee\nsupported models\n, e.g.,\n\"gpt-5-mini-audio\"\n.\nRefer to examples in the how-to guide\nhere\n.\nIn-line base64 data\nCopy\n# LangChain format\ncontent_block\n=\n{\n\"type\"\n:\n\"audio\"\n,\n\"mime_type\"\n:\n\"audio/wav\"\n,\n# or appropriate mime-type\n\"base64\"\n: base64_string,\n}\n# OpenAI Chat Completions format\ncontent_block\n=\n{\n\"type\"\n:\n\"input_audio\"\n,\n\"input_audio\"\n: {\n\"data\"\n: base64_string,\n\"format\"\n:\n\"wav\"\n},\n}\n\u200b\nPredicted output\nRequires\nlangchain-openai>=0.2.6\nSome OpenAI models (such as their\ngpt-5-mini\nand\ngpt-5-mini\nseries) support\nPredicted Outputs\n, which allow you to pass in a known portion of the LLM\u2019s expected output ahead of time to reduce latency. This is useful for cases such as editing text or code, where only a small part of the model\u2019s output will change.\nHere\u2019s an example:\nCopy\ncode\n=\n\"\"\"\n/// <summary>\n/// Represents a user with a first name, last name, and username.\n/// </summary>\npublic class User\n{\n/// <summary>\n/// Gets or sets the user's first name.\n/// </summary>\npublic string FirstName { get; set; }\n/// <summary>\n/// Gets or sets the user's last name.\n/// </summary>\npublic string LastName { get; set; }\n/// <summary>\n/// Gets or sets the user's username.\n/// </summary>\npublic string Username { get; set; }\n}\n\"\"\"\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\nquery\n=\n(\n\"Replace the Username property with an Email property. \"\n\"Respond only with code, and with no markdown formatting.\"\n)\nresponse\n=\nllm.invoke(\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}, {\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: code}],\nprediction\n=\n{\n\"type\"\n:\n\"content\"\n,\n\"content\"\n: code},\n)\nprint\n(response.content)\nprint\n(response.response_metadata)\nCopy\n///\n<\nsummary\n>\n///\nRepresents a user\nwith\na first name, last name,\nand\nemail.\n///\n</\nsummary\n>\npublic\nclass\nUser\n{\n///\n<\nsummary\n>\n///\nGets\nor\nsets the user\n's first name.\n///\n</\nsummary\n>\npublic string FirstName { get;\nset\n; }\n///\n<\nsummary\n>\n///\nGets\nor\nsets the user\n's last name.\n///\n</\nsummary\n>\npublic string LastName { get;\nset\n; }\n///\n<\nsummary\n>\n///\nGets\nor\nsets the user\n's email.\n///\n</\nsummary\n>\npublic string Email { get;\nset\n; }\n}\n{\n'token_usage'\n: {\n'completion_tokens'\n:\n226\n,\n'prompt_tokens'\n:\n166\n,\n'total_tokens'\n:\n392\n,\n'completion_tokens_details'\n: {\n'accepted_prediction_tokens'\n:\n49\n,\n'audio_tokens'\n:\nNone\n,\n'reasoning_tokens'\n:\n0\n,\n'rejected_prediction_tokens'\n:\n107\n},\n'prompt_tokens_details'\n: {\n'audio_tokens'\n:\nNone\n,\n'cached_tokens'\n:\n0\n}},\n'model_name'\n:\n'gpt-5-mini-08-06'\n,\n'system_fingerprint'\n:\n'fp_45cf54deae'\n,\n'finish_reason'\n:\n'stop'\n,\n'logprobs'\n:\nNone\n}\nPredictions are billed as additional tokens and may increase your usage and costs in exchange for this reduced latency.\n\u200b\nAudio Generation (Preview)\nRequires\nlangchain-openai>=0.2.3\nOpenAI has a new\naudio generation feature\nthat allows you to use audio inputs and outputs with the\ngpt-5-mini-audio\nmodel.\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini-audio\"\n,\ntemperature\n=\n0\n,\nmodel_kwargs\n=\n{\n\"modalities\"\n: [\n\"text\"\n,\n\"audio\"\n],\n\"audio\"\n: {\n\"voice\"\n:\n\"alloy\"\n,\n\"format\"\n:\n\"wav\"\n},\n},\n)\noutput_message\n=\nllm.invoke(\n[\n(\n\"human\"\n,\n\"Are you made by OpenAI? Just answer yes or no\"\n),\n]\n)\noutput_message.additional_kwargs['audio']\nwill contain a dictionary like\nCopy\n{\n'data'\n:\n'<audio data b64-encoded'\n,\n'expires_at'\n:\n1729268602\n,\n'id'\n:\n'audio_67127d6a44348190af62c1530ef0955a'\n,\n'transcript'\n:\n'Yes.'\n}\n\u2026and the format will be what was passed in\nmodel_kwargs['audio']['format']\n.\nWe can also pass this message with audio data back to the model as part of a message history before openai\nexpires_at\nis reached.\n**Output audio is stored under the\naudio\nkey in\nAIMessage.additional_kwargs\n, but input content blocks are typed with an\ninput_audio\ntype and key in\nHumanMessage.content\nlists. **\nFor more information, see OpenAI\u2019s\naudio docs\n.\nCopy\nhistory\n=\n[\n(\n\"human\"\n,\n\"Are you made by OpenAI? Just answer yes or no\"\n),\noutput_message,\n(\n\"human\"\n,\n\"And what is your name? Just give your name.\"\n),\n]\nsecond_output_message\n=\nllm.invoke(history)\n\u200b\nPrompt caching\nOpenAI\u2019s\nprompt caching\nfeature automatically caches prompts longer than 1024 tokens to reduce costs and improve response times. This feature is enabled for all recent models (\ngpt-5-mini\nand newer).\n\u200b\nManual caching\nYou can use the\nprompt_cache_key\nparameter to influence OpenAI\u2019s caching and optimize cache hit rates:\nCopy\nfrom\nlangchain_openai\nimport\nChatOpenAI\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\n# Use a cache key for repeated prompts\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant that translates English to French.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"I love programming.\"\n},\n]\nresponse\n=\nllm.invoke(\nmessages,\nprompt_cache_key\n=\n\"translation-assistant-v1\"\n)\n# Check cache usage\ncache_read_tokens\n=\nresponse.usage_metadata.input_token_details.cache_read\nprint\n(\nf\n\"Cached tokens used:\n{\ncache_read_tokens\n}\n\"\n)\nCache hits require the prompt prefix to match exactly\n\u200b\nCache key strategies\nYou can use different cache key strategies based on your application\u2019s needs:\nCopy\n# Static cache keys for consistent prompt templates\ncustomer_response\n=\nllm.invoke(\nmessages,\nprompt_cache_key\n=\n\"customer-support-v1\"\n)\nsupport_response\n=\nllm.invoke(\nmessages,\nprompt_cache_key\n=\n\"internal-support-v1\"\n)\n# Dynamic cache keys based on context\nuser_type\n=\n\"premium\"\ncache_key\n=\nf\n\"assistant-\n{\nuser_type\n}\n-v1\"\nresponse\n=\nllm.invoke(messages,\nprompt_cache_key\n=\ncache_key)\n\u200b\nModel-level caching\nYou can also set a default cache key at the model level using\nmodel_kwargs\n:\nCopy\nllm\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n,\nmodel_kwargs\n=\n{\n\"prompt_cache_key\"\n:\n\"default-cache-v1\"\n}\n)\n# Uses default cache key\nresponse1\n=\nllm.invoke(messages)\n# Override with specific cache key\nresponse2\n=\nllm.invoke(messages,\nprompt_cache_key\n=\n\"override-cache-v1\"\n)\n\u200b\nFlex processing\nOpenAI offers a variety of\nservice tiers\n. The \u201cflex\u201d tier offers cheaper pricing for requests, with the trade-off that responses may take longer and resources might not always be available. This approach is best suited for non-critical tasks, including model testing, data enhancement, or jobs that can be run asynchronously.\nTo use it, initialize the model with\nservice_tier=\"flex\"\n:\nCopy\nllm\n=\nChatOpenAI(\nmodel\n=\n\"o4-mini\"\n,\nservice_tier\n=\n\"flex\"\n)\nNote that this is a beta feature that is only available for a subset of models. See OpenAI\ndocs\nfor more detail.\n\u200b\nAPI reference\nFor detailed documentation of all features and configuration options, head to the\nChatOpenAI\nAPI reference.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/integrations/chat/openai",
      "title": "ChatOpenAI - Docs by LangChain",
      "heading": "ChatOpenAI"
    }
  },
  {
    "page_content": "Build a custom RAG agent with LangGraph - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangGraph\nBuild a custom RAG agent with LangGraph\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nMulti-agent\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nSetup\n1. Preprocess documents\n2. Create a retriever tool\n3. Generate query\n4. Grade documents\n5. Rewrite question\n6. Generate an answer\n7. Assemble the graph\n8. Run the agentic RAG\nTutorials\nLangGraph\nBuild a custom RAG agent with LangGraph\nCopy page\nCopy page\n\u200b\nOverview\nIn this tutorial we will build a\nretrieval\nagent using LangGraph.\nLangChain offers built-in\nagent\nimplementations, implemented using\nLangGraph\nprimitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a retrieval agent.\nRetrieval\nagents are useful when you want an LLM to make a decision about whether to retrieve context from a vectorstore or respond to the user directly.\nBy the end of the tutorial we will have done the following:\nFetch and preprocess documents that will be used for retrieval.\nIndex those documents for semantic search and create a retriever tool for the agent.\nBuild an agentic RAG system that can decide when to use the retriever tool.\n\u200b\nConcepts\nWe will cover the following concepts:\nRetrieval\nusing\ndocument loaders\n,\ntext splitters\n,\nembeddings\n, and\nvector stores\nThe LangGraph\nGraph API\n, including state, nodes, edges, and conditional edges.\n\u200b\nSetup\nLet\u2019s download the required packages and set our API keys:\nCopy\npip install\n-\nU langgraph\n\"langchain[openai]\"\nlangchain\n-\ncommunity langchain\n-\ntext\n-\nsplitters bs4\nCopy\nimport\ngetpass\nimport\nos\ndef\n_set_env\n(\nkey\n:\nstr\n):\nif\nkey\nnot\nin\nos.environ:\nos.environ[key]\n=\ngetpass.getpass(\nf\n\"\n{\nkey\n}\n:\"\n)\n_set_env(\n\"OPENAI_API_KEY\"\n)\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects.\nLangSmith\nlets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.\n\u200b\n1. Preprocess documents\nFetch documents to use in our RAG system. We will use three of the most recent pages from\nLilian Weng\u2019s excellent blog\n. We\u2019ll start by fetching the content of the pages using\nWebBaseLoader\nutility:\nCopy\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nurls\n=\n[\n\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"\n,\n\"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\n,\n\"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\"\n,\n]\ndocs\n=\n[WebBaseLoader(url).load()\nfor\nurl\nin\nurls]\nCopy\ndocs[\n0\n][\n0\n].page_content.strip()[:\n1000\n]\nSplit the fetched documents into smaller chunks for indexing into our vectorstore:\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ndocs_list\n=\n[item\nfor\nsublist\nin\ndocs\nfor\nitem\nin\nsublist]\ntext_splitter\n=\nRecursiveCharacterTextSplitter.from_tiktoken_encoder(\nchunk_size\n=\n100\n,\nchunk_overlap\n=\n50\n)\ndoc_splits\n=\ntext_splitter.split_documents(docs_list)\nCopy\ndoc_splits[\n0\n].page_content.strip()\n\u200b\n2. Create a retriever tool\nNow that we have our split documents, we can index them into a vector store that we\u2019ll use for semantic search.\nUse an in-memory vector store and OpenAI embeddings:\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nvectorstore\n=\nInMemoryVectorStore.from_documents(\ndocuments\n=\ndoc_splits,\nembedding\n=\nOpenAIEmbeddings()\n)\nretriever\n=\nvectorstore.as_retriever()\nCreate a retriever tool using the\n@tool\ndecorator:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\ndef\nretrieve_blog_posts\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search and return information about Lilian Weng blog posts.\"\"\"\ndocs\n=\nretriever.invoke(query)\nreturn\n\"\n\\n\\n\n\"\n.join([doc.page_content\nfor\ndoc\nin\ndocs])\nretriever_tool\n=\nretrieve_blog_posts\nTest the tool:\nCopy\nretriever_tool.invoke({\n\"query\"\n:\n\"types of reward hacking\"\n})\n\u200b\n3. Generate query\nNow we will start building components (\nnodes\nand\nedges\n) for our agentic RAG graph.\nNote that the components will operate on the\nMessagesState\n\u2014 graph state that contains a\nmessages\nkey with a list of\nchat messages\n.\nBuild a\ngenerate_query_or_respond\nnode. It will call an LLM to generate a response based on the current graph state (list of messages). Given the input messages, it will decide to retrieve using the retriever tool, or respond directly to the user. Note that we\u2019re giving the chat model access to the\nretriever_tool\nwe created earlier via\n.bind_tools\n:\nCopy\nfrom\nlanggraph.graph\nimport\nMessagesState\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nresponse_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n,\ntemperature\n=\n0\n)\ndef\ngenerate_query_or_respond\n(\nstate\n: MessagesState):\n\"\"\"Call the model to generate a response based on the current state. Given\nthe question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n\"\"\"\nresponse\n=\n(\nresponse_model\n.bind_tools([retriever_tool]).invoke(state[\n\"messages\"\n])\n)\nreturn\n{\n\"messages\"\n: [response]}\nTry it on a random input:\nCopy\ninput\n=\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hello!\"\n}]}\ngenerate_query_or_respond(\ninput\n)[\n\"messages\"\n][\n-\n1\n].pretty_print()\nOutput:\nCopy\n================================== Ai Message ==================================\nHello! How can I help you today?\nAsk a question that requires semantic search:\nCopy\ninput\n=\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Lilian Weng say about types of reward hacking?\"\n,\n}\n]\n}\ngenerate_query_or_respond(\ninput\n)[\n\"messages\"\n][\n-\n1\n].pretty_print()\nOutput:\nCopy\n================================== Ai Message ==================================\nTool Calls:\nretrieve_blog_posts (call_tYQxgfIlnQUDMdtAhdbXNwIM)\nCall ID: call_tYQxgfIlnQUDMdtAhdbXNwIM\nArgs:\nquery: types of reward hacking\n\u200b\n4. Grade documents\nAdd a\nconditional edge\n\u2014\ngrade_documents\n\u2014 to determine whether the retrieved documents are relevant to the question. We will use a model with a structured output schema\nGradeDocuments\nfor document grading. The\ngrade_documents\nfunction will return the name of the node to go to based on the grading decision (\ngenerate_answer\nor\nrewrite_question\n):\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\ntyping\nimport\nLiteral\nGRADE_PROMPT\n=\n(\n\"You are a grader assessing relevance of a retrieved document to a user question.\n\\n\n\"\n\"Here is the retrieved document:\n\\n\\n\n{context}\n\\n\\n\n\"\n\"Here is the user question:\n{question}\n\\n\n\"\n\"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n\\n\n\"\n\"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n)\nclass\nGradeDocuments\n(\nBaseModel\n):\n\"\"\"Grade documents using a binary score for relevance check.\"\"\"\nbinary_score:\nstr\n=\nField(\ndescription\n=\n\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n)\ngrader_model\n=\ninit_chat_model(\n\"gpt-5-mini\"\n,\ntemperature\n=\n0\n)\ndef\ngrade_documents\n(\nstate\n: MessagesState,\n) -> Literal[\n\"generate_answer\"\n,\n\"rewrite_question\"\n]:\n\"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\nquestion\n=\nstate[\n\"messages\"\n][\n0\n].content\ncontext\n=\nstate[\n\"messages\"\n][\n-\n1\n].content\nprompt\n=\nGRADE_PROMPT\n.format(\nquestion\n=\nquestion,\ncontext\n=\ncontext)\nresponse\n=\n(\ngrader_model\n.with_structured_output(GradeDocuments).invoke(\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: prompt}]\n)\n)\nscore\n=\nresponse.binary_score\nif\nscore\n==\n\"yes\"\n:\nreturn\n\"generate_answer\"\nelse\n:\nreturn\n\"rewrite_question\"\nRun this with irrelevant documents in the tool response:\nCopy\nfrom\nlangchain_core.messages\nimport\nconvert_to_messages\ninput\n=\n{\n\"messages\"\n: convert_to_messages(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Lilian Weng say about types of reward hacking?\"\n,\n},\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"\"\n,\n\"tool_calls\"\n: [\n{\n\"id\"\n:\n\"1\"\n,\n\"name\"\n:\n\"retrieve_blog_posts\"\n,\n\"args\"\n: {\n\"query\"\n:\n\"types of reward hacking\"\n},\n}\n],\n},\n{\n\"role\"\n:\n\"tool\"\n,\n\"content\"\n:\n\"meow\"\n,\n\"tool_call_id\"\n:\n\"1\"\n},\n]\n)\n}\ngrade_documents(\ninput\n)\nConfirm that the relevant documents are classified as such:\nCopy\ninput\n=\n{\n\"messages\"\n: convert_to_messages(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Lilian Weng say about types of reward hacking?\"\n,\n},\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"\"\n,\n\"tool_calls\"\n: [\n{\n\"id\"\n:\n\"1\"\n,\n\"name\"\n:\n\"retrieve_blog_posts\"\n,\n\"args\"\n: {\n\"query\"\n:\n\"types of reward hacking\"\n},\n}\n],\n},\n{\n\"role\"\n:\n\"tool\"\n,\n\"content\"\n:\n\"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\"\n,\n\"tool_call_id\"\n:\n\"1\"\n,\n},\n]\n)\n}\ngrade_documents(\ninput\n)\n\u200b\n5. Rewrite question\nBuild the\nrewrite_question\nnode. The retriever tool can return potentially irrelevant documents, which indicates a need to improve the original user question. To do so, we will call the\nrewrite_question\nnode:\nCopy\nfrom\nlangchain.messages\nimport\nHumanMessage\nREWRITE_PROMPT\n=\n(\n\"Look at the input and try to reason about the underlying semantic intent / meaning.\n\\n\n\"\n\"Here is the initial question:\"\n\"\n\\n\n-------\n\\n\n\"\n\"\n{question}\n\"\n\"\n\\n\n-------\n\\n\n\"\n\"Formulate an improved question:\"\n)\ndef\nrewrite_question\n(\nstate\n: MessagesState):\n\"\"\"Rewrite the original user question.\"\"\"\nmessages\n=\nstate[\n\"messages\"\n]\nquestion\n=\nmessages[\n0\n].content\nprompt\n=\nREWRITE_PROMPT\n.format(\nquestion\n=\nquestion)\nresponse\n=\nresponse_model.invoke([{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: prompt}])\nreturn\n{\n\"messages\"\n: [HumanMessage(\ncontent\n=\nresponse.content)]}\nTry it out:\nCopy\ninput\n=\n{\n\"messages\"\n: convert_to_messages(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Lilian Weng say about types of reward hacking?\"\n,\n},\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"\"\n,\n\"tool_calls\"\n: [\n{\n\"id\"\n:\n\"1\"\n,\n\"name\"\n:\n\"retrieve_blog_posts\"\n,\n\"args\"\n: {\n\"query\"\n:\n\"types of reward hacking\"\n},\n}\n],\n},\n{\n\"role\"\n:\n\"tool\"\n,\n\"content\"\n:\n\"meow\"\n,\n\"tool_call_id\"\n:\n\"1\"\n},\n]\n)\n}\nresponse\n=\nrewrite_question(\ninput\n)\nprint\n(response[\n\"messages\"\n][\n-\n1\n].content)\nOutput:\nCopy\nWhat are the different types of reward hacking described by Lilian Weng, and how does she explain them?\n\u200b\n6. Generate an answer\nBuild\ngenerate_answer\nnode: if we pass the grader checks, we can generate the final answer based on the original question and the retrieved context:\nCopy\nGENERATE_PROMPT\n=\n(\n\"You are an assistant for question-answering tasks. \"\n\"Use the following pieces of retrieved context to answer the question. \"\n\"If you don't know the answer, just say that you don't know. \"\n\"Use three sentences maximum and keep the answer concise.\n\\n\n\"\n\"Question:\n{question}\n\\n\n\"\n\"Context:\n{context}\n\"\n)\ndef\ngenerate_answer\n(\nstate\n: MessagesState):\n\"\"\"Generate an answer.\"\"\"\nquestion\n=\nstate[\n\"messages\"\n][\n0\n].content\ncontext\n=\nstate[\n\"messages\"\n][\n-\n1\n].content\nprompt\n=\nGENERATE_PROMPT\n.format(\nquestion\n=\nquestion,\ncontext\n=\ncontext)\nresponse\n=\nresponse_model.invoke([{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: prompt}])\nreturn\n{\n\"messages\"\n: [response]}\nTry it:\nCopy\ninput\n=\n{\n\"messages\"\n: convert_to_messages(\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Lilian Weng say about types of reward hacking?\"\n,\n},\n{\n\"role\"\n:\n\"assistant\"\n,\n\"content\"\n:\n\"\"\n,\n\"tool_calls\"\n: [\n{\n\"id\"\n:\n\"1\"\n,\n\"name\"\n:\n\"retrieve_blog_posts\"\n,\n\"args\"\n: {\n\"query\"\n:\n\"types of reward hacking\"\n},\n}\n],\n},\n{\n\"role\"\n:\n\"tool\"\n,\n\"content\"\n:\n\"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\"\n,\n\"tool_call_id\"\n:\n\"1\"\n,\n},\n]\n)\n}\nresponse\n=\ngenerate_answer(\ninput\n)\nresponse[\n\"messages\"\n][\n-\n1\n].pretty_print()\nOutput:\nCopy\n================================== Ai Message ==================================\nLilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors.\n\u200b\n7. Assemble the graph\nNow we\u2019ll assemble all the nodes and edges into a complete graph:\nStart with a\ngenerate_query_or_respond\nand determine if we need to call\nretriever_tool\nRoute to next step using\ntools_condition\n:\nIf\ngenerate_query_or_respond\nreturned\ntool_calls\n, call\nretriever_tool\nto retrieve context\nOtherwise, respond directly to the user\nGrade retrieved document content for relevance to the question (\ngrade_documents\n) and route to next step:\nIf not relevant, rewrite the question using\nrewrite_question\nand then call\ngenerate_query_or_respond\nagain\nIf relevant, proceed to\ngenerate_answer\nand generate final response using the\nToolMessage\nwith the retrieved document context\nCopy\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.prebuilt\nimport\nToolNode, tools_condition\nworkflow\n=\nStateGraph(MessagesState)\n# Define the nodes we will cycle between\nworkflow.add_node(generate_query_or_respond)\nworkflow.add_node(\n\"retrieve\"\n, ToolNode([retriever_tool]))\nworkflow.add_node(rewrite_question)\nworkflow.add_node(generate_answer)\nworkflow.add_edge(\nSTART\n,\n\"generate_query_or_respond\"\n)\n# Decide whether to retrieve\nworkflow.add_conditional_edges(\n\"generate_query_or_respond\"\n,\n# Assess LLM decision (call `retriever_tool` tool or respond to the user)\ntools_condition,\n{\n# Translate the condition outputs to nodes in our graph\n\"tools\"\n:\n\"retrieve\"\n,\nEND\n:\nEND\n,\n},\n)\n# Edges taken after the `action` node is called.\nworkflow.add_conditional_edges(\n\"retrieve\"\n,\n# Assess agent decision\ngrade_documents,\n)\nworkflow.add_edge(\n\"generate_answer\"\n,\nEND\n)\nworkflow.add_edge(\n\"rewrite_question\"\n,\n\"generate_query_or_respond\"\n)\n# Compile\ngraph\n=\nworkflow.compile()\nVisualize the graph:\nCopy\nfrom\nIPython.display\nimport\nImage, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n\u200b\n8. Run the agentic RAG\nNow let\u2019s test the complete graph by running it with a question:\nCopy\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Lilian Weng say about types of reward hacking?\"\n,\n}\n]\n}\n):\nfor\nnode, update\nin\nchunk.items():\nprint\n(\n\"Update from node\"\n, node)\nupdate[\n\"messages\"\n][\n-\n1\n].pretty_print()\nprint\n(\n\"\n\\n\\n\n\"\n)\nOutput:\nCopy\nUpdate from node generate_query_or_respond\n================================== Ai Message ==================================\nTool Calls:\nretrieve_blog_posts (call_NYu2vq4km9nNNEFqJwefWKu1)\nCall ID: call_NYu2vq4km9nNNEFqJwefWKu1\nArgs:\nquery: types of reward hacking\nUpdate from node retrieve\n================================= Tool Message ==================================\nName: retrieve_blog_posts\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\nWhy does Reward Hacking Exist?#\nPan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size, (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy of three types of misspecified proxy rewards:\nLet's Define Reward Hacking#\nReward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the task as designed. In recent years, several related concepts have been proposed, all referring to some form of reward hacking:\nUpdate from node generate_answer\n================================== Ai Message ==================================\nLilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a SQL assistant with on-demand skills\nPrevious\nBuild a custom SQL agent\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/agentic-rag",
      "title": "Build a custom RAG agent with LangGraph - Docs by LangChain",
      "heading": "Build a custom RAG agent with LangGraph"
    }
  },
  {
    "page_content": "Build a custom SQL agent - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangGraph\nBuild a custom SQL agent\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nMulti-agent\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nConcepts\nSetup\nInstallation\nLangSmith\n1. Select an LLM\n2. Configure the database\n3. Add tools for database interactions\n4. Define application steps\n5. Implement the agent\n6. Implement human-in-the-loop review\nNext steps\nTutorials\nLangGraph\nBuild a custom SQL agent\nCopy page\nCopy page\nIn this tutorial we will build a custom agent that can answer questions about a SQL database using LangGraph.\nLangChain offers built-in\nagent\nimplementations, implemented using\nLangGraph\nprimitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a SQL agent. You can find a tutorial building a SQL agent using higher-level LangChain abstractions\nhere\n.\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent\u2019s needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\nThe\nprebuilt agent\nlets us get started quickly, but we relied on the system prompt to constrain its behavior\u2014 for example, we instructed the agent to always start with the \u201clist tables\u201d tool, and to always run a query-checker tool before executing the query.\nWe can enforce a higher degree of control in LangGraph by customizing the agent. Here, we implement a simple ReAct-agent setup, with dedicated nodes for specific tool-calls. We will use the same [state] as the pre-built agent.\n\u200b\nConcepts\nWe will cover the following concepts:\nTools\nfor reading from SQL databases\nThe LangGraph\nGraph API\n, including state, nodes, edges, and conditional edges.\nHuman-in-the-loop\nprocesses\n\u200b\nSetup\n\u200b\nInstallation\npip\nCopy\npip\ninstall\nlangchain\nlanggraph\nlangchain-community\n\u200b\nLangSmith\nSet up\nLangSmith\nto inspect what is happening inside your chain or agent. Then set the following environment variables:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\n\u200b\n1. Select an LLM\nSelect a model that supports\ntool-calling\n:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nThe output shown in the examples below used OpenAI.\n\u200b\n2. Configure the database\nYou will be creating a\nSQLite database\nfor this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the\nchinook\ndatabase, which is a sample database that represents a digital media store.\nFor convenience, we have hosted the database (\nChinook.db\n) on a public GCS bucket.\nCopy\nimport\nrequests, pathlib\nurl\n=\n\"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\nlocal_path\n=\npathlib.Path(\n\"Chinook.db\"\n)\nif\nlocal_path.exists():\nprint\n(\nf\n\"\n{\nlocal_path\n}\nalready exists, skipping download.\"\n)\nelse\n:\nresponse\n=\nrequests.get(url)\nif\nresponse.status_code\n==\n200\n:\nlocal_path.write_bytes(response.content)\nprint\n(\nf\n\"File downloaded and saved as\n{\nlocal_path\n}\n\"\n)\nelse\n:\nprint\n(\nf\n\"Failed to download the file. Status code:\n{\nresponse.status_code\n}\n\"\n)\nWe will use a handy SQL database wrapper available in the\nlangchain_community\npackage to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\nCopy\nfrom\nlangchain_community.utilities\nimport\nSQLDatabase\ndb\n=\nSQLDatabase.from_uri(\n\"sqlite:///Chinook.db\"\n)\nprint\n(\nf\n\"Dialect:\n{\ndb.dialect\n}\n\"\n)\nprint\n(\nf\n\"Available tables:\n{\ndb.get_usable_table_names()\n}\n\"\n)\nprint\n(\nf\n'Sample output:\n{\ndb.run(\n\"SELECT * FROM Artist LIMIT 5;\"\n)\n}\n'\n)\nCopy\nDialect: sqlite\nAvailable tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\nSample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\n\u200b\n3. Add tools for database interactions\nUse the\nSQLDatabase\nwrapper available in the\nlangchain_community\npackage to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\nCopy\nfrom\nlangchain_community.agent_toolkits\nimport\nSQLDatabaseToolkit\ntoolkit\n=\nSQLDatabaseToolkit(\ndb\n=\ndb,\nllm\n=\nmodel)\ntools\n=\ntoolkit.get_tools()\nfor\ntool\nin\ntools:\nprint\n(\nf\n\"\n{\ntool.name\n}\n:\n{\ntool.description\n}\n\\n\n\"\n)\nCopy\nsql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n\u200b\n4. Define application steps\nWe construct dedicated nodes for the following steps:\nListing DB tables\nCalling the \u201cget schema\u201d tool\nGenerating a query\nChecking the query\nPutting these steps in dedicated nodes lets us (1) force tool-calls when needed, and (2) customize the prompts associated with each step.\nCopy\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain.messages\nimport\nAIMessage\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlanggraph.graph\nimport\nEND\n,\nSTART\n, MessagesState, StateGraph\nfrom\nlanggraph.prebuilt\nimport\nToolNode\nget_schema_tool\n=\nnext\n(tool\nfor\ntool\nin\ntools\nif\ntool.name\n==\n\"sql_db_schema\"\n)\nget_schema_node\n=\nToolNode([get_schema_tool],\nname\n=\n\"get_schema\"\n)\nrun_query_tool\n=\nnext\n(tool\nfor\ntool\nin\ntools\nif\ntool.name\n==\n\"sql_db_query\"\n)\nrun_query_node\n=\nToolNode([run_query_tool],\nname\n=\n\"run_query\"\n)\n# Example: create a predetermined tool call\ndef\nlist_tables\n(\nstate\n: MessagesState):\ntool_call\n=\n{\n\"name\"\n:\n\"sql_db_list_tables\"\n,\n\"args\"\n: {},\n\"id\"\n:\n\"abc123\"\n,\n\"type\"\n:\n\"tool_call\"\n,\n}\ntool_call_message\n=\nAIMessage(\ncontent\n=\n\"\"\n,\ntool_calls\n=\n[tool_call])\nlist_tables_tool\n=\nnext\n(tool\nfor\ntool\nin\ntools\nif\ntool.name\n==\n\"sql_db_list_tables\"\n)\ntool_message\n=\nlist_tables_tool.invoke(tool_call)\nresponse\n=\nAIMessage(\nf\n\"Available tables:\n{\ntool_message.content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [tool_call_message, tool_message, response]}\n# Example: force a model to create a tool call\ndef\ncall_get_schema\n(\nstate\n: MessagesState):\n# Note that LangChain enforces that all models accept `tool_choice=\"any\"`\n# as well as `tool_choice=<string name of tool>`.\nllm_with_tools\n=\nmodel.bind_tools([get_schema_tool],\ntool_choice\n=\n\"any\"\n)\nresponse\n=\nllm_with_tools.invoke(state[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: [response]}\ngenerate_query_system_prompt\n=\n\"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct\n{dialect}\nquery to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most\n{top_k}\nresults.\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n\"\"\"\n.format(\ndialect\n=\ndb.dialect,\ntop_k\n=\n5\n,\n)\ndef\ngenerate_query\n(\nstate\n: MessagesState):\nsystem_message\n=\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n: generate_query_system_prompt,\n}\n# We do not force a tool call here, to allow the model to\n# respond naturally when it obtains the solution.\nllm_with_tools\n=\nmodel.bind_tools([run_query_tool])\nresponse\n=\nllm_with_tools.invoke([system_message]\n+\nstate[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: [response]}\ncheck_query_system_prompt\n=\n\"\"\"\nYou are a SQL expert with a strong attention to detail.\nDouble check the\n{dialect}\nquery for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes,\njust reproduce the original query.\nYou will call the appropriate tool to execute the query after running this check.\n\"\"\"\n.format(\ndialect\n=\ndb.dialect)\ndef\ncheck_query\n(\nstate\n: MessagesState):\nsystem_message\n=\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n: check_query_system_prompt,\n}\n# Generate an artificial user message to check\ntool_call\n=\nstate[\n\"messages\"\n][\n-\n1\n].tool_calls[\n0\n]\nuser_message\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: tool_call[\n\"args\"\n][\n\"query\"\n]}\nllm_with_tools\n=\nmodel.bind_tools([run_query_tool],\ntool_choice\n=\n\"any\"\n)\nresponse\n=\nllm_with_tools.invoke([system_message, user_message])\nresponse.id\n=\nstate[\n\"messages\"\n][\n-\n1\n].id\nreturn\n{\n\"messages\"\n: [response]}\n\u200b\n5. Implement the agent\nWe can now assemble these steps into a workflow using the\nGraph API\n. We define a\nconditional edge\nat the query generation step that will route to the query checker if a query is generated, or end if there are no tool calls present, such that the LLM has delivered a response to the query.\nCopy\ndef\nshould_continue\n(\nstate\n: MessagesState) -> Literal[\nEND\n,\n\"check_query\"\n]:\nmessages\n=\nstate[\n\"messages\"\n]\nlast_message\n=\nmessages[\n-\n1\n]\nif\nnot\nlast_message.tool_calls:\nreturn\nEND\nelse\n:\nreturn\n\"check_query\"\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(list_tables)\nbuilder.add_node(call_get_schema)\nbuilder.add_node(get_schema_node,\n\"get_schema\"\n)\nbuilder.add_node(generate_query)\nbuilder.add_node(check_query)\nbuilder.add_node(run_query_node,\n\"run_query\"\n)\nbuilder.add_edge(\nSTART\n,\n\"list_tables\"\n)\nbuilder.add_edge(\n\"list_tables\"\n,\n\"call_get_schema\"\n)\nbuilder.add_edge(\n\"call_get_schema\"\n,\n\"get_schema\"\n)\nbuilder.add_edge(\n\"get_schema\"\n,\n\"generate_query\"\n)\nbuilder.add_conditional_edges(\n\"generate_query\"\n,\nshould_continue,\n)\nbuilder.add_edge(\n\"check_query\"\n,\n\"run_query\"\n)\nbuilder.add_edge(\n\"run_query\"\n,\n\"generate_query\"\n)\nagent\n=\nbuilder.compile()\nWe visualize the application below:\nCopy\nfrom\nIPython.display\nimport\nImage, display\nfrom\nlangchain_core.runnables.graph\nimport\nCurveStyle, MermaidDrawMethod, NodeStyles\ndisplay(Image(agent.get_graph().draw_mermaid_png()))\nWe can now invoke the graph:\nCopy\nquestion\n=\n\"Which genre on average has the longest tracks?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: question}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\nAvailable tables: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\nsql_db_schema (call_yzje0tj7JK3TEzDx4QnRR3lL)\nCall ID: call_yzje0tj7JK3TEzDx4QnRR3lL\nArgs:\ntable_names: Genre, Track\n================================= Tool Message =================================\nName: sql_db_schema\nCREATE TABLE \"Genre\" (\n\"GenreId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(120),\nPRIMARY KEY (\"GenreId\")\n)\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\nCREATE TABLE \"Track\" (\n\"TrackId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(200) NOT NULL,\n\"AlbumId\" INTEGER,\n\"MediaTypeId\" INTEGER NOT NULL,\n\"GenreId\" INTEGER,\n\"Composer\" NVARCHAR(220),\n\"Milliseconds\" INTEGER NOT NULL,\n\"Bytes\" INTEGER,\n\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\nPRIMARY KEY (\"TrackId\"),\nFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\nFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\nFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\nsql_db_query (call_cb9ApLfZLSq7CWg6jd0im90b)\nCall ID: call_cb9ApLfZLSq7CWg6jd0im90b\nArgs:\nquery: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;\n================================== Ai Message ==================================\nTool Calls:\nsql_db_query (call_DMVALfnQ4kJsuF3Yl6jxbeAU)\nCall ID: call_DMVALfnQ4kJsuF3Yl6jxbeAU\nArgs:\nquery: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\nThe genre with the longest tracks on average is \"Sci Fi & Fantasy,\" with an average track length of approximately 2,911,783 milliseconds. Other genres with relatively long tracks include \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\nSee\nLangSmith trace\nfor the above run.\n\u200b\n6. Implement human-in-the-loop review\nIt can be prudent to check the agent\u2019s SQL queries before they are executed for any unintended actions or inefficiencies.\nHere we leverage LangGraph\u2019s\nhuman-in-the-loop\nfeatures to pause the run before executing a SQL query and wait for human review. Using LangGraph\u2019s\npersistence layer\n, we can pause the run indefinitely (or at least as long as the persistence layer is alive).\nLet\u2019s wrap the\nsql_db_query\ntool in a node that receives human input. We can implement this using the\ninterrupt\nfunction. Below, we allow for input to approve the tool call, edit its arguments, or provide user feedback.\nCopy\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlanggraph.types\nimport\ninterrupt\n@tool\n(\nrun_query_tool.name,\ndescription\n=\nrun_query_tool.description,\nargs_schema\n=\nrun_query_tool.args_schema\n)\ndef\nrun_query_tool_with_interrupt\n(\nconfig\n: RunnableConfig,\n**\ntool_input\n):\nrequest\n=\n{\n\"action\"\n: run_query_tool.name,\n\"args\"\n: tool_input,\n\"description\"\n:\n\"Please review the tool call\"\n}\nresponse\n=\ninterrupt([request])\n# approve the tool call\nif\nresponse[\n\"type\"\n]\n==\n\"accept\"\n:\ntool_response\n=\nrun_query_tool.invoke(tool_input, config)\n# update tool call args\nelif\nresponse[\n\"type\"\n]\n==\n\"edit\"\n:\ntool_input\n=\nresponse[\n\"args\"\n][\n\"args\"\n]\ntool_response\n=\nrun_query_tool.invoke(tool_input, config)\n# respond to the LLM with user feedback\nelif\nresponse[\n\"type\"\n]\n==\n\"response\"\n:\nuser_feedback\n=\nresponse[\n\"args\"\n]\ntool_response\n=\nuser_feedback\nelse\n:\nraise\nValueError\n(\nf\n\"Unsupported interrupt response type:\n{\nresponse[\n'type'\n]\n}\n\"\n)\nreturn\ntool_response\n# Redefine the tool node to use the interrupt version\nrun_query_node\n=\nToolNode([run_query_tool_with_interrupt],\nname\n=\n\"run_query\"\n)\nThe above implementation follows the\ntool interrupt example\nin the broader\nhuman-in-the-loop\nguide. Refer to that guide for details and alternatives.\nLet\u2019s now re-assemble our graph. We will replace the programmatic check with human review. Note that we now include a\ncheckpointer\n; this is required to pause and resume the run.\nCopy\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\ndef\nshould_continue\n(\nstate\n: MessagesState) -> Literal[\nEND\n,\n\"run_query\"\n]:\nmessages\n=\nstate[\n\"messages\"\n]\nlast_message\n=\nmessages[\n-\n1\n]\nif\nnot\nlast_message.tool_calls:\nreturn\nEND\nelse\n:\nreturn\n\"run_query\"\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(list_tables)\nbuilder.add_node(call_get_schema)\nbuilder.add_node(get_schema_node,\n\"get_schema\"\n)\nbuilder.add_node(generate_query)\nbuilder.add_node(run_query_node,\n\"run_query\"\n)\nbuilder.add_edge(\nSTART\n,\n\"list_tables\"\n)\nbuilder.add_edge(\n\"list_tables\"\n,\n\"call_get_schema\"\n)\nbuilder.add_edge(\n\"call_get_schema\"\n,\n\"get_schema\"\n)\nbuilder.add_edge(\n\"get_schema\"\n,\n\"generate_query\"\n)\nbuilder.add_conditional_edges(\n\"generate_query\"\n,\nshould_continue,\n)\nbuilder.add_edge(\n\"run_query\"\n,\n\"generate_query\"\n)\ncheckpointer\n=\nInMemorySaver()\nagent\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nWe can invoke the graph as before. This time, execution is interrupted:\nCopy\nimport\njson\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\nquestion\n=\n\"Which genre on average has the longest tracks?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: question}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nif\n\"messages\"\nin\nstep:\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nelif\n\"__interrupt__\"\nin\nstep:\naction\n=\nstep[\n\"__interrupt__\"\n][\n0\n]\nprint\n(\n\"INTERRUPTED:\"\n)\nfor\nrequest\nin\naction.value:\nprint\n(json.dumps(request,\nindent\n=\n2\n))\nelse\n:\npass\nCopy\n...\nINTERRUPTED:\n{\n\"action\": \"sql_db_query\",\n\"args\": {\n\"query\": \"SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;\"\n},\n\"description\": \"Please review the tool call\"\n}\nWe can accept or edit the tool call using\nCommand\n:\nCopy\nfrom\nlanggraph.types\nimport\nCommand\nfor\nstep\nin\nagent.stream(\nCommand(\nresume\n=\n{\n\"type\"\n:\n\"accept\"\n}),\n# Command(resume={\"type\": \"edit\", \"args\": {\"query\": \"...\"}}),\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nif\n\"messages\"\nin\nstep:\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nelif\n\"__interrupt__\"\nin\nstep:\naction\n=\nstep[\n\"__interrupt__\"\n][\n0\n]\nprint\n(\n\"INTERRUPTED:\"\n)\nfor\nrequest\nin\naction.value:\nprint\n(json.dumps(request,\nindent\n=\n2\n))\nelse\n:\npass\nCopy\n================================== Ai Message ==================================\nTool Calls:\nsql_db_query (call_t4yXkD6shwdTPuelXEmY3sAY)\nCall ID: call_t4yXkD6shwdTPuelXEmY3sAY\nArgs:\nquery: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average length of about 2,911,783 milliseconds. Other genres with long average track lengths include \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\nRefer to the\nhuman-in-the-loop guide\nfor details.\n\u200b\nNext steps\nCheck out the\nEvaluate a graph\nguide for evaluating LangGraph applications, including SQL agents like this one, using LangSmith.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a custom RAG agent with LangGraph\nPrevious\nComponent architecture\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/sql-agent",
      "title": "Build a custom SQL agent - Docs by LangChain",
      "heading": "Build a custom SQL agent"
    }
  },
  {
    "page_content": "Build a semantic search engine with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain\nBuild a semantic search engine with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nSetup\nInstallation\nLangSmith\n1. Documents and Document Loaders\nLoading documents\nSplitting\n2. Embeddings\n3. Vector stores\n4. Retrievers\nNext steps\nTutorials\nLangChain\nBuild a semantic search engine with LangChain\nCopy page\nCopy page\n\u200b\nOverview\nThis tutorial will familiarize you with LangChain\u2019s\ndocument loader\n,\nembedding\n, and\nvector store\nabstractions. These abstractions are designed to support retrieval of data\u2014  from (vector) databases and other sources \u2014 for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or\nRAG\n.\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\n\u200b\nConcepts\nThis guide focuses on retrieval of text data. We will cover the following concepts:\nDocuments and document loaders\n;\nText splitters\n;\nEmbeddings\n;\nVector stores\nand\nretrievers\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires the\nlangchain-community\nand\npypdf\npackages:\npip\nconda\nCopy\npip\ninstall\nlangchain-community\npypdf\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, if in a notebook, you can set them with:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\n1. Documents and Document Loaders\nLangChain implements a\nDocument\nabstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\npage_content\n: a string representing the content;\nmetadata\n: a dict containing arbitrary metadata;\nid\n: (optional) a string identifier for the document.\nThe\nmetadata\nattribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual\nDocument\nobject often represents a chunk of a larger document.\nWe can generate sample documents when desired:\nCopy\nfrom\nlangchain_core.documents\nimport\nDocument\ndocuments\n=\n[\nDocument(\npage_content\n=\n\"Dogs are great companions, known for their loyalty and friendliness.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"mammal-pets-doc\"\n},\n),\nDocument(\npage_content\n=\n\"Cats are independent pets that often enjoy their own space.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"mammal-pets-doc\"\n},\n),\n]\nHowever, the LangChain ecosystem implements\ndocument loaders\nthat\nintegrate with hundreds of common sources\n. This makes it easy to incorporate data from these sources into your AI application.\n\u200b\nLoading documents\nLet\u2019s load a PDF into a sequence of\nDocument\nobjects.\nHere is a sample PDF\n\u2014 a 10-k filing for Nike from 2023. We can consult the LangChain documentation for\navailable PDF document loaders\n.\nCopy\nfrom\nlangchain_community.document_loaders\nimport\nPyPDFLoader\nfile_path\n=\n\"../example_data/nke-10k-2023.pdf\"\nloader\n=\nPyPDFLoader(file_path)\ndocs\n=\nloader.load()\nprint\n(\nlen\n(docs))\nCopy\n107\nPyPDFLoader\nloads one\nDocument\nobject per PDF page. For each, we can easily access:\nThe string content of the page;\nMetadata containing the file name and page number.\nCopy\nprint\n(\nf\n\"\n{\ndocs[\n0\n].page_content[:\n200\n]\n}\n\\n\n\"\n)\nprint\n(docs[\n0\n].metadata)\nCopy\nTable of Contents\nUNITED\nSTATES\nSECURITIES\nAND\nEXCHANGE\nCOMMISSION\nWashington, D.C. 20549\nFORM\n10\n-\nK\n(Mark One)\n\u2611\nANNUAL\nREPORT\nPURSUANT\nTO\nSECTION\n13\nOR\n15\n(D)\nOF\nTHE\nSECURITIES\nEXCHANGE\nACT\nOF\n1934\nFO\n{\n'source'\n:\n'../example_data/nke-10k-2023.pdf'\n,\n'page'\n:\n0\n}\n\u200b\nSplitting\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve\nDocument\nobjects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \u201cwashed out\u201d by surrounding text.\nWe can use\ntext splitters\nfor this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\nwith 200 characters of overlap between chunks. The overlap helps\nmitigate the possibility of separating a statement from important\ncontext related to it. We use the\nRecursiveCharacterTextSplitter\n,\nwhich will recursively split the document using common separators like\nnew lines until each chunk is the appropriate size. This is the\nrecommended text splitter for generic text use cases.\nWe set\nadd_start_index=True\nso that the character index where each\nsplit Document starts within the initial Document is preserved as\nmetadata attribute \u201cstart_index\u201d.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n,\nadd_start_index\n=\nTrue\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nlen\n(all_splits))\nCopy\n514\n\u200b\n2. Embeddings\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can\nembed\nit as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\nLangChain supports embeddings from\ndozens of providers\n. These models specify how text should be converted into a numeric vector. Let\u2019s select a model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nCopy\nvector_1\n=\nembeddings.embed_query(all_splits[\n0\n].page_content)\nvector_2\n=\nembeddings.embed_query(all_splits[\n1\n].page_content)\nassert\nlen\n(vector_1)\n==\nlen\n(vector_2)\nprint\n(\nf\n\"Generated vectors of length\n{\nlen\n(vector_1)\n}\n\\n\n\"\n)\nprint\n(vector_1[:\n10\n])\nCopy\nGenerated vectors of length 1536\n[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\n\u200b\n3. Vector stores\nLangChain\nVectorStore\nobjects contain methods for adding text and\nDocument\nobjects to the store, and querying them using various similarity metrics. They are often initialized with\nembedding\nmodels, which determine how text data is translated to numeric vectors.\nLangChain includes a suite of\nintegrations\nwith different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as\nPostgres\n) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\u2019s select a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\nHaving instantiated our vector store, we can now index the documents.\nCopy\nids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nNote that most vector store implementations will allow you to connect to an existing vector store\u2014  e.g., by providing a client, index name, or other information. See the documentation for a specific\nintegration\nfor more detail.\nOnce we\u2019ve instantiated a\nVectorStore\nthat contains documents, we can query it.\nVectorStore\nincludes methods for querying:\nSynchronously and asynchronously;\nBy string query and by vector;\nWith and without returning similarity scores;\nBy similarity and\nmaximum marginal relevance\n(to balance similarity with query to diversity in retrieved results).\nThe methods will generally include a list of\nDocument\nobjects in their outputs.\nUsage\nEmbeddings typically represent text as a \u201cdense\u201d vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\nReturn documents based on similarity to a string query:\nCopy\nresults\n=\nvector_store.similarity_search(\n\"How many distribution centers does Nike have in the US?\"\n)\nprint\n(results[\n0\n])\nCopy\npage_content\n=\n'direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S.\nRETAIL\nSTORES\nNUMBER\nNIKE\nBrand factory stores\n213\nNIKE\nBrand\nin\n-\nline stores (including employee\n-\nonly stores)\n74\nConverse stores (including factory stores)\n82\nTOTAL\n369\nIn the United States,\nNIKE\nhas eight significant distribution centers. Refer to Item\n2\n. Properties\nfor\nfurther information.\n2023\nFORM\n10\n-\nK\n2\n' metadata={'\npage\n': 4, '\nsource\n': '\n..\n/\nexample_data\n/\nnke\n-\n10k\n-\n2023.\npdf\n', '\nstart_index\n': 3125}\nAsync query:\nCopy\nresults\n=\nawait\nvector_store.asimilarity_search(\n\"When was Nike incorporated?\"\n)\nprint\n(results[\n0\n])\nCopy\npage_content\n=\n'Table of Contents\nPART\nI\nITEM\n1\n.\nBUSINESS\nGENERAL\nNIKE\n, Inc. was incorporated\nin\n1967\nunder the laws of the State of Oregon. As used\nin\nthis Annual Report on Form\n10\n-\nK (this\n\"Annual Report\"\n), the terms\n\"we,\"\n\"us,\"\n\"our,\"\n\"NIKE\"\nand\nthe\n\"Company\"\nrefer to\nNIKE\n, Inc.\nand\nits predecessors, subsidiaries\nand\naffiliates, collectively, unless the context indicates otherwise.\nOur principal business activity\nis\nthe design, development\nand\nworldwide marketing\nand\nselling of athletic footwear, apparel, equipment, accessories\nand\nservices.\nNIKE\nis\nthe largest seller of athletic footwear\nand\napparel\nin\nthe world. We sell our products through\nNIKE\nDirect operations, which are comprised of both\nNIKE\n-\nowned retail stores\nand\nsales through our digital platforms (also referred to\nas\n\"NIKE Brand Digital\"\n), to retail accounts\nand\nto a mix of independent distributors, licensees\nand\nsales\n' metadata={'\npage\n': 3, '\nsource\n': '\n..\n/\nexample_data\n/\nnke\n-\n10k\n-\n2023.\npdf\n', '\nstart_index\n': 0}\nReturn scores:\nCopy\n# Note that providers implement different scores; the score here\n# is a distance metric that varies inversely with similarity.\nresults\n=\nvector_store.similarity_search_with_score(\n\"What was Nike's revenue in 2023?\"\n)\ndoc, score\n=\nresults[\n0\n]\nprint\n(\nf\n\"Score:\n{\nscore\n}\n\\n\n\"\n)\nprint\n(doc)\nCopy\nScore:\n0.23699893057346344\npage_content\n=\n'Table of Contents\nFISCAL\n2023\nNIKE\nBRAND\nREVENUE\nHIGHLIGHTS\nThe following tables present\nNIKE\nBrand revenues disaggregated by reportable operating segment, distribution channel\nand\nmajor product line:\nFISCAL\n2023\nCOMPARED\nTO\nFISCAL\n2022\n\u2022\nNIKE\n, Inc. Revenues were\n$\n51.2\nbillion\nin\nfiscal\n2023\n, which increased\n10\n%\nand\n16\n%\ncompared to fiscal\n2022\non a reported\nand\ncurrency\n-\nneutral basis, respectively.\nThe increase was due to higher revenues\nin\nNorth America, Europe, Middle East\n&\nAfrica (\n\"EMEA\"\n),\nAPLA\nand\nGreater China, which contributed approximately\n7\n,\n6\n,\n2\nand\n1\npercentage points to\nNIKE\n, Inc. Revenues, respectively.\n\u2022\nNIKE\nBrand revenues, which represented over\n90\n%\nof\nNIKE\n, Inc. Revenues, increased\n10\n%\nand\n16\n%\non a reported\nand\ncurrency\n-\nneutral basis, respectively. This\nincrease was primarily due to higher revenues\nin\nMen\n's, the Jordan Brand, Women'\ns\nand\nKids\n' which grew 17%, 35%,11\n% a\nnd 10%, respectively, on a wholesale\nequivalent basis.\n' metadata={'\npage\n': 35, '\nsource\n': '\n..\n/\nexample_data\n/\nnke\n-\n10k\n-\n2023.\npdf\n', '\nstart_index\n': 0}\nReturn documents based on similarity to an embedded query:\nCopy\nembedding\n=\nembeddings.embed_query(\n\"How were Nike's margins impacted in 2023?\"\n)\nresults\n=\nvector_store.similarity_search_by_vector(embedding)\nprint\n(results[\n0\n])\nCopy\npage_content\n=\n'Table of Contents\nGROSS\nMARGIN\nFISCAL\n2023\nCOMPARED\nTO\nFISCAL\n2022\nFor fiscal\n2023\n, our consolidated gross profit increased\n4\n%\nto\n$\n22\n,\n292\nmillion compared to\n$\n21\n,\n479\nmillion\nfor\nfiscal\n2022\n. Gross margin decreased\n250\nbasis points to\n43.5\n%\nfor\nfiscal\n2023\ncompared to\n46.0\n%\nfor\nfiscal\n2022\ndue to the following:\n*\nWholesale equivalent\nThe decrease\nin\ngross margin\nfor\nfiscal\n2023\nwas primarily due to:\n\u2022Higher\nNIKE\nBrand product costs, on a wholesale equivalent basis, primarily due to higher\ninput\ncosts\nand\nelevated inbound freight\nand\nlogistics costs\nas\nwell\nas\nproduct mix;\n\u2022Lower margin\nin\nour\nNIKE\nDirect business, driven by higher promotional activity to liquidate inventory\nin\nthe current period compared to lower promotional activity\nin\nthe prior period resulting\nfrom\nlower available inventory supply\n;\n\u2022Unfavorable changes\nin\nnet foreign currency exchange rates, including hedges;\nand\n\u2022Lower off\n-\nprice margin, on a wholesale equivalent basis.\nThis was partially offset by:\n' metadata={'\npage\n': 36, '\nsource\n': '\n..\n/\nexample_data\n/\nnke\n-\n10k\n-\n2023.\npdf\n', '\nstart_index\n': 0}\nLearn more:\nAPI Reference\nIntegration-specific docs\n\u200b\n4. Retrievers\nLangChain\nVectorStore\nobjects do not subclass\nRunnable\n. LangChain\nRetrievers\nare Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous\ninvoke\nand\nbatch\noperations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\nWe can create a simple version of this ourselves, without subclassing\nRetriever\n. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the\nsimilarity_search\nmethod:\nCopy\nfrom\ntyping\nimport\nList\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain_core.runnables\nimport\nchain\n@chain\ndef\nretriever\n(\nquery\n:\nstr\n) -> List[Document]:\nreturn\nvector_store.similarity_search(query,\nk\n=\n1\n)\nretriever.batch(\n[\n\"How many distribution centers does Nike have in the US?\"\n,\n\"When was Nike incorporated?\"\n,\n],\n)\nCopy\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n[Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\nVectorstores implement an\nas_retriever\nmethod that will generate a Retriever, specifically a\nVectorStoreRetriever\n. These retrievers include specific\nsearch_type\nand\nsearch_kwargs\nattributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\nCopy\nretriever\n=\nvector_store.as_retriever(\nsearch_type\n=\n\"similarity\"\n,\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n},\n)\nretriever.batch(\n[\n\"How many distribution centers does Nike have in the US?\"\n,\n\"When was Nike incorporated?\"\n,\n],\n)\nCopy\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n[Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\nVectorStoreRetriever\nsupports search types of\n\"similarity\"\n(default),\n\"mmr\"\n(maximum marginal relevance, described above), and\n\"similarity_score_threshold\"\n. We can use the latter to threshold documents output by the retriever by similarity score.\nRetrievers can easily be incorporated into more complex applications, such as\nretrieval-augmented generation (RAG)\napplications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the\nRAG tutorial\ntutorial.\n\u200b\nNext steps\nYou\u2019ve now seen how to build a semantic search engine over a PDF document.\nFor more on document loaders:\nOverview\nAvailable integrations\nFor more on embeddings:\nOverview\nAvailable integrations\nFor more on vector stores:\nOverview\nAvailable integrations\nFor more on RAG, see:\nBuild a Retrieval Augmented Generation (RAG) App\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nLearn\nPrevious\nBuild a RAG agent with LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/knowledge-base",
      "title": "Build a semantic search engine with LangChain - Docs by LangChain",
      "heading": "Build a semantic search engine with LangChain"
    }
  },
  {
    "page_content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\nTutorials\nLangChain\nBuild a RAG agent with LangChain\nCopy page\nCopy page\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/rag",
      "title": "Build a RAG agent with LangChain - Docs by LangChain",
      "heading": "Build a RAG agent with LangChain"
    }
  },
  {
    "page_content": "Build a SQL agent - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain\nBuild a SQL agent\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nSetup\nInstallation\nLangSmith\n1. Select an LLM\n2. Configure the database\n3. Add tools for database interactions\n4. Use create_agent\n5. Run the agent\n(Optional) Use Studio\n6. Implement human-in-the-loop review\nNext steps\nTutorials\nLangChain\nBuild a SQL agent\nCopy page\nCopy page\n\u200b\nOverview\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain\nagents\n.\nAt a high level, the agent will:\n1\nFetch the available tables and schemas from the database\n2\nDecide which tables are relevant to the question\n3\nFetch the schemas for the relevant tables\n4\nGenerate a query based on the question and information from the schemas\n5\nDouble-check the query for common mistakes using an LLM\n6\nExecute the query and return the results\n7\nCorrect mistakes surfaced by the database engine until the query is successful\n8\nFormulate a response based on the results\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent\u2019s needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\n\u200b\nConcepts\nWe will cover the following concepts:\nTools\nfor reading from SQL databases\nLangChain\nagents\nHuman-in-the-loop\nprocesses\n\u200b\nSetup\n\u200b\nInstallation\npip\nCopy\npip\ninstall\nlangchain\nlanggraph\nlangchain-community\n\u200b\nLangSmith\nSet up\nLangSmith\nto inspect what is happening inside your chain or agent. Then set the following environment variables:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\n\u200b\n1. Select an LLM\nSelect a model that supports\ntool-calling\n:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nThe output shown in the examples below used OpenAI.\n\u200b\n2. Configure the database\nYou will be creating a\nSQLite database\nfor this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the\nchinook\ndatabase, which is a sample database that represents a digital media store.\nFor convenience, we have hosted the database (\nChinook.db\n) on a public GCS bucket.\nCopy\nimport\nrequests, pathlib\nurl\n=\n\"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\nlocal_path\n=\npathlib.Path(\n\"Chinook.db\"\n)\nif\nlocal_path.exists():\nprint\n(\nf\n\"\n{\nlocal_path\n}\nalready exists, skipping download.\"\n)\nelse\n:\nresponse\n=\nrequests.get(url)\nif\nresponse.status_code\n==\n200\n:\nlocal_path.write_bytes(response.content)\nprint\n(\nf\n\"File downloaded and saved as\n{\nlocal_path\n}\n\"\n)\nelse\n:\nprint\n(\nf\n\"Failed to download the file. Status code:\n{\nresponse.status_code\n}\n\"\n)\nWe will use a handy SQL database wrapper available in the\nlangchain_community\npackage to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\nCopy\nfrom\nlangchain_community.utilities\nimport\nSQLDatabase\ndb\n=\nSQLDatabase.from_uri(\n\"sqlite:///Chinook.db\"\n)\nprint\n(\nf\n\"Dialect:\n{\ndb.dialect\n}\n\"\n)\nprint\n(\nf\n\"Available tables:\n{\ndb.get_usable_table_names()\n}\n\"\n)\nprint\n(\nf\n'Sample output:\n{\ndb.run(\n\"SELECT * FROM Artist LIMIT 5;\"\n)\n}\n'\n)\nCopy\nDialect: sqlite\nAvailable tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\nSample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\n\u200b\n3. Add tools for database interactions\nUse the\nSQLDatabase\nwrapper available in the\nlangchain_community\npackage to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\nCopy\nfrom\nlangchain_community.agent_toolkits\nimport\nSQLDatabaseToolkit\ntoolkit\n=\nSQLDatabaseToolkit(\ndb\n=\ndb,\nllm\n=\nmodel)\ntools\n=\ntoolkit.get_tools()\nfor\ntool\nin\ntools:\nprint\n(\nf\n\"\n{\ntool.name\n}\n:\n{\ntool.description\n}\n\\n\n\"\n)\nCopy\nsql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n\u200b\n4. Use\ncreate_agent\nUse\ncreate_agent\nto build a\nReAct agent\nwith minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\nInitialize the agent with a descriptive system prompt to customize its behavior:\nCopy\nsystem_prompt\n=\n\"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct\n{dialect}\nquery to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most\n{top_k}\nresults.\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\nThen you should query the schema of the most relevant tables.\n\"\"\"\n.format(\ndialect\n=\ndb.dialect,\ntop_k\n=\n5\n,\n)\nNow, create an agent with the model, tools, and prompt:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nagent\n=\ncreate_agent(\nmodel,\ntools,\nsystem_prompt\n=\nsystem_prompt,\n)\n\u200b\n5. Run the agent\nRun the agent on a sample query and observe its behavior:\nCopy\nquestion\n=\n\"Which genre on average has the longest tracks?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: question}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\nTool Calls:\nsql_db_list_tables (call_BQsWg8P65apHc8BTJ1NPDvnM)\nCall ID: call_BQsWg8P65apHc8BTJ1NPDvnM\nArgs:\n================================= Tool Message =================================\nName: sql_db_list_tables\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\nsql_db_schema (call_i89tjKECFSeERbuACYm4w0cU)\nCall ID: call_i89tjKECFSeERbuACYm4w0cU\nArgs:\ntable_names: Track, Genre\n================================= Tool Message =================================\nName: sql_db_schema\nCREATE TABLE \"Genre\" (\n\"GenreId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(120),\nPRIMARY KEY (\"GenreId\")\n)\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\nCREATE TABLE \"Track\" (\n\"TrackId\" INTEGER NOT NULL,\n\"Name\" NVARCHAR(200) NOT NULL,\n\"AlbumId\" INTEGER,\n\"MediaTypeId\" INTEGER NOT NULL,\n\"GenreId\" INTEGER,\n\"Composer\" NVARCHAR(220),\n\"Milliseconds\" INTEGER NOT NULL,\n\"Bytes\" INTEGER,\n\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\nPRIMARY KEY (\"TrackId\"),\nFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\nFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\nFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\nsql_db_query_checker (call_G64yYm6R6UauiVPCXJZMA49b)\nCall ID: call_G64yYm6R6UauiVPCXJZMA49b\nArgs:\nquery: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query_checker\nSELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================== Ai Message ==================================\nTool Calls:\nsql_db_query (call_AnO3SrhD0ODJBxh6dHMwvHwZ)\nCall ID: call_AnO3SrhD0ODJBxh6dHMwvHwZ\nArgs:\nquery: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\nOn average, the genre with the longest tracks is \"Sci Fi & Fantasy\" with an average track length of approximately 2,911,783 milliseconds. This is followed by \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\nThe agent correctly wrote a query, checked the query, and ran it to inform its final response.\nYou can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the\nLangSmith trace\n.\n\u200b\n(Optional) Use Studio\nStudio\nprovides a \u201cclient side\u201d loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \u201cTell me the scheme of the database\u201d or \u201cShow me the invoices for the 5 top customers\u201d. You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\nRun your agent in Studio\nIn addition to the previously mentioned packages, you will need to:\nCopy\npip\ninstall\n-U\nlanggraph-cli[inmem\n]\n>\n=\n0.4.0\nIn directory you will run in, you will need a\nlanggraph.json\nfile with the following contents:\nCopy\n{\n\"dependencies\"\n: [\n\".\"\n],\n\"graphs\"\n: {\n\"agent\"\n:\n\"./sql_agent.py:agent\"\n,\n\"graph\"\n:\n\"./sql_agent_langgraph.py:graph\"\n},\n\"env\"\n:\n\".env\"\n}\nCreate a file\nsql_agent.py\nand insert this:\nCopy\n#sql_agent.py for studio\nimport\npathlib\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain_community.agent_toolkits\nimport\nSQLDatabaseToolkit\nfrom\nlangchain_community.utilities\nimport\nSQLDatabase\nimport\nrequests\n# Initialize an LLM\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n# Get the database, store it locally\nurl\n=\n\"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\nlocal_path\n=\npathlib.Path(\n\"Chinook.db\"\n)\nif\nlocal_path.exists():\nprint\n(\nf\n\"\n{\nlocal_path\n}\nalready exists, skipping download.\"\n)\nelse\n:\nresponse\n=\nrequests.get(url)\nif\nresponse.status_code\n==\n200\n:\nlocal_path.write_bytes(response.content)\nprint\n(\nf\n\"File downloaded and saved as\n{\nlocal_path\n}\n\"\n)\nelse\n:\nprint\n(\nf\n\"Failed to download the file. Status code:\n{\nresponse.status_code\n}\n\"\n)\ndb\n=\nSQLDatabase.from_uri(\n\"sqlite:///Chinook.db\"\n)\n# Create the tools\ntoolkit\n=\nSQLDatabaseToolkit(\ndb\n=\ndb,\nllm\n=\nmodel)\ntools\n=\ntoolkit.get_tools()\nfor\ntool\nin\ntools:\nprint\n(\nf\n\"\n{\ntool.name\n}\n:\n{\ntool.description\n}\n\\n\n\"\n)\n# Use create_agent\nsystem_prompt\n=\n\"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct\n{dialect}\nquery to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most\n{top_k}\nresults.\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\nThen you should query the schema of the most relevant tables.\n\"\"\"\n.format(\ndialect\n=\ndb.dialect,\ntop_k\n=\n5\n,\n)\nagent\n=\ncreate_agent(\nmodel,\ntools,\nsystem_prompt\n=\nsystem_prompt,\n)\n\u200b\n6. Implement human-in-the-loop review\nIt can be prudent to check the agent\u2019s SQL queries before they are executed for any unintended actions or inefficiencies.\nLangChain agents feature support for built-in\nhuman-in-the-loop middleware\nto add oversight to agent tool calls. Let\u2019s configure the agent to pause for human review on calling the\nsql_db_query\ntool:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nHumanInTheLoopMiddleware\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nagent\n=\ncreate_agent(\nmodel,\ntools,\nsystem_prompt\n=\nsystem_prompt,\nmiddleware\n=\n[\nHumanInTheLoopMiddleware(\ninterrupt_on\n=\n{\n\"sql_db_query\"\n:\nTrue\n},\ndescription_prefix\n=\n\"Tool execution pending approval\"\n,\n),\n],\ncheckpointer\n=\nInMemorySaver(),\n)\nWe\u2019ve added a\ncheckpointer\nto our agent to allow execution to be paused and resumed. See the\nhuman-in-the-loop guide\nfor detalis on this as well as available middleware configurations.\nOn running the agent, it will now pause for review before executing the\nsql_db_query\ntool:\nCopy\nquestion\n=\n\"Which genre on average has the longest tracks?\"\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: question}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nif\n\"__interrupt__\"\nin\nstep:\nprint\n(\n\"INTERRUPTED:\"\n)\ninterrupt\n=\nstep[\n\"__interrupt__\"\n][\n0\n]\nfor\nrequest\nin\ninterrupt.value[\n\"action_requests\"\n]:\nprint\n(request[\n\"description\"\n])\nelif\n\"messages\"\nin\nstep:\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nelse\n:\npass\nCopy\n...\nINTERRUPTED:\nTool execution pending approval\nTool: sql_db_query\nArgs: {'query': 'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;'}\nWe can resume execution, in this case accepting the query, using\nCommand\n:\nCopy\nfrom\nlanggraph.types\nimport\nCommand\nfor\nstep\nin\nagent.stream(\nCommand(\nresume\n=\n{\n\"decisions\"\n: [{\n\"type\"\n:\n\"approve\"\n}]}),\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nif\n\"messages\"\nin\nstep:\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nelif\n\"__interrupt__\"\nin\nstep:\nprint\n(\n\"INTERRUPTED:\"\n)\ninterrupt\n=\nstep[\n\"__interrupt__\"\n][\n0\n]\nfor\nrequest\nin\ninterrupt.value[\n\"action_requests\"\n]:\nprint\n(request[\n\"description\"\n])\nelse\n:\npass\nCopy\n================================== Ai Message ==================================\nTool Calls:\nsql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)\nCall ID: call_7oz86Epg7lYRqi9rQHbZPS1U\nArgs:\nquery: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average duration of about 2,911,783 milliseconds, followed by \"Science Fiction\" and \"Drama.\"\nRefer to the\nhuman-in-the-loop guide\nfor details.\n\u200b\nNext steps\nFor deeper customization, check out\nthis tutorial\nfor implementing a SQL agent directly using LangGraph primitives.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a RAG agent with LangChain\nPrevious\nBuild a voice agent with LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/sql-agent",
      "title": "Build a SQL agent - Docs by LangChain",
      "heading": "Build a SQL agent"
    }
  },
  {
    "page_content": "Build a voice agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain\nBuild a voice agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nWhat are voice agents?\nHow do voice agents work?\n1. STT > Agent > TTS Architecture (The \u201cSandwich\u201d)\n2. Speech-to-Speech Architecture (S2S)\nDemo Application Overview\nArchitecture\nSetup\n1. Speech-to-text\nKey Concepts\nImplementation\n2. LangChain agent\nKey Concepts\nImplementation\n3. Text-to-speech\nKey Concepts\nImplementation\nPutting It All Together\nTutorials\nLangChain\nBuild a voice agent with LangChain\nCopy page\nCopy page\n\u200b\nOverview\nChat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\n\u200b\nWhat are voice agents?\nVoice agents are\nagents\nthat can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\nThey\u2019re suited for a variety of use cases, including:\nCustomer support\nPersonal assistants\nHands-free interfaces\nCoaching and training\n\u200b\nHow do voice agents work?\nAt a high level, every voice agent needs to handle three tasks:\nListen\n- capture audio and transcribe it\nThink\n- interpret intent, reason, plan\nSpeak\n- generate audio and stream it back to the user\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\n\u200b\n1. STT > Agent > TTS Architecture (The \u201cSandwich\u201d)\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\nPros:\nFull control over each component (swap STT/TTS providers as needed)\nAccess to latest capabilities from modern text-modality models\nTransparent behavior with clear boundaries between components\nCons:\nRequires orchestrating multiple services\nAdditional complexity in managing the pipeline\nConversion from speech to text loses information (e.g., tone, emotion)\n\u200b\n2. Speech-to-Speech Architecture (S2S)\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\nPros:\nSimpler architecture with fewer moving parts\nTypically lower latency for simple interactions\nDirect audio processing captures tone and other nuances of speech\nCons:\nLimited model options, greater risk of provider lock-in\nFeatures may lag behind text-modality models\nLess transparency in how audio is processed\nReduced controllability and customization options\nThis guide demonstrates the\nsandwich architecture\nto balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\n\u200b\nDemo Application Overview\nWe\u2019ll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using\nAssemblyAI\nfor STT and\nCartesia\nfor TTS (although adapters can be built for most providers).\nAn end-to-end reference application is available in the\nvoice-sandwich-demo\nrepository. We will walk through that application here.\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\n\u200b\nArchitecture\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\nClient (Browser)\nCaptures microphone audio and encodes it as PCM\nEstablishes WebSocket connection to the backend server\nStreams audio chunks to the server in real-time\nReceives and plays back synthesized speech audio\nServer (Python)\nAccepts WebSocket connections from clients\nOrchestrates the three-step pipeline:\nSpeech-to-text (STT)\n: Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\nAgent\n: Processes transcripts with LangChain agent, streams response tokens\nText-to-speech (TTS)\n: Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\nReturns synthesized audio to the client for playback\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\n\u200b\nSetup\nFor detailed installation instructions and setup, see the\nrepository README\n.\n\u200b\n1. Speech-to-text\nThe STT stage transforms an incoming audio stream into text transcripts. The implementation uses a producer-consumer pattern to handle audio streaming and transcript reception concurrently.\n\u200b\nKey Concepts\nProducer-Consumer Pattern\n: Audio chunks are sent to the STT service concurrently with receiving transcript events. This allows transcription to begin before all audio has arrived.\nEvent Types\n:\nstt_chunk\n: Partial transcripts provided as the STT service processes audio\nstt_output\n: Final, formatted transcripts that trigger agent processing\nWebSocket Connection\n: Maintains a persistent connection to AssemblyAI\u2019s real-time STT API, configured for 16kHz PCM audio with automatic turn formatting.\n\u200b\nImplementation\nCopy\nfrom\ntyping\nimport\nAsyncIterator\nimport\nasyncio\nfrom\nassemblyai_stt\nimport\nAssemblyAISTT\nfrom\nevents\nimport\nVoiceAgentEvent\nasync\ndef\nstt_stream\n(\naudio_stream\n: AsyncIterator[\nbytes\n],\n) -> AsyncIterator[VoiceAgentEvent]:\n\"\"\"\nTransform stream: Audio (Bytes) \u2192 Voice Events (VoiceAgentEvent)\nUses a producer-consumer pattern where:\n- Producer: Reads audio chunks and sends them to AssemblyAI\n- Consumer: Receives transcription events from AssemblyAI\n\"\"\"\nstt\n=\nAssemblyAISTT(\nsample_rate\n=\n16000\n)\nasync\ndef\nsend_audio\n():\n\"\"\"Background task that pumps audio chunks to AssemblyAI.\"\"\"\ntry\n:\nasync\nfor\naudio_chunk\nin\naudio_stream:\nawait\nstt.send_audio(audio_chunk)\nfinally\n:\n# Signal completion when audio stream ends\nawait\nstt.close()\n# Launch audio sending in background\nsend_task\n=\nasyncio.create_task(send_audio())\ntry\n:\n# Receive and yield transcription events as they arrive\nasync\nfor\nevent\nin\nstt.receive_events():\nyield\nevent\nfinally\n:\n# Cleanup\nwith\ncontextlib.suppress(asyncio.CancelledError):\nsend_task.cancel()\nawait\nsend_task\nawait\nstt.close()\nThe application implements an AssemblyAI client to manage the WebSocket connection and message parsing. See below for implementations; similar adapters can be constructed for other STT providers.\nAssemblyAI Client\nCopy\nclass\nAssemblyAISTT\n:\ndef\n__init__\n(\nself\n,\napi_key\n:\nstr\n|\nNone\n=\nNone\n,\nsample_rate\n:\nint\n=\n16000\n):\nself\n.api_key\n=\napi_key\nor\nos.getenv(\n\"ASSEMBLYAI_API_KEY\"\n)\nself\n.sample_rate\n=\nsample_rate\nself\n._ws: WebSocketClientProtocol\n|\nNone\n=\nNone\nasync\ndef\nsend_audio\n(\nself\n,\naudio_chunk\n:\nbytes\n) ->\nNone\n:\n\"\"\"Send PCM audio bytes to AssemblyAI.\"\"\"\nws\n=\nawait\nself\n._ensure_connection()\nawait\nws.send(audio_chunk)\nasync\ndef\nreceive_events\n(\nself\n) -> AsyncIterator[STTEvent]:\n\"\"\"Yield STT events as they arrive from AssemblyAI.\"\"\"\nasync\nfor\nraw_message\nin\nself\n._ws:\nmessage\n=\njson.loads(raw_message)\nif\nmessage[\n\"type\"\n]\n==\n\"Turn\"\n:\n# Final formatted transcript\nif\nmessage.get(\n\"turn_is_formatted\"\n):\nyield\nSTTOutputEvent.create(message[\n\"transcript\"\n])\n# Partial transcript\nelse\n:\nyield\nSTTChunkEvent.create(message[\n\"transcript\"\n])\nasync\ndef\n_ensure_connection\n(\nself\n) -> WebSocketClientProtocol:\n\"\"\"Establish WebSocket connection if not already connected.\"\"\"\nif\nself\n._ws\nis\nNone\n:\nurl\n=\nf\n\"wss://streaming.assemblyai.com/v3/ws?sample_rate=\n{\nself\n.sample_rate\n}\n&format_turns=true\"\nself\n._ws\n=\nawait\nwebsockets.connect(\nurl,\nadditional_headers\n=\n{\n\"Authorization\"\n:\nself\n.api_key}\n)\nreturn\nself\n._ws\n\u200b\n2. LangChain agent\nThe agent stage processes text transcripts through a LangChain\nagent\nand streams the response tokens. In this case, we stream all\ntext content blocks\ngenerated by the agent.\n\u200b\nKey Concepts\nStreaming Responses\n: The agent uses\nstream_mode=\"messages\"\nto emit response tokens as they\u2019re generated, rather than waiting for the complete response. This enables the TTS stage to begin synthesis immediately.\nConversation Memory\n: A\ncheckpointer\nmaintains conversation state across turns using a unique thread ID. This allows the agent to reference previous exchanges in the conversation.\n\u200b\nImplementation\nCopy\nfrom\nuuid\nimport\nuuid4\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.messages\nimport\nHumanMessage\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\n# Define agent tools\ndef\nadd_to_order\n(\nitem\n:\nstr\n,\nquantity\n:\nint\n) ->\nstr\n:\n\"\"\"Add an item to the customer's sandwich order.\"\"\"\nreturn\nf\n\"Added\n{\nquantity\n}\nx\n{\nitem\n}\nto the order.\"\ndef\nconfirm_order\n(\norder_summary\n:\nstr\n) ->\nstr\n:\n\"\"\"Confirm the final order with the customer.\"\"\"\nreturn\nf\n\"Order confirmed:\n{\norder_summary\n}\n. Sending to kitchen.\"\n# Create agent with tools and memory\nagent\n=\ncreate_agent(\nmodel\n=\n\"anthropic:claude-haiku-4-5\"\n,\n# Select your model\ntools\n=\n[add_to_order, confirm_order],\nsystem_prompt\n=\n\"\"\"You are a helpful sandwich shop assistant.\nYour goal is to take the user's order. Be concise and friendly.\nDo NOT use emojis, special characters, or markdown.\nYour responses will be read by a text-to-speech engine.\"\"\"\n,\ncheckpointer\n=\nInMemorySaver(),\n)\nasync\ndef\nagent_stream\n(\nevent_stream\n: AsyncIterator[VoiceAgentEvent],\n) -> AsyncIterator[VoiceAgentEvent]:\n\"\"\"\nTransform stream: Voice Events \u2192 Voice Events (with Agent Responses)\nPasses through all upstream events and adds agent_chunk events\nwhen processing STT transcripts.\n\"\"\"\n# Generate unique thread ID for conversation memory\nthread_id\n=\nstr\n(uuid4())\nasync\nfor\nevent\nin\nevent_stream:\n# Pass through all upstream events\nyield\nevent\n# Process final transcripts through the agent\nif\nevent.type\n==\n\"stt_output\"\n:\n# Stream agent response with conversation context\nstream\n=\nagent.astream(\n{\n\"messages\"\n: [HumanMessage(\ncontent\n=\nevent.transcript)]},\n{\n\"configurable\"\n: {\n\"thread_id\"\n: thread_id}},\nstream_mode\n=\n\"messages\"\n,\n)\n# Yield agent response chunks as they arrive\nasync\nfor\nmessage, _\nin\nstream:\nif\nmessage.text:\nyield\nAgentChunkEvent.create(message.text)\n\u200b\n3. Text-to-speech\nThe TTS stage synthesizes agent response text into audio and streams it back to the client. Like the STT stage, it uses a producer-consumer pattern to handle concurrent text sending and audio reception.\n\u200b\nKey Concepts\nConcurrent Processing\n: The implementation merges two async streams:\nUpstream processing\n: Passes through all events and sends agent text chunks to the TTS provider\nAudio reception\n: Receives synthesized audio chunks from the TTS provider\nStreaming TTS\n: Some providers (such as\nCartesia\n) begin synthesizing audio as soon as it receives text, enabling audio playback to start before the agent finishes generating its complete response.\nEvent Passthrough\n: All upstream events flow through unchanged, allowing the client or other observers to track the full pipeline state.\n\u200b\nImplementation\nCopy\nfrom\ncartesia_tts\nimport\nCartesiaTTS\nfrom\nutils\nimport\nmerge_async_iters\nasync\ndef\ntts_stream\n(\nevent_stream\n: AsyncIterator[VoiceAgentEvent],\n) -> AsyncIterator[VoiceAgentEvent]:\n\"\"\"\nTransform stream: Voice Events \u2192 Voice Events (with Audio)\nMerges two concurrent streams:\n1. process_upstream(): passes through events and sends text to Cartesia\n2. tts.receive_events(): yields audio chunks from Cartesia\n\"\"\"\ntts\n=\nCartesiaTTS()\nasync\ndef\nprocess_upstream\n() -> AsyncIterator[VoiceAgentEvent]:\n\"\"\"Process upstream events and send agent text to Cartesia.\"\"\"\nasync\nfor\nevent\nin\nevent_stream:\n# Pass through all events\nyield\nevent\n# Send agent text to Cartesia for synthesis\nif\nevent.type\n==\n\"agent_chunk\"\n:\nawait\ntts.send_text(event.text)\ntry\n:\n# Merge upstream events with TTS audio events\n# Both streams run concurrently\nasync\nfor\nevent\nin\nmerge_async_iters(\nprocess_upstream(),\ntts.receive_events()\n):\nyield\nevent\nfinally\n:\nawait\ntts.close()\nThe application implements an Cartesia client to manage the WebSocket connection and audio streaming. See below for implementations; similar adapters can be constructed for other TTS providers.\nCartesia Client\nCopy\nimport\nbase64\nimport\njson\nimport\nwebsockets\nclass\nCartesiaTTS\n:\ndef\n__init__\n(\nself\n,\napi_key\n: Optional[\nstr\n]\n=\nNone\n,\nvoice_id\n:\nstr\n=\n\"f6ff7c0c-e396-40a9-a70b-f7607edb6937\"\n,\nmodel_id\n:\nstr\n=\n\"sonic-3\"\n,\nsample_rate\n:\nint\n=\n24000\n,\nencoding\n:\nstr\n=\n\"pcm_s16le\"\n,\n):\nself\n.api_key\n=\napi_key\nor\nos.getenv(\n\"CARTESIA_API_KEY\"\n)\nself\n.voice_id\n=\nvoice_id\nself\n.model_id\n=\nmodel_id\nself\n.sample_rate\n=\nsample_rate\nself\n.encoding\n=\nencoding\nself\n._ws: WebSocketClientProtocol\n|\nNone\n=\nNone\ndef\n_generate_context_id\n(\nself\n) ->\nstr\n:\n\"\"\"Generate a valid context_id for Cartesia.\"\"\"\ntimestamp\n=\nint\n(time.time()\n*\n1000\n)\ncounter\n=\nself\n._context_counter\nself\n._context_counter\n+=\n1\nreturn\nf\n\"ctx_\n{\ntimestamp\n}\n_\n{\ncounter\n}\n\"\nasync\ndef\nsend_text\n(\nself\n,\ntext\n:\nstr\n|\nNone\n) ->\nNone\n:\n\"\"\"Send text to Cartesia for synthesis.\"\"\"\nif\nnot\ntext\nor\nnot\ntext.strip():\nreturn\nws\n=\nawait\nself\n._ensure_connection()\npayload\n=\n{\n\"model_id\"\n:\nself\n.model_id,\n\"transcript\"\n: text,\n\"voice\"\n: {\n\"mode\"\n:\n\"id\"\n,\n\"id\"\n:\nself\n.voice_id,\n},\n\"output_format\"\n: {\n\"container\"\n:\n\"raw\"\n,\n\"encoding\"\n:\nself\n.encoding,\n\"sample_rate\"\n:\nself\n.sample_rate,\n},\n\"language\"\n:\nself\n.language,\n\"context_id\"\n:\nself\n._generate_context_id(),\n}\nawait\nws.send(json.dumps(payload))\nasync\ndef\nreceive_events\n(\nself\n) -> AsyncIterator[TTSChunkEvent]:\n\"\"\"Yield audio chunks as they arrive from Cartesia.\"\"\"\nasync\nfor\nraw_message\nin\nself\n._ws:\nmessage\n=\njson.loads(raw_message)\n# Decode and yield audio chunks\nif\n\"data\"\nin\nmessage\nand\nmessage[\n\"data\"\n]:\naudio_chunk\n=\nbase64.b64decode(message[\n\"data\"\n])\nif\naudio_chunk:\nyield\nTTSChunkEvent.create(audio_chunk)\nasync\ndef\n_ensure_connection\n(\nself\n) -> WebSocketClientProtocol:\n\"\"\"Establish WebSocket connection if not already connected.\"\"\"\nif\nself\n._ws\nis\nNone\n:\nurl\n=\n(\nf\n\"wss://api.cartesia.ai/tts/websocket\"\nf\n\"?api_key=\n{\nself\n.api_key\n}\n&cartesia_version=\n{\nself\n.cartesia_version\n}\n\"\n)\nself\n._ws\n=\nawait\nwebsockets.connect(url)\nreturn\nself\n._ws\n\u200b\nPutting It All Together\nThe complete pipeline chains the three stages together:\nCopy\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\npipeline\n=\n(\nRunnableGenerator(stt_stream)\n# Audio \u2192 STT events\n|\nRunnableGenerator(agent_stream)\n# STT events \u2192 Agent events\n|\nRunnableGenerator(tts_stream)\n# Agent events \u2192 TTS audio\n)\n# Use in WebSocket endpoint\n@app.websocket\n(\n\"/ws\"\n)\nasync\ndef\nwebsocket_endpoint\n(\nwebsocket\n: WebSocket):\nawait\nwebsocket.accept()\nasync\ndef\nwebsocket_audio_stream\n():\n\"\"\"Yield audio bytes from WebSocket.\"\"\"\nwhile\nTrue\n:\ndata\n=\nawait\nwebsocket.receive_bytes()\nyield\ndata\n# Transform audio through pipeline\noutput_stream\n=\npipeline.atransform(websocket_audio_stream())\n# Send TTS audio back to client\nasync\nfor\nevent\nin\noutput_stream:\nif\nevent.type\n==\n\"tts_chunk\"\n:\nawait\nwebsocket.send_bytes(event.audio)\nWe use\nRunnableGenerators\nto compose each step of the pipeline. This is an abstraction LangChain uses internally to manage\nstreaming across components\n.\nEach stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.\nFor more on building agents with LangChain, see the\nAgents guide\n.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a SQL agent\nPrevious\nBuild a personal assistant with subagents\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/voice-agent",
      "title": "Build a voice agent with LangChain - Docs by LangChain",
      "heading": "Build a voice agent with LangChain"
    }
  },
  {
    "page_content": "Component architecture - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nConceptual overviews\nComponent architecture\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nCore component ecosystem\nHow components connect\nComponent categories\nCommon patterns\nRAG (Retrieval-Augmented Generation)\nAgent with tools\nMulti-agent system\nLearn more\nConceptual overviews\nComponent architecture\nCopy page\nCopy page\nLangChain\u2019s power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components.\n\u200b\nCore component ecosystem\nThe diagram below shows how LangChain\u2019s major components connect to form complete AI applications:\n\u200b\nHow components connect\nEach component layer builds on the previous ones:\nInput processing\n\u2013 Transform raw data into structured documents\nEmbedding & storage\n\u2013 Convert text into searchable vector representations\nRetrieval\n\u2013 Find relevant information based on user queries\nGeneration\n\u2013 Use AI models to create responses, optionally with tools\nOrchestration\n\u2013 Coordinate everything through agents and memory systems\n\u200b\nComponent categories\nLangChain organizes components into these main categories:\nCategory\nPurpose\nKey Components\nUse Cases\nModels\nAI reasoning and generation\nChat models, LLMs, Embedding models\nText generation, reasoning, semantic understanding\nTools\nExternal capabilities\nAPIs, databases, etc.\nWeb search, data access, computations\nAgents\nOrchestration and reasoning\nReAct agents, tool calling agents\nNondeterministic workflows, decision making\nMemory\nContext preservation\nMessage history, custom state\nConversations, stateful interactions\nRetrievers\nInformation access\nVector retrievers, web retrievers\nRAG, knowledge base search\nDocument processing\nData ingestion\nLoaders, splitters, transformers\nPDF processing, web scraping\nVector Stores\nSemantic search\nChroma, Pinecone, FAISS\nSimilarity search, embeddings storage\n\u200b\nCommon patterns\n\u200b\nRAG (Retrieval-Augmented Generation)\n\u200b\nAgent with tools\n\u200b\nMulti-agent system\n\u200b\nLearn more\nNow that you understand how components relate to each other, explore specific areas:\nBuilding your first RAG system\nCreating agents\nWorking with tools\nSetting up memory\nBrowse integrations\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a custom SQL agent\nPrevious\nMemory overview\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/component-architecture",
      "title": "Component architecture - Docs by LangChain",
      "heading": "Component architecture"
    }
  },
  {
    "page_content": "Agents (LangGraph) | LangChain Reference\nSkip to content\nLangChain Reference\nAgents (LangGraph)\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nAgents\nTable of contents\nchat_agent_executor\nAgentState\ncreate_react_agent\nToolNode\ntool_node\nInjectedState\n__init__\nInjectedStore\ntools_condition\nValidationNode\ninterrupt\nHumanInterruptConfig\nActionRequest\nHumanInterrupt\nHumanResponse\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nchat_agent_executor\nAgentState\ncreate_react_agent\nToolNode\ntool_node\nInjectedState\n__init__\nInjectedStore\ntools_condition\nValidationNode\ninterrupt\nHumanInterruptConfig\nActionRequest\nHumanInterrupt\nHumanResponse\nAgents\nchat_agent_executor\n\u00b6\nFUNCTION\nDESCRIPTION\ncreate_react_agent\nCreates an agent graph that calls tools in a loop until a stopping condition is met.\nAgentState\ndeprecated\n\u00b6\nBases:\nTypedDict\nDeprecated\nAgentState has been moved to\nlangchain.agents\n. Please update your import to\nfrom langchain.agents import AgentState\n.\nThe state of the agent.\ncreate_react_agent\ndeprecated\n\u00b6\ncreate_react_agent\n(\nmodel\n:\nstr\n|\nLanguageModelLike\n|\nCallable\n[[\nStateSchema\n,\nRuntime\n[\nContextT\n]],\nBaseChatModel\n]\n|\nCallable\n[[\nStateSchema\n,\nRuntime\n[\nContextT\n]],\nAwaitable\n[\nBaseChatModel\n]]\n|\nCallable\n[\n[\nStateSchema\n,\nRuntime\n[\nContextT\n]],\nRunnable\n[\nLanguageModelInput\n,\nBaseMessage\n]\n]\n|\nCallable\n[\n[\nStateSchema\n,\nRuntime\n[\nContextT\n]],\nAwaitable\n[\nRunnable\n[\nLanguageModelInput\n,\nBaseMessage\n]],\n],\ntools\n:\nSequence\n[\nBaseTool\n|\nCallable\n|\ndict\n[\nstr\n,\nAny\n]]\n|\nToolNode\n,\n*\n,\nprompt\n:\nPrompt\n|\nNone\n=\nNone\n,\nresponse_format\n:\nStructuredResponseSchema\n|\ntuple\n[\nstr\n,\nStructuredResponseSchema\n]\n|\nNone\n=\nNone\n,\npre_model_hook\n:\nRunnableLike\n|\nNone\n=\nNone\n,\npost_model_hook\n:\nRunnableLike\n|\nNone\n=\nNone\n,\nstate_schema\n:\nStateSchemaType\n|\nNone\n=\nNone\n,\ncontext_schema\n:\ntype\n[\nAny\n]\n|\nNone\n=\nNone\n,\ncheckpointer\n:\nCheckpointer\n|\nNone\n=\nNone\n,\nstore\n:\nBaseStore\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndebug\n:\nbool\n=\nFalse\n,\nversion\n:\nLiteral\n[\n\"v1\"\n,\n\"v2\"\n]\n=\n\"v2\"\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n**\ndeprecated_kwargs\n:\nAny\n,\n)\n->\nCompiledStateGraph\nDeprecated\ncreate_react_agent has been moved to\nlangchain.agents\n. Please update your import to\nfrom langchain.agents import create_agent\n.\nCreates an agent graph that calls tools in a loop until a stopping condition is met.\nFor more details on using\ncreate_react_agent\n, visit\nAgents\ndocumentation.\nPARAMETER\nDESCRIPTION\nmodel\nThe language model for the agent. Supports static and dynamic\nmodel selection.\nStatic model\n: A chat model instance (e.g.,\nChatOpenAI\n) or string identifier (e.g.,\n\"openai:gpt-4\"\n)\nDynamic model\n: A callable with signature\n(state, runtime) -> BaseChatModel\nthat returns different models\nbased on runtime context\nIf the model has tools bound via\nbind_tools\nor other configurations,\nthe return type should be a\nRunnable[LanguageModelInput, BaseMessage]\nCoroutines are also supported, allowing for asynchronous model selection.\nDynamic functions receive graph state and runtime, enabling\ncontext-dependent model selection. Must return a\nBaseChatModel\ninstance. For tool calling, bind tools using\n.bind_tools()\n.\nBound tools must be a subset of the\ntools\nparameter.\nDynamic model\nfrom\ndataclasses\nimport\ndataclass\n@dataclass\nclass\nModelContext\n:\nmodel_name\n:\nstr\n=\n\"gpt-5-mini-turbo\"\n# Instantiate models globally\ngpt4_model\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-4\"\n)\ngpt35_model\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-5-mini-turbo\"\n)\ndef\nselect_model\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n[\nModelContext\n])\n->\nChatOpenAI\n:\nmodel_name\n=\nruntime\n.\ncontext\n.\nmodel_name\nmodel\n=\ngpt4_model\nif\nmodel_name\n==\n\"gpt-4\"\nelse\ngpt35_model\nreturn\nmodel\n.\nbind_tools\n(\ntools\n)\nDynamic Model Requirements\nEnsure returned models have appropriate tools bound via\n.bind_tools()\nand support required functionality. Bound tools\nmust be a subset of those specified in the\ntools\nparameter.\nTYPE:\nstr\n|\nLanguageModelLike\n|\nCallable\n[[\nStateSchema\n,\nRuntime\n[\nContextT\n]],\nBaseChatModel\n] |\nCallable\n[[\nStateSchema\n,\nRuntime\n[\nContextT\n]],\nAwaitable\n[\nBaseChatModel\n]] |\nCallable\n[[\nStateSchema\n,\nRuntime\n[\nContextT\n]],\nRunnable\n[\nLanguageModelInput\n,\nBaseMessage\n]] |\nCallable\n[[\nStateSchema\n,\nRuntime\n[\nContextT\n]],\nAwaitable\n[\nRunnable\n[\nLanguageModelInput\n,\nBaseMessage\n]]]\ntools\nA list of tools or a\nToolNode\ninstance.\nIf an empty list is provided, the agent will consist of a single LLM node without tool calling.\nTYPE:\nSequence\n[\nBaseTool\n|\nCallable\n|\ndict\n[\nstr\n,\nAny\n]] |\nToolNode\nprompt\nAn optional prompt for the LLM. Can take a few different forms:\nstr\n: This is converted to a\nSystemMessage\nand added to the beginning of the list of messages in\nstate[\"messages\"]\n.\nSystemMessage\n: this is added to the beginning of the list of messages in\nstate[\"messages\"]\n.\nCallable\n: This function should take in full graph state and the output is then passed to the language model.\nRunnable\n: This runnable should take in full graph state and the output is then passed to the language model.\nTYPE:\nPrompt\n| None\nDEFAULT:\nNone\nresponse_format\nAn optional schema for the final agent output.\nIf provided, output will be formatted to match the given schema and returned in the 'structured_response' state key.\nIf not provided,\nstructured_response\nwill not be present in the output state.\nCan be passed in as:\nAn OpenAI function/tool schema,\nA JSON Schema,\nA TypedDict class,\nA Pydantic class.\nA tuple\n(prompt, schema)\n, where schema is one of the above.\nThe prompt will be used together with the model that is being used to\ngenerate the structured response.\nImportant\nresponse_format\nrequires the model to support\n.with_structured_output\nNote\nThe graph will make a separate call to the LLM to generate the structured response after the agent loop is finished.\nThis is not the only strategy to get structured responses, see more options in\nthis guide\n.\nTYPE:\nStructuredResponseSchema\n|\ntuple\n[\nstr\n,\nStructuredResponseSchema\n] | None\nDEFAULT:\nNone\npre_model_hook\nAn optional node to add before the\nagent\nnode (i.e., the node that calls the LLM).\nUseful for managing long message histories (e.g., message trimming, summarization, etc.).\nPre-model hook must be a callable or a runnable that takes in current graph state and returns a state update in the form of\n# At least one of `messages` or `llm_input_messages` MUST be provided\n{\n# If provided, will UPDATE the `messages` in the state\n\"messages\"\n:\n[\nRemoveMessage\n(\nid\n=\nREMOVE_ALL_MESSAGES\n),\n...\n],\n# If provided, will be used as the input to the LLM,\n# and will NOT UPDATE `messages` in the state\n\"llm_input_messages\"\n:\n[\n...\n],\n# Any other state keys that need to be propagated\n...\n}\nImportant\nAt least one of\nmessages\nor\nllm_input_messages\nMUST be provided and will be used as an input to the\nagent\nnode.\nThe rest of the keys will be added to the graph state.\nWarning\nIf you are returning\nmessages\nin the pre-model hook, you should OVERWRITE the\nmessages\nkey by doing the following:\n{\n\"messages\"\n:\n[\nRemoveMessage\n(\nid\n=\nREMOVE_ALL_MESSAGES\n),\n*\nnew_messages\n]\n...\n}\nTYPE:\nRunnableLike\n| None\nDEFAULT:\nNone\npost_model_hook\nAn optional node to add after the\nagent\nnode (i.e., the node that calls the LLM).\nUseful for implementing human-in-the-loop, guardrails, validation, or other post-processing.\nPost-model hook must be a callable or a runnable that takes in current graph state and returns a state update.\nNote\nOnly available with\nversion=\"v2\"\n.\nTYPE:\nRunnableLike\n| None\nDEFAULT:\nNone\nstate_schema\nAn optional state schema that defines graph state.\nMust have\nmessages\nand\nremaining_steps\nkeys.\nDefaults to\nAgentState\nthat defines those two keys.\nNote\nremaining_steps\nis used to limit the number of steps the react agent can take.\nCalculated roughly as\nrecursion_limit\n-\ntotal_steps_taken\n.\nIf\nremaining_steps\nis less than 2 and tool calls are present in the response,\nthe react agent will return a final AI Message with\nthe content \"Sorry, need more steps to process this request.\".\nNo\nGraphRecusionError\nwill be raised in this case.\nTYPE:\nStateSchemaType\n| None\nDEFAULT:\nNone\ncontext_schema\nAn optional schema for runtime context.\nTYPE:\ntype\n[\nAny\n] | None\nDEFAULT:\nNone\ncheckpointer\nAn optional checkpoint saver object. This is used for persisting\nthe state of the graph (e.g., as chat memory) for a single thread (e.g., a single conversation).\nTYPE:\nCheckpointer\n| None\nDEFAULT:\nNone\nstore\nAn optional store object. This is used for persisting data\nacross multiple threads (e.g., multiple conversations / users).\nTYPE:\nBaseStore\n| None\nDEFAULT:\nNone\ninterrupt_before\nAn optional list of node names to interrupt before.\nShould be one of the following:\n\"agent\"\n,\n\"tools\"\n.\nThis is useful if you want to add a user confirmation or other interrupt before taking an action.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nAn optional list of node names to interrupt after.\nShould be one of the following:\n\"agent\"\n,\n\"tools\"\n.\nThis is useful if you want to return directly or run additional processing on an output.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ndebug\nA flag indicating whether to enable debug mode.\nTYPE:\nbool\nDEFAULT:\nFalse\nversion\nDetermines the version of the graph to create.\nCan be one of:\n\"v1\"\n: The tool node processes a single message. All tool\ncalls in the message are executed in parallel within the tool node.\n\"v2\"\n: The tool node processes a tool call.\nTool calls are distributed across multiple instances of the tool\nnode using the\nSend\nAPI.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\nname\nAn optional name for the\nCompiledStateGraph\n.\nThis name will be automatically used when adding ReAct agent graph to another graph as a subgraph node -\nparticularly useful for building multi-agent systems.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nconfig_schema\nDeprecated\nThe\nconfig_schema\nparameter is deprecated in v0.6.0 and support will be removed in v2.0.0.\nPlease use\ncontext_schema\ninstead to specify the schema for run-scoped context.\nRETURNS\nDESCRIPTION\nCompiledStateGraph\nA compiled LangChain\nRunnable\nthat can be used for chat interactions.\nThe \"agent\" node calls the language model with the messages list (after applying the prompt).\nIf the resulting AIMessage contains\ntool_calls\n, the graph will then call the\n\"tools\"\n.\nThe \"tools\" node executes the tools (1 tool per\ntool_call\n) and adds the responses to the messages list\nas\nToolMessage\nobjects. The agent node then calls the language model again.\nThe process repeats until no more\ntool_calls\nare present in the response.\nThe agent then returns the full list of messages as a dictionary containing the key\n'messages'\n.\nsequenceDiagram\nparticipant U as User\nparticipant A as LLM\nparticipant T as Tools\nU->>A: Initial input\nNote over A: Prompt + LLM\nloop while tool_calls present\nA->>T: Execute tools\nT-->>A: ToolMessage for each tool_calls\nend\nA->>U: Return final state\nExample\nfrom\nlanggraph.prebuilt\nimport\ncreate_react_agent\ndef\ncheck_weather\n(\nlocation\n:\nstr\n)\n->\nstr\n:\n'''Return the weather forecast for the specified location.'''\nreturn\nf\n\"It's always sunny in\n{\nlocation\n}\n\"\ngraph\n=\ncreate_react_agent\n(\n\"anthropic:claude-3-7-sonnet-latest\"\n,\ntools\n=\n[\ncheck_weather\n],\nprompt\n=\n\"You are a helpful assistant\"\n,\n)\ninputs\n=\n{\n\"messages\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\nfor\nchunk\nin\ngraph\n.\nstream\n(\ninputs\n,\nstream_mode\n=\n\"updates\"\n):\nprint\n(\nchunk\n)\nToolNode\n\u00b6\nBases:\nRunnableCallable\nA node for executing tools in LangGraph workflows.\nHandles tool execution patterns including function calls, state injection,\npersistent storage, and control flow. Manages parallel execution,\nerror handling.\nInput Formats\nGraph state with\nmessages\nkey that has a list of messages:\nCommon representation for agentic workflows\nSupports custom messages key via\nmessages_key\nparameter\nMessage List\n:\n[AIMessage(..., tool_calls=[...])]\nList of messages with tool calls in the last AIMessage\nDirect Tool Calls\n:\n[{\"name\": \"tool\", \"args\": {...}, \"id\": \"1\", \"type\": \"tool_call\"}]\nBypasses message parsing for direct tool execution\nFor programmatic tool invocation and testing\nOutput Formats\nOutput format depends on input type and tool behavior:\nFor Regular tools\n:\nDict input \u2192\n{\"messages\": [ToolMessage(...)]}\nList input \u2192\n[ToolMessage(...)]\nFor Command tools\n:\nReturns\n[Command(...)]\nor mixed list with regular tool outputs\nCommand\ncan update state, trigger navigation, or send messages\nPARAMETER\nDESCRIPTION\ntools\nA sequence of tools that can be invoked by this node.\nSupports:\nBaseTool instances\n: Tools with schemas and metadata\nPlain functions\n: Automatically converted to tools with inferred schemas\nTYPE:\nSequence\n[\nBaseTool\n|\nCallable\n]\nname\nThe name identifier for this node in the graph. Used for debugging\nand visualization.\nTYPE:\nstr\nDEFAULT:\n'tools'\ntags\nOptional metadata tags to associate with the node for filtering\nand organization.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nhandle_tool_errors\nConfiguration for error handling during tool execution.\nSupports multiple strategies:\nTrue\n: Catch all errors and return a\nToolMessage\nwith the default\nerror template containing the exception details.\nstr\n: Catch all errors and return a\nToolMessage\nwith this custom\nerror message string.\ntype[Exception]\n: Only catch exceptions with the specified type and\nreturn the default error message for it.\ntuple[type[Exception], ...]\n: Only catch exceptions with the specified\ntypes and return default error messages for them.\nCallable[..., str]\n: Catch exceptions matching the callable's signature\nand return the string result of calling it with the exception.\nFalse\n: Disable error handling entirely, allowing exceptions to\npropagate.\nDefaults to a callable that:\nCatches tool invocation errors (due to invalid arguments provided by the\nmodel) and returns a descriptive error message\nIgnores tool execution errors (they will be re-raised)\nTYPE:\nbool\n|\nstr\n|\nCallable\n[...,\nstr\n] |\ntype\n[\nException\n] |\ntuple\n[\ntype\n[\nException\n], ...]\nDEFAULT:\n_default_handle_tool_errors\nmessages_key\nThe key in the state dictionary that contains the message list.\nThis same key will be used for the output\nToolMessage\nobjects.\nAllows custom state schemas with different message field names.\nTYPE:\nstr\nDEFAULT:\n'messages'\nExamples:\nBasic usage:\nfrom\nlangchain.tools\nimport\nToolNode\nfrom\nlangchain_core.tools\nimport\ntool\n@tool\ndef\ncalculator\n(\na\n:\nint\n,\nb\n:\nint\n)\n->\nint\n:\n\"\"\"Add two numbers.\"\"\"\nreturn\na\n+\nb\ntool_node\n=\nToolNode\n([\ncalculator\n])\nState injection:\nfrom\ntyping_extensions\nimport\nAnnotated\nfrom\nlangchain.tools\nimport\nInjectedState\n@tool\ndef\ncontext_tool\n(\nquery\n:\nstr\n,\nstate\n:\nAnnotated\n[\ndict\n,\nInjectedState\n])\n->\nstr\n:\n\"\"\"Some tool that uses state.\"\"\"\nreturn\nf\n\"Query:\n{\nquery\n}\n, Messages:\n{\nlen\n(\nstate\n[\n'messages'\n])\n}\n\"\ntool_node\n=\nToolNode\n([\ncontext_tool\n])\nError handling:\ndef\nhandle_errors\n(\ne\n:\nValueError\n)\n->\nstr\n:\nreturn\n\"Invalid input provided\"\ntool_node\n=\nToolNode\n([\nmy_tool\n],\nhandle_tool_errors\n=\nhandle_errors\n)\ntool_node\n\u00b6\nTool execution node for LangGraph workflows.\nThis module provides prebuilt functionality for executing tools in LangGraph.\nTools are functions that models can call to interact with external systems,\nAPIs, databases, or perform computations.\nThe module implements design patterns for:\nParallel execution of multiple tool calls for efficiency\nRobust error handling with customizable error messages\nState injection for tools that need access to graph state\nStore injection for tools that need persistent storage\nCommand-based state updates for advanced control flow\nKey Components:\nToolNode\n: Main class for executing tools in LangGraph workflows\nInjectedState\n: Annotation for injecting graph state into tools\nInjectedStore\n: Annotation for injecting persistent store into tools\nToolRuntime\n: Runtime information for tools, bundling together\nstate\n,\ncontext\n,\nconfig\n,\nstream_writer\n,\ntool_call_id\n, and\nstore\ntools_condition\n: Utility function for conditional routing based on tool calls\nTypical Usage\nfrom\nlangchain_core.tools\nimport\ntool\nfrom\nlangchain.tools\nimport\nToolNode\n@tool\ndef\nmy_tool\n(\nx\n:\nint\n)\n->\nstr\n:\nreturn\nf\n\"Result:\n{\nx\n}\n\"\ntool_node\n=\nToolNode\n([\nmy_tool\n])\nFUNCTION\nDESCRIPTION\ntools_condition\nConditional routing function for tool-calling workflows.\nInjectedState\n\u00b6\nBases:\nInjectedToolArg\nAnnotation for injecting graph state into tool arguments.\nThis annotation enables tools to access graph state without exposing state\nmanagement details to the language model. Tools annotated with\nInjectedState\nreceive state data automatically during execution while remaining invisible\nto the model's tool-calling interface.\nPARAMETER\nDESCRIPTION\nfield\nOptional key to extract from the state dictionary. If\nNone\n, the entire\nstate is injected. If specified, only that field's value is injected.\nThis allows tools to request specific state components rather than\nprocessing the full state structure.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nExample\nfrom\ntyping\nimport\nList\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\nfrom\nlangchain_core.messages\nimport\nBaseMessage\n,\nAIMessage\nfrom\nlangchain.tools\nimport\nInjectedState\n,\nToolNode\n,\ntool\nclass\nAgentState\n(\nTypedDict\n):\nmessages\n:\nList\n[\nBaseMessage\n]\nfoo\n:\nstr\n@tool\ndef\nstate_tool\n(\nx\n:\nint\n,\nstate\n:\nAnnotated\n[\ndict\n,\nInjectedState\n])\n->\nstr\n:\n'''Do something with state.'''\nif\nlen\n(\nstate\n[\n\"messages\"\n])\n>\n2\n:\nreturn\nstate\n[\n\"foo\"\n]\n+\nstr\n(\nx\n)\nelse\n:\nreturn\n\"not enough messages\"\n@tool\ndef\nfoo_tool\n(\nx\n:\nint\n,\nfoo\n:\nAnnotated\n[\nstr\n,\nInjectedState\n(\n\"foo\"\n)])\n->\nstr\n:\n'''Do something else with state.'''\nreturn\nfoo\n+\nstr\n(\nx\n+\n1\n)\nnode\n=\nToolNode\n([\nstate_tool\n,\nfoo_tool\n])\ntool_call1\n=\n{\n\"name\"\n:\n\"state_tool\"\n,\n\"args\"\n:\n{\n\"x\"\n:\n1\n},\n\"id\"\n:\n\"1\"\n,\n\"type\"\n:\n\"tool_call\"\n}\ntool_call2\n=\n{\n\"name\"\n:\n\"foo_tool\"\n,\n\"args\"\n:\n{\n\"x\"\n:\n1\n},\n\"id\"\n:\n\"2\"\n,\n\"type\"\n:\n\"tool_call\"\n}\nstate\n=\n{\n\"messages\"\n:\n[\nAIMessage\n(\n\"\"\n,\ntool_calls\n=\n[\ntool_call1\n,\ntool_call2\n])],\n\"foo\"\n:\n\"bar\"\n,\n}\nnode\n.\ninvoke\n(\nstate\n)\n[\nToolMessage\n(\ncontent\n=\n\"not enough messages\"\n,\nname\n=\n\"state_tool\"\n,\ntool_call_id\n=\n\"1\"\n),\nToolMessage\n(\ncontent\n=\n\"bar2\"\n,\nname\n=\n\"foo_tool\"\n,\ntool_call_id\n=\n\"2\"\n),\n]\nNote\nInjectedState\narguments are automatically excluded from tool schemas\npresented to language models\nToolNode\nhandles the injection process during execution\nTools can mix regular arguments (controlled by the model) with injected\narguments (controlled by the system)\nState injection occurs after the model generates tool calls but before\ntool execution\nMETHOD\nDESCRIPTION\n__init__\nInitialize the\nInjectedState\nannotation.\n__init__\n\u00b6\n__init__\n(\nfield\n:\nstr\n|\nNone\n=\nNone\n)\n->\nNone\nInitialize the\nInjectedState\nannotation.\nInjectedStore\n\u00b6\nBases:\nInjectedToolArg\nAnnotation for injecting persistent store into tool arguments.\nThis annotation enables tools to access LangGraph's persistent storage system\nwithout exposing storage details to the language model. Tools annotated with\nInjectedStore\nreceive the store instance automatically during execution while\nremaining invisible to the model's tool-calling interface.\nThe store provides persistent, cross-session data storage that tools can use\nfor maintaining context, user preferences, or any other data that needs to\npersist beyond individual workflow executions.\nWarning\nInjectedStore\nannotation requires\nlangchain-core >= 0.3.8\nExample\nfrom\ntyping_extensions\nimport\nAnnotated\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nfrom\nlangchain.tools\nimport\nInjectedStore\n,\nToolNode\n,\ntool\n@tool\ndef\nsave_preference\n(\nkey\n:\nstr\n,\nvalue\n:\nstr\n,\nstore\n:\nAnnotated\n[\nAny\n,\nInjectedStore\n()]\n)\n->\nstr\n:\n\"\"\"Save user preference to persistent storage.\"\"\"\nstore\n.\nput\n((\n\"preferences\"\n,),\nkey\n,\nvalue\n)\nreturn\nf\n\"Saved\n{\nkey\n}\n=\n{\nvalue\n}\n\"\n@tool\ndef\nget_preference\n(\nkey\n:\nstr\n,\nstore\n:\nAnnotated\n[\nAny\n,\nInjectedStore\n()]\n)\n->\nstr\n:\n\"\"\"Retrieve user preference from persistent storage.\"\"\"\nresult\n=\nstore\n.\nget\n((\n\"preferences\"\n,),\nkey\n)\nreturn\nresult\n.\nvalue\nif\nresult\nelse\n\"Not found\"\nUsage with\nToolNode\nand graph compilation:\nfrom\nlanggraph.graph\nimport\nStateGraph\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nstore\n=\nInMemoryStore\n()\ntool_node\n=\nToolNode\n([\nsave_preference\n,\nget_preference\n])\ngraph\n=\nStateGraph\n(\nState\n)\ngraph\n.\nadd_node\n(\n\"tools\"\n,\ntool_node\n)\ncompiled_graph\n=\ngraph\n.\ncompile\n(\nstore\n=\nstore\n)\n# Store is injected automatically\nCross-session persistence:\n# First session\nresult1\n=\ngraph\n.\ninvoke\n({\n\"messages\"\n:\n[\nHumanMessage\n(\n\"Save my favorite color as blue\"\n)]})\n# Later session - data persists\nresult2\n=\ngraph\n.\ninvoke\n({\n\"messages\"\n:\n[\nHumanMessage\n(\n\"What's my favorite color?\"\n)]})\nNote\nInjectedStore\narguments are automatically excluded from tool schemas\npresented to language models\nThe store instance is automatically injected by\nToolNode\nduring execution\nTools can access namespaced storage using the store's get/put methods\nStore injection requires the graph to be compiled with a store instance\nMultiple tools can share the same store instance for data consistency\ntools_condition\n\u00b6\ntools_condition\n(\nstate\n:\nlist\n[\nAnyMessage\n]\n|\ndict\n[\nstr\n,\nAny\n]\n|\nBaseModel\n,\nmessages_key\n:\nstr\n=\n\"messages\"\n)\n->\nLiteral\n[\n\"tools\"\n,\n\"__end__\"\n]\nConditional routing function for tool-calling workflows.\nThis utility function implements the standard conditional logic for ReAct-style\nagents: if the last\nAIMessage\ncontains tool calls, route to the tool execution\nnode; otherwise, end the workflow. This pattern is fundamental to most tool-calling\nagent architectures.\nThe function handles multiple state formats commonly used in LangGraph applications,\nmaking it flexible for different graph designs while maintaining consistent behavior.\nPARAMETER\nDESCRIPTION\nstate\nThe current graph state to examine for tool calls. Supported formats:\n- Dictionary containing a messages key (for\nStateGraph\n)\n-\nBaseModel\ninstance with a messages attribute\nTYPE:\nlist\n[\nAnyMessage\n] |\ndict\n[\nstr\n,\nAny\n] |\nBaseModel\nmessages_key\nThe key or attribute name containing the message list in the state.\nThis allows customization for graphs using different state schemas.\nTYPE:\nstr\nDEFAULT:\n'messages'\nRETURNS\nDESCRIPTION\nLiteral\n['tools', '__end__']\nEither\n'tools'\nif tool calls are present in the last\nAIMessage\n, or\n'__end__'\nto terminate the workflow. These are the standard routing destinations for\ntool-calling conditional edges.\nRAISES\nDESCRIPTION\nValueError\nIf no messages can be found in the provided state format.\nExample\nBasic usage in a ReAct agent:\nfrom\nlanggraph.graph\nimport\nStateGraph\nfrom\nlangchain.tools\nimport\nToolNode\nfrom\nlangchain.tools.tool_node\nimport\ntools_condition\nfrom\ntyping_extensions\nimport\nTypedDict\nclass\nState\n(\nTypedDict\n):\nmessages\n:\nlist\ngraph\n=\nStateGraph\n(\nState\n)\ngraph\n.\nadd_node\n(\n\"llm\"\n,\ncall_model\n)\ngraph\n.\nadd_node\n(\n\"tools\"\n,\nToolNode\n([\nmy_tool\n]))\ngraph\n.\nadd_conditional_edges\n(\n\"llm\"\n,\ntools_condition\n,\n# Routes to \"tools\" or \"__end__\"\n{\n\"tools\"\n:\n\"tools\"\n,\n\"__end__\"\n:\n\"__end__\"\n},\n)\nCustom messages key:\ndef\ncustom_condition\n(\nstate\n):\nreturn\ntools_condition\n(\nstate\n,\nmessages_key\n=\n\"chat_history\"\n)\nNote\nThis function is designed to work seamlessly with\nToolNode\nand standard\nLangGraph patterns. It expects the last message to be an\nAIMessage\nwhen\ntool calls are present, which is the standard output format for tool-calling\nlanguage models.\nValidationNode\ndeprecated\n\u00b6\nBases:\nRunnableCallable\nDeprecated\nValidationNode is deprecated. Please use\ncreate_agent\nfrom\nlangchain.agents\nwith custom tool error handling.\nA node that validates all tools requests from the last\nAIMessage\n.\nIt can be used either in\nStateGraph\nwith a\n'messages'\nkey.\nNote\nThis node does not actually\nrun\nthe tools, it only validates the tool calls,\nwhich is useful for extraction and other use cases where you need to generate\nstructured output that conforms to a complex schema without losing the original\nmessages and tool IDs (for use in multi-turn conversations).\nRETURNS\nDESCRIPTION\nUnion\n[\nDict\n[\nstr\n,\nList\n[\nToolMessage\n]],\nSequence\n[\nToolMessage\n]]\nA list of\nToolMessage\nobjects with the validated content or error messages.\nExample\nExample usage for re-prompting the model to generate a valid response:\nfrom\ntyping\nimport\nLiteral\n,\nAnnotated\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\npydantic\nimport\nBaseModel\n,\nfield_validator\nfrom\nlanggraph.graph\nimport\nEND\n,\nSTART\n,\nStateGraph\nfrom\nlanggraph.prebuilt\nimport\nValidationNode\nfrom\nlanggraph.graph.message\nimport\nadd_messages\nclass\nSelectNumber\n(\nBaseModel\n):\na\n:\nint\n@field_validator\n(\n\"a\"\n)\ndef\na_must_be_meaningful\n(\ncls\n,\nv\n):\nif\nv\n!=\n37\n:\nraise\nValueError\n(\n\"Only 37 is allowed\"\n)\nreturn\nv\nbuilder\n=\nStateGraph\n(\nAnnotated\n[\nlist\n,\nadd_messages\n])\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-5-haiku-latest\"\n)\n.\nbind_tools\n([\nSelectNumber\n])\nbuilder\n.\nadd_node\n(\n\"model\"\n,\nllm\n)\nbuilder\n.\nadd_node\n(\n\"validation\"\n,\nValidationNode\n([\nSelectNumber\n]))\nbuilder\n.\nadd_edge\n(\nSTART\n,\n\"model\"\n)\ndef\nshould_validate\n(\nstate\n:\nlist\n)\n->\nLiteral\n[\n\"validation\"\n,\n\"__end__\"\n]:\nif\nstate\n[\n-\n1\n]\n.\ntool_calls\n:\nreturn\n\"validation\"\nreturn\nEND\nbuilder\n.\nadd_conditional_edges\n(\n\"model\"\n,\nshould_validate\n)\ndef\nshould_reprompt\n(\nstate\n:\nlist\n)\n->\nLiteral\n[\n\"model\"\n,\n\"__end__\"\n]:\nfor\nmsg\nin\nstate\n[::\n-\n1\n]:\n# None of the tool calls were errors\nif\nmsg\n.\ntype\n==\n\"ai\"\n:\nreturn\nEND\nif\nmsg\n.\nadditional_kwargs\n.\nget\n(\n\"is_error\"\n):\nreturn\n\"model\"\nreturn\nEND\nbuilder\n.\nadd_conditional_edges\n(\n\"validation\"\n,\nshould_reprompt\n)\ngraph\n=\nbuilder\n.\ncompile\n()\nres\n=\ngraph\n.\ninvoke\n((\n\"user\"\n,\n\"Select a number, any number\"\n))\n# Show the retry logic\nfor\nmsg\nin\nres\n:\nmsg\n.\npretty_print\n()\ninterrupt\n\u00b6\nHumanInterruptConfig\ndeprecated\n\u00b6\nBases:\nTypedDict\nDeprecated\nHumanInterruptConfig has been moved to\nlangchain.agents.interrupt\n. Please update your import to\nfrom langchain.agents.interrupt import HumanInterruptConfig\n.\nConfiguration that defines what actions are allowed for a human interrupt.\nThis controls the available interaction options when the graph is paused for human input.\nATTRIBUTE\nDESCRIPTION\nallow_ignore\nWhether the human can choose to ignore/skip the current step\nallow_respond\nWhether the human can provide a text response/feedback\nallow_edit\nWhether the human can edit the provided content/state\nallow_accept\nWhether the human can accept/approve the current state\nActionRequest\ndeprecated\n\u00b6\nBases:\nTypedDict\nDeprecated\nActionRequest has been moved to\nlangchain.agents.interrupt\n. Please update your import to\nfrom langchain.agents.interrupt import ActionRequest\n.\nRepresents a request for human action within the graph execution.\nContains the action type and any associated arguments needed for the action.\nATTRIBUTE\nDESCRIPTION\naction\nThe type or name of action being requested (e.g.,\n\"Approve XYZ action\"\n)\nargs\nKey-value pairs of arguments needed for the action\nHumanInterrupt\ndeprecated\n\u00b6\nBases:\nTypedDict\nDeprecated\nHumanInterrupt has been moved to\nlangchain.agents.interrupt\n. Please update your import to\nfrom langchain.agents.interrupt import HumanInterrupt\n.\nRepresents an interrupt triggered by the graph that requires human intervention.\nThis is passed to the\ninterrupt\nfunction when execution is paused for human input.\nATTRIBUTE\nDESCRIPTION\naction_request\nThe specific action being requested from the human\nconfig\nConfiguration defining what actions are allowed\ndescription\nOptional detailed description of what input is needed\nExample\n# Extract a tool call from the state and create an interrupt request\nrequest\n=\nHumanInterrupt\n(\naction_request\n=\nActionRequest\n(\naction\n=\n\"run_command\"\n,\n# The action being requested\nargs\n=\n{\n\"command\"\n:\n\"ls\"\n,\n\"args\"\n:\n[\n\"-l\"\n]}\n# Arguments for the action\n),\nconfig\n=\nHumanInterruptConfig\n(\nallow_ignore\n=\nTrue\n,\n# Allow skipping this step\nallow_respond\n=\nTrue\n,\n# Allow text feedback\nallow_edit\n=\nFalse\n,\n# Don't allow editing\nallow_accept\n=\nTrue\n# Allow direct acceptance\n),\ndescription\n=\n\"Please review the command before execution\"\n)\n# Send the interrupt request and get the response\nresponse\n=\ninterrupt\n([\nrequest\n])[\n0\n]\nHumanResponse\n\u00b6\nBases:\nTypedDict\nThe response provided by a human to an interrupt, which is returned when graph execution resumes.\nATTRIBUTE\nDESCRIPTION\ntype\nThe type of response:\n'accept'\n: Approves the current state without changes\n'ignore'\n: Skips/ignores the current step\n'response'\n: Provides text feedback or instructions\n'edit'\n: Modifies the current state/content\nTYPE:\nLiteral\n['accept', 'ignore', 'response', 'edit']\nargs\nThe response payload:\nNone\n: For ignore/accept actions\nstr\n: For text responses\nActionRequest\n: For edit actions with updated content\nTYPE:\nNone |\nstr\n|\nActionRequest\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/agents/",
      "title": "Agents (LangGraph) | LangChain Reference",
      "heading": "Agents"
    }
  },
  {
    "page_content": "Supervisor | LangChain Reference\nSkip to content\nLangChain Reference\nSupervisor\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSupervisor\nTable of contents\nsupervisor\ncreate_supervisor\nhandoff\ncreate_handoff_tool\ncreate_forward_message_tool\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nsupervisor\ncreate_supervisor\nhandoff\ncreate_handoff_tool\ncreate_forward_message_tool\nlanggraph-supervisor\n\u00b6\nNote\nWe now recommend using the\nsupervisor pattern directly via tools\nrather than this library for most use cases. The tool-calling approach gives you more control over context engineering and is the recommended pattern in the\nLangChain multi-agent guide\n.\nSee our\nsupervisor tutorial\nfor a step-by-step guide.\nWe're making this library compatible with LangChain 1.0 to help users upgrade their existing code. If you find this library solves a problem that can't be easily addressed with the manual supervisor pattern, we'd love to hear about your use case!\nSee the\nproject description\nfor more details.\nsupervisor\n\u00b6\nFUNCTION\nDESCRIPTION\ncreate_supervisor\nCreate a multi-agent supervisor.\ncreate_supervisor\n\u00b6\ncreate_supervisor\n(\nagents\n:\nlist\n[\nPregel\n],\n*\n,\nmodel\n:\nLanguageModelLike\n,\ntools\n:\nlist\n[\nBaseTool\n|\nCallable\n]\n|\nToolNode\n|\nNone\n=\nNone\n,\nprompt\n:\nPrompt\n|\nNone\n=\nNone\n,\nresponse_format\n:\nStructuredResponseSchema\n|\ntuple\n[\nstr\n,\nStructuredResponseSchema\n]\n|\nNone\n=\nNone\n,\npre_model_hook\n:\nRunnableLike\n|\nNone\n=\nNone\n,\npost_model_hook\n:\nRunnableLike\n|\nNone\n=\nNone\n,\nparallel_tool_calls\n:\nbool\n=\nFalse\n,\nstate_schema\n:\nStateSchemaType\n|\nNone\n=\nNone\n,\ncontext_schema\n:\nType\n[\nAny\n]\n|\nNone\n=\nNone\n,\noutput_mode\n:\nOutputMode\n=\n\"last_message\"\n,\nadd_handoff_messages\n:\nbool\n=\nTrue\n,\nhandoff_tool_prefix\n:\nstr\n|\nNone\n=\nNone\n,\nadd_handoff_back_messages\n:\nbool\n|\nNone\n=\nNone\n,\nsupervisor_name\n:\nstr\n=\n\"supervisor\"\n,\ninclude_agent_name\n:\nAgentNameMode\n|\nNone\n=\nNone\n,\n**\ndeprecated_kwargs\n:\nUnpack\n[\nDeprecatedKwargs\n],\n)\n->\nStateGraph\nCreate a multi-agent supervisor.\nPARAMETER\nDESCRIPTION\nagents\nList of agents to manage.\nAn agent can be a LangGraph\nCompiledStateGraph\n,\na functional API workflow, or any other\nPregel\nobject.\nTYPE:\nlist\n[\nPregel\n]\nmodel\nLanguage model to use for the supervisor\nTYPE:\nLanguageModelLike\ntools\nTools to use for the supervisor\nTYPE:\nlist\n[\nBaseTool\n|\nCallable\n] |\nToolNode\n| None\nDEFAULT:\nNone\nprompt\nOptional prompt to use for the supervisor.\nCan be one of:\nstr\n: This is converted to a\nSystemMessage\nand added to the beginning of the list of messages in\nstate[\"messages\"]\n.\nSystemMessage\n: this is added to the beginning of the list of messages in\nstate[\"messages\"]\n.\nCallable\n: This function should take in full graph state and the output is then passed to the language model.\nRunnable\n: This runnable should take in full graph state and the output is then passed to the language model.\nTYPE:\nPrompt\n| None\nDEFAULT:\nNone\nresponse_format\nAn optional schema for the final supervisor output.\nIf provided, output will be formatted to match the given schema and returned in the\n'structured_response'\nstate key.\nIf not provided,\nstructured_response\nwill not be present in the output state.\nCan be passed in as:\nAn OpenAI function/tool schema,\nA JSON Schema,\nA TypedDict class,\nA Pydantic class.\nA tuple\n(prompt, schema)\n, where schema is one of the above.\nThe prompt will be used together with the model that is being used to generate the structured response.\nImportant\nresponse_format\nrequires the model to support\n.with_structured_output\nNote\nresponse_format\nrequires\nstructured_response\nkey in your state schema.\nYou can use the prebuilt\nlanggraph.prebuilt.chat_agent_executor.AgentStateWithStructuredResponse\n.\nTYPE:\nStructuredResponseSchema\n|\ntuple\n[\nstr\n,\nStructuredResponseSchema\n] | None\nDEFAULT:\nNone\npre_model_hook\nAn optional node to add before the LLM node in the supervisor agent (i.e., the node that calls the LLM).\nUseful for managing long message histories (e.g., message trimming, summarization, etc.).\nPre-model hook must be a callable or a runnable that takes in current graph state and returns a state update in the form of\n# At least one of `messages` or `llm_input_messages` MUST be provided\n{\n# If provided, will UPDATE the `messages` in the state\n\"messages\"\n:\n[\nRemoveMessage\n(\nid\n=\nREMOVE_ALL_MESSAGES\n),\n...\n],\n# If provided, will be used as the input to the LLM,\n# and will NOT UPDATE `messages` in the state\n\"llm_input_messages\"\n:\n[\n...\n],\n# Any other state keys that need to be propagated\n...\n}\nImportant\nAt least one of\nmessages\nor\nllm_input_messages\nMUST be provided and will be used as an input to the\nagent\nnode.\nThe rest of the keys will be added to the graph state.\nWarning\nIf you are returning\nmessages\nin the pre-model hook, you should OVERWRITE the\nmessages\nkey by doing the following:\n{\n\"messages\"\n:\n[\nRemoveMessage\n(\nid\n=\nREMOVE_ALL_MESSAGES\n),\n*\nnew_messages\n]\n...\n}\nTYPE:\nRunnableLike\n| None\nDEFAULT:\nNone\npost_model_hook\nAn optional node to add after the LLM node in the supervisor agent (i.e., the node that calls the LLM).\nUseful for implementing human-in-the-loop, guardrails, validation, or other post-processing.\nPost-model hook must be a callable or a runnable that takes in current graph state and returns a state update.\nTYPE:\nRunnableLike\n| None\nDEFAULT:\nNone\nparallel_tool_calls\nWhether to allow the supervisor LLM to call tools in parallel (only OpenAI and Anthropic).\nUse this to control whether the supervisor can hand off to multiple agents at once.\nIf\nTrue\n, will enable parallel tool calls.\nIf\nFalse\n, will disable parallel tool calls.\nImportant\nThis is currently supported only by OpenAI and Anthropic models.\nTo control parallel tool calling for other providers, add explicit instructions for tool use to the system prompt.\nTYPE:\nbool\nDEFAULT:\nFalse\nstate_schema\nState schema to use for the supervisor graph.\nTYPE:\nStateSchemaType\n| None\nDEFAULT:\nNone\ncontext_schema\nSpecifies the schema for the context object that will be passed to the workflow.\nTYPE:\nType\n[\nAny\n] | None\nDEFAULT:\nNone\noutput_mode\nMode for adding managed agents' outputs to the message history in the multi-agent workflow.\nCan be one of:\nfull_history\n: Add the entire agent message history\nlast_message\n: Add only the last message\nTYPE:\nOutputMode\nDEFAULT:\n'last_message'\nadd_handoff_messages\nWhether to add a pair of\n(AIMessage, ToolMessage)\nto the message history\nwhen a handoff occurs.\nTYPE:\nbool\nDEFAULT:\nTrue\nhandoff_tool_prefix\nOptional prefix for the handoff tools (e.g.,\n'delegate_to_'\nor\n'transfer_to_'\n)\nIf provided, the handoff tools will be named\nhandoff_tool_prefix_agent_name\n.\nIf not provided, the handoff tools will be named\ntransfer_to_agent_name\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nadd_handoff_back_messages\nWhether to add a pair of\n(AIMessage, ToolMessage)\nto the message history\nwhen returning control to the supervisor to indicate that a handoff has occurred.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nsupervisor_name\nName of the supervisor node.\nTYPE:\nstr\nDEFAULT:\n'supervisor'\ninclude_agent_name\nUse to specify how to expose the agent name to the underlying supervisor LLM.\nNone\n: Relies on the LLM provider using the name attribute on the AI message. Currently, only OpenAI supports this.\n'inline'\n: Add the agent name directly into the content field of the AI message using XML-style tags.\nExample:\n\"How can I help you\"\n->\n\"<name>agent_name</name><content>How can I help you?</content>\"\nTYPE:\nAgentNameMode\n| None\nDEFAULT:\nNone\nExample\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph_supervisor\nimport\ncreate_supervisor\nfrom\nlanggraph.prebuilt\nimport\ncreate_react_agent\n# Create specialized agents\ndef\nadd\n(\na\n:\nfloat\n,\nb\n:\nfloat\n)\n->\nfloat\n:\n'''Add two numbers.'''\nreturn\na\n+\nb\ndef\nweb_search\n(\nquery\n:\nstr\n)\n->\nstr\n:\n'''Search the web for information.'''\nreturn\n'Here are the headcounts for each of the FAANG companies in 2024...'\nmath_agent\n=\ncreate_react_agent\n(\nmodel\n=\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[\nadd\n],\nname\n=\n\"math_expert\"\n,\n)\nresearch_agent\n=\ncreate_react_agent\n(\nmodel\n=\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[\nweb_search\n],\nname\n=\n\"research_expert\"\n,\n)\n# Create supervisor workflow\nworkflow\n=\ncreate_supervisor\n(\n[\nresearch_agent\n,\nmath_agent\n],\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-5-mini\"\n),\n)\n# Compile and run\napp\n=\nworkflow\n.\ncompile\n()\nresult\n=\napp\n.\ninvoke\n({\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's the combined headcount of the FAANG companies in 2024?\"\n}\n]\n})\nhandoff\n\u00b6\nFUNCTION\nDESCRIPTION\ncreate_handoff_tool\nCreate a tool that can handoff control to the requested agent.\ncreate_forward_message_tool\nCreate a tool the supervisor can use to forward a worker message by name.\ncreate_handoff_tool\n\u00b6\ncreate_handoff_tool\n(\n*\n,\nagent_name\n:\nstr\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\nadd_handoff_messages\n:\nbool\n=\nTrue\n,\n)\n->\nBaseTool\nCreate a tool that can handoff control to the requested agent.\nPARAMETER\nDESCRIPTION\nagent_name\nThe name of the agent to handoff control to, i.e. the name of the\nagent node in the multi-agent graph.\nAgent names should be simple, clear and unique, preferably in snake_case,\nalthough you are only limited to the names accepted by LangGraph\nnodes as well as the tool names accepted by LLM providers\n(the tool name will look like this:\ntransfer_to_<agent_name>\n).\nTYPE:\nstr\nname\nOptional name of the tool to use for the handoff.\nIf not provided, the tool name will be\ntransfer_to_<agent_name>\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nOptional description for the handoff tool.\nIf not provided, the description will be\nAsk agent <agent_name> for help\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nadd_handoff_messages\nWhether to add handoff messages to the message history.\nIf\nFalse\n, the handoff messages will be omitted from the message history.\nTYPE:\nbool\nDEFAULT:\nTrue\ncreate_forward_message_tool\n\u00b6\ncreate_forward_message_tool\n(\nsupervisor_name\n:\nstr\n=\n'supervisor'\n)\n->\nBaseTool\nCreate a tool the supervisor can use to forward a worker message by name.\nThis helps avoid information loss any time the supervisor rewrites a worker query\nto the user and also can save some tokens.\nPARAMETER\nDESCRIPTION\nsupervisor_name\nThe name of the supervisor node (used for namespacing the tool).\nTYPE:\nstr\nDEFAULT:\n'supervisor'\nRETURNS\nDESCRIPTION\nBaseTool\nThe\n'forward_message'\ntool.\nTYPE:\nBaseTool\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/supervisor/",
      "title": "Supervisor | LangChain Reference",
      "heading": "langgraph-supervisor\u00b6"
    }
  },
  {
    "page_content": "Swarm | LangChain Reference\nSkip to content\nLangChain Reference\nSwarm\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nSwarm\nTable of contents\nswarm\nSwarmState\ncreate_swarm\nadd_active_agent_router\nhandoff\ncreate_handoff_tool\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nswarm\nSwarmState\ncreate_swarm\nadd_active_agent_router\nhandoff\ncreate_handoff_tool\nlanggraph-swarm\n\u00b6\nSee the\nproject description\nfor more details.\nswarm\n\u00b6\nFUNCTION\nDESCRIPTION\ncreate_swarm\nCreate a multi-agent swarm.\nadd_active_agent_router\nAdd a router to the currently active agent to the\nStateGraph\n.\nSwarmState\n\u00b6\nBases:\nMessagesState\nState schema for the multi-agent swarm.\ncreate_swarm\n\u00b6\ncreate_swarm\n(\nagents\n:\nlist\n[\nPregel\n],\n*\n,\ndefault_active_agent\n:\nstr\n,\nstate_schema\n:\nStateSchemaType\n=\nSwarmState\n,\ncontext_schema\n:\ntype\n[\nAny\n]\n|\nNone\n=\nNone\n,\n**\ndeprecated_kwargs\n:\nUnpack\n[\nDeprecatedKwargs\n],\n)\n->\nStateGraph\nCreate a multi-agent swarm.\nPARAMETER\nDESCRIPTION\nagents\nList of agents to add to the swarm\nAn agent can be a LangGraph\nCompiledStateGraph\n,\na functional API\nworkflow\n,\nor any other\nPregel\nobject.\nTYPE:\nlist\n[\nPregel\n]\ndefault_active_agent\nName of the agent to route to by default (if no agents are currently active).\nTYPE:\nstr\nstate_schema\nState schema to use for the multi-agent graph.\nTYPE:\nStateSchemaType\nDEFAULT:\nSwarmState\ncontext_schema\nSpecifies the schema for the context object that will be passed to the workflow.\nTYPE:\ntype\n[\nAny\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nStateGraph\nA multi-agent swarm\nStateGraph\n.\nExample\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph_swarm\nimport\ncreate_handoff_tool\n,\ncreate_swarm\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n->\nint\n:\n'''Add two numbers'''\nreturn\na\n+\nb\nalice\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[\nadd\n,\ncreate_handoff_tool\n(\nagent_name\n=\n\"Bob\"\n,\ndescription\n=\n\"Transfer to Bob\"\n,\n),\n],\nsystem_prompt\n=\n\"You are Alice, an addition expert.\"\n,\nname\n=\n\"Alice\"\n,\n)\nbob\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[\ncreate_handoff_tool\n(\nagent_name\n=\n\"Alice\"\n,\ndescription\n=\n\"Transfer to Alice, she can help with math\"\n,\n),\n],\nsystem_prompt\n=\n\"You are Bob, you speak like a pirate.\"\n,\nname\n=\n\"Bob\"\n,\n)\ncheckpointer\n=\nInMemorySaver\n()\nworkflow\n=\ncreate_swarm\n(\n[\nalice\n,\nbob\n],\ndefault_active_agent\n=\n\"Alice\"\n)\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\ncheckpointer\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}}\nturn_1\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"i'd like to speak to Bob\"\n}]},\nconfig\n,\n)\nturn_2\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's 5 + 7?\"\n}]},\nconfig\n,\n)\nadd_active_agent_router\n\u00b6\nadd_active_agent_router\n(\nbuilder\n:\nStateGraph\n,\n*\n,\nroute_to\n:\nlist\n[\nstr\n],\ndefault_active_agent\n:\nstr\n)\n->\nStateGraph\nAdd a router to the currently active agent to the\nStateGraph\n.\nPARAMETER\nDESCRIPTION\nbuilder\nThe graph builder (\nStateGraph\n) to add the router to.\nTYPE:\nStateGraph\nroute_to\nA list of agent (node) names to route to.\nTYPE:\nlist\n[\nstr\n]\ndefault_active_agent\nName of the agent to route to by default (if no agents are currently active).\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nStateGraph\nStateGraph\nwith the router added.\nExample\nfrom\nlangchain.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.graph\nimport\nStateGraph\nfrom\nlanggraph_swarm\nimport\nSwarmState\n,\ncreate_handoff_tool\n,\nadd_active_agent_router\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n)\n->\nint\n:\n'''Add two numbers'''\nreturn\na\n+\nb\nalice\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[\nadd\n,\ncreate_handoff_tool\n(\nagent_name\n=\n\"Bob\"\n,\ndescription\n=\n\"Transfer to Bob\"\n,\n),\n],\nsystem_prompt\n=\n\"You are Alice, an addition expert.\"\n,\nname\n=\n\"Alice\"\n,\n)\nbob\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[\ncreate_handoff_tool\n(\nagent_name\n=\n\"Alice\"\n,\ndescription\n=\n\"Transfer to Alice, she can help with math\"\n,\n),\n],\nsystem_prompt\n=\n\"You are Bob, you speak like a pirate.\"\n,\nname\n=\n\"Bob\"\n,\n)\ncheckpointer\n=\nInMemorySaver\n()\nworkflow\n=\n(\nStateGraph\n(\nSwarmState\n)\n.\nadd_node\n(\nalice\n,\ndestinations\n=\n(\n\"Bob\"\n,))\n.\nadd_node\n(\nbob\n,\ndestinations\n=\n(\n\"Alice\"\n,))\n)\n# this is the router that enables us to keep track of the last active agent\nworkflow\n=\nadd_active_agent_router\n(\nbuilder\n=\nworkflow\n,\nroute_to\n=\n[\n\"Alice\"\n,\n\"Bob\"\n],\ndefault_active_agent\n=\n\"Alice\"\n,\n)\n# compile the workflow\napp\n=\nworkflow\n.\ncompile\n(\ncheckpointer\n=\ncheckpointer\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}}\nturn_1\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"i'd like to speak to Bob\"\n}]},\nconfig\n,\n)\nturn_2\n=\napp\n.\ninvoke\n(\n{\n\"messages\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's 5 + 7?\"\n}]},\nconfig\n,\n)\nhandoff\n\u00b6\nFUNCTION\nDESCRIPTION\ncreate_handoff_tool\nCreate a tool that can handoff control to the requested agent.\ncreate_handoff_tool\n\u00b6\ncreate_handoff_tool\n(\n*\n,\nagent_name\n:\nstr\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n)\n->\nBaseTool\nCreate a tool that can handoff control to the requested agent.\nPARAMETER\nDESCRIPTION\nagent_name\nThe name of the agent to handoff control to, i.e.\nthe name of the agent node in the multi-agent graph.\nAgent names should be simple, clear and unique, preferably in\nsnake_case\n,\nalthough you are only limited to the names accepted by LangGraph\nnodes as well as the tool names accepted by LLM providers\n(the tool name will look like this:\ntransfer_to_<agent_name>\n).\nTYPE:\nstr\nname\nOptional name of the tool to use for the handoff.\nIf not provided, the tool name will be\ntransfer_to_<agent_name>\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nOptional description for the handoff tool.\nIf not provided, the tool description will be\nAsk agent <agent_name> for help\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/swarm/",
      "title": "Swarm | LangChain Reference",
      "heading": "langgraph-swarm\u00b6"
    }
  },
  {
    "page_content": "Graphs | LangChain Reference\nSkip to content\nLangChain Reference\nGraphs\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nGraphs\nTable of contents\nStateGraph\nadd_node\nadd_edge\nadd_conditional_edges\nadd_sequence\ncompile\nCompiledStateGraph\nstream\nastream\ninvoke\nainvoke\nget_state\naget_state\nget_state_history\naget_state_history\nupdate_state\naupdate_state\nbulk_update_state\nabulk_update_state\nget_graph\naget_graph\nget_subgraphs\naget_subgraphs\nwith_config\nmessage\nadd_messages\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nStateGraph\nadd_node\nadd_edge\nadd_conditional_edges\nadd_sequence\ncompile\nCompiledStateGraph\nstream\nastream\ninvoke\nainvoke\nget_state\naget_state\nget_state_history\naget_state_history\nupdate_state\naupdate_state\nbulk_update_state\nabulk_update_state\nget_graph\naget_graph\nget_subgraphs\naget_subgraphs\nwith_config\nmessage\nadd_messages\nGraphs\nStateGraph\n\u00b6\nBases:\nGeneric\n[\nStateT\n,\nContextT\n,\nInputT\n,\nOutputT\n]\nA graph whose nodes communicate by reading and writing to a shared state.\nThe signature of each node is\nState -> Partial<State>\n.\nEach state key can optionally be annotated with a reducer function that\nwill be used to aggregate the values of that key received from multiple nodes.\nThe signature of a reducer function is\n(Value, Value) -> Value\n.\nWarning\nStateGraph\nis a builder class and cannot be used directly for execution.\nYou must first call\n.compile()\nto create an executable graph that supports\nmethods like\ninvoke()\n,\nstream()\n,\nastream()\n, and\nainvoke()\n. See the\nCompiledStateGraph\ndocumentation for more details.\nPARAMETER\nDESCRIPTION\nstate_schema\nThe schema class that defines the state.\nTYPE:\ntype\n[\nStateT\n]\ncontext_schema\nThe schema class that defines the runtime context.\nUse this to expose immutable context data to your nodes, like\nuser_id\n,\ndb_conn\n, etc.\nTYPE:\ntype\n[\nContextT\n] | None\nDEFAULT:\nNone\ninput_schema\nThe schema class that defines the input to the graph.\nTYPE:\ntype\n[\nInputT\n] | None\nDEFAULT:\nNone\noutput_schema\nThe schema class that defines the output from the graph.\nTYPE:\ntype\n[\nOutputT\n] | None\nDEFAULT:\nNone\nconfig_schema\nDeprecated\nThe\nconfig_schema\nparameter is deprecated in v0.6.0 and support will be removed in v2.0.0.\nPlease use\ncontext_schema\ninstead to specify the schema for run-scoped context.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.graph\nimport\nStateGraph\nfrom\nlanggraph.runtime\nimport\nRuntime\ndef\nreducer\n(\na\n:\nlist\n,\nb\n:\nint\n|\nNone\n)\n->\nlist\n:\nif\nb\nis\nnot\nNone\n:\nreturn\na\n+\n[\nb\n]\nreturn\na\nclass\nState\n(\nTypedDict\n):\nx\n:\nAnnotated\n[\nlist\n,\nreducer\n]\nclass\nContext\n(\nTypedDict\n):\nr\n:\nfloat\ngraph\n=\nStateGraph\n(\nstate_schema\n=\nState\n,\ncontext_schema\n=\nContext\n)\ndef\nnode\n(\nstate\n:\nState\n,\nruntime\n:\nRuntime\n[\nContext\n])\n->\ndict\n:\nr\n=\nruntime\n.\ncontext\n.\nget\n(\n\"r\"\n,\n1.0\n)\nx\n=\nstate\n[\n\"x\"\n][\n-\n1\n]\nnext_value\n=\nx\n*\nr\n*\n(\n1\n-\nx\n)\nreturn\n{\n\"x\"\n:\nnext_value\n}\ngraph\n.\nadd_node\n(\n\"A\"\n,\nnode\n)\ngraph\n.\nset_entry_point\n(\n\"A\"\n)\ngraph\n.\nset_finish_point\n(\n\"A\"\n)\ncompiled\n=\ngraph\n.\ncompile\n()\nstep1\n=\ncompiled\n.\ninvoke\n({\n\"x\"\n:\n0.5\n},\ncontext\n=\n{\n\"r\"\n:\n3.0\n})\n# {'x': [0.5, 0.75]}\nMETHOD\nDESCRIPTION\nadd_node\nAdd a new node to the\nStateGraph\n.\nadd_edge\nAdd a directed edge from the start node (or list of start nodes) to the end node.\nadd_conditional_edges\nAdd a conditional edge from the starting node to any number of destination nodes.\nadd_sequence\nAdd a sequence of nodes that will be executed in the provided order.\ncompile\nCompiles the\nStateGraph\ninto a\nCompiledStateGraph\nobject.\nadd_node\n\u00b6\nadd_node\n(\nnode\n:\nstr\n|\nStateNode\n[\nNodeInputT\n,\nContextT\n],\naction\n:\nStateNode\n[\nNodeInputT\n,\nContextT\n]\n|\nNone\n=\nNone\n,\n*\n,\ndefer\n:\nbool\n=\nFalse\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ninput_schema\n:\ntype\n[\nNodeInputT\n]\n|\nNone\n=\nNone\n,\nretry_policy\n:\nRetryPolicy\n|\nSequence\n[\nRetryPolicy\n]\n|\nNone\n=\nNone\n,\ncache_policy\n:\nCachePolicy\n|\nNone\n=\nNone\n,\ndestinations\n:\ndict\n[\nstr\n,\nstr\n]\n|\ntuple\n[\nstr\n,\n...\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nUnpack\n[\nDeprecatedKwargs\n],\n)\n->\nSelf\nAdd a new node to the\nStateGraph\n.\nPARAMETER\nDESCRIPTION\nnode\nThe function or runnable this node will run.\nIf a string is provided, it will be used as the node name, and action will be used as the function or runnable.\nTYPE:\nstr\n|\nStateNode\n[\nNodeInputT\n,\nContextT\n]\naction\nThe action associated with the node.\nWill be used as the node function or runnable if\nnode\nis a string (node name).\nTYPE:\nStateNode\n[\nNodeInputT\n,\nContextT\n] | None\nDEFAULT:\nNone\ndefer\nWhether to defer the execution of the node until the run is about to end.\nTYPE:\nbool\nDEFAULT:\nFalse\nmetadata\nThe metadata associated with the node.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ninput_schema\nThe input schema for the node. (Default: the graph's state schema)\nTYPE:\ntype\n[\nNodeInputT\n] | None\nDEFAULT:\nNone\nretry_policy\nThe retry policy for the node.\nIf a sequence is provided, the first matching policy will be applied.\nTYPE:\nRetryPolicy\n|\nSequence\n[\nRetryPolicy\n] | None\nDEFAULT:\nNone\ncache_policy\nThe cache policy for the node.\nTYPE:\nCachePolicy\n| None\nDEFAULT:\nNone\ndestinations\nDestinations that indicate where a node can route to.\nUseful for edgeless graphs with nodes that return\nCommand\nobjects.\nIf a\ndict\nis provided, the keys will be used as the target node names and the values will be used as the labels for the edges.\nIf a\ntuple\nis provided, the values will be used as the target node names.\nNote\nThis is only used for graph rendering and doesn't have any effect on the graph execution.\nTYPE:\ndict\n[\nstr\n,\nstr\n] |\ntuple\n[\nstr\n, ...] | None\nDEFAULT:\nNone\nExample\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlanggraph.graph\nimport\nSTART\n,\nStateGraph\nclass\nState\n(\nTypedDict\n):\nx\n:\nint\ndef\nmy_node\n(\nstate\n:\nState\n,\nconfig\n:\nRunnableConfig\n)\n->\nState\n:\nreturn\n{\n\"x\"\n:\nstate\n[\n\"x\"\n]\n+\n1\n}\nbuilder\n=\nStateGraph\n(\nState\n)\nbuilder\n.\nadd_node\n(\nmy_node\n)\n# node name will be 'my_node'\nbuilder\n.\nadd_edge\n(\nSTART\n,\n\"my_node\"\n)\ngraph\n=\nbuilder\n.\ncompile\n()\ngraph\n.\ninvoke\n({\n\"x\"\n:\n1\n})\n# {'x': 2}\nCustomize the name:\nbuilder\n=\nStateGraph\n(\nState\n)\nbuilder\n.\nadd_node\n(\n\"my_fair_node\"\n,\nmy_node\n)\nbuilder\n.\nadd_edge\n(\nSTART\n,\n\"my_fair_node\"\n)\ngraph\n=\nbuilder\n.\ncompile\n()\ngraph\n.\ninvoke\n({\n\"x\"\n:\n1\n})\n# {'x': 2}\nRETURNS\nDESCRIPTION\nSelf\nThe instance of the\nStateGraph\n, allowing for method chaining.\nTYPE:\nSelf\nadd_edge\n\u00b6\nadd_edge\n(\nstart_key\n:\nstr\n|\nlist\n[\nstr\n],\nend_key\n:\nstr\n)\n->\nSelf\nAdd a directed edge from the start node (or list of start nodes) to the end node.\nWhen a single start node is provided, the graph will wait for that node to complete\nbefore executing the end node. When multiple start nodes are provided,\nthe graph will wait for ALL of the start nodes to complete before executing the end node.\nPARAMETER\nDESCRIPTION\nstart_key\nThe key(s) of the start node(s) of the edge.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nend_key\nThe key of the end node of the edge.\nTYPE:\nstr\nRAISES\nDESCRIPTION\nValueError\nIf the start key is\n'END'\nor if the start key or end key is not present in the graph.\nRETURNS\nDESCRIPTION\nSelf\nThe instance of the\nStateGraph\n, allowing for method chaining.\nTYPE:\nSelf\nadd_conditional_edges\n\u00b6\nadd_conditional_edges\n(\nsource\n:\nstr\n,\npath\n:\nCallable\n[\n...\n,\nHashable\n|\nSequence\n[\nHashable\n]]\n|\nCallable\n[\n...\n,\nAwaitable\n[\nHashable\n|\nSequence\n[\nHashable\n]]]\n|\nRunnable\n[\nAny\n,\nHashable\n|\nSequence\n[\nHashable\n]],\npath_map\n:\ndict\n[\nHashable\n,\nstr\n]\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n)\n->\nSelf\nAdd a conditional edge from the starting node to any number of destination nodes.\nPARAMETER\nDESCRIPTION\nsource\nThe starting node. This conditional edge will run when\nexiting this node.\nTYPE:\nstr\npath\nThe callable that determines the next node or nodes.\nIf not specifying\npath_map\nit should return one or more nodes.\nIf it returns\n'END'\n, the graph will stop execution.\nTYPE:\nCallable\n[...,\nHashable\n|\nSequence\n[\nHashable\n]] |\nCallable\n[...,\nAwaitable\n[\nHashable\n|\nSequence\n[\nHashable\n]]] |\nRunnable\n[\nAny\n,\nHashable\n|\nSequence\n[\nHashable\n]]\npath_map\nOptional mapping of paths to node names.\nIf omitted the paths returned by\npath\nshould be node names.\nTYPE:\ndict\n[\nHashable\n,\nstr\n] |\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nSelf\nThe instance of the graph, allowing for method chaining.\nTYPE:\nSelf\nWarning\nWithout type hints on the\npath\nfunction's return value (e.g.,\n-> Literal[\"foo\", \"__end__\"]:\n)\nor a path_map, the graph visualization assumes the edge could transition to any node in the graph.\nadd_sequence\n\u00b6\nadd_sequence\n(\nnodes\n:\nSequence\n[\nStateNode\n[\nNodeInputT\n,\nContextT\n]\n|\ntuple\n[\nstr\n,\nStateNode\n[\nNodeInputT\n,\nContextT\n]]\n],\n)\n->\nSelf\nAdd a sequence of nodes that will be executed in the provided order.\nPARAMETER\nDESCRIPTION\nnodes\nA sequence of\nStateNode\n(callables that accept a\nstate\narg) or\n(name, StateNode)\ntuples.\nIf no names are provided, the name will be inferred from the node object (e.g. a\nRunnable\nor a\nCallable\nname).\nEach node will be executed in the order provided.\nTYPE:\nSequence\n[\nStateNode\n[\nNodeInputT\n,\nContextT\n] |\ntuple\n[\nstr\n,\nStateNode\n[\nNodeInputT\n,\nContextT\n]]]\nRAISES\nDESCRIPTION\nValueError\nIf the sequence is empty.\nValueError\nIf the sequence contains duplicate node names.\nRETURNS\nDESCRIPTION\nSelf\nThe instance of the\nStateGraph\n, allowing for method chaining.\nTYPE:\nSelf\ncompile\n\u00b6\ncompile\n(\ncheckpointer\n:\nCheckpointer\n=\nNone\n,\n*\n,\ncache\n:\nBaseCache\n|\nNone\n=\nNone\n,\nstore\n:\nBaseStore\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nAll\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nAll\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndebug\n:\nbool\n=\nFalse\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nCompiledStateGraph\n[\nStateT\n,\nContextT\n,\nInputT\n,\nOutputT\n]\nCompiles the\nStateGraph\ninto a\nCompiledStateGraph\nobject.\nThe compiled graph implements the\nRunnable\ninterface and can be invoked,\nstreamed, batched, and run asynchronously.\nPARAMETER\nDESCRIPTION\ncheckpointer\nA checkpoint saver object or flag.\nIf provided, this\nCheckpointer\nserves as a fully versioned \"short-term memory\" for the graph,\nallowing it to be paused, resumed, and replayed from any point.\nIf\nNone\n, it may inherit the parent graph's checkpointer when used as a subgraph.\nIf\nFalse\n, it will not use or inherit any checkpointer.\nTYPE:\nCheckpointer\nDEFAULT:\nNone\ninterrupt_before\nAn optional list of node names to interrupt before.\nTYPE:\nAll\n|\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nAn optional list of node names to interrupt after.\nTYPE:\nAll\n|\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ndebug\nA flag indicating whether to enable debug mode.\nTYPE:\nbool\nDEFAULT:\nFalse\nname\nThe name to use for the compiled graph.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCompiledStateGraph\nThe compiled\nStateGraph\n.\nTYPE:\nCompiledStateGraph\n[\nStateT\n,\nContextT\n,\nInputT\n,\nOutputT\n]\nCompiledStateGraph\n\u00b6\nBases:\nPregel\n[\nStateT\n,\nContextT\n,\nInputT\n,\nOutputT\n]\n,\nGeneric\n[\nStateT\n,\nContextT\n,\nInputT\n,\nOutputT\n]\nMETHOD\nDESCRIPTION\nstream\nStream graph steps for a single input.\nastream\nAsynchronously stream graph steps for a single input.\ninvoke\nRun the graph with a single input and config.\nainvoke\nAsynchronously run the graph with a single input and config.\nget_state\nGet the current state of the graph.\naget_state\nGet the current state of the graph.\nget_state_history\nGet the history of the state of the graph.\naget_state_history\nAsynchronously get the history of the state of the graph.\nupdate_state\nUpdate the state of the graph with the given values, as if they came from\naupdate_state\nAsynchronously update the state of the graph with the given values, as if they came from\nbulk_update_state\nApply updates to the graph state in bulk. Requires a checkpointer to be set.\nabulk_update_state\nAsynchronously apply updates to the graph state in bulk. Requires a checkpointer to be set.\nget_graph\nReturn a drawable representation of the computation graph.\naget_graph\nReturn a drawable representation of the computation graph.\nget_subgraphs\nGet the subgraphs of the graph.\naget_subgraphs\nGet the subgraphs of the graph.\nwith_config\nCreate a copy of the Pregel object with an updated config.\nstream\n\u00b6\nstream\n(\ninput\n:\nInputT\n|\nCommand\n|\nNone\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ncontext\n:\nContextT\n|\nNone\n=\nNone\n,\nstream_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n|\nNone\n=\nNone\n,\nprint_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n=\n(),\noutput_keys\n:\nstr\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndurability\n:\nDurability\n|\nNone\n=\nNone\n,\nsubgraphs\n:\nbool\n=\nFalse\n,\ndebug\n:\nbool\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nUnpack\n[\nDeprecatedKwargs\n],\n)\n->\nIterator\n[\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n]\nStream graph steps for a single input.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the graph.\nTYPE:\nInputT\n|\nCommand\n| None\nconfig\nThe configuration to use for the run.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ncontext\nThe static context to use for the run.\nAdded in version 0.6.0\nTYPE:\nContextT\n| None\nDEFAULT:\nNone\nstream_mode\nThe mode to stream output, defaults to\nself.stream_mode\n.\nOptions are:\n\"values\"\n: Emit all values in the state after each step, including interrupts.\nWhen used with functional API, values are emitted once at the end of the workflow.\n\"updates\"\n: Emit only the node or task names and updates returned by the nodes or tasks after each step.\nIf multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n\"custom\"\n: Emit custom data from inside nodes or tasks using\nStreamWriter\n.\n\"messages\"\n: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\nWill be emitted as 2-tuples\n(LLM token, metadata)\n.\n\"checkpoints\"\n: Emit an event when a checkpoint is created, in the same format as returned by\nget_state()\n.\n\"tasks\"\n: Emit events when tasks start and finish, including their results and errors.\n\"debug\"\n: Emit debug events with as much information as possible for each step.\nYou can pass a list as the\nstream_mode\nparameter to stream multiple modes at once.\nThe streamed outputs will be tuples of\n(mode, data)\n.\nSee\nLangGraph streaming guide\nfor more details.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n] | None\nDEFAULT:\nNone\nprint_mode\nAccepts the same values as\nstream_mode\n, but only prints the output to the console, for debugging purposes.\nDoes not affect the output of the graph in any way.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\nDEFAULT:\n()\noutput_keys\nThe keys to stream, defaults to all non-context channels.\nTYPE:\nstr\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_before\nNodes to interrupt before, defaults to all nodes in the graph.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nNodes to interrupt after, defaults to all nodes in the graph.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ndurability\nThe durability mode for the graph execution, defaults to\n\"async\"\n.\nOptions are:\n\"sync\"\n: Changes are persisted synchronously before the next step starts.\n\"async\"\n: Changes are persisted asynchronously while the next step executes.\n\"exit\"\n: Changes are persisted only when the graph exits.\nTYPE:\nDurability\n| None\nDEFAULT:\nNone\nsubgraphs\nWhether to stream events from inside subgraphs, defaults to\nFalse\n.\nIf\nTrue\n, the events will be emitted as tuples\n(namespace, data)\n,\nor\n(namespace, mode, data)\nif\nstream_mode\nis a list,\nwhere\nnamespace\nis a tuple with the path to the node where a subgraph is invoked,\ne.g.\n(\"parent_node:<task_id>\", \"child_node:<task_id>\")\n.\nSee\nLangGraph streaming guide\nfor more details.\nTYPE:\nbool\nDEFAULT:\nFalse\nYIELDS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n] |\nAny\nThe output of each step in the graph. The output shape depends on the\nstream_mode\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInputT\n|\nCommand\n|\nNone\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ncontext\n:\nContextT\n|\nNone\n=\nNone\n,\nstream_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n|\nNone\n=\nNone\n,\nprint_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n=\n(),\noutput_keys\n:\nstr\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndurability\n:\nDurability\n|\nNone\n=\nNone\n,\nsubgraphs\n:\nbool\n=\nFalse\n,\ndebug\n:\nbool\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nUnpack\n[\nDeprecatedKwargs\n],\n)\n->\nAsyncIterator\n[\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n]\nAsynchronously stream graph steps for a single input.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the graph.\nTYPE:\nInputT\n|\nCommand\n| None\nconfig\nThe configuration to use for the run.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ncontext\nThe static context to use for the run.\nAdded in version 0.6.0\nTYPE:\nContextT\n| None\nDEFAULT:\nNone\nstream_mode\nThe mode to stream output, defaults to\nself.stream_mode\n.\nOptions are:\n\"values\"\n: Emit all values in the state after each step, including interrupts.\nWhen used with functional API, values are emitted once at the end of the workflow.\n\"updates\"\n: Emit only the node or task names and updates returned by the nodes or tasks after each step.\nIf multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n\"custom\"\n: Emit custom data from inside nodes or tasks using\nStreamWriter\n.\n\"messages\"\n: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\nWill be emitted as 2-tuples\n(LLM token, metadata)\n.\n\"checkpoints\"\n: Emit an event when a checkpoint is created, in the same format as returned by\nget_state()\n.\n\"tasks\"\n: Emit events when tasks start and finish, including their results and errors.\n\"debug\"\n: Emit debug events with as much information as possible for each step.\nYou can pass a list as the\nstream_mode\nparameter to stream multiple modes at once.\nThe streamed outputs will be tuples of\n(mode, data)\n.\nSee\nLangGraph streaming guide\nfor more details.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n] | None\nDEFAULT:\nNone\nprint_mode\nAccepts the same values as\nstream_mode\n, but only prints the output to the console, for debugging purposes.\nDoes not affect the output of the graph in any way.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\nDEFAULT:\n()\noutput_keys\nThe keys to stream, defaults to all non-context channels.\nTYPE:\nstr\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_before\nNodes to interrupt before, defaults to all nodes in the graph.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nNodes to interrupt after, defaults to all nodes in the graph.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ndurability\nThe durability mode for the graph execution, defaults to\n\"async\"\n.\nOptions are:\n\"sync\"\n: Changes are persisted synchronously before the next step starts.\n\"async\"\n: Changes are persisted asynchronously while the next step executes.\n\"exit\"\n: Changes are persisted only when the graph exits.\nTYPE:\nDurability\n| None\nDEFAULT:\nNone\nsubgraphs\nWhether to stream events from inside subgraphs, defaults to\nFalse\n.\nIf\nTrue\n, the events will be emitted as tuples\n(namespace, data)\n,\nor\n(namespace, mode, data)\nif\nstream_mode\nis a list,\nwhere\nnamespace\nis a tuple with the path to the node where a subgraph is invoked,\ne.g.\n(\"parent_node:<task_id>\", \"child_node:<task_id>\")\n.\nSee\nLangGraph streaming guide\nfor more details.\nTYPE:\nbool\nDEFAULT:\nFalse\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ndict\n[\nstr\n,\nAny\n] |\nAny\n]\nThe output of each step in the graph. The output shape depends on the\nstream_mode\n.\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\nInputT\n|\nCommand\n|\nNone\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ncontext\n:\nContextT\n|\nNone\n=\nNone\n,\nstream_mode\n:\nStreamMode\n=\n\"values\"\n,\nprint_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n=\n(),\noutput_keys\n:\nstr\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndurability\n:\nDurability\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nAny\nRun the graph with a single input and config.\nPARAMETER\nDESCRIPTION\ninput\nThe input data for the graph. It can be a dictionary or any other type.\nTYPE:\nInputT\n|\nCommand\n| None\nconfig\nThe configuration for the graph run.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ncontext\nThe static context to use for the run.\nAdded in version 0.6.0\nTYPE:\nContextT\n| None\nDEFAULT:\nNone\nstream_mode\nThe stream mode for the graph run.\nTYPE:\nStreamMode\nDEFAULT:\n'values'\nprint_mode\nAccepts the same values as\nstream_mode\n, but only prints the output to the console, for debugging purposes.\nDoes not affect the output of the graph in any way.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\nDEFAULT:\n()\noutput_keys\nThe output keys to retrieve from the graph run.\nTYPE:\nstr\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_before\nThe nodes to interrupt the graph run before.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nThe nodes to interrupt the graph run after.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ndurability\nThe durability mode for the graph execution, defaults to\n\"async\"\n.\nOptions are:\n\"sync\"\n: Changes are persisted synchronously before the next step starts.\n\"async\"\n: Changes are persisted asynchronously while the next step executes.\n\"exit\"\n: Changes are persisted only when the graph exits.\nTYPE:\nDurability\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the graph run.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n] |\nAny\nThe output of the graph run. If\nstream_mode\nis\n\"values\"\n, it returns the latest output.\ndict\n[\nstr\n,\nAny\n] |\nAny\nIf\nstream_mode\nis not\n\"values\"\n, it returns a list of output chunks.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nInputT\n|\nCommand\n|\nNone\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ncontext\n:\nContextT\n|\nNone\n=\nNone\n,\nstream_mode\n:\nStreamMode\n=\n\"values\"\n,\nprint_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n=\n(),\noutput_keys\n:\nstr\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndurability\n:\nDurability\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nAny\nAsynchronously run the graph with a single input and config.\nPARAMETER\nDESCRIPTION\ninput\nThe input data for the graph. It can be a dictionary or any other type.\nTYPE:\nInputT\n|\nCommand\n| None\nconfig\nThe configuration for the graph run.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ncontext\nThe static context to use for the run.\nAdded in version 0.6.0\nTYPE:\nContextT\n| None\nDEFAULT:\nNone\nstream_mode\nThe stream mode for the graph run.\nTYPE:\nStreamMode\nDEFAULT:\n'values'\nprint_mode\nAccepts the same values as\nstream_mode\n, but only prints the output to the console, for debugging purposes.\nDoes not affect the output of the graph in any way.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\nDEFAULT:\n()\noutput_keys\nThe output keys to retrieve from the graph run.\nTYPE:\nstr\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_before\nThe nodes to interrupt the graph run before.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nThe nodes to interrupt the graph run after.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ndurability\nThe durability mode for the graph execution, defaults to\n\"async\"\n.\nOptions are:\n\"sync\"\n: Changes are persisted synchronously before the next step starts.\n\"async\"\n: Changes are persisted asynchronously while the next step executes.\n\"exit\"\n: Changes are persisted only when the graph exits.\nTYPE:\nDurability\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the graph run.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n] |\nAny\nThe output of the graph run. If\nstream_mode\nis\n\"values\"\n, it returns the latest output.\ndict\n[\nstr\n,\nAny\n] |\nAny\nIf\nstream_mode\nis not\n\"values\"\n, it returns a list of output chunks.\nget_state\n\u00b6\nget_state\n(\nconfig\n:\nRunnableConfig\n,\n*\n,\nsubgraphs\n:\nbool\n=\nFalse\n)\n->\nStateSnapshot\nGet the current state of the graph.\naget_state\nasync\n\u00b6\naget_state\n(\nconfig\n:\nRunnableConfig\n,\n*\n,\nsubgraphs\n:\nbool\n=\nFalse\n)\n->\nStateSnapshot\nGet the current state of the graph.\nget_state_history\n\u00b6\nget_state_history\n(\nconfig\n:\nRunnableConfig\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nIterator\n[\nStateSnapshot\n]\nGet the history of the state of the graph.\naget_state_history\nasync\n\u00b6\naget_state_history\n(\nconfig\n:\nRunnableConfig\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nAsyncIterator\n[\nStateSnapshot\n]\nAsynchronously get the history of the state of the graph.\nupdate_state\n\u00b6\nupdate_state\n(\nconfig\n:\nRunnableConfig\n,\nvalues\n:\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n|\nNone\n,\nas_node\n:\nstr\n|\nNone\n=\nNone\n,\ntask_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableConfig\nUpdate the state of the graph with the given values, as if they came from\nnode\nas_node\n. If\nas_node\nis not provided, it will be set to the last node\nthat updated the state, if not ambiguous.\naupdate_state\nasync\n\u00b6\naupdate_state\n(\nconfig\n:\nRunnableConfig\n,\nvalues\n:\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n,\nas_node\n:\nstr\n|\nNone\n=\nNone\n,\ntask_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableConfig\nAsynchronously update the state of the graph with the given values, as if they came from\nnode\nas_node\n. If\nas_node\nis not provided, it will be set to the last node\nthat updated the state, if not ambiguous.\nbulk_update_state\n\u00b6\nbulk_update_state\n(\nconfig\n:\nRunnableConfig\n,\nsupersteps\n:\nSequence\n[\nSequence\n[\nStateUpdate\n]]\n)\n->\nRunnableConfig\nApply updates to the graph state in bulk. Requires a checkpointer to be set.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to apply the updates to.\nTYPE:\nRunnableConfig\nsupersteps\nA list of supersteps, each including a list of updates to apply sequentially to a graph state.\nEach update is a tuple of the form\n(values, as_node, task_id)\nwhere\ntask_id\nis optional.\nTYPE:\nSequence\n[\nSequence\n[\nStateUpdate\n]]\nRAISES\nDESCRIPTION\nValueError\nIf no checkpointer is set or no updates are provided.\nInvalidUpdateError\nIf an invalid update is provided.\nRETURNS\nDESCRIPTION\nRunnableConfig\nThe updated config.\nTYPE:\nRunnableConfig\nabulk_update_state\nasync\n\u00b6\nabulk_update_state\n(\nconfig\n:\nRunnableConfig\n,\nsupersteps\n:\nSequence\n[\nSequence\n[\nStateUpdate\n]]\n)\n->\nRunnableConfig\nAsynchronously apply updates to the graph state in bulk. Requires a checkpointer to be set.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to apply the updates to.\nTYPE:\nRunnableConfig\nsupersteps\nA list of supersteps, each including a list of updates to apply sequentially to a graph state.\nEach update is a tuple of the form\n(values, as_node, task_id)\nwhere\ntask_id\nis optional.\nTYPE:\nSequence\n[\nSequence\n[\nStateUpdate\n]]\nRAISES\nDESCRIPTION\nValueError\nIf no checkpointer is set or no updates are provided.\nInvalidUpdateError\nIf an invalid update is provided.\nRETURNS\nDESCRIPTION\nRunnableConfig\nThe updated config.\nTYPE:\nRunnableConfig\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nxray\n:\nint\n|\nbool\n=\nFalse\n)\n->\nGraph\nReturn a drawable representation of the computation graph.\naget_graph\nasync\n\u00b6\naget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nxray\n:\nint\n|\nbool\n=\nFalse\n)\n->\nGraph\nReturn a drawable representation of the computation graph.\nget_subgraphs\n\u00b6\nget_subgraphs\n(\n*\n,\nnamespace\n:\nstr\n|\nNone\n=\nNone\n,\nrecurse\n:\nbool\n=\nFalse\n)\n->\nIterator\n[\ntuple\n[\nstr\n,\nPregelProtocol\n]]\nGet the subgraphs of the graph.\nPARAMETER\nDESCRIPTION\nnamespace\nThe namespace to filter the subgraphs by.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nrecurse\nWhether to recurse into the subgraphs.\nIf\nFalse\n, only the immediate subgraphs will be returned.\nTYPE:\nbool\nDEFAULT:\nFalse\nRETURNS\nDESCRIPTION\nIterator\n[\ntuple\n[\nstr\n,\nPregelProtocol\n]]\nAn iterator of the\n(namespace, subgraph)\npairs.\naget_subgraphs\nasync\n\u00b6\naget_subgraphs\n(\n*\n,\nnamespace\n:\nstr\n|\nNone\n=\nNone\n,\nrecurse\n:\nbool\n=\nFalse\n)\n->\nAsyncIterator\n[\ntuple\n[\nstr\n,\nPregelProtocol\n]]\nGet the subgraphs of the graph.\nPARAMETER\nDESCRIPTION\nnamespace\nThe namespace to filter the subgraphs by.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nrecurse\nWhether to recurse into the subgraphs.\nIf\nFalse\n, only the immediate subgraphs will be returned.\nTYPE:\nbool\nDEFAULT:\nFalse\nRETURNS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nstr\n,\nPregelProtocol\n]]\nAn iterator of the\n(namespace, subgraph)\npairs.\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nSelf\nCreate a copy of the Pregel object with an updated config.\nmessage\n\u00b6\nFUNCTION\nDESCRIPTION\nadd_messages\nMerges two lists of messages, updating existing messages by ID.\nadd_messages\n\u00b6\nadd_messages\n(\nleft\n:\nMessages\n,\nright\n:\nMessages\n,\n*\n,\nformat\n:\nLiteral\n[\n\"langchain-openai\"\n]\n|\nNone\n=\nNone\n,\n)\n->\nMessages\nMerges two lists of messages, updating existing messages by ID.\nBy default, this ensures the state is \"append-only\", unless the\nnew message has the same ID as an existing message.\nPARAMETER\nDESCRIPTION\nleft\nThe base list of\nMessages\n.\nTYPE:\nMessages\nright\nThe list of\nMessages\n(or single\nMessage\n) to merge\ninto the base list.\nTYPE:\nMessages\nformat\nThe format to return messages in. If\nNone\nthen\nMessages\nwill be\nreturned as is. If\nlangchain-openai\nthen\nMessages\nwill be returned as\nBaseMessage\nobjects with their contents formatted to match OpenAI message\nformat, meaning contents can be string,\n'text'\nblocks, or\n'image_url'\nblocks\nand tool responses are returned as their own\nToolMessage\nobjects.\nRequirement\nMust have\nlangchain-core>=0.3.11\ninstalled to use this feature.\nTYPE:\nLiteral\n['langchain-openai'] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nMessages\nA new list of messages with the messages from\nright\nmerged into\nleft\n.\nMessages\nIf a message in\nright\nhas the same ID as a message in\nleft\n, the\nmessage from\nright\nwill replace the message from\nleft\n.\nBasic usage\nfrom\nlangchain_core.messages\nimport\nAIMessage\n,\nHumanMessage\nmsgs1\n=\n[\nHumanMessage\n(\ncontent\n=\n\"Hello\"\n,\nid\n=\n\"1\"\n)]\nmsgs2\n=\n[\nAIMessage\n(\ncontent\n=\n\"Hi there!\"\n,\nid\n=\n\"2\"\n)]\nadd_messages\n(\nmsgs1\n,\nmsgs2\n)\n# [HumanMessage(content='Hello', id='1'), AIMessage(content='Hi there!', id='2')]\nOverwrite existing message\nmsgs1\n=\n[\nHumanMessage\n(\ncontent\n=\n\"Hello\"\n,\nid\n=\n\"1\"\n)]\nmsgs2\n=\n[\nHumanMessage\n(\ncontent\n=\n\"Hello again\"\n,\nid\n=\n\"1\"\n)]\nadd_messages\n(\nmsgs1\n,\nmsgs2\n)\n# [HumanMessage(content='Hello again', id='1')]\nUse in a StateGraph\nfrom\ntyping\nimport\nAnnotated\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph\nclass\nState\n(\nTypedDict\n):\nmessages\n:\nAnnotated\n[\nlist\n,\nadd_messages\n]\nbuilder\n=\nStateGraph\n(\nState\n)\nbuilder\n.\nadd_node\n(\n\"chatbot\"\n,\nlambda\nstate\n:\n{\n\"messages\"\n:\n[(\n\"assistant\"\n,\n\"Hello\"\n)]})\nbuilder\n.\nset_entry_point\n(\n\"chatbot\"\n)\nbuilder\n.\nset_finish_point\n(\n\"chatbot\"\n)\ngraph\n=\nbuilder\n.\ncompile\n()\ngraph\n.\ninvoke\n({})\n# {'messages': [AIMessage(content='Hello', id=...)]}\nUse OpenAI message format\nfrom\ntyping\nimport\nAnnotated\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph\n,\nadd_messages\nclass\nState\n(\nTypedDict\n):\nmessages\n:\nAnnotated\n[\nlist\n,\nadd_messages\n(\nformat\n=\n\"langchain-openai\"\n)]\ndef\nchatbot_node\n(\nstate\n:\nState\n)\n->\nlist\n:\nreturn\n{\n\"messages\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"Here's an image:\"\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n},\n},\n{\n\"type\"\n:\n\"image\"\n,\n\"source\"\n:\n{\n\"type\"\n:\n\"base64\"\n,\n\"media_type\"\n:\n\"image/jpeg\"\n,\n\"data\"\n:\n\"1234\"\n,\n},\n},\n],\n},\n]\n}\nbuilder\n=\nStateGraph\n(\nState\n)\nbuilder\n.\nadd_node\n(\n\"chatbot\"\n,\nchatbot_node\n)\nbuilder\n.\nset_entry_point\n(\n\"chatbot\"\n)\nbuilder\n.\nset_finish_point\n(\n\"chatbot\"\n)\ngraph\n=\nbuilder\n.\ncompile\n()\ngraph\n.\ninvoke\n({\n\"messages\"\n:\n[]})\n# {\n#     'messages': [\n#         HumanMessage(\n#             content=[\n#                 {\"type\": \"text\", \"text\": \"Here's an image:\"},\n#                 {\n#                     \"type\": \"image_url\",\n#                     \"image_url\": {\"url\": \"data:image/jpeg;base64,1234\"},\n#                 },\n#             ],\n#         ),\n#     ]\n# }\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/graphs/",
      "title": "Graphs | LangChain Reference",
      "heading": "Graphs"
    }
  },
  {
    "page_content": "Functional API | LangChain Reference\nSkip to content\nLangChain Reference\nFunctional API\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nFunctional API\nTable of contents\nfunc\nentrypoint\nFunction signature\nInjectable parameters\nState management\nfinal\nvalue\nsave\n__init__\n__call__\ntask\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nfunc\nentrypoint\nFunction signature\nInjectable parameters\nState management\nfinal\nvalue\nsave\n__init__\n__call__\ntask\nFunctional API\nfunc\n\u00b6\nFUNCTION\nDESCRIPTION\ntask\nDefine a LangGraph task using the\ntask\ndecorator.\nentrypoint\n\u00b6\nBases:\nGeneric\n[\nContextT\n]\nDefine a LangGraph workflow using the\nentrypoint\ndecorator.\nFunction signature\n\u00b6\nThe decorated function must accept a\nsingle parameter\n, which serves as the input\nto the function. This input parameter can be of any type. Use a dictionary\nto pass\nmultiple parameters\nto the function.\nInjectable parameters\n\u00b6\nThe decorated function can request access to additional parameters\nthat will be injected automatically at run time. These parameters include:\nParameter\nDescription\nconfig\nA configuration object (aka\nRunnableConfig\n) that holds run-time configuration values.\nprevious\nThe previous return value for the given thread (available only when a checkpointer is provided).\nruntime\nA\nRuntime\nobject that contains information about the current run, including context, store, writer\nThe entrypoint decorator can be applied to sync functions or async functions.\nState management\n\u00b6\nThe\nprevious\nparameter can be used to access the return value of the previous\ninvocation of the entrypoint on the same thread id. This value is only available\nwhen a checkpointer is provided.\nIf you want\nprevious\nto be different from the return value, you can use the\nentrypoint.final\nobject to return a value while saving a different value to the\ncheckpoint.\nPARAMETER\nDESCRIPTION\ncheckpointer\nSpecify a checkpointer to create a workflow that can persist\nits state across runs.\nTYPE:\nBaseCheckpointSaver\n| None\nDEFAULT:\nNone\nstore\nA generalized key-value store. Some implementations may support\nsemantic search capabilities through an optional\nindex\nconfiguration.\nTYPE:\nBaseStore\n| None\nDEFAULT:\nNone\ncache\nA cache to use for caching the results of the workflow.\nTYPE:\nBaseCache\n| None\nDEFAULT:\nNone\ncontext_schema\nSpecifies the schema for the context object that will be\npassed to the workflow.\nTYPE:\ntype\n[\nContextT\n] | None\nDEFAULT:\nNone\ncache_policy\nA cache policy to use for caching the results of the workflow.\nTYPE:\nCachePolicy\n| None\nDEFAULT:\nNone\nretry_policy\nA retry policy (or list of policies) to use for the workflow in case of a failure.\nTYPE:\nRetryPolicy\n|\nSequence\n[\nRetryPolicy\n] | None\nDEFAULT:\nNone\nconfig_schema\nDeprecated\nThe\nconfig_schema\nparameter is deprecated in v0.6.0 and support will be removed in v2.0.0.\nPlease use\ncontext_schema\ninstead to specify the schema for run-scoped context.\nUsing entrypoint and tasks\nimport\ntime\nfrom\nlanggraph.func\nimport\nentrypoint\n,\ntask\nfrom\nlanggraph.types\nimport\ninterrupt\n,\nCommand\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\n@task\ndef\ncompose_essay\n(\ntopic\n:\nstr\n)\n->\nstr\n:\ntime\n.\nsleep\n(\n1.0\n)\n# Simulate slow operation\nreturn\nf\n\"An essay about\n{\ntopic\n}\n\"\n@entrypoint\n(\ncheckpointer\n=\nInMemorySaver\n())\ndef\nreview_workflow\n(\ntopic\n:\nstr\n)\n->\ndict\n:\n\"\"\"Manages the workflow for generating and reviewing an essay.\nThe workflow includes:\n1. Generating an essay about the given topic.\n2. Interrupting the workflow for human review of the generated essay.\nUpon resuming the workflow, compose_essay task will not be re-executed\nas its result is cached by the checkpointer.\nArgs:\ntopic: The subject of the essay.\nReturns:\ndict: A dictionary containing the generated essay and the human review.\n\"\"\"\nessay_future\n=\ncompose_essay\n(\ntopic\n)\nessay\n=\nessay_future\n.\nresult\n()\nhuman_review\n=\ninterrupt\n({\n\"question\"\n:\n\"Please provide a review\"\n,\n\"essay\"\n:\nessay\n})\nreturn\n{\n\"essay\"\n:\nessay\n,\n\"review\"\n:\nhuman_review\n,\n}\n# Example configuration for the workflow\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"some_thread\"\n}\n}\n# Topic for the essay\ntopic\n=\n\"cats\"\n# Stream the workflow to generate the essay and await human review\nfor\nresult\nin\nreview_workflow\n.\nstream\n(\ntopic\n,\nconfig\n):\nprint\n(\nresult\n)\n# Example human review provided after the interrupt\nhuman_review\n=\n\"This essay is great.\"\n# Resume the workflow with the provided human review\nfor\nresult\nin\nreview_workflow\n.\nstream\n(\nCommand\n(\nresume\n=\nhuman_review\n),\nconfig\n):\nprint\n(\nresult\n)\nAccessing the previous return value\nWhen a checkpointer is enabled the function can access the previous return value\nof the previous invocation on the same thread id.\nfrom\ntyping\nimport\nOptional\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\nfrom\nlanggraph.func\nimport\nentrypoint\n@entrypoint\n(\ncheckpointer\n=\nInMemorySaver\n())\ndef\nmy_workflow\n(\ninput_data\n:\nstr\n,\nprevious\n:\nOptional\n[\nstr\n]\n=\nNone\n)\n->\nstr\n:\nreturn\n\"world\"\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"some_thread\"\n}}\nmy_workflow\n.\ninvoke\n(\n\"hello\"\n,\nconfig\n)\nUsing\nentrypoint.final\nto save a value\nThe\nentrypoint.final\nobject allows you to return a value while saving\na different value to the checkpoint. This value will be accessible\nin the next invocation of the entrypoint via the\nprevious\nparameter, as\nlong as the same thread id is used.\nfrom\ntyping\nimport\nAny\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\nfrom\nlanggraph.func\nimport\nentrypoint\n@entrypoint\n(\ncheckpointer\n=\nInMemorySaver\n())\ndef\nmy_workflow\n(\nnumber\n:\nint\n,\n*\n,\nprevious\n:\nAny\n=\nNone\n,\n)\n->\nentrypoint\n.\nfinal\n[\nint\n,\nint\n]:\nprevious\n=\nprevious\nor\n0\n# This will return the previous value to the caller, saving\n# 2 * number to the checkpoint, which will be used in the next invocation\n# for the `previous` parameter.\nreturn\nentrypoint\n.\nfinal\n(\nvalue\n=\nprevious\n,\nsave\n=\n2\n*\nnumber\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"some_thread\"\n}}\nmy_workflow\n.\ninvoke\n(\n3\n,\nconfig\n)\n# 0 (previous was None)\nmy_workflow\n.\ninvoke\n(\n1\n,\nconfig\n)\n# 6 (previous was 3 * 2 from the previous invocation)\nMETHOD\nDESCRIPTION\n__init__\nInitialize the entrypoint decorator.\n__call__\nConvert a function into a Pregel graph.\nfinal\ndataclass\n\u00b6\nBases:\nGeneric\n[\nR\n,\nS\n]\nA primitive that can be returned from an entrypoint.\nThis primitive allows to save a value to the checkpointer distinct from the\nreturn value from the entrypoint.\nDecoupling the return value and the save value\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.func\nimport\nentrypoint\n@entrypoint\n(\ncheckpointer\n=\nInMemorySaver\n())\ndef\nmy_workflow\n(\nnumber\n:\nint\n,\n*\n,\nprevious\n:\nAny\n=\nNone\n,\n)\n->\nentrypoint\n.\nfinal\n[\nint\n,\nint\n]:\nprevious\n=\nprevious\nor\n0\n# This will return the previous value to the caller, saving\n# 2 * number to the checkpoint, which will be used in the next invocation\n# for the `previous` parameter.\nreturn\nentrypoint\n.\nfinal\n(\nvalue\n=\nprevious\n,\nsave\n=\n2\n*\nnumber\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}}\nmy_workflow\n.\ninvoke\n(\n3\n,\nconfig\n)\n# 0 (previous was None)\nmy_workflow\n.\ninvoke\n(\n1\n,\nconfig\n)\n# 6 (previous was 3 * 2 from the previous invocation)\nvalue\ninstance-attribute\n\u00b6\nvalue\n:\nR\nValue to return. A value will always be returned even if it is\nNone\n.\nsave\ninstance-attribute\n\u00b6\nsave\n:\nS\nThe value for the state for the next checkpoint.\nA value will always be saved even if it is\nNone\n.\n__init__\n\u00b6\n__init__\n(\ncheckpointer\n:\nBaseCheckpointSaver\n|\nNone\n=\nNone\n,\nstore\n:\nBaseStore\n|\nNone\n=\nNone\n,\ncache\n:\nBaseCache\n|\nNone\n=\nNone\n,\ncontext_schema\n:\ntype\n[\nContextT\n]\n|\nNone\n=\nNone\n,\ncache_policy\n:\nCachePolicy\n|\nNone\n=\nNone\n,\nretry_policy\n:\nRetryPolicy\n|\nSequence\n[\nRetryPolicy\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nUnpack\n[\nDeprecatedKwargs\n],\n)\n->\nNone\nInitialize the entrypoint decorator.\n__call__\n\u00b6\n__call__\n(\nfunc\n:\nCallable\n[\n...\n,\nAny\n])\n->\nPregel\nConvert a function into a Pregel graph.\nPARAMETER\nDESCRIPTION\nfunc\nThe function to convert. Support both sync and async functions.\nTYPE:\nCallable\n[...,\nAny\n]\nRETURNS\nDESCRIPTION\nPregel\nA Pregel graph.\ntask\n\u00b6\ntask\n(\n__func_or_none__\n:\nCallable\n[\nP\n,\nAwaitable\n[\nT\n]]\n|\nCallable\n[\nP\n,\nT\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\nretry_policy\n:\nRetryPolicy\n|\nSequence\n[\nRetryPolicy\n]\n|\nNone\n=\nNone\n,\ncache_policy\n:\nCachePolicy\n[\nCallable\n[\nP\n,\nstr\n|\nbytes\n]]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nUnpack\n[\nDeprecatedKwargs\n],\n)\n->\n(\nCallable\n[[\nCallable\n[\nP\n,\nAwaitable\n[\nT\n]]\n|\nCallable\n[\nP\n,\nT\n]],\n_TaskFunction\n[\nP\n,\nT\n]]\n|\n_TaskFunction\n[\nP\n,\nT\n]\n)\nDefine a LangGraph task using the\ntask\ndecorator.\nRequires python 3.11 or higher for async functions\nThe\ntask\ndecorator supports both sync and async functions. To use async\nfunctions, ensure that you are using Python 3.11 or higher.\nTasks can only be called from within an\nentrypoint\nor\nfrom within a\nStateGraph\n. A task can be called like a regular function with the\nfollowing differences:\nWhen a checkpointer is enabled, the function inputs and outputs must be serializable.\nThe decorated function can only be called from within an entrypoint or\nStateGraph\n.\nCalling the function produces a future. This makes it easy to parallelize tasks.\nPARAMETER\nDESCRIPTION\nname\nAn optional name for the task. If not provided, the function name will be used.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nretry_policy\nAn optional retry policy (or list of policies) to use for the task in case of a failure.\nTYPE:\nRetryPolicy\n|\nSequence\n[\nRetryPolicy\n] | None\nDEFAULT:\nNone\ncache_policy\nAn optional cache policy to use for the task. This allows caching of the task results.\nTYPE:\nCachePolicy\n[\nCallable\n[\nP\n,\nstr\n|\nbytes\n]] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallable\n[[\nCallable\n[\nP\n,\nAwaitable\n[\nT\n]] |\nCallable\n[\nP\n,\nT\n]],\n_TaskFunction\n[\nP\n,\nT\n]] |\n_TaskFunction\n[\nP\n,\nT\n]\nA callable function when used as a decorator.\nSync Task\nfrom\nlanggraph.func\nimport\nentrypoint\n,\ntask\n@task\ndef\nadd_one_task\n(\na\n:\nint\n)\n->\nint\n:\nreturn\na\n+\n1\n@entrypoint\n()\ndef\nadd_one\n(\nnumbers\n:\nlist\n[\nint\n])\n->\nlist\n[\nint\n]:\nfutures\n=\n[\nadd_one_task\n(\nn\n)\nfor\nn\nin\nnumbers\n]\nresults\n=\n[\nf\n.\nresult\n()\nfor\nf\nin\nfutures\n]\nreturn\nresults\n# Call the entrypoint\nadd_one\n.\ninvoke\n([\n1\n,\n2\n,\n3\n])\n# Returns [2, 3, 4]\nAsync Task\nimport\nasyncio\nfrom\nlanggraph.func\nimport\nentrypoint\n,\ntask\n@task\nasync\ndef\nadd_one_task\n(\na\n:\nint\n)\n->\nint\n:\nreturn\na\n+\n1\n@entrypoint\n()\nasync\ndef\nadd_one\n(\nnumbers\n:\nlist\n[\nint\n])\n->\nlist\n[\nint\n]:\nfutures\n=\n[\nadd_one_task\n(\nn\n)\nfor\nn\nin\nnumbers\n]\nreturn\nasyncio\n.\ngather\n(\n*\nfutures\n)\n# Call the entrypoint\nawait\nadd_one\n.\nainvoke\n([\n1\n,\n2\n,\n3\n])\n# Returns [2, 3, 4]\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/func/",
      "title": "Functional API | LangChain Reference",
      "heading": "Functional API"
    }
  },
  {
    "page_content": "Pregel | LangChain Reference\nSkip to content\nLangChain Reference\nPregel\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nPregel\nTable of contents\nNodeBuilder\nsubscribe_only\nsubscribe_to\nread_from\ndo\nwrite_to\nmeta\nbuild\nPregel\nOverview\nActors\nChannels\nBasic channels: LastValue and Topic\nAdvanced channels: Context and BinaryOperatorAggregate\nExamples\nstream\nastream\ninvoke\nainvoke\nget_state\naget_state\nget_state_history\naget_state_history\nupdate_state\naupdate_state\nbulk_update_state\nabulk_update_state\nget_graph\naget_graph\nget_subgraphs\naget_subgraphs\nwith_config\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nNodeBuilder\nsubscribe_only\nsubscribe_to\nread_from\ndo\nwrite_to\nmeta\nbuild\nPregel\nOverview\nActors\nChannels\nBasic channels: LastValue and Topic\nAdvanced channels: Context and BinaryOperatorAggregate\nExamples\nstream\nastream\ninvoke\nainvoke\nget_state\naget_state\nget_state_history\naget_state_history\nupdate_state\naupdate_state\nbulk_update_state\nabulk_update_state\nget_graph\naget_graph\nget_subgraphs\naget_subgraphs\nwith_config\nPregel\nNodeBuilder\n\u00b6\nMETHOD\nDESCRIPTION\nsubscribe_only\nSubscribe to a single channel.\nsubscribe_to\nAdd channels to subscribe to.\nread_from\nAdds the specified channels to read from, without subscribing to them.\ndo\nAdds the specified node.\nwrite_to\nAdd channel writes.\nmeta\nAdd tags or metadata to the node.\nbuild\nBuilds the node.\nsubscribe_only\n\u00b6\nsubscribe_only\n(\nchannel\n:\nstr\n)\n->\nSelf\nSubscribe to a single channel.\nsubscribe_to\n\u00b6\nsubscribe_to\n(\n*\nchannels\n:\nstr\n,\nread\n:\nbool\n=\nTrue\n)\n->\nSelf\nAdd channels to subscribe to.\nNode will be invoked when any of these channels are updated, with a dict of the\nchannel values as input.\nPARAMETER\nDESCRIPTION\nchannels\nChannel name(s) to subscribe to\nTYPE:\nstr\nDEFAULT:\n()\nread\nIf\nTrue\n, the channels will be included in the input to the node.\nOtherwise, they will trigger the node without being sent in input.\nTYPE:\nbool\nDEFAULT:\nTrue\nRETURNS\nDESCRIPTION\nSelf\nSelf for chaining\nread_from\n\u00b6\nread_from\n(\n*\nchannels\n:\nstr\n)\n->\nSelf\nAdds the specified channels to read from, without subscribing to them.\ndo\n\u00b6\ndo\n(\nnode\n:\nRunnableLike\n)\n->\nSelf\nAdds the specified node.\nwrite_to\n\u00b6\nwrite_to\n(\n*\nchannels\n:\nstr\n|\nChannelWriteEntry\n,\n**\nkwargs\n:\n_WriteValue\n)\n->\nSelf\nAdd channel writes.\nPARAMETER\nDESCRIPTION\n*channels\nChannel names to write to.\nTYPE:\nstr\n|\nChannelWriteEntry\nDEFAULT:\n()\n**kwargs\nChannel name and value mappings.\nTYPE:\n_WriteValue\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nSelf\nSelf for chaining\nmeta\n\u00b6\nmeta\n(\n*\ntags\n:\nstr\n,\n**\nmetadata\n:\nAny\n)\n->\nSelf\nAdd tags or metadata to the node.\nbuild\n\u00b6\nbuild\n()\n->\nPregelNode\nBuilds the node.\nPregel\n\u00b6\nBases:\nPregelProtocol\n[\nStateT\n,\nContextT\n,\nInputT\n,\nOutputT\n]\n,\nGeneric\n[\nStateT\n,\nContextT\n,\nInputT\n,\nOutputT\n]\nPregel manages the runtime behavior for LangGraph applications.\nOverview\n\u00b6\nPregel combines\nactors\nand\nchannels\ninto a single application.\nActors\nread data from channels and write data to channels.\nPregel organizes the execution of the application into multiple steps,\nfollowing the\nPregel Algorithm\n/\nBulk Synchronous Parallel\nmodel.\nEach step consists of three phases:\nPlan\n: Determine which\nactors\nto execute in this step. For example,\nin the first step, select the\nactors\nthat subscribe to the special\ninput\nchannels; in subsequent steps,\nselect the\nactors\nthat subscribe to channels updated in the previous step.\nExecution\n: Execute all selected\nactors\nin parallel,\nuntil all complete, or one fails, or a timeout is reached. During this\nphase, channel updates are invisible to actors until the next step.\nUpdate\n: Update the channels with the values written by the\nactors\nin this step.\nRepeat until no\nactors\nare selected for execution, or a maximum number of\nsteps is reached.\nActors\n\u00b6\nAn\nactor\nis a\nPregelNode\n.\nIt subscribes to channels, reads data from them, and writes data to them.\nIt can be thought of as an\nactor\nin the Pregel algorithm.\nPregelNodes\nimplement LangChain's\nRunnable interface.\nChannels\n\u00b6\nChannels are used to communicate between actors (\nPregelNodes\n).\nEach channel has a value type, an update type, and an update function \u2013 which\ntakes a sequence of updates and\nmodifies the stored value. Channels can be used to send data from one chain to\nanother, or to send data from a chain to itself in a future step. LangGraph\nprovides a number of built-in channels:\nBasic channels: LastValue and Topic\n\u00b6\nLastValue\n: The default channel, stores the last value sent to the channel,\nuseful for input and output values, or for sending data from one step to the next\nTopic\n: A configurable PubSub Topic, useful for sending multiple values\nbetween\nactors\n, or for accumulating output. Can be configured to deduplicate\nvalues, and/or to accumulate values over the course of multiple steps.\nAdvanced channels: Context and BinaryOperatorAggregate\n\u00b6\nContext\n: exposes the value of a context manager, managing its lifecycle.\nUseful for accessing external resources that require setup and/or teardown. e.g.\nclient = Context(httpx.Client)\nBinaryOperatorAggregate\n: stores a persistent value, updated by applying\na binary operator to the current value and each update\nsent to the channel, useful for computing aggregates over multiple steps. e.g.\ntotal = BinaryOperatorAggregate(int, operator.add)\nExamples\n\u00b6\nMost users will interact with Pregel via a\nStateGraph (Graph API)\nor via an\nentrypoint (Functional API)\n.\nHowever, for\nadvanced\nuse cases, Pregel can be used directly. If you're\nnot sure whether you need to use Pregel directly, then the answer is probably no\n- you should use the Graph API or Functional API instead. These are higher-level\ninterfaces that will compile down to Pregel under the hood.\nHere are some examples to give you a sense of how it works:\nSingle node application\nfrom\nlanggraph.channels\nimport\nEphemeralValue\nfrom\nlanggraph.pregel\nimport\nPregel\n,\nNodeBuilder\nnode1\n=\n(\nNodeBuilder\n()\n.\nsubscribe_only\n(\n\"a\"\n)\n.\ndo\n(\nlambda\nx\n:\nx\n+\nx\n)\n.\nwrite_to\n(\n\"b\"\n)\n)\napp\n=\nPregel\n(\nnodes\n=\n{\n\"node1\"\n:\nnode1\n},\nchannels\n=\n{\n\"a\"\n:\nEphemeralValue\n(\nstr\n),\n\"b\"\n:\nEphemeralValue\n(\nstr\n),\n},\ninput_channels\n=\n[\n\"a\"\n],\noutput_channels\n=\n[\n\"b\"\n],\n)\napp\n.\ninvoke\n({\n\"a\"\n:\n\"foo\"\n})\n{'b': 'foofoo'}\nUsing multiple nodes and multiple output channels\nfrom\nlanggraph.channels\nimport\nLastValue\n,\nEphemeralValue\nfrom\nlanggraph.pregel\nimport\nPregel\n,\nNodeBuilder\nnode1\n=\n(\nNodeBuilder\n()\n.\nsubscribe_only\n(\n\"a\"\n)\n.\ndo\n(\nlambda\nx\n:\nx\n+\nx\n)\n.\nwrite_to\n(\n\"b\"\n)\n)\nnode2\n=\n(\nNodeBuilder\n()\n.\nsubscribe_to\n(\n\"b\"\n)\n.\ndo\n(\nlambda\nx\n:\nx\n[\n\"b\"\n]\n+\nx\n[\n\"b\"\n])\n.\nwrite_to\n(\n\"c\"\n)\n)\napp\n=\nPregel\n(\nnodes\n=\n{\n\"node1\"\n:\nnode1\n,\n\"node2\"\n:\nnode2\n},\nchannels\n=\n{\n\"a\"\n:\nEphemeralValue\n(\nstr\n),\n\"b\"\n:\nLastValue\n(\nstr\n),\n\"c\"\n:\nEphemeralValue\n(\nstr\n),\n},\ninput_channels\n=\n[\n\"a\"\n],\noutput_channels\n=\n[\n\"b\"\n,\n\"c\"\n],\n)\napp\n.\ninvoke\n({\n\"a\"\n:\n\"foo\"\n})\n{'b': 'foofoo', 'c': 'foofoofoofoo'}\nUsing a Topic channel\nfrom\nlanggraph.channels\nimport\nLastValue\n,\nEphemeralValue\n,\nTopic\nfrom\nlanggraph.pregel\nimport\nPregel\n,\nNodeBuilder\nnode1\n=\n(\nNodeBuilder\n()\n.\nsubscribe_only\n(\n\"a\"\n)\n.\ndo\n(\nlambda\nx\n:\nx\n+\nx\n)\n.\nwrite_to\n(\n\"b\"\n,\n\"c\"\n)\n)\nnode2\n=\n(\nNodeBuilder\n()\n.\nsubscribe_only\n(\n\"b\"\n)\n.\ndo\n(\nlambda\nx\n:\nx\n+\nx\n)\n.\nwrite_to\n(\n\"c\"\n)\n)\napp\n=\nPregel\n(\nnodes\n=\n{\n\"node1\"\n:\nnode1\n,\n\"node2\"\n:\nnode2\n},\nchannels\n=\n{\n\"a\"\n:\nEphemeralValue\n(\nstr\n),\n\"b\"\n:\nEphemeralValue\n(\nstr\n),\n\"c\"\n:\nTopic\n(\nstr\n,\naccumulate\n=\nTrue\n),\n},\ninput_channels\n=\n[\n\"a\"\n],\noutput_channels\n=\n[\n\"c\"\n],\n)\napp\n.\ninvoke\n({\n\"a\"\n:\n\"foo\"\n})\n{\"c\": [\"foofoo\", \"foofoofoofoo\"]}\nUsing a\nBinaryOperatorAggregate\nchannel\nfrom\nlanggraph.channels\nimport\nEphemeralValue\n,\nBinaryOperatorAggregate\nfrom\nlanggraph.pregel\nimport\nPregel\n,\nNodeBuilder\nnode1\n=\n(\nNodeBuilder\n()\n.\nsubscribe_only\n(\n\"a\"\n)\n.\ndo\n(\nlambda\nx\n:\nx\n+\nx\n)\n.\nwrite_to\n(\n\"b\"\n,\n\"c\"\n)\n)\nnode2\n=\n(\nNodeBuilder\n()\n.\nsubscribe_only\n(\n\"b\"\n)\n.\ndo\n(\nlambda\nx\n:\nx\n+\nx\n)\n.\nwrite_to\n(\n\"c\"\n)\n)\ndef\nreducer\n(\ncurrent\n,\nupdate\n):\nif\ncurrent\n:\nreturn\ncurrent\n+\n\" | \"\n+\nupdate\nelse\n:\nreturn\nupdate\napp\n=\nPregel\n(\nnodes\n=\n{\n\"node1\"\n:\nnode1\n,\n\"node2\"\n:\nnode2\n},\nchannels\n=\n{\n\"a\"\n:\nEphemeralValue\n(\nstr\n),\n\"b\"\n:\nEphemeralValue\n(\nstr\n),\n\"c\"\n:\nBinaryOperatorAggregate\n(\nstr\n,\noperator\n=\nreducer\n),\n},\ninput_channels\n=\n[\n\"a\"\n],\noutput_channels\n=\n[\n\"c\"\n],\n)\napp\n.\ninvoke\n({\n\"a\"\n:\n\"foo\"\n})\n{'c': 'foofoo | foofoofoofoo'}\nIntroducing a cycle\nThis example demonstrates how to introduce a cycle in the graph, by having\na chain write to a channel it subscribes to.\nExecution will continue until a\nNone\nvalue is written to the channel.\nfrom\nlanggraph.channels\nimport\nEphemeralValue\nfrom\nlanggraph.pregel\nimport\nPregel\n,\nNodeBuilder\n,\nChannelWriteEntry\nexample_node\n=\n(\nNodeBuilder\n()\n.\nsubscribe_only\n(\n\"value\"\n)\n.\ndo\n(\nlambda\nx\n:\nx\n+\nx\nif\nlen\n(\nx\n)\n<\n10\nelse\nNone\n)\n.\nwrite_to\n(\nChannelWriteEntry\n(\nchannel\n=\n\"value\"\n,\nskip_none\n=\nTrue\n))\n)\napp\n=\nPregel\n(\nnodes\n=\n{\n\"example_node\"\n:\nexample_node\n},\nchannels\n=\n{\n\"value\"\n:\nEphemeralValue\n(\nstr\n),\n},\ninput_channels\n=\n[\n\"value\"\n],\noutput_channels\n=\n[\n\"value\"\n],\n)\napp\n.\ninvoke\n({\n\"value\"\n:\n\"a\"\n})\n{'value': 'aaaaaaaaaaaaaaaa'}\nMETHOD\nDESCRIPTION\nstream\nStream graph steps for a single input.\nastream\nAsynchronously stream graph steps for a single input.\ninvoke\nRun the graph with a single input and config.\nainvoke\nAsynchronously run the graph with a single input and config.\nget_state\nGet the current state of the graph.\naget_state\nGet the current state of the graph.\nget_state_history\nGet the history of the state of the graph.\naget_state_history\nAsynchronously get the history of the state of the graph.\nupdate_state\nUpdate the state of the graph with the given values, as if they came from\naupdate_state\nAsynchronously update the state of the graph with the given values, as if they came from\nbulk_update_state\nApply updates to the graph state in bulk. Requires a checkpointer to be set.\nabulk_update_state\nAsynchronously apply updates to the graph state in bulk. Requires a checkpointer to be set.\nget_graph\nReturn a drawable representation of the computation graph.\naget_graph\nReturn a drawable representation of the computation graph.\nget_subgraphs\nGet the subgraphs of the graph.\naget_subgraphs\nGet the subgraphs of the graph.\nwith_config\nCreate a copy of the Pregel object with an updated config.\nstream\n\u00b6\nstream\n(\ninput\n:\nInputT\n|\nCommand\n|\nNone\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ncontext\n:\nContextT\n|\nNone\n=\nNone\n,\nstream_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n|\nNone\n=\nNone\n,\nprint_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n=\n(),\noutput_keys\n:\nstr\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndurability\n:\nDurability\n|\nNone\n=\nNone\n,\nsubgraphs\n:\nbool\n=\nFalse\n,\ndebug\n:\nbool\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nUnpack\n[\nDeprecatedKwargs\n],\n)\n->\nIterator\n[\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n]\nStream graph steps for a single input.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the graph.\nTYPE:\nInputT\n|\nCommand\n| None\nconfig\nThe configuration to use for the run.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ncontext\nThe static context to use for the run.\nAdded in version 0.6.0\nTYPE:\nContextT\n| None\nDEFAULT:\nNone\nstream_mode\nThe mode to stream output, defaults to\nself.stream_mode\n.\nOptions are:\n\"values\"\n: Emit all values in the state after each step, including interrupts.\nWhen used with functional API, values are emitted once at the end of the workflow.\n\"updates\"\n: Emit only the node or task names and updates returned by the nodes or tasks after each step.\nIf multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n\"custom\"\n: Emit custom data from inside nodes or tasks using\nStreamWriter\n.\n\"messages\"\n: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\nWill be emitted as 2-tuples\n(LLM token, metadata)\n.\n\"checkpoints\"\n: Emit an event when a checkpoint is created, in the same format as returned by\nget_state()\n.\n\"tasks\"\n: Emit events when tasks start and finish, including their results and errors.\n\"debug\"\n: Emit debug events with as much information as possible for each step.\nYou can pass a list as the\nstream_mode\nparameter to stream multiple modes at once.\nThe streamed outputs will be tuples of\n(mode, data)\n.\nSee\nLangGraph streaming guide\nfor more details.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n] | None\nDEFAULT:\nNone\nprint_mode\nAccepts the same values as\nstream_mode\n, but only prints the output to the console, for debugging purposes.\nDoes not affect the output of the graph in any way.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\nDEFAULT:\n()\noutput_keys\nThe keys to stream, defaults to all non-context channels.\nTYPE:\nstr\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_before\nNodes to interrupt before, defaults to all nodes in the graph.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nNodes to interrupt after, defaults to all nodes in the graph.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ndurability\nThe durability mode for the graph execution, defaults to\n\"async\"\n.\nOptions are:\n\"sync\"\n: Changes are persisted synchronously before the next step starts.\n\"async\"\n: Changes are persisted asynchronously while the next step executes.\n\"exit\"\n: Changes are persisted only when the graph exits.\nTYPE:\nDurability\n| None\nDEFAULT:\nNone\nsubgraphs\nWhether to stream events from inside subgraphs, defaults to\nFalse\n.\nIf\nTrue\n, the events will be emitted as tuples\n(namespace, data)\n,\nor\n(namespace, mode, data)\nif\nstream_mode\nis a list,\nwhere\nnamespace\nis a tuple with the path to the node where a subgraph is invoked,\ne.g.\n(\"parent_node:<task_id>\", \"child_node:<task_id>\")\n.\nSee\nLangGraph streaming guide\nfor more details.\nTYPE:\nbool\nDEFAULT:\nFalse\nYIELDS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n] |\nAny\nThe output of each step in the graph. The output shape depends on the\nstream_mode\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInputT\n|\nCommand\n|\nNone\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ncontext\n:\nContextT\n|\nNone\n=\nNone\n,\nstream_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n|\nNone\n=\nNone\n,\nprint_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n=\n(),\noutput_keys\n:\nstr\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndurability\n:\nDurability\n|\nNone\n=\nNone\n,\nsubgraphs\n:\nbool\n=\nFalse\n,\ndebug\n:\nbool\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nUnpack\n[\nDeprecatedKwargs\n],\n)\n->\nAsyncIterator\n[\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n]\nAsynchronously stream graph steps for a single input.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the graph.\nTYPE:\nInputT\n|\nCommand\n| None\nconfig\nThe configuration to use for the run.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ncontext\nThe static context to use for the run.\nAdded in version 0.6.0\nTYPE:\nContextT\n| None\nDEFAULT:\nNone\nstream_mode\nThe mode to stream output, defaults to\nself.stream_mode\n.\nOptions are:\n\"values\"\n: Emit all values in the state after each step, including interrupts.\nWhen used with functional API, values are emitted once at the end of the workflow.\n\"updates\"\n: Emit only the node or task names and updates returned by the nodes or tasks after each step.\nIf multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n\"custom\"\n: Emit custom data from inside nodes or tasks using\nStreamWriter\n.\n\"messages\"\n: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\nWill be emitted as 2-tuples\n(LLM token, metadata)\n.\n\"checkpoints\"\n: Emit an event when a checkpoint is created, in the same format as returned by\nget_state()\n.\n\"tasks\"\n: Emit events when tasks start and finish, including their results and errors.\n\"debug\"\n: Emit debug events with as much information as possible for each step.\nYou can pass a list as the\nstream_mode\nparameter to stream multiple modes at once.\nThe streamed outputs will be tuples of\n(mode, data)\n.\nSee\nLangGraph streaming guide\nfor more details.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n] | None\nDEFAULT:\nNone\nprint_mode\nAccepts the same values as\nstream_mode\n, but only prints the output to the console, for debugging purposes.\nDoes not affect the output of the graph in any way.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\nDEFAULT:\n()\noutput_keys\nThe keys to stream, defaults to all non-context channels.\nTYPE:\nstr\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_before\nNodes to interrupt before, defaults to all nodes in the graph.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nNodes to interrupt after, defaults to all nodes in the graph.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ndurability\nThe durability mode for the graph execution, defaults to\n\"async\"\n.\nOptions are:\n\"sync\"\n: Changes are persisted synchronously before the next step starts.\n\"async\"\n: Changes are persisted asynchronously while the next step executes.\n\"exit\"\n: Changes are persisted only when the graph exits.\nTYPE:\nDurability\n| None\nDEFAULT:\nNone\nsubgraphs\nWhether to stream events from inside subgraphs, defaults to\nFalse\n.\nIf\nTrue\n, the events will be emitted as tuples\n(namespace, data)\n,\nor\n(namespace, mode, data)\nif\nstream_mode\nis a list,\nwhere\nnamespace\nis a tuple with the path to the node where a subgraph is invoked,\ne.g.\n(\"parent_node:<task_id>\", \"child_node:<task_id>\")\n.\nSee\nLangGraph streaming guide\nfor more details.\nTYPE:\nbool\nDEFAULT:\nFalse\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ndict\n[\nstr\n,\nAny\n] |\nAny\n]\nThe output of each step in the graph. The output shape depends on the\nstream_mode\n.\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\nInputT\n|\nCommand\n|\nNone\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ncontext\n:\nContextT\n|\nNone\n=\nNone\n,\nstream_mode\n:\nStreamMode\n=\n\"values\"\n,\nprint_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n=\n(),\noutput_keys\n:\nstr\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndurability\n:\nDurability\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nAny\nRun the graph with a single input and config.\nPARAMETER\nDESCRIPTION\ninput\nThe input data for the graph. It can be a dictionary or any other type.\nTYPE:\nInputT\n|\nCommand\n| None\nconfig\nThe configuration for the graph run.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ncontext\nThe static context to use for the run.\nAdded in version 0.6.0\nTYPE:\nContextT\n| None\nDEFAULT:\nNone\nstream_mode\nThe stream mode for the graph run.\nTYPE:\nStreamMode\nDEFAULT:\n'values'\nprint_mode\nAccepts the same values as\nstream_mode\n, but only prints the output to the console, for debugging purposes.\nDoes not affect the output of the graph in any way.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\nDEFAULT:\n()\noutput_keys\nThe output keys to retrieve from the graph run.\nTYPE:\nstr\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_before\nThe nodes to interrupt the graph run before.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nThe nodes to interrupt the graph run after.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ndurability\nThe durability mode for the graph execution, defaults to\n\"async\"\n.\nOptions are:\n\"sync\"\n: Changes are persisted synchronously before the next step starts.\n\"async\"\n: Changes are persisted asynchronously while the next step executes.\n\"exit\"\n: Changes are persisted only when the graph exits.\nTYPE:\nDurability\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the graph run.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n] |\nAny\nThe output of the graph run. If\nstream_mode\nis\n\"values\"\n, it returns the latest output.\ndict\n[\nstr\n,\nAny\n] |\nAny\nIf\nstream_mode\nis not\n\"values\"\n, it returns a list of output chunks.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nInputT\n|\nCommand\n|\nNone\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ncontext\n:\nContextT\n|\nNone\n=\nNone\n,\nstream_mode\n:\nStreamMode\n=\n\"values\"\n,\nprint_mode\n:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\n=\n(),\noutput_keys\n:\nstr\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nAll\n|\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndurability\n:\nDurability\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nAny\nAsynchronously run the graph with a single input and config.\nPARAMETER\nDESCRIPTION\ninput\nThe input data for the graph. It can be a dictionary or any other type.\nTYPE:\nInputT\n|\nCommand\n| None\nconfig\nThe configuration for the graph run.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ncontext\nThe static context to use for the run.\nAdded in version 0.6.0\nTYPE:\nContextT\n| None\nDEFAULT:\nNone\nstream_mode\nThe stream mode for the graph run.\nTYPE:\nStreamMode\nDEFAULT:\n'values'\nprint_mode\nAccepts the same values as\nstream_mode\n, but only prints the output to the console, for debugging purposes.\nDoes not affect the output of the graph in any way.\nTYPE:\nStreamMode\n|\nSequence\n[\nStreamMode\n]\nDEFAULT:\n()\noutput_keys\nThe output keys to retrieve from the graph run.\nTYPE:\nstr\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_before\nThe nodes to interrupt the graph run before.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\nThe nodes to interrupt the graph run after.\nTYPE:\nAll\n|\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ndurability\nThe durability mode for the graph execution, defaults to\n\"async\"\n.\nOptions are:\n\"sync\"\n: Changes are persisted synchronously before the next step starts.\n\"async\"\n: Changes are persisted asynchronously while the next step executes.\n\"exit\"\n: Changes are persisted only when the graph exits.\nTYPE:\nDurability\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the graph run.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n] |\nAny\nThe output of the graph run. If\nstream_mode\nis\n\"values\"\n, it returns the latest output.\ndict\n[\nstr\n,\nAny\n] |\nAny\nIf\nstream_mode\nis not\n\"values\"\n, it returns a list of output chunks.\nget_state\n\u00b6\nget_state\n(\nconfig\n:\nRunnableConfig\n,\n*\n,\nsubgraphs\n:\nbool\n=\nFalse\n)\n->\nStateSnapshot\nGet the current state of the graph.\naget_state\nasync\n\u00b6\naget_state\n(\nconfig\n:\nRunnableConfig\n,\n*\n,\nsubgraphs\n:\nbool\n=\nFalse\n)\n->\nStateSnapshot\nGet the current state of the graph.\nget_state_history\n\u00b6\nget_state_history\n(\nconfig\n:\nRunnableConfig\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nIterator\n[\nStateSnapshot\n]\nGet the history of the state of the graph.\naget_state_history\nasync\n\u00b6\naget_state_history\n(\nconfig\n:\nRunnableConfig\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nAsyncIterator\n[\nStateSnapshot\n]\nAsynchronously get the history of the state of the graph.\nupdate_state\n\u00b6\nupdate_state\n(\nconfig\n:\nRunnableConfig\n,\nvalues\n:\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n|\nNone\n,\nas_node\n:\nstr\n|\nNone\n=\nNone\n,\ntask_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableConfig\nUpdate the state of the graph with the given values, as if they came from\nnode\nas_node\n. If\nas_node\nis not provided, it will be set to the last node\nthat updated the state, if not ambiguous.\naupdate_state\nasync\n\u00b6\naupdate_state\n(\nconfig\n:\nRunnableConfig\n,\nvalues\n:\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n,\nas_node\n:\nstr\n|\nNone\n=\nNone\n,\ntask_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableConfig\nAsynchronously update the state of the graph with the given values, as if they came from\nnode\nas_node\n. If\nas_node\nis not provided, it will be set to the last node\nthat updated the state, if not ambiguous.\nbulk_update_state\n\u00b6\nbulk_update_state\n(\nconfig\n:\nRunnableConfig\n,\nsupersteps\n:\nSequence\n[\nSequence\n[\nStateUpdate\n]]\n)\n->\nRunnableConfig\nApply updates to the graph state in bulk. Requires a checkpointer to be set.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to apply the updates to.\nTYPE:\nRunnableConfig\nsupersteps\nA list of supersteps, each including a list of updates to apply sequentially to a graph state.\nEach update is a tuple of the form\n(values, as_node, task_id)\nwhere\ntask_id\nis optional.\nTYPE:\nSequence\n[\nSequence\n[\nStateUpdate\n]]\nRAISES\nDESCRIPTION\nValueError\nIf no checkpointer is set or no updates are provided.\nInvalidUpdateError\nIf an invalid update is provided.\nRETURNS\nDESCRIPTION\nRunnableConfig\nThe updated config.\nTYPE:\nRunnableConfig\nabulk_update_state\nasync\n\u00b6\nabulk_update_state\n(\nconfig\n:\nRunnableConfig\n,\nsupersteps\n:\nSequence\n[\nSequence\n[\nStateUpdate\n]]\n)\n->\nRunnableConfig\nAsynchronously apply updates to the graph state in bulk. Requires a checkpointer to be set.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to apply the updates to.\nTYPE:\nRunnableConfig\nsupersteps\nA list of supersteps, each including a list of updates to apply sequentially to a graph state.\nEach update is a tuple of the form\n(values, as_node, task_id)\nwhere\ntask_id\nis optional.\nTYPE:\nSequence\n[\nSequence\n[\nStateUpdate\n]]\nRAISES\nDESCRIPTION\nValueError\nIf no checkpointer is set or no updates are provided.\nInvalidUpdateError\nIf an invalid update is provided.\nRETURNS\nDESCRIPTION\nRunnableConfig\nThe updated config.\nTYPE:\nRunnableConfig\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nxray\n:\nint\n|\nbool\n=\nFalse\n)\n->\nGraph\nReturn a drawable representation of the computation graph.\naget_graph\nasync\n\u00b6\naget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nxray\n:\nint\n|\nbool\n=\nFalse\n)\n->\nGraph\nReturn a drawable representation of the computation graph.\nget_subgraphs\n\u00b6\nget_subgraphs\n(\n*\n,\nnamespace\n:\nstr\n|\nNone\n=\nNone\n,\nrecurse\n:\nbool\n=\nFalse\n)\n->\nIterator\n[\ntuple\n[\nstr\n,\nPregelProtocol\n]]\nGet the subgraphs of the graph.\nPARAMETER\nDESCRIPTION\nnamespace\nThe namespace to filter the subgraphs by.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nrecurse\nWhether to recurse into the subgraphs.\nIf\nFalse\n, only the immediate subgraphs will be returned.\nTYPE:\nbool\nDEFAULT:\nFalse\nRETURNS\nDESCRIPTION\nIterator\n[\ntuple\n[\nstr\n,\nPregelProtocol\n]]\nAn iterator of the\n(namespace, subgraph)\npairs.\naget_subgraphs\nasync\n\u00b6\naget_subgraphs\n(\n*\n,\nnamespace\n:\nstr\n|\nNone\n=\nNone\n,\nrecurse\n:\nbool\n=\nFalse\n)\n->\nAsyncIterator\n[\ntuple\n[\nstr\n,\nPregelProtocol\n]]\nGet the subgraphs of the graph.\nPARAMETER\nDESCRIPTION\nnamespace\nThe namespace to filter the subgraphs by.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nrecurse\nWhether to recurse into the subgraphs.\nIf\nFalse\n, only the immediate subgraphs will be returned.\nTYPE:\nbool\nDEFAULT:\nFalse\nRETURNS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nstr\n,\nPregelProtocol\n]]\nAn iterator of the\n(namespace, subgraph)\npairs.\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nSelf\nCreate a copy of the Pregel object with an updated config.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/pregel/",
      "title": "Pregel | LangChain Reference",
      "heading": "Pregel"
    }
  },
  {
    "page_content": "Checkpointing | LangChain Reference\nSkip to content\nLangChain Reference\nCheckpointing\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nCheckpointing\nTable of contents\nbase\nCheckpointMetadata\nsource\nstep\nparents\nCheckpoint\nv\nid\nts\nchannel_values\nchannel_versions\nversions_seen\nupdated_channels\nBaseCheckpointSaver\nconfig_specs\nget\nget_tuple\nlist\nput\nput_writes\ndelete_thread\naget\naget_tuple\nalist\naput\naput_writes\nadelete_thread\nget_next_version\ncreate_checkpoint\nbase\nSerializerProtocol\nCipherProtocol\nencrypt\ndecrypt\njsonplus\nJsonPlusSerializer\nencrypted\nEncryptedSerializer\ndumps_typed\nfrom_pycryptodome_aes\nmemory\nInMemorySaver\nconfig_specs\nget_tuple\nlist\nput\nput_writes\ndelete_thread\naget_tuple\nalist\naput\naput_writes\nadelete_thread\nget_next_version\nget\naget\nPersistentDict\nsync\nsqlite\nSqliteSaver\nconfig_specs\nfrom_conn_string\nsetup\ncursor\nget_tuple\nlist\nput\nput_writes\ndelete_thread\naget_tuple\nalist\naput\nget_next_version\nget\naget\naput_writes\nadelete_thread\naio\nAsyncSqliteSaver\nconfig_specs\nfrom_conn_string\nget_tuple\nlist\nput\nput_writes\ndelete_thread\nsetup\naget_tuple\nalist\naput\naput_writes\nadelete_thread\nget_next_version\nget\naget\npostgres\nPostgresSaver\nconfig_specs\nfrom_conn_string\nsetup\nlist\nget_tuple\nput\nput_writes\ndelete_thread\nget\naget\naget_tuple\nalist\naput\naput_writes\nadelete_thread\nget_next_version\naio\nAsyncPostgresSaver\nconfig_specs\nfrom_conn_string\nsetup\nalist\naget_tuple\naput\naput_writes\nadelete_thread\nlist\nget_tuple\nput\nput_writes\ndelete_thread\nget\naget\nget_next_version\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nbase\nCheckpointMetadata\nsource\nstep\nparents\nCheckpoint\nv\nid\nts\nchannel_values\nchannel_versions\nversions_seen\nupdated_channels\nBaseCheckpointSaver\nconfig_specs\nget\nget_tuple\nlist\nput\nput_writes\ndelete_thread\naget\naget_tuple\nalist\naput\naput_writes\nadelete_thread\nget_next_version\ncreate_checkpoint\nbase\nSerializerProtocol\nCipherProtocol\nencrypt\ndecrypt\njsonplus\nJsonPlusSerializer\nencrypted\nEncryptedSerializer\ndumps_typed\nfrom_pycryptodome_aes\nmemory\nInMemorySaver\nconfig_specs\nget_tuple\nlist\nput\nput_writes\ndelete_thread\naget_tuple\nalist\naput\naput_writes\nadelete_thread\nget_next_version\nget\naget\nPersistentDict\nsync\nsqlite\nSqliteSaver\nconfig_specs\nfrom_conn_string\nsetup\ncursor\nget_tuple\nlist\nput\nput_writes\ndelete_thread\naget_tuple\nalist\naput\nget_next_version\nget\naget\naput_writes\nadelete_thread\naio\nAsyncSqliteSaver\nconfig_specs\nfrom_conn_string\nget_tuple\nlist\nput\nput_writes\ndelete_thread\nsetup\naget_tuple\nalist\naput\naput_writes\nadelete_thread\nget_next_version\nget\naget\npostgres\nPostgresSaver\nconfig_specs\nfrom_conn_string\nsetup\nlist\nget_tuple\nput\nput_writes\ndelete_thread\nget\naget\naget_tuple\nalist\naput\naput_writes\nadelete_thread\nget_next_version\naio\nAsyncPostgresSaver\nconfig_specs\nfrom_conn_string\nsetup\nalist\naget_tuple\naput\naput_writes\nadelete_thread\nlist\nget_tuple\nput\nput_writes\ndelete_thread\nget\naget\nget_next_version\nCheckpointing\nbase\n\u00b6\nFUNCTION\nDESCRIPTION\ncreate_checkpoint\nCreate a checkpoint for the given channels.\nCheckpointMetadata\n\u00b6\nBases:\nTypedDict\nMetadata associated with a checkpoint.\nsource\ninstance-attribute\n\u00b6\nsource\n:\nLiteral\n[\n'input'\n,\n'loop'\n,\n'update'\n,\n'fork'\n]\nThe source of the checkpoint.\n\"input\"\n: The checkpoint was created from an input to invoke/stream/batch.\n\"loop\"\n: The checkpoint was created from inside the pregel loop.\n\"update\"\n: The checkpoint was created from a manual state update.\n\"fork\"\n: The checkpoint was created as a copy of another checkpoint.\nstep\ninstance-attribute\n\u00b6\nstep\n:\nint\nThe step number of the checkpoint.\n-1\nfor the first\n\"input\"\ncheckpoint.\n0\nfor the first\n\"loop\"\ncheckpoint.\n...\nfor the\nnth\ncheckpoint afterwards.\nparents\ninstance-attribute\n\u00b6\nparents\n:\ndict\n[\nstr\n,\nstr\n]\nThe IDs of the parent checkpoints.\nMapping from checkpoint namespace to checkpoint ID.\nCheckpoint\n\u00b6\nBases:\nTypedDict\nState snapshot at a given point in time.\nv\ninstance-attribute\n\u00b6\nv\n:\nint\nThe version of the checkpoint format. Currently\n1\n.\nid\ninstance-attribute\n\u00b6\nid\n:\nstr\nThe ID of the checkpoint.\nThis is both unique and monotonically increasing, so can be used for sorting\ncheckpoints from first to last.\nts\ninstance-attribute\n\u00b6\nts\n:\nstr\nThe timestamp of the checkpoint in ISO 8601 format.\nchannel_values\ninstance-attribute\n\u00b6\nchannel_values\n:\ndict\n[\nstr\n,\nAny\n]\nThe values of the channels at the time of the checkpoint.\nMapping from channel name to deserialized channel snapshot value.\nchannel_versions\ninstance-attribute\n\u00b6\nchannel_versions\n:\nChannelVersions\nThe versions of the channels at the time of the checkpoint.\nThe keys are channel names and the values are monotonically increasing\nversion strings for each channel.\nversions_seen\ninstance-attribute\n\u00b6\nversions_seen\n:\ndict\n[\nstr\n,\nChannelVersions\n]\nMap from node ID to map from channel name to version seen.\nThis keeps track of the versions of the channels that each node has seen.\nUsed to determine which nodes to execute next.\nupdated_channels\ninstance-attribute\n\u00b6\nupdated_channels\n:\nlist\n[\nstr\n]\n|\nNone\nThe channels that were updated in this checkpoint.\nBaseCheckpointSaver\n\u00b6\nBases:\nGeneric\n[\nV\n]\nBase class for creating a graph checkpointer.\nCheckpointers allow LangGraph agents to persist their state\nwithin and across multiple interactions.\nATTRIBUTE\nDESCRIPTION\nserde\nSerializer for encoding/decoding checkpoints.\nTYPE:\nSerializerProtocol\nNote\nWhen creating a custom checkpoint saver, consider implementing async\nversions to avoid blocking the main thread.\nMETHOD\nDESCRIPTION\nget\nFetch a checkpoint using the given configuration.\nget_tuple\nFetch a checkpoint tuple using the given configuration.\nlist\nList checkpoints that match the given criteria.\nput\nStore a checkpoint with its configuration and metadata.\nput_writes\nStore intermediate writes linked to a checkpoint.\ndelete_thread\nDelete all checkpoints and writes associated with a specific thread ID.\naget\nAsynchronously fetch a checkpoint using the given configuration.\naget_tuple\nAsynchronously fetch a checkpoint tuple using the given configuration.\nalist\nAsynchronously list checkpoints that match the given criteria.\naput\nAsynchronously store a checkpoint with its configuration and metadata.\naput_writes\nAsynchronously store intermediate writes linked to a checkpoint.\nadelete_thread\nDelete all checkpoints and writes associated with a specific thread ID.\nget_next_version\nGenerate the next version ID for a channel.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\nDefine the configuration options for the checkpoint saver.\nRETURNS\nDESCRIPTION\nlist\nList of configuration field specs.\nTYPE:\nlist\nget\n\u00b6\nget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nFetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\nget_tuple\n\u00b6\nget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nFetch a checkpoint tuple using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe requested checkpoint tuple, or\nNone\nif not found.\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\nlist\n\u00b6\nlist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nIterator\n[\nCheckpointTuple\n]\nList checkpoints that match the given criteria.\nPARAMETER\nDESCRIPTION\nconfig\nBase configuration for filtering checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nList checkpoints created before this configuration.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nIterator\n[\nCheckpointTuple\n]\nIterator of matching checkpoint tuples.\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\nput\n\u00b6\nput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nStore a checkpoint with its configuration and metadata.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration for the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to store.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata for the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew channel versions as of this write.\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nUpdated configuration after storing the checkpoint.\nTYPE:\nRunnableConfig\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\nput_writes\n\u00b6\nput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nStore intermediate writes linked to a checkpoint.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\ndelete_thread\n\u00b6\ndelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a specific thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID whose checkpoints should be deleted.\nTYPE:\nstr\naget\nasync\n\u00b6\naget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nAsynchronously fetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\naget_tuple\nasync\n\u00b6\naget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nAsynchronously fetch a checkpoint tuple using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe requested checkpoint tuple, or\nNone\nif not found.\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\nalist\nasync\n\u00b6\nalist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nAsyncIterator\n[\nCheckpointTuple\n]\nAsynchronously list checkpoints that match the given criteria.\nPARAMETER\nDESCRIPTION\nconfig\nBase configuration for filtering checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria for metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nList checkpoints created before this configuration.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nAsyncIterator\n[\nCheckpointTuple\n]\nAsync iterator of matching checkpoint tuples.\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\naput\nasync\n\u00b6\naput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nAsynchronously store a checkpoint with its configuration and metadata.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration for the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to store.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata for the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew channel versions as of this write.\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nUpdated configuration after storing the checkpoint.\nTYPE:\nRunnableConfig\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\naput_writes\nasync\n\u00b6\naput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nAsynchronously store intermediate writes linked to a checkpoint.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\nadelete_thread\nasync\n\u00b6\nadelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a specific thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID whose checkpoints should be deleted.\nTYPE:\nstr\nget_next_version\n\u00b6\nget_next_version\n(\ncurrent\n:\nV\n|\nNone\n,\nchannel\n:\nNone\n)\n->\nV\nGenerate the next version ID for a channel.\nDefault is to use integer versions, incrementing by\n1\n.\nIf you override, you can use\nstr\n/\nint\n/\nfloat\nversions, as long as they are monotonically increasing.\nPARAMETER\nDESCRIPTION\ncurrent\nThe current version identifier (\nint\n,\nfloat\n, or\nstr\n).\nTYPE:\nV\n| None\nchannel\nDeprecated argument, kept for backwards compatibility.\nTYPE:\nNone\nRETURNS\nDESCRIPTION\nV\nThe next version identifier, which must be increasing.\nTYPE:\nV\ncreate_checkpoint\n\u00b6\ncreate_checkpoint\n(\ncheckpoint\n:\nCheckpoint\n,\nchannels\n:\nMapping\n[\nstr\n,\nChannelProtocol\n]\n|\nNone\n,\nstep\n:\nint\n,\n*\n,\nid\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nCheckpoint\nCreate a checkpoint for the given channels.\nbase\n\u00b6\nSerializerProtocol\n\u00b6\nBases:\nProtocol\nProtocol for serialization and deserialization of objects.\ndumps_typed\n: Serialize an object to a tuple\n(type, bytes)\n.\nloads_typed\n: Deserialize an object from a tuple\n(type, bytes)\n.\nValid implementations include the\npickle\n,\njson\nand\norjson\nmodules.\nCipherProtocol\n\u00b6\nBases:\nProtocol\nProtocol for encryption and decryption of data.\nencrypt\n: Encrypt plaintext.\ndecrypt\n: Decrypt ciphertext.\nMETHOD\nDESCRIPTION\nencrypt\nEncrypt plaintext. Returns a tuple\n(cipher name, ciphertext)\n.\ndecrypt\nDecrypt ciphertext. Returns the plaintext.\nencrypt\n\u00b6\nencrypt\n(\nplaintext\n:\nbytes\n)\n->\ntuple\n[\nstr\n,\nbytes\n]\nEncrypt plaintext. Returns a tuple\n(cipher name, ciphertext)\n.\ndecrypt\n\u00b6\ndecrypt\n(\nciphername\n:\nstr\n,\nciphertext\n:\nbytes\n)\n->\nbytes\nDecrypt ciphertext. Returns the plaintext.\njsonplus\n\u00b6\nJsonPlusSerializer\n\u00b6\nBases:\nSerializerProtocol\nSerializer that uses ormsgpack, with optional fallbacks.\nWarning\nSecurity note: This serializer is intended for use within the\nBaseCheckpointSaver\nclass and called within the Pregel loop. It should not be used on untrusted\npython objects. If an attacker can write directly to your checkpoint database,\nthey may be able to trigger code execution when data is deserialized.\nencrypted\n\u00b6\nEncryptedSerializer\n\u00b6\nBases:\nSerializerProtocol\nSerializer that encrypts and decrypts data using an encryption protocol.\nMETHOD\nDESCRIPTION\ndumps_typed\nSerialize an object to a tuple\n(type, bytes)\nand encrypt the bytes.\nfrom_pycryptodome_aes\nCreate an\nEncryptedSerializer\nusing AES encryption.\ndumps_typed\n\u00b6\ndumps_typed\n(\nobj\n:\nAny\n)\n->\ntuple\n[\nstr\n,\nbytes\n]\nSerialize an object to a tuple\n(type, bytes)\nand encrypt the bytes.\nfrom_pycryptodome_aes\nclassmethod\n\u00b6\nfrom_pycryptodome_aes\n(\nserde\n:\nSerializerProtocol\n=\nJsonPlusSerializer\n(),\n**\nkwargs\n:\nAny\n)\n->\nEncryptedSerializer\nCreate an\nEncryptedSerializer\nusing AES encryption.\nmemory\n\u00b6\nInMemorySaver\n\u00b6\nBases:\nBaseCheckpointSaver\n[\nstr\n]\n,\nAbstractContextManager\n,\nAbstractAsyncContextManager\nAn in-memory checkpoint saver.\nThis checkpoint saver stores checkpoints in memory using a\ndefaultdict\n.\nNote\nOnly use\nInMemorySaver\nfor debugging or testing purposes.\nFor production use cases we recommend installing\nlanggraph-checkpoint-postgres\nand using\nPostgresSaver\n/\nAsyncPostgresSaver\n.\nIf you are using LangSmith Deployment, no checkpointer needs to be specified. The correct managed checkpointer will be used automatically.\nPARAMETER\nDESCRIPTION\nserde\nThe serializer to use for serializing and deserializing checkpoints.\nTYPE:\nSerializerProtocol\n| None\nDEFAULT:\nNone\nExample\nimport\nasyncio\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.graph\nimport\nStateGraph\nbuilder\n=\nStateGraph\n(\nint\n)\nbuilder\n.\nadd_node\n(\n\"add_one\"\n,\nlambda\nx\n:\nx\n+\n1\n)\nbuilder\n.\nset_entry_point\n(\n\"add_one\"\n)\nbuilder\n.\nset_finish_point\n(\n\"add_one\"\n)\nmemory\n=\nInMemorySaver\n()\ngraph\n=\nbuilder\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\ncoro\n=\ngraph\n.\nainvoke\n(\n1\n,\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"thread-1\"\n}})\nasyncio\n.\nrun\n(\ncoro\n)\n# Output: 2\nMETHOD\nDESCRIPTION\nget_tuple\nGet a checkpoint tuple from the in-memory storage.\nlist\nList checkpoints from the in-memory storage.\nput\nSave a checkpoint to the in-memory storage.\nput_writes\nSave a list of writes to the in-memory storage.\ndelete_thread\nDelete all checkpoints and writes associated with a thread ID.\naget_tuple\nAsynchronous version of\nget_tuple\n.\nalist\nAsynchronous version of\nlist\n.\naput\nAsynchronous version of\nput\n.\naput_writes\nAsynchronous version of\nput_writes\n.\nadelete_thread\nDelete all checkpoints and writes associated with a thread ID.\nget_next_version\nGenerate the next version ID for a channel.\nget\nFetch a checkpoint using the given configuration.\naget\nAsynchronously fetch a checkpoint using the given configuration.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\nDefine the configuration options for the checkpoint saver.\nRETURNS\nDESCRIPTION\nlist\nList of configuration field specs.\nTYPE:\nlist\nget_tuple\n\u00b6\nget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nGet a checkpoint tuple from the in-memory storage.\nThis method retrieves a checkpoint tuple from the in-memory storage based on the\nprovided config. If the config contains a\ncheckpoint_id\nkey, the checkpoint with\nthe matching thread ID and timestamp is retrieved. Otherwise, the latest checkpoint\nfor the given thread ID is retrieved.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for retrieving the checkpoint.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe retrieved checkpoint tuple, or None if no matching checkpoint was found.\nlist\n\u00b6\nlist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nIterator\n[\nCheckpointTuple\n]\nList checkpoints from the in-memory storage.\nThis method retrieves a list of checkpoint tuples from the in-memory storage based\non the provided criteria.\nPARAMETER\nDESCRIPTION\nconfig\nBase configuration for filtering checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria for metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nList checkpoints created before this configuration.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nYIELDS\nDESCRIPTION\nCheckpointTuple\nAn iterator of matching checkpoint tuples.\nput\n\u00b6\nput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nSave a checkpoint to the in-memory storage.\nThis method saves a checkpoint to the in-memory storage. The checkpoint is associated\nwith the provided config.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to save.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata to save with the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew versions as of this write\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nThe updated config containing the saved checkpoint's timestamp.\nTYPE:\nRunnableConfig\nput_writes\n\u00b6\nput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nSave a list of writes to the in-memory storage.\nThis method saves a list of writes to the in-memory storage. The writes are associated\nwith the provided config.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the writes.\nTYPE:\nRunnableConfig\nwrites\nThe writes to save.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\nRETURNS\nDESCRIPTION\nRunnableConfig\nThe updated config containing the saved writes' timestamp.\nTYPE:\nNone\ndelete_thread\n\u00b6\ndelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID to delete.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nNone\nNone\naget_tuple\nasync\n\u00b6\naget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nAsynchronous version of\nget_tuple\n.\nThis method is an asynchronous wrapper around\nget_tuple\nthat runs the synchronous\nmethod in a separate thread using asyncio.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for retrieving the checkpoint.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe retrieved checkpoint tuple, or None if no matching checkpoint was found.\nalist\nasync\n\u00b6\nalist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nAsyncIterator\n[\nCheckpointTuple\n]\nAsynchronous version of\nlist\n.\nThis method is an asynchronous wrapper around\nlist\nthat runs the synchronous\nmethod in a separate thread using asyncio.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for listing the checkpoints.\nTYPE:\nRunnableConfig\n| None\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nCheckpointTuple\n]\nAn asynchronous iterator of checkpoint tuples.\naput\nasync\n\u00b6\naput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nAsynchronous version of\nput\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to save.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata to save with the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew versions as of this write\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nThe updated config containing the saved checkpoint's timestamp.\nTYPE:\nRunnableConfig\naput_writes\nasync\n\u00b6\naput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nAsynchronous version of\nput_writes\n.\nThis method is an asynchronous wrapper around\nput_writes\nthat runs the synchronous\nmethod in a separate thread using asyncio.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the writes.\nTYPE:\nRunnableConfig\nwrites\nThe writes to save, each as a (channel, value) pair.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\nRETURNS\nDESCRIPTION\nNone\nNone\nadelete_thread\nasync\n\u00b6\nadelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID to delete.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nNone\nNone\nget_next_version\n\u00b6\nget_next_version\n(\ncurrent\n:\nstr\n|\nNone\n,\nchannel\n:\nNone\n)\n->\nstr\nGenerate the next version ID for a channel.\nDefault is to use integer versions, incrementing by\n1\n.\nIf you override, you can use\nstr\n/\nint\n/\nfloat\nversions, as long as they are monotonically increasing.\nPARAMETER\nDESCRIPTION\ncurrent\nThe current version identifier (\nint\n,\nfloat\n, or\nstr\n).\nTYPE:\nV\n| None\nchannel\nDeprecated argument, kept for backwards compatibility.\nTYPE:\nNone\nRETURNS\nDESCRIPTION\nV\nThe next version identifier, which must be increasing.\nTYPE:\nV\nget\n\u00b6\nget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nFetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\naget\nasync\n\u00b6\naget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nAsynchronously fetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\nPersistentDict\n\u00b6\nBases:\ndefaultdict\nPersistent dictionary with an API compatible with shelve and anydbm.\nThe dict is kept in memory, so the dictionary operations run as fast as\na regular dictionary.\nWrite to disk is delayed until close or sync (similar to gdbm's fast mode).\nInput file format is automatically discovered.\nOutput file format is selectable between pickle, json, and csv.\nAll three serialization formats are backed by fast C implementations.\nAdapted from\nhttps://code.activestate.com/recipes/576642-persistent-dict-with-multiple-standard-file-format/\nMETHOD\nDESCRIPTION\nsync\nWrite dict to disk\nsync\n\u00b6\nsync\n()\n->\nNone\nWrite dict to disk\nsqlite\n\u00b6\nSqliteSaver\n\u00b6\nBases:\nBaseCheckpointSaver\n[\nstr\n]\nA checkpoint saver that stores checkpoints in a SQLite database.\nNote\nThis class is meant for lightweight, synchronous use cases\n(demos and small projects) and does not\nscale to multiple threads.\nFor a similar sqlite saver with\nasync\nsupport,\nconsider using\nAsyncSqliteSaver\n.\nPARAMETER\nDESCRIPTION\nconn\nThe SQLite database connection.\nTYPE:\nConnection\nserde\nThe serializer to use for serializing and deserializing checkpoints. Defaults to JsonPlusSerializerCompat.\nTYPE:\nOptional\n[\nSerializerProtocol\n]\nDEFAULT:\nNone\nExamples:\n>>> import sqlite3\n>>> from langgraph.checkpoint.sqlite import SqliteSaver\n>>> from langgraph.graph import StateGraph\n>>>\n>>> builder = StateGraph(int)\n>>> builder.add_node(\"add_one\", lambda x: x + 1)\n>>> builder.set_entry_point(\"add_one\")\n>>> builder.set_finish_point(\"add_one\")\n>>> # Create a new SqliteSaver instance\n>>> # Note: check_same_thread=False is OK as the implementation uses a lock\n>>> # to ensure thread safety.\n>>> conn = sqlite3.connect(\"checkpoints.sqlite\", check_same_thread=False)\n>>> memory = SqliteSaver(conn)\n>>> graph = builder.compile(checkpointer=memory)\n>>> config = {\"configurable\": {\"thread_id\": \"1\"}}\n>>> graph.get_state(config)\n>>> result = graph.invoke(3, config)\n>>> graph.get_state(config)\nStateSnapshot(values=4, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '0c62ca34-ac19-445d-bbb0-5b4984975b2a'}}, parent_config=None)\nMETHOD\nDESCRIPTION\nfrom_conn_string\nCreate a new SqliteSaver instance from a connection string.\nsetup\nSet up the checkpoint database.\ncursor\nGet a cursor for the SQLite database.\nget_tuple\nGet a checkpoint tuple from the database.\nlist\nList checkpoints from the database.\nput\nSave a checkpoint to the database.\nput_writes\nStore intermediate writes linked to a checkpoint.\ndelete_thread\nDelete all checkpoints and writes associated with a thread ID.\naget_tuple\nGet a checkpoint tuple from the database asynchronously.\nalist\nList checkpoints from the database asynchronously.\naput\nSave a checkpoint to the database asynchronously.\nget_next_version\nGenerate the next version ID for a channel.\nget\nFetch a checkpoint using the given configuration.\naget\nAsynchronously fetch a checkpoint using the given configuration.\naput_writes\nAsynchronously store intermediate writes linked to a checkpoint.\nadelete_thread\nDelete all checkpoints and writes associated with a specific thread ID.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\nDefine the configuration options for the checkpoint saver.\nRETURNS\nDESCRIPTION\nlist\nList of configuration field specs.\nTYPE:\nlist\nfrom_conn_string\nclassmethod\n\u00b6\nfrom_conn_string\n(\nconn_string\n:\nstr\n)\n->\nIterator\n[\nSqliteSaver\n]\nCreate a new SqliteSaver instance from a connection string.\nPARAMETER\nDESCRIPTION\nconn_string\nThe SQLite connection string.\nTYPE:\nstr\nYIELDS\nDESCRIPTION\nSqliteSaver\nA new SqliteSaver instance.\nTYPE::\nSqliteSaver\nExamples:\nIn memory:\nwith SqliteSaver.from_conn_string(\":memory:\") as memory:\n...\nTo disk:\nwith SqliteSaver.from_conn_string(\"checkpoints.sqlite\") as memory:\n...\nsetup\n\u00b6\nsetup\n()\n->\nNone\nSet up the checkpoint database.\nThis method creates the necessary tables in the SQLite database if they don't\nalready exist. It is called automatically when needed and should not be called\ndirectly by the user.\ncursor\n\u00b6\ncursor\n(\ntransaction\n:\nbool\n=\nTrue\n)\n->\nIterator\n[\nCursor\n]\nGet a cursor for the SQLite database.\nThis method returns a cursor for the SQLite database. It is used internally\nby the SqliteSaver and should not be called directly by the user.\nPARAMETER\nDESCRIPTION\ntransaction\nWhether to commit the transaction when the cursor is closed. Defaults to True.\nTYPE:\nbool\nDEFAULT:\nTrue\nYIELDS\nDESCRIPTION\nCursor\nsqlite3.Cursor: A cursor for the SQLite database.\nget_tuple\n\u00b6\nget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nGet a checkpoint tuple from the database.\nThis method retrieves a checkpoint tuple from the SQLite database based on the\nprovided config. If the config contains a\ncheckpoint_id\nkey, the checkpoint with\nthe matching thread ID and checkpoint ID is retrieved. Otherwise, the latest checkpoint\nfor the given thread ID is retrieved.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for retrieving the checkpoint.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe retrieved checkpoint tuple, or None if no matching checkpoint was found.\nExamples:\nBasic:\n>>> config = {\"configurable\": {\"thread_id\": \"1\"}}\n>>> checkpoint_tuple = memory.get_tuple(config)\n>>> print(checkpoint_tuple)\nCheckpointTuple(...)\nWith checkpoint ID:\n>>> config = {\n...    \"configurable\": {\n...        \"thread_id\": \"1\",\n...        \"checkpoint_ns\": \"\",\n...        \"checkpoint_id\": \"1ef4f797-8335-6428-8001-8a1503f9b875\",\n...    }\n... }\n>>> checkpoint_tuple = memory.get_tuple(config)\n>>> print(checkpoint_tuple)\nCheckpointTuple(...)\nlist\n\u00b6\nlist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nIterator\n[\nCheckpointTuple\n]\nList checkpoints from the database.\nThis method retrieves a list of checkpoint tuples from the SQLite database based\non the provided config. The checkpoints are ordered by checkpoint ID in descending order (newest first).\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for listing the checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria for metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nIf provided, only checkpoints before the specified checkpoint ID are returned.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nThe maximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nYIELDS\nDESCRIPTION\nCheckpointTuple\nAn iterator of checkpoint tuples.\nExamples:\n>>>\nfrom\nlanggraph.checkpoint.sqlite\nimport\nSqliteSaver\n>>>\nwith\nSqliteSaver\n.\nfrom_conn_string\n(\n\":memory:\"\n)\nas\nmemory\n:\n...\n# Run a graph, then list the checkpoints\n>>>\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}}\n>>>\ncheckpoints\n=\nlist\n(\nmemory\n.\nlist\n(\nconfig\n,\nlimit\n=\n2\n))\n>>>\nprint\n(\ncheckpoints\n)\n[CheckpointTuple(...), CheckpointTuple(...)]\n>>>\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}}\n>>>\nbefore\n=\n{\n\"configurable\"\n:\n{\n\"checkpoint_id\"\n:\n\"1ef4f797-8335-6428-8001-8a1503f9b875\"\n}}\n>>>\nwith\nSqliteSaver\n.\nfrom_conn_string\n(\n\":memory:\"\n)\nas\nmemory\n:\n...\n# Run a graph, then list the checkpoints\n>>>\ncheckpoints\n=\nlist\n(\nmemory\n.\nlist\n(\nconfig\n,\nbefore\n=\nbefore\n))\n>>>\nprint\n(\ncheckpoints\n)\n[CheckpointTuple(...), ...]\nput\n\u00b6\nput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nSave a checkpoint to the database.\nThis method saves a checkpoint to the SQLite database. The checkpoint is associated\nwith the provided config and its parent config (if any).\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to save.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata to save with the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew channel versions as of this write.\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nUpdated configuration after storing the checkpoint.\nTYPE:\nRunnableConfig\nExamples:\n>>> from langgraph.checkpoint.sqlite import SqliteSaver\n>>> with SqliteSaver.from_conn_string(\":memory:\") as memory:\n>>>     config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_ns\": \"\"}}\n>>>     checkpoint = {\"ts\": \"2024-05-04T06:32:42.235444+00:00\", \"id\": \"1ef4f797-8335-6428-8001-8a1503f9b875\", \"channel_values\": {\"key\": \"value\"}}\n>>>     saved_config = memory.put(config, checkpoint, {\"source\": \"input\", \"step\": 1, \"writes\": {\"key\": \"value\"}}, {})\n>>> print(saved_config)\n{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef4f797-8335-6428-8001-8a1503f9b875'}}\nput_writes\n\u00b6\nput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nStore intermediate writes linked to a checkpoint.\nThis method saves intermediate writes associated with a checkpoint to the SQLite database.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store, each as (channel, value) pair.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\ndelete_thread\n\u00b6\ndelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID to delete.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nNone\nNone\naget_tuple\nasync\n\u00b6\naget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nGet a checkpoint tuple from the database asynchronously.\nNote\nThis async method is not supported by the SqliteSaver class.\nUse get_tuple() instead, or consider using\nAsyncSqliteSaver\n.\nalist\nasync\n\u00b6\nalist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nAsyncIterator\n[\nCheckpointTuple\n]\nList checkpoints from the database asynchronously.\nNote\nThis async method is not supported by the SqliteSaver class.\nUse list() instead, or consider using\nAsyncSqliteSaver\n.\naput\nasync\n\u00b6\naput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nSave a checkpoint to the database asynchronously.\nNote\nThis async method is not supported by the SqliteSaver class.\nUse put() instead, or consider using\nAsyncSqliteSaver\n.\nget_next_version\n\u00b6\nget_next_version\n(\ncurrent\n:\nstr\n|\nNone\n,\nchannel\n:\nNone\n)\n->\nstr\nGenerate the next version ID for a channel.\nThis method creates a new version identifier for a channel based on its current version.\nPARAMETER\nDESCRIPTION\ncurrent\nThe current version identifier of the channel.\nTYPE:\nOptional\n[\nstr\n]\nRETURNS\nDESCRIPTION\nstr\nThe next version identifier, which is guaranteed to be monotonically increasing.\nTYPE:\nstr\nget\n\u00b6\nget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nFetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\naget\nasync\n\u00b6\naget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nAsynchronously fetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\naput_writes\nasync\n\u00b6\naput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nAsynchronously store intermediate writes linked to a checkpoint.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\nadelete_thread\nasync\n\u00b6\nadelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a specific thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID whose checkpoints should be deleted.\nTYPE:\nstr\naio\n\u00b6\nAsyncSqliteSaver\n\u00b6\nBases:\nBaseCheckpointSaver\n[\nstr\n]\nAn asynchronous checkpoint saver that stores checkpoints in a SQLite database.\nThis class provides an asynchronous interface for saving and retrieving checkpoints\nusing a SQLite database. It's designed for use in asynchronous environments and\noffers better performance for I/O-bound operations compared to synchronous alternatives.\nATTRIBUTE\nDESCRIPTION\nconn\nThe asynchronous SQLite database connection.\nTYPE:\nConnection\nserde\nThe serializer used for encoding/decoding checkpoints.\nTYPE:\nSerializerProtocol\nTip\nRequires the\naiosqlite\npackage.\nInstall it with\npip install aiosqlite\n.\nWarning\nWhile this class supports asynchronous checkpointing, it is not recommended\nfor production workloads due to limitations in SQLite's write performance.\nFor production use, consider a more robust database like PostgreSQL.\nTip\nRemember to\nclose the database connection\nafter executing your code,\notherwise, you may see the graph \"hang\" after execution (since the program\nwill not exit until the connection is closed).\nThe easiest way is to use the\nasync with\nstatement as shown in the examples.\nasync\nwith\nAsyncSqliteSaver\n.\nfrom_conn_string\n(\n\"checkpoints.sqlite\"\n)\nas\nsaver\n:\n# Your code here\ngraph\n=\nbuilder\n.\ncompile\n(\ncheckpointer\n=\nsaver\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"thread-1\"\n}}\nasync\nfor\nevent\nin\ngraph\n.\nastream_events\n(\n...\n,\nconfig\n,\nversion\n=\n\"v1\"\n):\nprint\n(\nevent\n)\nExamples:\nUsage within StateGraph:\n>>>\nimport\nasyncio\n>>>\n>>>\nfrom\nlanggraph.checkpoint.sqlite.aio\nimport\nAsyncSqliteSaver\n>>>\nfrom\nlanggraph.graph\nimport\nStateGraph\n>>>\n>>>\nasync\ndef\nmain\n():\n>>>\nbuilder\n=\nStateGraph\n(\nint\n)\n>>>\nbuilder\n.\nadd_node\n(\n\"add_one\"\n,\nlambda\nx\n:\nx\n+\n1\n)\n>>>\nbuilder\n.\nset_entry_point\n(\n\"add_one\"\n)\n>>>\nbuilder\n.\nset_finish_point\n(\n\"add_one\"\n)\n>>>\nasync\nwith\nAsyncSqliteSaver\n.\nfrom_conn_string\n(\n\"checkpoints.db\"\n)\nas\nmemory\n:\n>>>\ngraph\n=\nbuilder\n.\ncompile\n(\ncheckpointer\n=\nmemory\n)\n>>>\ncoro\n=\ngraph\n.\nainvoke\n(\n1\n,\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"thread-1\"\n}})\n>>>\nprint\n(\nawait\nasyncio\n.\ngather\n(\ncoro\n))\n>>>\n>>>\nasyncio\n.\nrun\n(\nmain\n())\nOutput: [2]\nRaw usage:\n>>>\nimport\nasyncio\n>>>\nimport\naiosqlite\n>>>\nfrom\nlanggraph.checkpoint.sqlite.aio\nimport\nAsyncSqliteSaver\n>>>\n>>>\nasync\ndef\nmain\n():\n>>>\nasync\nwith\naiosqlite\n.\nconnect\n(\n\"checkpoints.db\"\n)\nas\nconn\n:\n...\nsaver\n=\nAsyncSqliteSaver\n(\nconn\n)\n...\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n,\n\"checkpoint_ns\"\n:\n\"\"\n}}\n...\ncheckpoint\n=\n{\n\"ts\"\n:\n\"2023-05-03T10:00:00Z\"\n,\n\"data\"\n:\n{\n\"key\"\n:\n\"value\"\n},\n\"id\"\n:\n\"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"\n}\n...\nsaved_config\n=\nawait\nsaver\n.\naput\n(\nconfig\n,\ncheckpoint\n,\n{},\n{})\n...\nprint\n(\nsaved_config\n)\n>>>\nasyncio\n.\nrun\n(\nmain\n())\n{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '0c62ca34-ac19-445d-bbb0-5b4984975b2a'}}\nMETHOD\nDESCRIPTION\nfrom_conn_string\nCreate a new AsyncSqliteSaver instance from a connection string.\nget_tuple\nGet a checkpoint tuple from the database.\nlist\nList checkpoints from the database asynchronously.\nput\nSave a checkpoint to the database.\nput_writes\nStore intermediate writes linked to a checkpoint.\ndelete_thread\nDelete all checkpoints and writes associated with a thread ID.\nsetup\nSet up the checkpoint database asynchronously.\naget_tuple\nGet a checkpoint tuple from the database asynchronously.\nalist\nList checkpoints from the database asynchronously.\naput\nSave a checkpoint to the database asynchronously.\naput_writes\nStore intermediate writes linked to a checkpoint asynchronously.\nadelete_thread\nDelete all checkpoints and writes associated with a thread ID.\nget_next_version\nGenerate the next version ID for a channel.\nget\nFetch a checkpoint using the given configuration.\naget\nAsynchronously fetch a checkpoint using the given configuration.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\nDefine the configuration options for the checkpoint saver.\nRETURNS\nDESCRIPTION\nlist\nList of configuration field specs.\nTYPE:\nlist\nfrom_conn_string\nasync\nclassmethod\n\u00b6\nfrom_conn_string\n(\nconn_string\n:\nstr\n)\n->\nAsyncIterator\n[\nAsyncSqliteSaver\n]\nCreate a new AsyncSqliteSaver instance from a connection string.\nPARAMETER\nDESCRIPTION\nconn_string\nThe SQLite connection string.\nTYPE:\nstr\nYIELDS\nDESCRIPTION\nAsyncSqliteSaver\nA new AsyncSqliteSaver instance.\nTYPE::\nAsyncIterator\n[\nAsyncSqliteSaver\n]\nget_tuple\n\u00b6\nget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nGet a checkpoint tuple from the database.\nThis method retrieves a checkpoint tuple from the SQLite database based on the\nprovided config. If the config contains a\ncheckpoint_id\nkey, the checkpoint with\nthe matching thread ID and checkpoint ID is retrieved. Otherwise, the latest checkpoint\nfor the given thread ID is retrieved.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for retrieving the checkpoint.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe retrieved checkpoint tuple, or None if no matching checkpoint was found.\nlist\n\u00b6\nlist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nIterator\n[\nCheckpointTuple\n]\nList checkpoints from the database asynchronously.\nThis method retrieves a list of checkpoint tuples from the SQLite database based\non the provided config. The checkpoints are ordered by checkpoint ID in descending order (newest first).\nPARAMETER\nDESCRIPTION\nconfig\nBase configuration for filtering checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria for metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nIf provided, only checkpoints before the specified checkpoint ID are returned.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nYIELDS\nDESCRIPTION\nCheckpointTuple\nAn iterator of matching checkpoint tuples.\nput\n\u00b6\nput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nSave a checkpoint to the database.\nThis method saves a checkpoint to the SQLite database. The checkpoint is associated\nwith the provided config and its parent config (if any).\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to save.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata to save with the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew channel versions as of this write.\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nUpdated configuration after storing the checkpoint.\nTYPE:\nRunnableConfig\nput_writes\n\u00b6\nput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nStore intermediate writes linked to a checkpoint.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\ndelete_thread\n\u00b6\ndelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID to delete.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nNone\nNone\nsetup\nasync\n\u00b6\nsetup\n()\n->\nNone\nSet up the checkpoint database asynchronously.\nThis method creates the necessary tables in the SQLite database if they don't\nalready exist. It is called automatically when needed and should not be called\ndirectly by the user.\naget_tuple\nasync\n\u00b6\naget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nGet a checkpoint tuple from the database asynchronously.\nThis method retrieves a checkpoint tuple from the SQLite database based on the\nprovided config. If the config contains a\ncheckpoint_id\nkey, the checkpoint with\nthe matching thread ID and checkpoint ID is retrieved. Otherwise, the latest checkpoint\nfor the given thread ID is retrieved.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for retrieving the checkpoint.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe retrieved checkpoint tuple, or None if no matching checkpoint was found.\nalist\nasync\n\u00b6\nalist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nAsyncIterator\n[\nCheckpointTuple\n]\nList checkpoints from the database asynchronously.\nThis method retrieves a list of checkpoint tuples from the SQLite database based\non the provided config. The checkpoints are ordered by checkpoint ID in descending order (newest first).\nPARAMETER\nDESCRIPTION\nconfig\nBase configuration for filtering checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria for metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nIf provided, only checkpoints before the specified checkpoint ID are returned.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nCheckpointTuple\n]\nAn asynchronous iterator of matching checkpoint tuples.\naput\nasync\n\u00b6\naput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nSave a checkpoint to the database asynchronously.\nThis method saves a checkpoint to the SQLite database. The checkpoint is associated\nwith the provided config and its parent config (if any).\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to save.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata to save with the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew channel versions as of this write.\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nUpdated configuration after storing the checkpoint.\nTYPE:\nRunnableConfig\naput_writes\nasync\n\u00b6\naput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nStore intermediate writes linked to a checkpoint asynchronously.\nThis method saves intermediate writes associated with a checkpoint to the database.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store, each as (channel, value) pair.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\nadelete_thread\nasync\n\u00b6\nadelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID to delete.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nNone\nNone\nget_next_version\n\u00b6\nget_next_version\n(\ncurrent\n:\nstr\n|\nNone\n,\nchannel\n:\nNone\n)\n->\nstr\nGenerate the next version ID for a channel.\nThis method creates a new version identifier for a channel based on its current version.\nPARAMETER\nDESCRIPTION\ncurrent\nThe current version identifier of the channel.\nTYPE:\nOptional\n[\nstr\n]\nRETURNS\nDESCRIPTION\nstr\nThe next version identifier, which is guaranteed to be monotonically increasing.\nTYPE:\nstr\nget\n\u00b6\nget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nFetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\naget\nasync\n\u00b6\naget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nAsynchronously fetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\npostgres\n\u00b6\nPostgresSaver\n\u00b6\nBases:\nBasePostgresSaver\nCheckpointer that stores checkpoints in a Postgres database.\nMETHOD\nDESCRIPTION\nfrom_conn_string\nCreate a new PostgresSaver instance from a connection string.\nsetup\nSet up the checkpoint database asynchronously.\nlist\nList checkpoints from the database.\nget_tuple\nGet a checkpoint tuple from the database.\nput\nSave a checkpoint to the database.\nput_writes\nStore intermediate writes linked to a checkpoint.\ndelete_thread\nDelete all checkpoints and writes associated with a thread ID.\nget\nFetch a checkpoint using the given configuration.\naget\nAsynchronously fetch a checkpoint using the given configuration.\naget_tuple\nAsynchronously fetch a checkpoint tuple using the given configuration.\nalist\nAsynchronously list checkpoints that match the given criteria.\naput\nAsynchronously store a checkpoint with its configuration and metadata.\naput_writes\nAsynchronously store intermediate writes linked to a checkpoint.\nadelete_thread\nDelete all checkpoints and writes associated with a specific thread ID.\nget_next_version\nGenerate the next version ID for a channel.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\nDefine the configuration options for the checkpoint saver.\nRETURNS\nDESCRIPTION\nlist\nList of configuration field specs.\nTYPE:\nlist\nfrom_conn_string\nclassmethod\n\u00b6\nfrom_conn_string\n(\nconn_string\n:\nstr\n,\n*\n,\npipeline\n:\nbool\n=\nFalse\n)\n->\nIterator\n[\nPostgresSaver\n]\nCreate a new PostgresSaver instance from a connection string.\nPARAMETER\nDESCRIPTION\nconn_string\nThe Postgres connection info string.\nTYPE:\nstr\npipeline\nwhether to use Pipeline\nTYPE:\nbool\nDEFAULT:\nFalse\nRETURNS\nDESCRIPTION\nPostgresSaver\nA new PostgresSaver instance.\nTYPE:\nIterator\n[\nPostgresSaver\n]\nsetup\n\u00b6\nsetup\n()\n->\nNone\nSet up the checkpoint database asynchronously.\nThis method creates the necessary tables in the Postgres database if they don't\nalready exist and runs database migrations. It MUST be called directly by the user\nthe first time checkpointer is used.\nlist\n\u00b6\nlist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nIterator\n[\nCheckpointTuple\n]\nList checkpoints from the database.\nThis method retrieves a list of checkpoint tuples from the Postgres database based\non the provided config. The checkpoints are ordered by checkpoint ID in descending order (newest first).\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for listing the checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria for metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nIf provided, only checkpoints before the specified checkpoint ID are returned.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nThe maximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nYIELDS\nDESCRIPTION\nCheckpointTuple\nAn iterator of checkpoint tuples.\nExamples:\n>>>\nfrom\nlanggraph.checkpoint.postgres\nimport\nPostgresSaver\n>>>\nDB_URI\n=\n\"postgres://postgres:postgres@localhost:5432/postgres?sslmode=disable\"\n>>>\nwith\nPostgresSaver\n.\nfrom_conn_string\n(\nDB_URI\n)\nas\nmemory\n:\n...\n# Run a graph, then list the checkpoints\n>>>\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}}\n>>>\ncheckpoints\n=\nlist\n(\nmemory\n.\nlist\n(\nconfig\n,\nlimit\n=\n2\n))\n>>>\nprint\n(\ncheckpoints\n)\n[CheckpointTuple(...), CheckpointTuple(...)]\n>>>\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\n\"1\"\n}}\n>>>\nbefore\n=\n{\n\"configurable\"\n:\n{\n\"checkpoint_id\"\n:\n\"1ef4f797-8335-6428-8001-8a1503f9b875\"\n}}\n>>>\nwith\nPostgresSaver\n.\nfrom_conn_string\n(\nDB_URI\n)\nas\nmemory\n:\n...\n# Run a graph, then list the checkpoints\n>>>\ncheckpoints\n=\nlist\n(\nmemory\n.\nlist\n(\nconfig\n,\nbefore\n=\nbefore\n))\n>>>\nprint\n(\ncheckpoints\n)\n[CheckpointTuple(...), ...]\nget_tuple\n\u00b6\nget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nGet a checkpoint tuple from the database.\nThis method retrieves a checkpoint tuple from the Postgres database based on the\nprovided config. If the config contains a\ncheckpoint_id\nkey, the checkpoint with\nthe matching thread ID and timestamp is retrieved. Otherwise, the latest checkpoint\nfor the given thread ID is retrieved.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for retrieving the checkpoint.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe retrieved checkpoint tuple, or None if no matching checkpoint was found.\nExamples:\nBasic:\n>>> config = {\"configurable\": {\"thread_id\": \"1\"}}\n>>> checkpoint_tuple = memory.get_tuple(config)\n>>> print(checkpoint_tuple)\nCheckpointTuple(...)\nWith timestamp:\n>>> config = {\n...    \"configurable\": {\n...        \"thread_id\": \"1\",\n...        \"checkpoint_ns\": \"\",\n...        \"checkpoint_id\": \"1ef4f797-8335-6428-8001-8a1503f9b875\",\n...    }\n... }\n>>> checkpoint_tuple = memory.get_tuple(config)\n>>> print(checkpoint_tuple)\nCheckpointTuple(...)\nput\n\u00b6\nput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nSave a checkpoint to the database.\nThis method saves a checkpoint to the Postgres database. The checkpoint is associated\nwith the provided config and its parent config (if any).\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to save.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata to save with the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew channel versions as of this write.\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nUpdated configuration after storing the checkpoint.\nTYPE:\nRunnableConfig\nExamples:\n>>> from langgraph.checkpoint.postgres import PostgresSaver\n>>> DB_URI = \"postgres://postgres:postgres@localhost:5432/postgres?sslmode=disable\"\n>>> with PostgresSaver.from_conn_string(DB_URI) as memory:\n>>>     config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_ns\": \"\"}}\n>>>     checkpoint = {\"ts\": \"2024-05-04T06:32:42.235444+00:00\", \"id\": \"1ef4f797-8335-6428-8001-8a1503f9b875\", \"channel_values\": {\"key\": \"value\"}}\n>>>     saved_config = memory.put(config, checkpoint, {\"source\": \"input\", \"step\": 1, \"writes\": {\"key\": \"value\"}}, {})\n>>> print(saved_config)\n{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef4f797-8335-6428-8001-8a1503f9b875'}}\nput_writes\n\u00b6\nput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nStore intermediate writes linked to a checkpoint.\nThis method saves intermediate writes associated with a checkpoint to the Postgres database.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ndelete_thread\n\u00b6\ndelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID to delete.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nNone\nNone\nget\n\u00b6\nget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nFetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\naget\nasync\n\u00b6\naget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nAsynchronously fetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\naget_tuple\nasync\n\u00b6\naget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nAsynchronously fetch a checkpoint tuple using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe requested checkpoint tuple, or\nNone\nif not found.\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\nalist\nasync\n\u00b6\nalist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nAsyncIterator\n[\nCheckpointTuple\n]\nAsynchronously list checkpoints that match the given criteria.\nPARAMETER\nDESCRIPTION\nconfig\nBase configuration for filtering checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria for metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nList checkpoints created before this configuration.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nAsyncIterator\n[\nCheckpointTuple\n]\nAsync iterator of matching checkpoint tuples.\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\naput\nasync\n\u00b6\naput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nAsynchronously store a checkpoint with its configuration and metadata.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration for the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to store.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata for the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew channel versions as of this write.\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nUpdated configuration after storing the checkpoint.\nTYPE:\nRunnableConfig\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\naput_writes\nasync\n\u00b6\naput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nAsynchronously store intermediate writes linked to a checkpoint.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\nRAISES\nDESCRIPTION\nNotImplementedError\nImplement this method in your custom checkpoint saver.\nadelete_thread\nasync\n\u00b6\nadelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a specific thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID whose checkpoints should be deleted.\nTYPE:\nstr\nget_next_version\n\u00b6\nget_next_version\n(\ncurrent\n:\nstr\n|\nNone\n,\nchannel\n:\nNone\n)\n->\nstr\nGenerate the next version ID for a channel.\nDefault is to use integer versions, incrementing by\n1\n.\nIf you override, you can use\nstr\n/\nint\n/\nfloat\nversions, as long as they are monotonically increasing.\nPARAMETER\nDESCRIPTION\ncurrent\nThe current version identifier (\nint\n,\nfloat\n, or\nstr\n).\nTYPE:\nV\n| None\nchannel\nDeprecated argument, kept for backwards compatibility.\nTYPE:\nNone\nRETURNS\nDESCRIPTION\nV\nThe next version identifier, which must be increasing.\nTYPE:\nV\naio\n\u00b6\nAsyncPostgresSaver\n\u00b6\nBases:\nBasePostgresSaver\nAsynchronous checkpointer that stores checkpoints in a Postgres database.\nMETHOD\nDESCRIPTION\nfrom_conn_string\nCreate a new AsyncPostgresSaver instance from a connection string.\nsetup\nSet up the checkpoint database asynchronously.\nalist\nList checkpoints from the database asynchronously.\naget_tuple\nGet a checkpoint tuple from the database asynchronously.\naput\nSave a checkpoint to the database asynchronously.\naput_writes\nStore intermediate writes linked to a checkpoint asynchronously.\nadelete_thread\nDelete all checkpoints and writes associated with a thread ID.\nlist\nList checkpoints from the database.\nget_tuple\nGet a checkpoint tuple from the database.\nput\nSave a checkpoint to the database.\nput_writes\nStore intermediate writes linked to a checkpoint.\ndelete_thread\nDelete all checkpoints and writes associated with a thread ID.\nget\nFetch a checkpoint using the given configuration.\naget\nAsynchronously fetch a checkpoint using the given configuration.\nget_next_version\nGenerate the next version ID for a channel.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\nDefine the configuration options for the checkpoint saver.\nRETURNS\nDESCRIPTION\nlist\nList of configuration field specs.\nTYPE:\nlist\nfrom_conn_string\nasync\nclassmethod\n\u00b6\nfrom_conn_string\n(\nconn_string\n:\nstr\n,\n*\n,\npipeline\n:\nbool\n=\nFalse\n,\nserde\n:\nSerializerProtocol\n|\nNone\n=\nNone\n)\n->\nAsyncIterator\n[\nAsyncPostgresSaver\n]\nCreate a new AsyncPostgresSaver instance from a connection string.\nPARAMETER\nDESCRIPTION\nconn_string\nThe Postgres connection info string.\nTYPE:\nstr\npipeline\nwhether to use AsyncPipeline\nTYPE:\nbool\nDEFAULT:\nFalse\nRETURNS\nDESCRIPTION\nAsyncPostgresSaver\nA new AsyncPostgresSaver instance.\nTYPE:\nAsyncIterator\n[\nAsyncPostgresSaver\n]\nsetup\nasync\n\u00b6\nsetup\n()\n->\nNone\nSet up the checkpoint database asynchronously.\nThis method creates the necessary tables in the Postgres database if they don't\nalready exist and runs database migrations. It MUST be called directly by the user\nthe first time checkpointer is used.\nalist\nasync\n\u00b6\nalist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nAsyncIterator\n[\nCheckpointTuple\n]\nList checkpoints from the database asynchronously.\nThis method retrieves a list of checkpoint tuples from the Postgres database based\non the provided config. The checkpoints are ordered by checkpoint ID in descending order (newest first).\nPARAMETER\nDESCRIPTION\nconfig\nBase configuration for filtering checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria for metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nIf provided, only checkpoints before the specified checkpoint ID are returned.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nCheckpointTuple\n]\nAn asynchronous iterator of matching checkpoint tuples.\naget_tuple\nasync\n\u00b6\naget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nGet a checkpoint tuple from the database asynchronously.\nThis method retrieves a checkpoint tuple from the Postgres database based on the\nprovided config. If the config contains a\ncheckpoint_id\nkey, the checkpoint with\nthe matching thread ID and \"checkpoint_id\" is retrieved. Otherwise, the latest checkpoint\nfor the given thread ID is retrieved.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for retrieving the checkpoint.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe retrieved checkpoint tuple, or None if no matching checkpoint was found.\naput\nasync\n\u00b6\naput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nSave a checkpoint to the database asynchronously.\nThis method saves a checkpoint to the Postgres database. The checkpoint is associated\nwith the provided config and its parent config (if any).\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to save.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata to save with the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew channel versions as of this write.\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nUpdated configuration after storing the checkpoint.\nTYPE:\nRunnableConfig\naput_writes\nasync\n\u00b6\naput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nStore intermediate writes linked to a checkpoint asynchronously.\nThis method saves intermediate writes associated with a checkpoint to the database.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store, each as (channel, value) pair.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\nadelete_thread\nasync\n\u00b6\nadelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID to delete.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nNone\nNone\nlist\n\u00b6\nlist\n(\nconfig\n:\nRunnableConfig\n|\nNone\n,\n*\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nbefore\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\n)\n->\nIterator\n[\nCheckpointTuple\n]\nList checkpoints from the database.\nThis method retrieves a list of checkpoint tuples from the Postgres database based\non the provided config. The checkpoints are ordered by checkpoint ID in descending order (newest first).\nPARAMETER\nDESCRIPTION\nconfig\nBase configuration for filtering checkpoints.\nTYPE:\nRunnableConfig\n| None\nfilter\nAdditional filtering criteria for metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nbefore\nIf provided, only checkpoints before the specified checkpoint ID are returned.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of checkpoints to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nYIELDS\nDESCRIPTION\nCheckpointTuple\nAn iterator of matching checkpoint tuples.\nget_tuple\n\u00b6\nget_tuple\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpointTuple\n|\nNone\nGet a checkpoint tuple from the database.\nThis method retrieves a checkpoint tuple from the Postgres database based on the\nprovided config. If the config contains a\ncheckpoint_id\nkey, the checkpoint with\nthe matching thread ID and \"checkpoint_id\" is retrieved. Otherwise, the latest checkpoint\nfor the given thread ID is retrieved.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use for retrieving the checkpoint.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpointTuple\n| None\nThe retrieved checkpoint tuple, or None if no matching checkpoint was found.\nput\n\u00b6\nput\n(\nconfig\n:\nRunnableConfig\n,\ncheckpoint\n:\nCheckpoint\n,\nmetadata\n:\nCheckpointMetadata\n,\nnew_versions\n:\nChannelVersions\n,\n)\n->\nRunnableConfig\nSave a checkpoint to the database.\nThis method saves a checkpoint to the Postgres database. The checkpoint is associated\nwith the provided config and its parent config (if any).\nPARAMETER\nDESCRIPTION\nconfig\nThe config to associate with the checkpoint.\nTYPE:\nRunnableConfig\ncheckpoint\nThe checkpoint to save.\nTYPE:\nCheckpoint\nmetadata\nAdditional metadata to save with the checkpoint.\nTYPE:\nCheckpointMetadata\nnew_versions\nNew channel versions as of this write.\nTYPE:\nChannelVersions\nRETURNS\nDESCRIPTION\nRunnableConfig\nUpdated configuration after storing the checkpoint.\nTYPE:\nRunnableConfig\nput_writes\n\u00b6\nput_writes\n(\nconfig\n:\nRunnableConfig\n,\nwrites\n:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]],\ntask_id\n:\nstr\n,\ntask_path\n:\nstr\n=\n\"\"\n,\n)\n->\nNone\nStore intermediate writes linked to a checkpoint.\nThis method saves intermediate writes associated with a checkpoint to the database.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration of the related checkpoint.\nTYPE:\nRunnableConfig\nwrites\nList of writes to store, each as (channel, value) pair.\nTYPE:\nSequence\n[\ntuple\n[\nstr\n,\nAny\n]]\ntask_id\nIdentifier for the task creating the writes.\nTYPE:\nstr\ntask_path\nPath of the task creating the writes.\nTYPE:\nstr\nDEFAULT:\n''\ndelete_thread\n\u00b6\ndelete_thread\n(\nthread_id\n:\nstr\n)\n->\nNone\nDelete all checkpoints and writes associated with a thread ID.\nPARAMETER\nDESCRIPTION\nthread_id\nThe thread ID to delete.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nNone\nNone\nget\n\u00b6\nget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nFetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\naget\nasync\n\u00b6\naget\n(\nconfig\n:\nRunnableConfig\n)\n->\nCheckpoint\n|\nNone\nAsynchronously fetch a checkpoint using the given configuration.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration specifying which checkpoint to retrieve.\nTYPE:\nRunnableConfig\nRETURNS\nDESCRIPTION\nCheckpoint\n| None\nThe requested checkpoint, or\nNone\nif not found.\nget_next_version\n\u00b6\nget_next_version\n(\ncurrent\n:\nstr\n|\nNone\n,\nchannel\n:\nNone\n)\n->\nstr\nGenerate the next version ID for a channel.\nDefault is to use integer versions, incrementing by\n1\n.\nIf you override, you can use\nstr\n/\nint\n/\nfloat\nversions, as long as they are monotonically increasing.\nPARAMETER\nDESCRIPTION\ncurrent\nThe current version identifier (\nint\n,\nfloat\n, or\nstr\n).\nTYPE:\nV\n| None\nchannel\nDeprecated argument, kept for backwards compatibility.\nTYPE:\nNone\nRETURNS\nDESCRIPTION\nV\nThe next version identifier, which must be increasing.\nTYPE:\nV\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/checkpoints/",
      "title": "Checkpointing | LangChain Reference",
      "heading": "Checkpointing"
    }
  },
  {
    "page_content": "Storage (LangGraph) | LangChain Reference\nSkip to content\nLangChain Reference\nStorage (LangGraph)\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nStorage\nTable of contents\nbase\nNamespacePath\nNamespaceMatchType\nEmbeddings\nembed_documents\nembed_query\naembed_documents\naembed_query\nNotProvided\nItem\nSearchItem\n__init__\nGetOp\nnamespace\nkey\nrefresh_ttl\nSearchOp\nnamespace_prefix\nfilter\nlimit\noffset\nquery\nrefresh_ttl\nMatchCondition\nmatch_type\npath\nListNamespacesOp\nmatch_conditions\nmax_depth\nlimit\noffset\nPutOp\nnamespace\nkey\nvalue\nindex\nttl\nInvalidNamespaceError\nTTLConfig\nrefresh_on_read\ndefault_ttl\nsweep_interval_minutes\nIndexConfig\ndims\nembed\nfields\nBaseStore\nbatch\nabatch\nget\nsearch\nput\ndelete\nlist_namespaces\naget\nasearch\naput\nadelete\nalist_namespaces\nensure_embeddings\nget_text_at_path\ntokenize_path\npostgres\nAsyncPostgresStore\nbatch\nget\nsearch\nput\ndelete\nlist_namespaces\naget\nasearch\naput\nadelete\nalist_namespaces\nabatch\nfrom_conn_string\nsetup\nsweep_ttl\nstart_ttl_sweeper\nstop_ttl_sweeper\nPoolConfig\nmin_size\nmax_size\nkwargs\nPostgresStore\nget\nsearch\nput\ndelete\nlist_namespaces\naget\nasearch\naput\nadelete\nalist_namespaces\nfrom_conn_string\nsweep_ttl\nstart_ttl_sweeper\nstop_ttl_sweeper\n__del__\nbatch\nabatch\nsetup\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nbase\nNamespacePath\nNamespaceMatchType\nEmbeddings\nembed_documents\nembed_query\naembed_documents\naembed_query\nNotProvided\nItem\nSearchItem\n__init__\nGetOp\nnamespace\nkey\nrefresh_ttl\nSearchOp\nnamespace_prefix\nfilter\nlimit\noffset\nquery\nrefresh_ttl\nMatchCondition\nmatch_type\npath\nListNamespacesOp\nmatch_conditions\nmax_depth\nlimit\noffset\nPutOp\nnamespace\nkey\nvalue\nindex\nttl\nInvalidNamespaceError\nTTLConfig\nrefresh_on_read\ndefault_ttl\nsweep_interval_minutes\nIndexConfig\ndims\nembed\nfields\nBaseStore\nbatch\nabatch\nget\nsearch\nput\ndelete\nlist_namespaces\naget\nasearch\naput\nadelete\nalist_namespaces\nensure_embeddings\nget_text_at_path\ntokenize_path\npostgres\nAsyncPostgresStore\nbatch\nget\nsearch\nput\ndelete\nlist_namespaces\naget\nasearch\naput\nadelete\nalist_namespaces\nabatch\nfrom_conn_string\nsetup\nsweep_ttl\nstart_ttl_sweeper\nstop_ttl_sweeper\nPoolConfig\nmin_size\nmax_size\nkwargs\nPostgresStore\nget\nsearch\nput\ndelete\nlist_namespaces\naget\nasearch\naput\nadelete\nalist_namespaces\nfrom_conn_string\nsweep_ttl\nstart_ttl_sweeper\nstop_ttl_sweeper\n__del__\nbatch\nabatch\nsetup\nStorage\nbase\n\u00b6\nBase classes and types for persistent key-value stores.\nStores provide long-term memory that persists across threads and conversations.\nSupports hierarchical namespaces, key-value storage, and optional vector search.\nCore types\nBaseStore\n: Store interface with sync/async operations\nItem\n: Stored key-value pairs with metadata\nOp\n: Get/Put/Search/List operations\nFUNCTION\nDESCRIPTION\nensure_embeddings\nEnsure that an embedding function conforms to LangChain's Embeddings interface.\nget_text_at_path\nExtract text from an object using a path expression or pre-tokenized path.\ntokenize_path\nTokenize a path into components.\nNamespacePath\nmodule-attribute\n\u00b6\nNamespacePath\n=\ntuple\n[\nstr\n|\nLiteral\n[\n'*'\n],\n...\n]\nA tuple representing a namespace path that can include wildcards.\nExamples\n(\n\"users\"\n,)\n# Exact users namespace\n(\n\"documents\"\n,\n\"*\"\n)\n# Any sub-namespace under documents\n(\n\"cache\"\n,\n\"*\"\n,\n\"v1\"\n)\n# Any cache category with v1 version\nNamespaceMatchType\nmodule-attribute\n\u00b6\nNamespaceMatchType\n=\nLiteral\n[\n'prefix'\n,\n'suffix'\n]\nSpecifies how to match namespace paths.\nValues\n\"prefix\": Match from the start of the namespace\n\"suffix\": Match from the end of the namespace\nEmbeddings\n\u00b6\nBases:\nABC\nInterface for embedding models.\nThis is an interface meant for implementing text embedding models.\nText embedding models are used to map text to a vector (a point in n-dimensional\nspace).\nTexts that are similar will usually be mapped to points that are close to each\nother in this space. The exact details of what's considered \"similar\" and how\n\"distance\" is measured in this space are dependent on the specific embedding model.\nThis abstraction contains a method for embedding a list of documents and a method\nfor embedding a query text. The embedding of a query text is expected to be a single\nvector, while the embedding of a list of documents is expected to be a list of\nvectors.\nUsually the query embedding is identical to the document embedding, but the\nabstraction allows treating them independently.\nIn addition to the synchronous methods, this interface also provides asynchronous\nversions of the methods.\nBy default, the asynchronous methods are implemented using the synchronous methods;\nhowever, implementations may choose to override the asynchronous methods with\nan async native implementation for performance reasons.\nMETHOD\nDESCRIPTION\nembed_documents\nEmbed search docs.\nembed_query\nEmbed query text.\naembed_documents\nAsynchronous Embed search docs.\naembed_query\nAsynchronous Embed query text.\nembed_documents\nabstractmethod\n\u00b6\nembed_documents\n(\ntexts\n:\nlist\n[\nstr\n])\n->\nlist\n[\nlist\n[\nfloat\n]]\nEmbed search docs.\nPARAMETER\nDESCRIPTION\ntexts\nList of text to embed.\nTYPE:\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nlist\n[\nlist\n[\nfloat\n]]\nList of embeddings.\nembed_query\nabstractmethod\n\u00b6\nembed_query\n(\ntext\n:\nstr\n)\n->\nlist\n[\nfloat\n]\nEmbed query text.\nPARAMETER\nDESCRIPTION\ntext\nText to embed.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nlist\n[\nfloat\n]\nEmbedding.\naembed_documents\nasync\n\u00b6\naembed_documents\n(\ntexts\n:\nlist\n[\nstr\n])\n->\nlist\n[\nlist\n[\nfloat\n]]\nAsynchronous Embed search docs.\nPARAMETER\nDESCRIPTION\ntexts\nList of text to embed.\nTYPE:\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nlist\n[\nlist\n[\nfloat\n]]\nList of embeddings.\naembed_query\nasync\n\u00b6\naembed_query\n(\ntext\n:\nstr\n)\n->\nlist\n[\nfloat\n]\nAsynchronous Embed query text.\nPARAMETER\nDESCRIPTION\ntext\nText to embed.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nlist\n[\nfloat\n]\nEmbedding.\nNotProvided\n\u00b6\nSentinel singleton.\nItem\n\u00b6\nRepresents a stored item with metadata.\nPARAMETER\nDESCRIPTION\nvalue\nThe stored data as a dictionary. Keys are filterable.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nnamespace\nHierarchical path defining the collection in which this document resides.\nRepresented as a tuple of strings, allowing for nested categorization.\nFor example:\n(\"documents\", 'user123')\nTYPE:\ntuple\n[\nstr\n, ...]\ncreated_at\nTimestamp of item creation.\nTYPE:\ndatetime\nupdated_at\nTimestamp of last update.\nTYPE:\ndatetime\nSearchItem\n\u00b6\nBases:\nItem\nRepresents an item returned from a search operation with additional metadata.\nMETHOD\nDESCRIPTION\n__init__\nInitialize a result item.\n__init__\n\u00b6\n__init__\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\nvalue\n:\ndict\n[\nstr\n,\nAny\n],\ncreated_at\n:\ndatetime\n,\nupdated_at\n:\ndatetime\n,\nscore\n:\nfloat\n|\nNone\n=\nNone\n,\n)\n->\nNone\nInitialize a result item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path to the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nvalue\nThe stored value.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ncreated_at\nWhen the item was first created.\nTYPE:\ndatetime\nupdated_at\nWhen the item was last updated.\nTYPE:\ndatetime\nscore\nRelevance/similarity score if from a ranked operation.\nTYPE:\nfloat\n| None\nDEFAULT:\nNone\nGetOp\n\u00b6\nBases:\nNamedTuple\nOperation to retrieve a specific item by its namespace and key.\nThis operation allows precise retrieval of stored items using their full path\n(namespace) and unique identifier (key) combination.\nExamples\nBasic item retrieval:\nGetOp\n(\nnamespace\n=\n(\n\"users\"\n,\n\"profiles\"\n),\nkey\n=\n\"user123\"\n)\nGetOp\n(\nnamespace\n=\n(\n\"cache\"\n,\n\"embeddings\"\n),\nkey\n=\n\"doc456\"\n)\nnamespace\ninstance-attribute\n\u00b6\nnamespace\n:\ntuple\n[\nstr\n,\n...\n]\nHierarchical path that uniquely identifies the item's location.\nExamples\n(\n\"users\"\n,)\n# Root level users namespace\n(\n\"users\"\n,\n\"profiles\"\n)\n# Profiles within users namespace\nkey\ninstance-attribute\n\u00b6\nkey\n:\nstr\nUnique identifier for the item within its specific namespace.\nExamples\n\"user123\"\n# For a user profile\n\"doc456\"\n# For a document\nrefresh_ttl\nclass-attribute\ninstance-attribute\n\u00b6\nrefresh_ttl\n:\nbool\n=\nTrue\nWhether to refresh TTLs for the returned item.\nIf no TTL was specified for the original item(s),\nor if TTL support is not enabled for your adapter,\nthis argument is ignored.\nSearchOp\n\u00b6\nBases:\nNamedTuple\nOperation to search for items within a specified namespace hierarchy.\nThis operation supports both structured filtering and natural language search\nwithin a given namespace prefix. It provides pagination through limit and offset\nparameters.\nNote\nNatural language search support depends on your store implementation.\nExamples\nSearch with filters and pagination:\nSearchOp\n(\nnamespace_prefix\n=\n(\n\"documents\"\n,),\nfilter\n=\n{\n\"type\"\n:\n\"report\"\n,\n\"status\"\n:\n\"active\"\n},\nlimit\n=\n5\n,\noffset\n=\n10\n)\nNatural language search:\nSearchOp\n(\nnamespace_prefix\n=\n(\n\"users\"\n,\n\"content\"\n),\nquery\n=\n\"technical documentation about APIs\"\n,\nlimit\n=\n20\n)\nnamespace_prefix\ninstance-attribute\n\u00b6\nnamespace_prefix\n:\ntuple\n[\nstr\n,\n...\n]\nHierarchical path prefix defining the search scope.\nExamples\n()\n# Search entire store\n(\n\"documents\"\n,)\n# Search all documents\n(\n\"users\"\n,\n\"content\"\n)\n# Search within user content\nfilter\nclass-attribute\ninstance-attribute\n\u00b6\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\nKey-value pairs for filtering results based on exact matches or comparison operators.\nThe filter supports both exact matches and operator-based comparisons.\nSupported Operators\n$eq\n: Equal to (same as direct value comparison)\n$ne\n: Not equal to\n$gt\n: Greater than\n$gte\n: Greater than or equal to\n$lt\n: Less than\n$lte\n: Less than or equal to\nExamples\nSimple exact match:\n{\n\"status\"\n:\n\"active\"\n}\nComparison operators:\n{\n\"score\"\n:\n{\n\"$gt\"\n:\n4.99\n}}\n# Score greater than 4.99\nMultiple conditions:\n{\n\"score\"\n:\n{\n\"$gte\"\n:\n3.0\n},\n\"color\"\n:\n\"red\"\n}\nlimit\nclass-attribute\ninstance-attribute\n\u00b6\nlimit\n:\nint\n=\n10\nMaximum number of items to return in the search results.\noffset\nclass-attribute\ninstance-attribute\n\u00b6\noffset\n:\nint\n=\n0\nNumber of matching items to skip for pagination.\nquery\nclass-attribute\ninstance-attribute\n\u00b6\nquery\n:\nstr\n|\nNone\n=\nNone\nNatural language search query for semantic search capabilities.\nExamples\n\"technical documentation about REST APIs\"\n\"machine learning papers from 2023\"\nrefresh_ttl\nclass-attribute\ninstance-attribute\n\u00b6\nrefresh_ttl\n:\nbool\n=\nTrue\nWhether to refresh TTLs for the returned item.\nIf no TTL was specified for the original item(s),\nor if TTL support is not enabled for your adapter,\nthis argument is ignored.\nMatchCondition\n\u00b6\nBases:\nNamedTuple\nRepresents a pattern for matching namespaces in the store.\nThis class combines a match type (prefix or suffix) with a namespace path\npattern that can include wildcards to flexibly match different namespace\nhierarchies.\nExamples\nPrefix matching:\nMatchCondition\n(\nmatch_type\n=\n\"prefix\"\n,\npath\n=\n(\n\"users\"\n,\n\"profiles\"\n))\nSuffix matching with wildcard:\nMatchCondition\n(\nmatch_type\n=\n\"suffix\"\n,\npath\n=\n(\n\"cache\"\n,\n\"*\"\n))\nSimple suffix matching:\nMatchCondition\n(\nmatch_type\n=\n\"suffix\"\n,\npath\n=\n(\n\"v1\"\n,))\nmatch_type\ninstance-attribute\n\u00b6\nmatch_type\n:\nNamespaceMatchType\nType of namespace matching to perform.\npath\ninstance-attribute\n\u00b6\npath\n:\nNamespacePath\nNamespace path pattern that can include wildcards.\nListNamespacesOp\n\u00b6\nBases:\nNamedTuple\nOperation to list and filter namespaces in the store.\nThis operation allows exploring the organization of data, finding specific\ncollections, and navigating the namespace hierarchy.\nExamples\nList all namespaces under the\n\"documents\"\npath:\nListNamespacesOp\n(\nmatch_conditions\n=\n(\nMatchCondition\n(\nmatch_type\n=\n\"prefix\"\n,\npath\n=\n(\n\"documents\"\n,)),),\nmax_depth\n=\n2\n)\nList all namespaces that end with\n\"v1\"\n:\nListNamespacesOp\n(\nmatch_conditions\n=\n(\nMatchCondition\n(\nmatch_type\n=\n\"suffix\"\n,\npath\n=\n(\n\"v1\"\n,)),),\nlimit\n=\n50\n)\nmatch_conditions\nclass-attribute\ninstance-attribute\n\u00b6\nmatch_conditions\n:\ntuple\n[\nMatchCondition\n,\n...\n]\n|\nNone\n=\nNone\nOptional conditions for filtering namespaces.\nExamples\nAll user namespaces:\n(\nMatchCondition\n(\nmatch_type\n=\n\"prefix\"\n,\npath\n=\n(\n\"users\"\n,)),)\nAll namespaces that start with\n\"docs\"\nand end with\n\"draft\"\n:\n(\nMatchCondition\n(\nmatch_type\n=\n\"prefix\"\n,\npath\n=\n(\n\"docs\"\n,)),\nMatchCondition\n(\nmatch_type\n=\n\"suffix\"\n,\npath\n=\n(\n\"draft\"\n,))\n)\nmax_depth\nclass-attribute\ninstance-attribute\n\u00b6\nmax_depth\n:\nint\n|\nNone\n=\nNone\nMaximum depth of namespace hierarchy to return.\nNote\nNamespaces deeper than this level will be truncated.\nlimit\nclass-attribute\ninstance-attribute\n\u00b6\nlimit\n:\nint\n=\n100\nMaximum number of namespaces to return.\noffset\nclass-attribute\ninstance-attribute\n\u00b6\noffset\n:\nint\n=\n0\nNumber of namespaces to skip for pagination.\nPutOp\n\u00b6\nBases:\nNamedTuple\nOperation to store, update, or delete an item in the store.\nThis class represents a single operation to modify the store's contents,\nwhether adding new items, updating existing ones, or removing them.\nnamespace\ninstance-attribute\n\u00b6\nnamespace\n:\ntuple\n[\nstr\n,\n...\n]\nHierarchical path that identifies the location of the item.\nThe namespace acts as a folder-like structure to organize items.\nEach element in the tuple represents one level in the hierarchy.\nExamples\nRoot level documents:\n(\n\"documents\"\n,)\nUser-specific documents:\n(\n\"documents\"\n,\n\"user123\"\n)\nNested cache structure:\n(\n\"cache\"\n,\n\"embeddings\"\n,\n\"v1\"\n)\nkey\ninstance-attribute\n\u00b6\nkey\n:\nstr\nUnique identifier for the item within its namespace.\nThe key must be unique within the specific namespace to avoid conflicts.\nTogether with the namespace, it forms a complete path to the item.\nExample\nIf namespace is\n(\"documents\", \"user123\")\nand key is\n\"report1\"\n,\nthe full path would effectively be\n\"documents/user123/report1\"\nvalue\ninstance-attribute\n\u00b6\nvalue\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nThe data to store, or\nNone\nto mark the item for deletion.\nThe value must be a dictionary with string keys and JSON-serializable values.\nSetting this to\nNone\nsignals that the item should be deleted.\nExample\n{\n\"field1\": \"string value\",\n\"field2\": 123,\n\"nested\": {\"can\": \"contain\", \"any\": \"serializable data\"}\n}\nindex\nclass-attribute\ninstance-attribute\n\u00b6\nindex\n:\nLiteral\n[\nFalse\n]\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\nControls how the item's fields are indexed for search operations.\nIndexing configuration determines how the item can be found through search\nNone\n(default): Uses the store's default indexing configuration (if provided)\nFalse\n: Disables indexing for this item\nlist[str]\n: Specifies which json path fields to index for search\nThe item remains accessible through direct get() operations regardless of indexing.\nWhen indexed, fields can be searched using natural language queries through\nvector similarity search (if supported by the store implementation).\nPath Syntax\nSimple field access:\n\"field\"\nNested fields:\n\"parent.child.grandchild\"\nArray indexing:\nSpecific index:\n\"array[0]\"\nLast element:\n\"array[-1]\"\nAll elements (each individually):\n\"array[*]\"\nExamples\nNone\n- Use store defaults (whole item)\nlist[str]\n- List of fields to index\n[\n\"metadata.title\"\n,\n# Nested field access\n\"context[*].content\"\n,\n# Index content from all context as separate vectors\n\"authors[0].name\"\n,\n# First author's name\n\"revisions[-1].changes\"\n,\n# Most recent revision's changes\n\"sections[*].paragraphs[*].text\"\n,\n# All text from all paragraphs in all sections\n\"metadata.tags[*]\"\n,\n# All tags in metadata\n]\nttl\nclass-attribute\ninstance-attribute\n\u00b6\nttl\n:\nfloat\n|\nNone\n=\nNone\nControls the TTL (time-to-live) for the item in minutes.\nIf provided, and if the store you are using supports this feature, the item\nwill expire this many minutes after it was last accessed. The expiration timer\nrefreshes on both read operations (get/search) and write operations (put/update).\nWhen the TTL expires, the item will be scheduled for deletion on a best-effort basis.\nDefaults to\nNone\n(no expiration).\nInvalidNamespaceError\n\u00b6\nBases:\nValueError\nProvided namespace is invalid.\nTTLConfig\n\u00b6\nBases:\nTypedDict\nConfiguration for TTL (time-to-live) behavior in the store.\nrefresh_on_read\ninstance-attribute\n\u00b6\nrefresh_on_read\n:\nbool\nDefault behavior for refreshing TTLs on read operations (\nGET\nand\nSEARCH\n).\nIf\nTrue\n, TTLs will be refreshed on read operations (get/search) by default.\nThis can be overridden per-operation by explicitly setting\nrefresh_ttl\n.\nDefaults to\nTrue\nif not configured.\ndefault_ttl\ninstance-attribute\n\u00b6\ndefault_ttl\n:\nfloat\n|\nNone\nDefault TTL (time-to-live) in minutes for new items.\nIf provided, new items will expire after this many minutes after their last access.\nThe expiration timer refreshes on both read and write operations.\nDefaults to\nNone\n(no expiration).\nsweep_interval_minutes\ninstance-attribute\n\u00b6\nsweep_interval_minutes\n:\nint\n|\nNone\nInterval in minutes between TTL sweep operations.\nIf provided, the store will periodically delete expired items based on TTL.\nDefaults to None (no sweeping).\nIndexConfig\n\u00b6\nBases:\nTypedDict\nConfiguration for indexing documents for semantic search in the store.\nIf not provided to the store, the store will not support vector search.\nIn that case, all\nindex\narguments to\nput()\nand\naput()\noperations will be ignored.\ndims\ninstance-attribute\n\u00b6\ndims\n:\nint\nNumber of dimensions in the embedding vectors.\nCommon embedding models have the following dimensions\nopenai:text-embedding-3-large\n:\n3072\nopenai:text-embedding-3-small\n:\n1536\nopenai:text-embedding-ada-002\n:\n1536\ncohere:embed-english-v3.0\n:\n1024\ncohere:embed-english-light-v3.0\n:\n384\ncohere:embed-multilingual-v3.0\n:\n1024\ncohere:embed-multilingual-light-v3.0\n:\n384\nembed\ninstance-attribute\n\u00b6\nembed\n:\nEmbeddings\n|\nEmbeddingsFunc\n|\nAEmbeddingsFunc\n|\nstr\nOptional function to generate embeddings from text.\nCan be specified in three ways\nA LangChain\nEmbeddings\ninstance\nA synchronous embedding function (\nEmbeddingsFunc\n)\nAn asynchronous embedding function (\nAEmbeddingsFunc\n)\nA provider string (e.g.,\n\"openai:text-embedding-3-small\"\n)\nExamples\nUsing LangChain's initialization with\nInMemoryStore\n:\nfrom\nlangchain.embeddings\nimport\ninit_embeddings\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nstore\n=\nInMemoryStore\n(\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n\"embed\"\n:\ninit_embeddings\n(\n\"openai:text-embedding-3-small\"\n)\n}\n)\nUsing a custom embedding function with\nInMemoryStore\n:\nfrom\nopenai\nimport\nOpenAI\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nclient\n=\nOpenAI\n()\ndef\nembed_texts\n(\ntexts\n:\nlist\n[\nstr\n])\n->\nlist\n[\nlist\n[\nfloat\n]]:\nresponse\n=\nclient\n.\nembeddings\n.\ncreate\n(\nmodel\n=\n\"text-embedding-3-small\"\n,\ninput\n=\ntexts\n)\nreturn\n[\ne\n.\nembedding\nfor\ne\nin\nresponse\n.\ndata\n]\nstore\n=\nInMemoryStore\n(\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n\"embed\"\n:\nembed_texts\n}\n)\nUsing an asynchronous embedding function with\nInMemoryStore\n:\nfrom\nopenai\nimport\nAsyncOpenAI\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nclient\n=\nAsyncOpenAI\n()\nasync\ndef\naembed_texts\n(\ntexts\n:\nlist\n[\nstr\n])\n->\nlist\n[\nlist\n[\nfloat\n]]:\nresponse\n=\nawait\nclient\n.\nembeddings\n.\ncreate\n(\nmodel\n=\n\"text-embedding-3-small\"\n,\ninput\n=\ntexts\n)\nreturn\n[\ne\n.\nembedding\nfor\ne\nin\nresponse\n.\ndata\n]\nstore\n=\nInMemoryStore\n(\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n\"embed\"\n:\naembed_texts\n}\n)\nfields\ninstance-attribute\n\u00b6\nfields\n:\nlist\n[\nstr\n]\n|\nNone\nFields to extract text from for embedding generation.\nControls which parts of stored items are embedded for semantic search. Follows JSON path syntax:\n[\"$\"]\n: Embeds the entire JSON object as one vector  (default)\n[\"field1\", \"field2\"]\n: Embeds specific top-level fields\n[\"parent.child\"]\n: Embeds nested fields using dot notation\n[\"array[*].field\"]\n: Embeds field from each array element separately\nNote\nYou can always override this behavior when storing an item using the\nindex\nparameter in the\nput\nor\naput\noperations.\nExamples\n# Embed entire document (default)\nfields\n=\n[\n\"$\"\n]\n# Embed specific fields\nfields\n=\n[\n\"text\"\n,\n\"summary\"\n]\n# Embed nested fields\nfields\n=\n[\n\"metadata.title\"\n,\n\"content.body\"\n]\n# Embed from arrays\nfields\n=\n[\n\"messages[*].content\"\n]\n# Each message content separately\nfields\n=\n[\n\"context[0].text\"\n]\n# First context item's text\nNote\nFields missing from a document are skipped\nArray notation creates separate embeddings for each element\nComplex nested paths are supported (e.g.,\n\"a.b[*].c.d\"\n)\nBaseStore\n\u00b6\nBases:\nABC\nAbstract base class for persistent key-value stores.\nStores enable persistence and memory that can be shared across threads,\nscoped to user IDs, assistant IDs, or other arbitrary namespaces.\nSome implementations may support semantic search capabilities through\nan optional\nindex\nconfiguration.\nNote\nSemantic search capabilities vary by implementation and are typically\ndisabled by default. Stores that support this feature can be configured\nby providing an\nindex\nconfiguration at creation time. Without this\nconfiguration, semantic search is disabled and any\nindex\narguments\nto storage operations will have no effect.\nSimilarly, TTL (time-to-live) support is disabled by default.\nSubclasses must explicitly set\nsupports_ttl = True\nto enable this feature.\nMETHOD\nDESCRIPTION\nbatch\nExecute multiple operations synchronously in a single batch.\nabatch\nExecute multiple operations asynchronously in a single batch.\nget\nRetrieve a single item.\nsearch\nSearch for items within a namespace prefix.\nput\nStore or update an item in the store.\ndelete\nDelete an item.\nlist_namespaces\nList and filter namespaces in the store.\naget\nAsynchronously retrieve a single item.\nasearch\nAsynchronously search for items within a namespace prefix.\naput\nAsynchronously store or update an item in the store.\nadelete\nAsynchronously delete an item.\nalist_namespaces\nList and filter namespaces in the store asynchronously.\nbatch\nabstractmethod\n\u00b6\nbatch\n(\nops\n:\nIterable\n[\nOp\n])\n->\nlist\n[\nResult\n]\nExecute multiple operations synchronously in a single batch.\nPARAMETER\nDESCRIPTION\nops\nAn iterable of operations to execute.\nTYPE:\nIterable\n[\nOp\n]\nRETURNS\nDESCRIPTION\nlist\n[\nResult\n]\nA list of results, where each result corresponds to an operation in the input.\nlist\n[\nResult\n]\nThe order of results matches the order of input operations.\nabatch\nabstractmethod\nasync\n\u00b6\nabatch\n(\nops\n:\nIterable\n[\nOp\n])\n->\nlist\n[\nResult\n]\nExecute multiple operations asynchronously in a single batch.\nPARAMETER\nDESCRIPTION\nops\nAn iterable of operations to execute.\nTYPE:\nIterable\n[\nOp\n]\nRETURNS\nDESCRIPTION\nlist\n[\nResult\n]\nA list of results, where each result corresponds to an operation in the input.\nlist\n[\nResult\n]\nThe order of results matches the order of input operations.\nget\n\u00b6\nget\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\n*\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n)\n->\nItem\n|\nNone\nRetrieve a single item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nrefresh_ttl\nWhether to refresh TTLs for the returned item.\nIf\nNone\n, uses the store's default\nrefresh_ttl\nsetting.\nIf no TTL is specified, this argument is ignored.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nItem\n| None\nThe retrieved item or\nNone\nif not found.\nsearch\n\u00b6\nsearch\n(\nnamespace_prefix\n:\ntuple\n[\nstr\n,\n...\n],\n/\n,\n*\n,\nquery\n:\nstr\n|\nNone\n=\nNone\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n10\n,\noffset\n:\nint\n=\n0\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n,\n)\n->\nlist\n[\nSearchItem\n]\nSearch for items within a namespace prefix.\nPARAMETER\nDESCRIPTION\nnamespace_prefix\nHierarchical path prefix to search within.\nTYPE:\ntuple\n[\nstr\n, ...]\nquery\nOptional query for natural language search.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nfilter\nKey-value pairs to filter results.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nlimit\nMaximum number of items to return.\nTYPE:\nint\nDEFAULT:\n10\noffset\nNumber of items to skip before returning results.\nTYPE:\nint\nDEFAULT:\n0\nrefresh_ttl\nWhether to refresh TTLs for the returned items.\nIf no TTL is specified, this argument is ignored.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nlist\n[\nSearchItem\n]\nList of items matching the search criteria.\nExamples\nBasic filtering:\n# Search for documents with specific metadata\nresults\n=\nstore\n.\nsearch\n(\n(\n\"docs\"\n,),\nfilter\n=\n{\n\"type\"\n:\n\"article\"\n,\n\"status\"\n:\n\"published\"\n}\n)\nNatural language search (requires vector store implementation):\n# Initialize store with embedding configuration\nstore\n=\nYourStore\n(\n# e.g., InMemoryStore, AsyncPostgresStore\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n# embedding dimensions\n\"embed\"\n:\nyour_embedding_function\n,\n# function to create embeddings\n\"fields\"\n:\n[\n\"text\"\n]\n# fields to embed. Defaults to [\"$\"]\n}\n)\n# Search for semantically similar documents\nresults\n=\nstore\n.\nsearch\n(\n(\n\"docs\"\n,),\nquery\n=\n\"machine learning applications in healthcare\"\n,\nfilter\n=\n{\n\"type\"\n:\n\"research_paper\"\n},\nlimit\n=\n5\n)\nNote\nNatural language search support depends on your store implementation\nand requires proper embedding configuration.\nput\n\u00b6\nput\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\nvalue\n:\ndict\n[\nstr\n,\nAny\n],\nindex\n:\nLiteral\n[\nFalse\n]\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n*\n,\nttl\n:\nfloat\n|\nNone\n|\nNotProvided\n=\nNOT_PROVIDED\n,\n)\n->\nNone\nStore or update an item in the store.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item, represented as a tuple of strings.\nExample:\n(\"documents\", \"user123\")\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace. Together with namespace forms\nthe complete path to the item.\nTYPE:\nstr\nvalue\nDictionary containing the item's data. Must contain string keys\nand JSON-serializable values.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nindex\nControls how the item's fields are indexed for search:\nNone (default): Use\nfields\nyou configured when creating the store (if any)\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored\nFalse: Disable indexing for this item\nlist[str]\n: List of field paths to index, supporting:\nNested fields:\n\"metadata.title\"\nArray access:\n\"chapters[*].content\"\n(each indexed separately)\nSpecific indices:\n\"authors[0].name\"\nTYPE:\nLiteral\n[False] |\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nttl\nTime to live in minutes. Support for this argument depends on your store adapter.\nIf specified, the item will expire after this many minutes from when it was last accessed.\nNone means no expiration. Expired runs will be deleted opportunistically.\nBy default, the expiration timer refreshes on both read operations (get/search)\nand write operations (put/update), whenever the item is included in the operation.\nTYPE:\nfloat\n| None |\nNotProvided\nDEFAULT:\nNOT_PROVIDED\nNote\nIndexing support depends on your store implementation.\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored.\nSimilarly, TTL support depends on the specific store implementation.\nSome implementations may not support expiration of items.\nExamples\nStore item. Indexing depends on how you configure the store:\nstore\n.\nput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n})\nDo not index item for semantic search. Still accessible through\nget()\nand\nsearch()\noperations but won't have a vector representation.\nstore\n.\nput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n},\nindex\n=\nFalse\n)\nIndex specific fields for search:\nstore\n.\nput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n},\nindex\n=\n[\n\"memory\"\n])\ndelete\n\u00b6\ndelete\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n)\n->\nNone\nDelete an item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nlist_namespaces\n\u00b6\nlist_namespaces\n(\n*\n,\nprefix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nsuffix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nmax_depth\n:\nint\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n100\n,\noffset\n:\nint\n=\n0\n,\n)\n->\nlist\n[\ntuple\n[\nstr\n,\n...\n]]\nList and filter namespaces in the store.\nUsed to explore the organization of data,\nfind specific collections, or navigate the namespace hierarchy.\nPARAMETER\nDESCRIPTION\nprefix\nFilter namespaces that start with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nsuffix\nFilter namespaces that end with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nmax_depth\nReturn namespaces up to this depth in the hierarchy.\nNamespaces deeper than this level will be truncated.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of namespaces to return.\nTYPE:\nint\nDEFAULT:\n100\noffset\nNumber of namespaces to skip for pagination.\nTYPE:\nint\nDEFAULT:\n0\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nstr\n, ...]]\nA list of namespace tuples that match the criteria. Each tuple represents a\nfull namespace path up to\nmax_depth\n.\n???+ example \"Examples\":\nSetting `max_depth=3`. Given the namespaces:\n```python\n# Example if you have the following namespaces:\n# (\"a\", \"b\", \"c\")\n# (\"a\", \"b\", \"d\", \"e\")\n# (\"a\", \"b\", \"d\", \"i\")\n# (\"a\", \"b\", \"f\")\n# (\"a\", \"c\", \"f\")\nstore.list_namespaces(prefix=(\"a\", \"b\"), max_depth=3)\n# [(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"b\", \"f\")]\n```\naget\nasync\n\u00b6\naget\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\n*\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n)\n->\nItem\n|\nNone\nAsynchronously retrieve a single item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nItem\n| None\nThe retrieved item or\nNone\nif not found.\nasearch\nasync\n\u00b6\nasearch\n(\nnamespace_prefix\n:\ntuple\n[\nstr\n,\n...\n],\n/\n,\n*\n,\nquery\n:\nstr\n|\nNone\n=\nNone\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n10\n,\noffset\n:\nint\n=\n0\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n,\n)\n->\nlist\n[\nSearchItem\n]\nAsynchronously search for items within a namespace prefix.\nPARAMETER\nDESCRIPTION\nnamespace_prefix\nHierarchical path prefix to search within.\nTYPE:\ntuple\n[\nstr\n, ...]\nquery\nOptional query for natural language search.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nfilter\nKey-value pairs to filter results.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nlimit\nMaximum number of items to return.\nTYPE:\nint\nDEFAULT:\n10\noffset\nNumber of items to skip before returning results.\nTYPE:\nint\nDEFAULT:\n0\nrefresh_ttl\nWhether to refresh TTLs for the returned items.\nIf\nNone\n, uses the store's\nTTLConfig.refresh_default\nsetting.\nIf\nTTLConfig\nis not provided or no TTL is specified, this argument is ignored.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nlist\n[\nSearchItem\n]\nList of items matching the search criteria.\nExamples\nBasic filtering:\n# Search for documents with specific metadata\nresults\n=\nawait\nstore\n.\nasearch\n(\n(\n\"docs\"\n,),\nfilter\n=\n{\n\"type\"\n:\n\"article\"\n,\n\"status\"\n:\n\"published\"\n}\n)\nNatural language search (requires vector store implementation):\n# Initialize store with embedding configuration\nstore\n=\nYourStore\n(\n# e.g., InMemoryStore, AsyncPostgresStore\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n# embedding dimensions\n\"embed\"\n:\nyour_embedding_function\n,\n# function to create embeddings\n\"fields\"\n:\n[\n\"text\"\n]\n# fields to embed\n}\n)\n# Search for semantically similar documents\nresults\n=\nawait\nstore\n.\nasearch\n(\n(\n\"docs\"\n,),\nquery\n=\n\"machine learning applications in healthcare\"\n,\nfilter\n=\n{\n\"type\"\n:\n\"research_paper\"\n},\nlimit\n=\n5\n)\nNote\nNatural language search support depends on your store implementation\nand requires proper embedding configuration.\naput\nasync\n\u00b6\naput\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\nvalue\n:\ndict\n[\nstr\n,\nAny\n],\nindex\n:\nLiteral\n[\nFalse\n]\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n*\n,\nttl\n:\nfloat\n|\nNone\n|\nNotProvided\n=\nNOT_PROVIDED\n,\n)\n->\nNone\nAsynchronously store or update an item in the store.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item, represented as a tuple of strings.\nExample:\n(\"documents\", \"user123\")\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace. Together with namespace forms\nthe complete path to the item.\nTYPE:\nstr\nvalue\nDictionary containing the item's data. Must contain string keys\nand JSON-serializable values.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nindex\nControls how the item's fields are indexed for search:\nNone (default): Use\nfields\nyou configured when creating the store (if any)\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored\nFalse: Disable indexing for this item\nlist[str]\n: List of field paths to index, supporting:\nNested fields:\n\"metadata.title\"\nArray access:\n\"chapters[*].content\"\n(each indexed separately)\nSpecific indices:\n\"authors[0].name\"\nTYPE:\nLiteral\n[False] |\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nttl\nTime to live in minutes. Support for this argument depends on your store adapter.\nIf specified, the item will expire after this many minutes from when it was last accessed.\nNone means no expiration. Expired runs will be deleted opportunistically.\nBy default, the expiration timer refreshes on both read operations (get/search)\nand write operations (put/update), whenever the item is included in the operation.\nTYPE:\nfloat\n| None |\nNotProvided\nDEFAULT:\nNOT_PROVIDED\nNote\nIndexing support depends on your store implementation.\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored.\nSimilarly, TTL support depends on the specific store implementation.\nSome implementations may not support expiration of items.\nExamples\nStore item. Indexing depends on how you configure the store:\nawait\nstore\n.\naput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n})\nDo not index item for semantic search. Still accessible through\nget()\nand\nsearch()\noperations but won't have a vector representation.\nawait\nstore\n.\naput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n},\nindex\n=\nFalse\n)\nIndex specific fields for search (if store configured to index items):\nawait\nstore\n.\naput\n(\n(\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n,\n\"context\"\n:\n[{\n\"content\"\n:\n\"...\"\n},\n{\n\"content\"\n:\n\"...\"\n}]\n},\nindex\n=\n[\n\"memory\"\n,\n\"context[*].content\"\n]\n)\nadelete\nasync\n\u00b6\nadelete\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n)\n->\nNone\nAsynchronously delete an item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nalist_namespaces\nasync\n\u00b6\nalist_namespaces\n(\n*\n,\nprefix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nsuffix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nmax_depth\n:\nint\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n100\n,\noffset\n:\nint\n=\n0\n,\n)\n->\nlist\n[\ntuple\n[\nstr\n,\n...\n]]\nList and filter namespaces in the store asynchronously.\nUsed to explore the organization of data,\nfind specific collections, or navigate the namespace hierarchy.\nPARAMETER\nDESCRIPTION\nprefix\nFilter namespaces that start with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nsuffix\nFilter namespaces that end with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nmax_depth\nReturn namespaces up to this depth in the hierarchy.\nNamespaces deeper than this level will be truncated to this depth.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of namespaces to return.\nTYPE:\nint\nDEFAULT:\n100\noffset\nNumber of namespaces to skip for pagination.\nTYPE:\nint\nDEFAULT:\n0\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nstr\n, ...]]\nA list of namespace tuples that match the criteria. Each tuple represents a\nfull namespace path up to\nmax_depth\n.\nExamples\nSetting\nmax_depth=3\nwith existing namespaces:\n# Given the following namespaces:\n# (\"a\", \"b\", \"c\")\n# (\"a\", \"b\", \"d\", \"e\")\n# (\"a\", \"b\", \"d\", \"i\")\n# (\"a\", \"b\", \"f\")\n# (\"a\", \"c\", \"f\")\nawait\nstore\n.\nalist_namespaces\n(\nprefix\n=\n(\n\"a\"\n,\n\"b\"\n),\nmax_depth\n=\n3\n)\n# Returns: [(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"b\", \"f\")]\nensure_embeddings\n\u00b6\nensure_embeddings\n(\nembed\n:\nEmbeddings\n|\nEmbeddingsFunc\n|\nAEmbeddingsFunc\n|\nstr\n|\nNone\n,\n)\n->\nEmbeddings\nEnsure that an embedding function conforms to LangChain's Embeddings interface.\nThis function wraps arbitrary embedding functions to make them compatible with\nLangChain's Embeddings interface. It handles both synchronous and asynchronous\nfunctions.\nPARAMETER\nDESCRIPTION\nembed\nEither an existing Embeddings instance, or a function that converts\ntext to embeddings. If the function is async, it will be used for both\nsync and async operations.\nTYPE:\nEmbeddings\n|\nEmbeddingsFunc\n|\nAEmbeddingsFunc\n|\nstr\n| None\nRETURNS\nDESCRIPTION\nEmbeddings\nAn Embeddings instance that wraps the provided function(s).\nExamples\nWrap a synchronous embedding function:\ndef\nmy_embed_fn\n(\ntexts\n):\nreturn\n[[\n0.1\n,\n0.2\n]\nfor\n_\nin\ntexts\n]\nembeddings\n=\nensure_embeddings\n(\nmy_embed_fn\n)\nresult\n=\nembeddings\n.\nembed_query\n(\n\"hello\"\n)\n# Returns [0.1, 0.2]\nWrap an asynchronous embedding function:\nasync\ndef\nmy_async_fn\n(\ntexts\n):\nreturn\n[[\n0.1\n,\n0.2\n]\nfor\n_\nin\ntexts\n]\nembeddings\n=\nensure_embeddings\n(\nmy_async_fn\n)\nresult\n=\nawait\nembeddings\n.\naembed_query\n(\n\"hello\"\n)\n# Returns [0.1, 0.2]\nInitialize embeddings using a provider string:\n# Requires langchain>=0.3.9 and langgraph-checkpoint>=2.0.11\nembeddings\n=\nensure_embeddings\n(\n\"openai:text-embedding-3-small\"\n)\nresult\n=\nembeddings\n.\nembed_query\n(\n\"hello\"\n)\nget_text_at_path\n\u00b6\nget_text_at_path\n(\nobj\n:\nAny\n,\npath\n:\nstr\n|\nlist\n[\nstr\n])\n->\nlist\n[\nstr\n]\nExtract text from an object using a path expression or pre-tokenized path.\nPARAMETER\nDESCRIPTION\nobj\nThe object to extract text from\nTYPE:\nAny\npath\nEither a path string or pre-tokenized path list.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nPath types handled\nSimple paths: \"field1.field2\"\nArray indexing: \"[0]\", \"[*]\", \"[-1]\"\nWildcards: \"*\"\nMulti-field selection: \"{field1,field2}\"\nNested paths in multi-field: \"{field1,nested.field2}\"\ntokenize_path\n\u00b6\ntokenize_path\n(\npath\n:\nstr\n)\n->\nlist\n[\nstr\n]\nTokenize a path into components.\nTypes handled\nSimple paths: \"field1.field2\"\nArray indexing: \"[0]\", \"[*]\", \"[-1]\"\nWildcards: \"*\"\nMulti-field selection: \"{field1,field2}\"\npostgres\n\u00b6\nAsyncPostgresStore\n\u00b6\nBases:\nAsyncBatchedBaseStore\n,\nBasePostgresStore\n[\nConn\n]\nAsynchronous Postgres-backed store with optional vector search using pgvector.\nExamples\nBasic setup and usage:\nfrom\nlanggraph.store.postgres\nimport\nAsyncPostgresStore\nconn_string\n=\n\"postgresql://user:pass@localhost:5432/dbname\"\nasync\nwith\nAsyncPostgresStore\n.\nfrom_conn_string\n(\nconn_string\n)\nas\nstore\n:\nawait\nstore\n.\nsetup\n()\n# Run migrations. Done once\n# Store and retrieve data\nawait\nstore\n.\naput\n((\n\"users\"\n,\n\"123\"\n),\n\"prefs\"\n,\n{\n\"theme\"\n:\n\"dark\"\n})\nitem\n=\nawait\nstore\n.\naget\n((\n\"users\"\n,\n\"123\"\n),\n\"prefs\"\n)\nVector search using LangChain embeddings:\nfrom\nlangchain.embeddings\nimport\ninit_embeddings\nfrom\nlanggraph.store.postgres\nimport\nAsyncPostgresStore\nconn_string\n=\n\"postgresql://user:pass@localhost:5432/dbname\"\nasync\nwith\nAsyncPostgresStore\n.\nfrom_conn_string\n(\nconn_string\n,\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n\"embed\"\n:\ninit_embeddings\n(\n\"openai:text-embedding-3-small\"\n),\n\"fields\"\n:\n[\n\"text\"\n]\n# specify which fields to embed. Default is the whole serialized value\n}\n)\nas\nstore\n:\nawait\nstore\n.\nsetup\n()\n# Run migrations. Done once\n# Store documents\nawait\nstore\n.\naput\n((\n\"docs\"\n,),\n\"doc1\"\n,\n{\n\"text\"\n:\n\"Python tutorial\"\n})\nawait\nstore\n.\naput\n((\n\"docs\"\n,),\n\"doc2\"\n,\n{\n\"text\"\n:\n\"TypeScript guide\"\n})\nawait\nstore\n.\naput\n((\n\"docs\"\n,),\n\"doc3\"\n,\n{\n\"text\"\n:\n\"Other guide\"\n},\nindex\n=\nFalse\n)\n# don't index\n# Search by similarity\nresults\n=\nawait\nstore\n.\nasearch\n((\n\"docs\"\n,),\nquery\n=\n\"programming guides\"\n,\nlimit\n=\n2\n)\nUsing connection pooling for better performance:\nfrom\nlanggraph.store.postgres\nimport\nAsyncPostgresStore\n,\nPoolConfig\nconn_string\n=\n\"postgresql://user:pass@localhost:5432/dbname\"\nasync\nwith\nAsyncPostgresStore\n.\nfrom_conn_string\n(\nconn_string\n,\npool_config\n=\nPoolConfig\n(\nmin_size\n=\n5\n,\nmax_size\n=\n20\n)\n)\nas\nstore\n:\nawait\nstore\n.\nsetup\n()\n# Run migrations. Done once\n# Use store with connection pooling...\nWarning\nMake sure to:\n1. Call\nsetup()\nbefore first use to create necessary tables and indexes\n2. Have the pgvector extension available to use vector search\n3. Use Python 3.10+ for async functionality\nNote\nSemantic search is disabled by default. You can enable it by providing an\nindex\nconfiguration\nwhen creating the store. Without this configuration, all\nindex\narguments passed to\nput\nor\naput\nwill have no effect.\nNote\nIf you provide a TTL configuration, you must explicitly call\nstart_ttl_sweeper()\nto begin\nthe background task that removes expired items. Call\nstop_ttl_sweeper()\nto properly\nclean up resources when you're done with the store.\nMETHOD\nDESCRIPTION\nbatch\nExecute multiple operations synchronously in a single batch.\nget\nRetrieve a single item.\nsearch\nSearch for items within a namespace prefix.\nput\nStore or update an item in the store.\ndelete\nDelete an item.\nlist_namespaces\nList and filter namespaces in the store.\naget\nAsynchronously retrieve a single item.\nasearch\nAsynchronously search for items within a namespace prefix.\naput\nAsynchronously store or update an item in the store.\nadelete\nAsynchronously delete an item.\nalist_namespaces\nList and filter namespaces in the store asynchronously.\nabatch\nExecute multiple operations asynchronously in a single batch.\nfrom_conn_string\nCreate a new AsyncPostgresStore instance from a connection string.\nsetup\nSet up the store database asynchronously.\nsweep_ttl\nDelete expired store items based on TTL.\nstart_ttl_sweeper\nPeriodically delete expired store items based on TTL.\nstop_ttl_sweeper\nStop the TTL sweeper task if it's running.\nbatch\n\u00b6\nbatch\n(\nops\n:\nIterable\n[\nOp\n])\n->\nlist\n[\nResult\n]\nExecute multiple operations synchronously in a single batch.\nPARAMETER\nDESCRIPTION\nops\nAn iterable of operations to execute.\nTYPE:\nIterable\n[\nOp\n]\nRETURNS\nDESCRIPTION\nlist\n[\nResult\n]\nA list of results, where each result corresponds to an operation in the input.\nlist\n[\nResult\n]\nThe order of results matches the order of input operations.\nget\n\u00b6\nget\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\n*\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n)\n->\nItem\n|\nNone\nRetrieve a single item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nrefresh_ttl\nWhether to refresh TTLs for the returned item.\nIf\nNone\n, uses the store's default\nrefresh_ttl\nsetting.\nIf no TTL is specified, this argument is ignored.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nItem\n| None\nThe retrieved item or\nNone\nif not found.\nsearch\n\u00b6\nsearch\n(\nnamespace_prefix\n:\ntuple\n[\nstr\n,\n...\n],\n/\n,\n*\n,\nquery\n:\nstr\n|\nNone\n=\nNone\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n10\n,\noffset\n:\nint\n=\n0\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n,\n)\n->\nlist\n[\nSearchItem\n]\nSearch for items within a namespace prefix.\nPARAMETER\nDESCRIPTION\nnamespace_prefix\nHierarchical path prefix to search within.\nTYPE:\ntuple\n[\nstr\n, ...]\nquery\nOptional query for natural language search.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nfilter\nKey-value pairs to filter results.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nlimit\nMaximum number of items to return.\nTYPE:\nint\nDEFAULT:\n10\noffset\nNumber of items to skip before returning results.\nTYPE:\nint\nDEFAULT:\n0\nrefresh_ttl\nWhether to refresh TTLs for the returned items.\nIf no TTL is specified, this argument is ignored.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nlist\n[\nSearchItem\n]\nList of items matching the search criteria.\nExamples\nBasic filtering:\n# Search for documents with specific metadata\nresults\n=\nstore\n.\nsearch\n(\n(\n\"docs\"\n,),\nfilter\n=\n{\n\"type\"\n:\n\"article\"\n,\n\"status\"\n:\n\"published\"\n}\n)\nNatural language search (requires vector store implementation):\n# Initialize store with embedding configuration\nstore\n=\nYourStore\n(\n# e.g., InMemoryStore, AsyncPostgresStore\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n# embedding dimensions\n\"embed\"\n:\nyour_embedding_function\n,\n# function to create embeddings\n\"fields\"\n:\n[\n\"text\"\n]\n# fields to embed. Defaults to [\"$\"]\n}\n)\n# Search for semantically similar documents\nresults\n=\nstore\n.\nsearch\n(\n(\n\"docs\"\n,),\nquery\n=\n\"machine learning applications in healthcare\"\n,\nfilter\n=\n{\n\"type\"\n:\n\"research_paper\"\n},\nlimit\n=\n5\n)\nNote\nNatural language search support depends on your store implementation\nand requires proper embedding configuration.\nput\n\u00b6\nput\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\nvalue\n:\ndict\n[\nstr\n,\nAny\n],\nindex\n:\nLiteral\n[\nFalse\n]\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n*\n,\nttl\n:\nfloat\n|\nNone\n|\nNotProvided\n=\nNOT_PROVIDED\n,\n)\n->\nNone\nStore or update an item in the store.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item, represented as a tuple of strings.\nExample:\n(\"documents\", \"user123\")\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace. Together with namespace forms\nthe complete path to the item.\nTYPE:\nstr\nvalue\nDictionary containing the item's data. Must contain string keys\nand JSON-serializable values.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nindex\nControls how the item's fields are indexed for search:\nNone (default): Use\nfields\nyou configured when creating the store (if any)\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored\nFalse: Disable indexing for this item\nlist[str]\n: List of field paths to index, supporting:\nNested fields:\n\"metadata.title\"\nArray access:\n\"chapters[*].content\"\n(each indexed separately)\nSpecific indices:\n\"authors[0].name\"\nTYPE:\nLiteral\n[False] |\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nttl\nTime to live in minutes. Support for this argument depends on your store adapter.\nIf specified, the item will expire after this many minutes from when it was last accessed.\nNone means no expiration. Expired runs will be deleted opportunistically.\nBy default, the expiration timer refreshes on both read operations (get/search)\nand write operations (put/update), whenever the item is included in the operation.\nTYPE:\nfloat\n| None |\nNotProvided\nDEFAULT:\nNOT_PROVIDED\nNote\nIndexing support depends on your store implementation.\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored.\nSimilarly, TTL support depends on the specific store implementation.\nSome implementations may not support expiration of items.\nExamples\nStore item. Indexing depends on how you configure the store:\nstore\n.\nput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n})\nDo not index item for semantic search. Still accessible through\nget()\nand\nsearch()\noperations but won't have a vector representation.\nstore\n.\nput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n},\nindex\n=\nFalse\n)\nIndex specific fields for search:\nstore\n.\nput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n},\nindex\n=\n[\n\"memory\"\n])\ndelete\n\u00b6\ndelete\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n)\n->\nNone\nDelete an item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nlist_namespaces\n\u00b6\nlist_namespaces\n(\n*\n,\nprefix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nsuffix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nmax_depth\n:\nint\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n100\n,\noffset\n:\nint\n=\n0\n,\n)\n->\nlist\n[\ntuple\n[\nstr\n,\n...\n]]\nList and filter namespaces in the store.\nUsed to explore the organization of data,\nfind specific collections, or navigate the namespace hierarchy.\nPARAMETER\nDESCRIPTION\nprefix\nFilter namespaces that start with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nsuffix\nFilter namespaces that end with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nmax_depth\nReturn namespaces up to this depth in the hierarchy.\nNamespaces deeper than this level will be truncated.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of namespaces to return.\nTYPE:\nint\nDEFAULT:\n100\noffset\nNumber of namespaces to skip for pagination.\nTYPE:\nint\nDEFAULT:\n0\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nstr\n, ...]]\nA list of namespace tuples that match the criteria. Each tuple represents a\nfull namespace path up to\nmax_depth\n.\n???+ example \"Examples\":\nSetting `max_depth=3`. Given the namespaces:\n```python\n# Example if you have the following namespaces:\n# (\"a\", \"b\", \"c\")\n# (\"a\", \"b\", \"d\", \"e\")\n# (\"a\", \"b\", \"d\", \"i\")\n# (\"a\", \"b\", \"f\")\n# (\"a\", \"c\", \"f\")\nstore.list_namespaces(prefix=(\"a\", \"b\"), max_depth=3)\n# [(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"b\", \"f\")]\n```\naget\nasync\n\u00b6\naget\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\n*\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n)\n->\nItem\n|\nNone\nAsynchronously retrieve a single item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nItem\n| None\nThe retrieved item or\nNone\nif not found.\nasearch\nasync\n\u00b6\nasearch\n(\nnamespace_prefix\n:\ntuple\n[\nstr\n,\n...\n],\n/\n,\n*\n,\nquery\n:\nstr\n|\nNone\n=\nNone\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n10\n,\noffset\n:\nint\n=\n0\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n,\n)\n->\nlist\n[\nSearchItem\n]\nAsynchronously search for items within a namespace prefix.\nPARAMETER\nDESCRIPTION\nnamespace_prefix\nHierarchical path prefix to search within.\nTYPE:\ntuple\n[\nstr\n, ...]\nquery\nOptional query for natural language search.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nfilter\nKey-value pairs to filter results.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nlimit\nMaximum number of items to return.\nTYPE:\nint\nDEFAULT:\n10\noffset\nNumber of items to skip before returning results.\nTYPE:\nint\nDEFAULT:\n0\nrefresh_ttl\nWhether to refresh TTLs for the returned items.\nIf\nNone\n, uses the store's\nTTLConfig.refresh_default\nsetting.\nIf\nTTLConfig\nis not provided or no TTL is specified, this argument is ignored.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nlist\n[\nSearchItem\n]\nList of items matching the search criteria.\nExamples\nBasic filtering:\n# Search for documents with specific metadata\nresults\n=\nawait\nstore\n.\nasearch\n(\n(\n\"docs\"\n,),\nfilter\n=\n{\n\"type\"\n:\n\"article\"\n,\n\"status\"\n:\n\"published\"\n}\n)\nNatural language search (requires vector store implementation):\n# Initialize store with embedding configuration\nstore\n=\nYourStore\n(\n# e.g., InMemoryStore, AsyncPostgresStore\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n# embedding dimensions\n\"embed\"\n:\nyour_embedding_function\n,\n# function to create embeddings\n\"fields\"\n:\n[\n\"text\"\n]\n# fields to embed\n}\n)\n# Search for semantically similar documents\nresults\n=\nawait\nstore\n.\nasearch\n(\n(\n\"docs\"\n,),\nquery\n=\n\"machine learning applications in healthcare\"\n,\nfilter\n=\n{\n\"type\"\n:\n\"research_paper\"\n},\nlimit\n=\n5\n)\nNote\nNatural language search support depends on your store implementation\nand requires proper embedding configuration.\naput\nasync\n\u00b6\naput\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\nvalue\n:\ndict\n[\nstr\n,\nAny\n],\nindex\n:\nLiteral\n[\nFalse\n]\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n*\n,\nttl\n:\nfloat\n|\nNone\n|\nNotProvided\n=\nNOT_PROVIDED\n,\n)\n->\nNone\nAsynchronously store or update an item in the store.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item, represented as a tuple of strings.\nExample:\n(\"documents\", \"user123\")\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace. Together with namespace forms\nthe complete path to the item.\nTYPE:\nstr\nvalue\nDictionary containing the item's data. Must contain string keys\nand JSON-serializable values.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nindex\nControls how the item's fields are indexed for search:\nNone (default): Use\nfields\nyou configured when creating the store (if any)\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored\nFalse: Disable indexing for this item\nlist[str]\n: List of field paths to index, supporting:\nNested fields:\n\"metadata.title\"\nArray access:\n\"chapters[*].content\"\n(each indexed separately)\nSpecific indices:\n\"authors[0].name\"\nTYPE:\nLiteral\n[False] |\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nttl\nTime to live in minutes. Support for this argument depends on your store adapter.\nIf specified, the item will expire after this many minutes from when it was last accessed.\nNone means no expiration. Expired runs will be deleted opportunistically.\nBy default, the expiration timer refreshes on both read operations (get/search)\nand write operations (put/update), whenever the item is included in the operation.\nTYPE:\nfloat\n| None |\nNotProvided\nDEFAULT:\nNOT_PROVIDED\nNote\nIndexing support depends on your store implementation.\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored.\nSimilarly, TTL support depends on the specific store implementation.\nSome implementations may not support expiration of items.\nExamples\nStore item. Indexing depends on how you configure the store:\nawait\nstore\n.\naput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n})\nDo not index item for semantic search. Still accessible through\nget()\nand\nsearch()\noperations but won't have a vector representation.\nawait\nstore\n.\naput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n},\nindex\n=\nFalse\n)\nIndex specific fields for search (if store configured to index items):\nawait\nstore\n.\naput\n(\n(\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n,\n\"context\"\n:\n[{\n\"content\"\n:\n\"...\"\n},\n{\n\"content\"\n:\n\"...\"\n}]\n},\nindex\n=\n[\n\"memory\"\n,\n\"context[*].content\"\n]\n)\nadelete\nasync\n\u00b6\nadelete\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n)\n->\nNone\nAsynchronously delete an item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nalist_namespaces\nasync\n\u00b6\nalist_namespaces\n(\n*\n,\nprefix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nsuffix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nmax_depth\n:\nint\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n100\n,\noffset\n:\nint\n=\n0\n,\n)\n->\nlist\n[\ntuple\n[\nstr\n,\n...\n]]\nList and filter namespaces in the store asynchronously.\nUsed to explore the organization of data,\nfind specific collections, or navigate the namespace hierarchy.\nPARAMETER\nDESCRIPTION\nprefix\nFilter namespaces that start with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nsuffix\nFilter namespaces that end with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nmax_depth\nReturn namespaces up to this depth in the hierarchy.\nNamespaces deeper than this level will be truncated to this depth.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of namespaces to return.\nTYPE:\nint\nDEFAULT:\n100\noffset\nNumber of namespaces to skip for pagination.\nTYPE:\nint\nDEFAULT:\n0\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nstr\n, ...]]\nA list of namespace tuples that match the criteria. Each tuple represents a\nfull namespace path up to\nmax_depth\n.\nExamples\nSetting\nmax_depth=3\nwith existing namespaces:\n# Given the following namespaces:\n# (\"a\", \"b\", \"c\")\n# (\"a\", \"b\", \"d\", \"e\")\n# (\"a\", \"b\", \"d\", \"i\")\n# (\"a\", \"b\", \"f\")\n# (\"a\", \"c\", \"f\")\nawait\nstore\n.\nalist_namespaces\n(\nprefix\n=\n(\n\"a\"\n,\n\"b\"\n),\nmax_depth\n=\n3\n)\n# Returns: [(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"b\", \"f\")]\nabatch\nasync\n\u00b6\nabatch\n(\nops\n:\nIterable\n[\nOp\n])\n->\nlist\n[\nResult\n]\nExecute multiple operations asynchronously in a single batch.\nPARAMETER\nDESCRIPTION\nops\nAn iterable of operations to execute.\nTYPE:\nIterable\n[\nOp\n]\nRETURNS\nDESCRIPTION\nlist\n[\nResult\n]\nA list of results, where each result corresponds to an operation in the input.\nlist\n[\nResult\n]\nThe order of results matches the order of input operations.\nfrom_conn_string\nasync\nclassmethod\n\u00b6\nfrom_conn_string\n(\nconn_string\n:\nstr\n,\n*\n,\npipeline\n:\nbool\n=\nFalse\n,\npool_config\n:\nPoolConfig\n|\nNone\n=\nNone\n,\nindex\n:\nPostgresIndexConfig\n|\nNone\n=\nNone\n,\nttl\n:\nTTLConfig\n|\nNone\n=\nNone\n,\n)\n->\nAsyncIterator\n[\nAsyncPostgresStore\n]\nCreate a new AsyncPostgresStore instance from a connection string.\nPARAMETER\nDESCRIPTION\nconn_string\nThe Postgres connection info string.\nTYPE:\nstr\npipeline\nWhether to use AsyncPipeline (only for single connections)\nTYPE:\nbool\nDEFAULT:\nFalse\npool_config\nConfiguration for the connection pool.\nIf provided, will create a connection pool and use it instead of a single connection.\nThis overrides the\npipeline\nargument.\nTYPE:\nPoolConfig\n| None\nDEFAULT:\nNone\nindex\nThe embedding config.\nTYPE:\nPostgresIndexConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nAsyncPostgresStore\nA new AsyncPostgresStore instance.\nTYPE:\nAsyncIterator\n[\nAsyncPostgresStore\n]\nsetup\nasync\n\u00b6\nsetup\n()\n->\nNone\nSet up the store database asynchronously.\nThis method creates the necessary tables in the Postgres database if they don't\nalready exist and runs database migrations. It MUST be called directly by the user\nthe first time the store is used.\nsweep_ttl\nasync\n\u00b6\nsweep_ttl\n()\n->\nint\nDelete expired store items based on TTL.\nRETURNS\nDESCRIPTION\nint\nThe number of deleted items.\nTYPE:\nint\nstart_ttl_sweeper\nasync\n\u00b6\nstart_ttl_sweeper\n(\nsweep_interval_minutes\n:\nint\n|\nNone\n=\nNone\n)\n->\nTask\n[\nNone\n]\nPeriodically delete expired store items based on TTL.\nRETURNS\nDESCRIPTION\nTask\n[None]\nTask that can be awaited or cancelled.\nstop_ttl_sweeper\nasync\n\u00b6\nstop_ttl_sweeper\n(\ntimeout\n:\nfloat\n|\nNone\n=\nNone\n)\n->\nbool\nStop the TTL sweeper task if it's running.\nPARAMETER\nDESCRIPTION\ntimeout\nMaximum time to wait for the task to stop, in seconds.\nIf\nNone\n, wait indefinitely.\nTYPE:\nfloat\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nbool\nTrue if the task was successfully stopped or wasn't running,\nFalse if the timeout was reached before the task stopped.\nTYPE:\nbool\nPoolConfig\n\u00b6\nBases:\nTypedDict\nConnection pool settings for PostgreSQL connections.\nControls connection lifecycle and resource utilization:\n- Small pools (1-5) suit low-concurrency workloads\n- Larger pools handle concurrent requests but consume more resources\n- Setting max_size prevents resource exhaustion under load\nmin_size\ninstance-attribute\n\u00b6\nmin_size\n:\nint\nMinimum number of connections maintained in the pool. Defaults to 1.\nmax_size\ninstance-attribute\n\u00b6\nmax_size\n:\nint\n|\nNone\nMaximum number of connections allowed in the pool. None means unlimited.\nkwargs\ninstance-attribute\n\u00b6\nkwargs\n:\ndict\nAdditional connection arguments passed to each connection in the pool.\nDefault kwargs set automatically:\n- autocommit: True\n- prepare_threshold: 0\n- row_factory: dict_row\nPostgresStore\n\u00b6\nBases:\nBaseStore\n,\nBasePostgresStore\n[\nConn\n]\nPostgres-backed store with optional vector search using pgvector.\nExamples\nBasic setup and usage:\nfrom\nlanggraph.store.postgres\nimport\nPostgresStore\nfrom\npsycopg\nimport\nConnection\nconn_string\n=\n\"postgresql://user:pass@localhost:5432/dbname\"\n# Using direct connection\nwith\nConnection\n.\nconnect\n(\nconn_string\n)\nas\nconn\n:\nstore\n=\nPostgresStore\n(\nconn\n)\nstore\n.\nsetup\n()\n# Run migrations. Done once\n# Store and retrieve data\nstore\n.\nput\n((\n\"users\"\n,\n\"123\"\n),\n\"prefs\"\n,\n{\n\"theme\"\n:\n\"dark\"\n})\nitem\n=\nstore\n.\nget\n((\n\"users\"\n,\n\"123\"\n),\n\"prefs\"\n)\nOr using the convenient from_conn_string helper:\nfrom\nlanggraph.store.postgres\nimport\nPostgresStore\nconn_string\n=\n\"postgresql://user:pass@localhost:5432/dbname\"\nwith\nPostgresStore\n.\nfrom_conn_string\n(\nconn_string\n)\nas\nstore\n:\nstore\n.\nsetup\n()\n# Store and retrieve data\nstore\n.\nput\n((\n\"users\"\n,\n\"123\"\n),\n\"prefs\"\n,\n{\n\"theme\"\n:\n\"dark\"\n})\nitem\n=\nstore\n.\nget\n((\n\"users\"\n,\n\"123\"\n),\n\"prefs\"\n)\nVector search using LangChain embeddings:\nfrom\nlangchain.embeddings\nimport\ninit_embeddings\nfrom\nlanggraph.store.postgres\nimport\nPostgresStore\nconn_string\n=\n\"postgresql://user:pass@localhost:5432/dbname\"\nwith\nPostgresStore\n.\nfrom_conn_string\n(\nconn_string\n,\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n\"embed\"\n:\ninit_embeddings\n(\n\"openai:text-embedding-3-small\"\n),\n\"fields\"\n:\n[\n\"text\"\n]\n# specify which fields to embed. Default is the whole serialized value\n}\n)\nas\nstore\n:\nstore\n.\nsetup\n()\n# Do this once to run migrations\n# Store documents\nstore\n.\nput\n((\n\"docs\"\n,),\n\"doc1\"\n,\n{\n\"text\"\n:\n\"Python tutorial\"\n})\nstore\n.\nput\n((\n\"docs\"\n,),\n\"doc2\"\n,\n{\n\"text\"\n:\n\"TypeScript guide\"\n})\nstore\n.\nput\n((\n\"docs\"\n,),\n\"doc2\"\n,\n{\n\"text\"\n:\n\"Other guide\"\n},\nindex\n=\nFalse\n)\n# don't index\n# Search by similarity\nresults\n=\nstore\n.\nsearch\n((\n\"docs\"\n,),\nquery\n=\n\"programming guides\"\n,\nlimit\n=\n2\n)\nNote\nSemantic search is disabled by default. You can enable it by providing an\nindex\nconfiguration\nwhen creating the store. Without this configuration, all\nindex\narguments passed to\nput\nor\naput\nwill have no effect.\nWarning\nMake sure to call\nsetup()\nbefore first use to create necessary tables and indexes.\nThe pgvector extension must be available to use vector search.\nNote\nIf you provide a TTL configuration, you must explicitly call\nstart_ttl_sweeper()\nto begin\nthe background thread that removes expired items. Call\nstop_ttl_sweeper()\nto properly\nclean up resources when you're done with the store.\nMETHOD\nDESCRIPTION\nget\nRetrieve a single item.\nsearch\nSearch for items within a namespace prefix.\nput\nStore or update an item in the store.\ndelete\nDelete an item.\nlist_namespaces\nList and filter namespaces in the store.\naget\nAsynchronously retrieve a single item.\nasearch\nAsynchronously search for items within a namespace prefix.\naput\nAsynchronously store or update an item in the store.\nadelete\nAsynchronously delete an item.\nalist_namespaces\nList and filter namespaces in the store asynchronously.\nfrom_conn_string\nCreate a new PostgresStore instance from a connection string.\nsweep_ttl\nDelete expired store items based on TTL.\nstart_ttl_sweeper\nPeriodically delete expired store items based on TTL.\nstop_ttl_sweeper\nStop the TTL sweeper thread if it's running.\n__del__\nEnsure the TTL sweeper thread is stopped when the object is garbage collected.\nbatch\nExecute multiple operations synchronously in a single batch.\nabatch\nExecute multiple operations asynchronously in a single batch.\nsetup\nSet up the store database.\nget\n\u00b6\nget\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\n*\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n)\n->\nItem\n|\nNone\nRetrieve a single item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nrefresh_ttl\nWhether to refresh TTLs for the returned item.\nIf\nNone\n, uses the store's default\nrefresh_ttl\nsetting.\nIf no TTL is specified, this argument is ignored.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nItem\n| None\nThe retrieved item or\nNone\nif not found.\nsearch\n\u00b6\nsearch\n(\nnamespace_prefix\n:\ntuple\n[\nstr\n,\n...\n],\n/\n,\n*\n,\nquery\n:\nstr\n|\nNone\n=\nNone\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n10\n,\noffset\n:\nint\n=\n0\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n,\n)\n->\nlist\n[\nSearchItem\n]\nSearch for items within a namespace prefix.\nPARAMETER\nDESCRIPTION\nnamespace_prefix\nHierarchical path prefix to search within.\nTYPE:\ntuple\n[\nstr\n, ...]\nquery\nOptional query for natural language search.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nfilter\nKey-value pairs to filter results.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nlimit\nMaximum number of items to return.\nTYPE:\nint\nDEFAULT:\n10\noffset\nNumber of items to skip before returning results.\nTYPE:\nint\nDEFAULT:\n0\nrefresh_ttl\nWhether to refresh TTLs for the returned items.\nIf no TTL is specified, this argument is ignored.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nlist\n[\nSearchItem\n]\nList of items matching the search criteria.\nExamples\nBasic filtering:\n# Search for documents with specific metadata\nresults\n=\nstore\n.\nsearch\n(\n(\n\"docs\"\n,),\nfilter\n=\n{\n\"type\"\n:\n\"article\"\n,\n\"status\"\n:\n\"published\"\n}\n)\nNatural language search (requires vector store implementation):\n# Initialize store with embedding configuration\nstore\n=\nYourStore\n(\n# e.g., InMemoryStore, AsyncPostgresStore\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n# embedding dimensions\n\"embed\"\n:\nyour_embedding_function\n,\n# function to create embeddings\n\"fields\"\n:\n[\n\"text\"\n]\n# fields to embed. Defaults to [\"$\"]\n}\n)\n# Search for semantically similar documents\nresults\n=\nstore\n.\nsearch\n(\n(\n\"docs\"\n,),\nquery\n=\n\"machine learning applications in healthcare\"\n,\nfilter\n=\n{\n\"type\"\n:\n\"research_paper\"\n},\nlimit\n=\n5\n)\nNote\nNatural language search support depends on your store implementation\nand requires proper embedding configuration.\nput\n\u00b6\nput\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\nvalue\n:\ndict\n[\nstr\n,\nAny\n],\nindex\n:\nLiteral\n[\nFalse\n]\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n*\n,\nttl\n:\nfloat\n|\nNone\n|\nNotProvided\n=\nNOT_PROVIDED\n,\n)\n->\nNone\nStore or update an item in the store.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item, represented as a tuple of strings.\nExample:\n(\"documents\", \"user123\")\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace. Together with namespace forms\nthe complete path to the item.\nTYPE:\nstr\nvalue\nDictionary containing the item's data. Must contain string keys\nand JSON-serializable values.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nindex\nControls how the item's fields are indexed for search:\nNone (default): Use\nfields\nyou configured when creating the store (if any)\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored\nFalse: Disable indexing for this item\nlist[str]\n: List of field paths to index, supporting:\nNested fields:\n\"metadata.title\"\nArray access:\n\"chapters[*].content\"\n(each indexed separately)\nSpecific indices:\n\"authors[0].name\"\nTYPE:\nLiteral\n[False] |\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nttl\nTime to live in minutes. Support for this argument depends on your store adapter.\nIf specified, the item will expire after this many minutes from when it was last accessed.\nNone means no expiration. Expired runs will be deleted opportunistically.\nBy default, the expiration timer refreshes on both read operations (get/search)\nand write operations (put/update), whenever the item is included in the operation.\nTYPE:\nfloat\n| None |\nNotProvided\nDEFAULT:\nNOT_PROVIDED\nNote\nIndexing support depends on your store implementation.\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored.\nSimilarly, TTL support depends on the specific store implementation.\nSome implementations may not support expiration of items.\nExamples\nStore item. Indexing depends on how you configure the store:\nstore\n.\nput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n})\nDo not index item for semantic search. Still accessible through\nget()\nand\nsearch()\noperations but won't have a vector representation.\nstore\n.\nput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n},\nindex\n=\nFalse\n)\nIndex specific fields for search:\nstore\n.\nput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n},\nindex\n=\n[\n\"memory\"\n])\ndelete\n\u00b6\ndelete\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n)\n->\nNone\nDelete an item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nlist_namespaces\n\u00b6\nlist_namespaces\n(\n*\n,\nprefix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nsuffix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nmax_depth\n:\nint\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n100\n,\noffset\n:\nint\n=\n0\n,\n)\n->\nlist\n[\ntuple\n[\nstr\n,\n...\n]]\nList and filter namespaces in the store.\nUsed to explore the organization of data,\nfind specific collections, or navigate the namespace hierarchy.\nPARAMETER\nDESCRIPTION\nprefix\nFilter namespaces that start with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nsuffix\nFilter namespaces that end with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nmax_depth\nReturn namespaces up to this depth in the hierarchy.\nNamespaces deeper than this level will be truncated.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of namespaces to return.\nTYPE:\nint\nDEFAULT:\n100\noffset\nNumber of namespaces to skip for pagination.\nTYPE:\nint\nDEFAULT:\n0\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nstr\n, ...]]\nA list of namespace tuples that match the criteria. Each tuple represents a\nfull namespace path up to\nmax_depth\n.\n???+ example \"Examples\":\nSetting `max_depth=3`. Given the namespaces:\n```python\n# Example if you have the following namespaces:\n# (\"a\", \"b\", \"c\")\n# (\"a\", \"b\", \"d\", \"e\")\n# (\"a\", \"b\", \"d\", \"i\")\n# (\"a\", \"b\", \"f\")\n# (\"a\", \"c\", \"f\")\nstore.list_namespaces(prefix=(\"a\", \"b\"), max_depth=3)\n# [(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"b\", \"f\")]\n```\naget\nasync\n\u00b6\naget\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\n*\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n)\n->\nItem\n|\nNone\nAsynchronously retrieve a single item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nItem\n| None\nThe retrieved item or\nNone\nif not found.\nasearch\nasync\n\u00b6\nasearch\n(\nnamespace_prefix\n:\ntuple\n[\nstr\n,\n...\n],\n/\n,\n*\n,\nquery\n:\nstr\n|\nNone\n=\nNone\n,\nfilter\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n10\n,\noffset\n:\nint\n=\n0\n,\nrefresh_ttl\n:\nbool\n|\nNone\n=\nNone\n,\n)\n->\nlist\n[\nSearchItem\n]\nAsynchronously search for items within a namespace prefix.\nPARAMETER\nDESCRIPTION\nnamespace_prefix\nHierarchical path prefix to search within.\nTYPE:\ntuple\n[\nstr\n, ...]\nquery\nOptional query for natural language search.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nfilter\nKey-value pairs to filter results.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nlimit\nMaximum number of items to return.\nTYPE:\nint\nDEFAULT:\n10\noffset\nNumber of items to skip before returning results.\nTYPE:\nint\nDEFAULT:\n0\nrefresh_ttl\nWhether to refresh TTLs for the returned items.\nIf\nNone\n, uses the store's\nTTLConfig.refresh_default\nsetting.\nIf\nTTLConfig\nis not provided or no TTL is specified, this argument is ignored.\nTYPE:\nbool\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nlist\n[\nSearchItem\n]\nList of items matching the search criteria.\nExamples\nBasic filtering:\n# Search for documents with specific metadata\nresults\n=\nawait\nstore\n.\nasearch\n(\n(\n\"docs\"\n,),\nfilter\n=\n{\n\"type\"\n:\n\"article\"\n,\n\"status\"\n:\n\"published\"\n}\n)\nNatural language search (requires vector store implementation):\n# Initialize store with embedding configuration\nstore\n=\nYourStore\n(\n# e.g., InMemoryStore, AsyncPostgresStore\nindex\n=\n{\n\"dims\"\n:\n1536\n,\n# embedding dimensions\n\"embed\"\n:\nyour_embedding_function\n,\n# function to create embeddings\n\"fields\"\n:\n[\n\"text\"\n]\n# fields to embed\n}\n)\n# Search for semantically similar documents\nresults\n=\nawait\nstore\n.\nasearch\n(\n(\n\"docs\"\n,),\nquery\n=\n\"machine learning applications in healthcare\"\n,\nfilter\n=\n{\n\"type\"\n:\n\"research_paper\"\n},\nlimit\n=\n5\n)\nNote\nNatural language search support depends on your store implementation\nand requires proper embedding configuration.\naput\nasync\n\u00b6\naput\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n,\nvalue\n:\ndict\n[\nstr\n,\nAny\n],\nindex\n:\nLiteral\n[\nFalse\n]\n|\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n*\n,\nttl\n:\nfloat\n|\nNone\n|\nNotProvided\n=\nNOT_PROVIDED\n,\n)\n->\nNone\nAsynchronously store or update an item in the store.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item, represented as a tuple of strings.\nExample:\n(\"documents\", \"user123\")\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace. Together with namespace forms\nthe complete path to the item.\nTYPE:\nstr\nvalue\nDictionary containing the item's data. Must contain string keys\nand JSON-serializable values.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nindex\nControls how the item's fields are indexed for search:\nNone (default): Use\nfields\nyou configured when creating the store (if any)\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored\nFalse: Disable indexing for this item\nlist[str]\n: List of field paths to index, supporting:\nNested fields:\n\"metadata.title\"\nArray access:\n\"chapters[*].content\"\n(each indexed separately)\nSpecific indices:\n\"authors[0].name\"\nTYPE:\nLiteral\n[False] |\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nttl\nTime to live in minutes. Support for this argument depends on your store adapter.\nIf specified, the item will expire after this many minutes from when it was last accessed.\nNone means no expiration. Expired runs will be deleted opportunistically.\nBy default, the expiration timer refreshes on both read operations (get/search)\nand write operations (put/update), whenever the item is included in the operation.\nTYPE:\nfloat\n| None |\nNotProvided\nDEFAULT:\nNOT_PROVIDED\nNote\nIndexing support depends on your store implementation.\nIf you do not initialize the store with indexing capabilities,\nthe\nindex\nparameter will be ignored.\nSimilarly, TTL support depends on the specific store implementation.\nSome implementations may not support expiration of items.\nExamples\nStore item. Indexing depends on how you configure the store:\nawait\nstore\n.\naput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n})\nDo not index item for semantic search. Still accessible through\nget()\nand\nsearch()\noperations but won't have a vector representation.\nawait\nstore\n.\naput\n((\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n},\nindex\n=\nFalse\n)\nIndex specific fields for search (if store configured to index items):\nawait\nstore\n.\naput\n(\n(\n\"docs\"\n,),\n\"report\"\n,\n{\n\"memory\"\n:\n\"Will likes ai\"\n,\n\"context\"\n:\n[{\n\"content\"\n:\n\"...\"\n},\n{\n\"content\"\n:\n\"...\"\n}]\n},\nindex\n=\n[\n\"memory\"\n,\n\"context[*].content\"\n]\n)\nadelete\nasync\n\u00b6\nadelete\n(\nnamespace\n:\ntuple\n[\nstr\n,\n...\n],\nkey\n:\nstr\n)\n->\nNone\nAsynchronously delete an item.\nPARAMETER\nDESCRIPTION\nnamespace\nHierarchical path for the item.\nTYPE:\ntuple\n[\nstr\n, ...]\nkey\nUnique identifier within the namespace.\nTYPE:\nstr\nalist_namespaces\nasync\n\u00b6\nalist_namespaces\n(\n*\n,\nprefix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nsuffix\n:\nNamespacePath\n|\nNone\n=\nNone\n,\nmax_depth\n:\nint\n|\nNone\n=\nNone\n,\nlimit\n:\nint\n=\n100\n,\noffset\n:\nint\n=\n0\n,\n)\n->\nlist\n[\ntuple\n[\nstr\n,\n...\n]]\nList and filter namespaces in the store asynchronously.\nUsed to explore the organization of data,\nfind specific collections, or navigate the namespace hierarchy.\nPARAMETER\nDESCRIPTION\nprefix\nFilter namespaces that start with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nsuffix\nFilter namespaces that end with this path.\nTYPE:\nNamespacePath\n| None\nDEFAULT:\nNone\nmax_depth\nReturn namespaces up to this depth in the hierarchy.\nNamespaces deeper than this level will be truncated to this depth.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nlimit\nMaximum number of namespaces to return.\nTYPE:\nint\nDEFAULT:\n100\noffset\nNumber of namespaces to skip for pagination.\nTYPE:\nint\nDEFAULT:\n0\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nstr\n, ...]]\nA list of namespace tuples that match the criteria. Each tuple represents a\nfull namespace path up to\nmax_depth\n.\nExamples\nSetting\nmax_depth=3\nwith existing namespaces:\n# Given the following namespaces:\n# (\"a\", \"b\", \"c\")\n# (\"a\", \"b\", \"d\", \"e\")\n# (\"a\", \"b\", \"d\", \"i\")\n# (\"a\", \"b\", \"f\")\n# (\"a\", \"c\", \"f\")\nawait\nstore\n.\nalist_namespaces\n(\nprefix\n=\n(\n\"a\"\n,\n\"b\"\n),\nmax_depth\n=\n3\n)\n# Returns: [(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"b\", \"f\")]\nfrom_conn_string\nclassmethod\n\u00b6\nfrom_conn_string\n(\nconn_string\n:\nstr\n,\n*\n,\npipeline\n:\nbool\n=\nFalse\n,\npool_config\n:\nPoolConfig\n|\nNone\n=\nNone\n,\nindex\n:\nPostgresIndexConfig\n|\nNone\n=\nNone\n,\nttl\n:\nTTLConfig\n|\nNone\n=\nNone\n,\n)\n->\nIterator\n[\nPostgresStore\n]\nCreate a new PostgresStore instance from a connection string.\nPARAMETER\nDESCRIPTION\nconn_string\nThe Postgres connection info string.\nTYPE:\nstr\npipeline\nwhether to use Pipeline\nTYPE:\nbool\nDEFAULT:\nFalse\npool_config\nConfiguration for the connection pool.\nIf provided, will create a connection pool and use it instead of a single connection.\nThis overrides the\npipeline\nargument.\nTYPE:\nPoolConfig\n| None\nDEFAULT:\nNone\nindex\nThe index configuration for the store.\nTYPE:\nPostgresIndexConfig\n| None\nDEFAULT:\nNone\nttl\nThe TTL configuration for the store.\nTYPE:\nTTLConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nPostgresStore\nA new PostgresStore instance.\nTYPE:\nIterator\n[\nPostgresStore\n]\nsweep_ttl\n\u00b6\nsweep_ttl\n()\n->\nint\nDelete expired store items based on TTL.\nRETURNS\nDESCRIPTION\nint\nThe number of deleted items.\nTYPE:\nint\nstart_ttl_sweeper\n\u00b6\nstart_ttl_sweeper\n(\nsweep_interval_minutes\n:\nint\n|\nNone\n=\nNone\n)\n->\nFuture\n[\nNone\n]\nPeriodically delete expired store items based on TTL.\nRETURNS\nDESCRIPTION\nFuture\n[None]\nFuture that can be waited on or cancelled.\nstop_ttl_sweeper\n\u00b6\nstop_ttl_sweeper\n(\ntimeout\n:\nfloat\n|\nNone\n=\nNone\n)\n->\nbool\nStop the TTL sweeper thread if it's running.\nPARAMETER\nDESCRIPTION\ntimeout\nMaximum time to wait for the thread to stop, in seconds.\nIf\nNone\n, wait indefinitely.\nTYPE:\nfloat\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nbool\nTrue if the thread was successfully stopped or wasn't running,\nFalse if the timeout was reached before the thread stopped.\nTYPE:\nbool\n__del__\n\u00b6\n__del__\n()\n->\nNone\nEnsure the TTL sweeper thread is stopped when the object is garbage collected.\nbatch\n\u00b6\nbatch\n(\nops\n:\nIterable\n[\nOp\n])\n->\nlist\n[\nResult\n]\nExecute multiple operations synchronously in a single batch.\nPARAMETER\nDESCRIPTION\nops\nAn iterable of operations to execute.\nTYPE:\nIterable\n[\nOp\n]\nRETURNS\nDESCRIPTION\nlist\n[\nResult\n]\nA list of results, where each result corresponds to an operation in the input.\nlist\n[\nResult\n]\nThe order of results matches the order of input operations.\nabatch\nasync\n\u00b6\nabatch\n(\nops\n:\nIterable\n[\nOp\n])\n->\nlist\n[\nResult\n]\nExecute multiple operations asynchronously in a single batch.\nPARAMETER\nDESCRIPTION\nops\nAn iterable of operations to execute.\nTYPE:\nIterable\n[\nOp\n]\nRETURNS\nDESCRIPTION\nlist\n[\nResult\n]\nA list of results, where each result corresponds to an operation in the input.\nlist\n[\nResult\n]\nThe order of results matches the order of input operations.\nsetup\n\u00b6\nsetup\n()\n->\nNone\nSet up the store database.\nThis method creates the necessary tables in the Postgres database if they don't\nalready exist and runs database migrations. It MUST be called directly by the user\nthe first time the store is used.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/store/",
      "title": "Storage (LangGraph) | LangChain Reference",
      "heading": "Storage"
    }
  },
  {
    "page_content": "Caching (LangGraph) | LangChain Reference\nSkip to content\nLangChain Reference\nCaching (LangGraph)\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nCaching\nTable of contents\nbase\nBaseCache\n__init__\nget\naget\nset\naset\nclear\naclear\nmemory\nInMemoryCache\n__init__\nget\naget\nset\naset\nclear\naclear\nsqlite\nSqliteCache\n__init__\nget\naget\nset\naset\nclear\naclear\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nbase\nBaseCache\n__init__\nget\naget\nset\naset\nclear\naclear\nmemory\nInMemoryCache\n__init__\nget\naget\nset\naset\nclear\naclear\nsqlite\nSqliteCache\n__init__\nget\naget\nset\naset\nclear\naclear\nCaching\nbase\n\u00b6\nBaseCache\n\u00b6\nBases:\nABC\n,\nGeneric\n[\nValueT\n]\nBase class for a cache.\nMETHOD\nDESCRIPTION\n__init__\nInitialize the cache with a serializer.\nget\nGet the cached values for the given keys.\naget\nAsynchronously get the cached values for the given keys.\nset\nSet the cached values for the given keys and TTLs.\naset\nAsynchronously set the cached values for the given keys and TTLs.\nclear\nDelete the cached values for the given namespaces.\naclear\nAsynchronously delete the cached values for the given namespaces.\n__init__\n\u00b6\n__init__\n(\n*\n,\nserde\n:\nSerializerProtocol\n|\nNone\n=\nNone\n)\n->\nNone\nInitialize the cache with a serializer.\nget\nabstractmethod\n\u00b6\nget\n(\nkeys\n:\nSequence\n[\nFullKey\n])\n->\ndict\n[\nFullKey\n,\nValueT\n]\nGet the cached values for the given keys.\naget\nabstractmethod\nasync\n\u00b6\naget\n(\nkeys\n:\nSequence\n[\nFullKey\n])\n->\ndict\n[\nFullKey\n,\nValueT\n]\nAsynchronously get the cached values for the given keys.\nset\nabstractmethod\n\u00b6\nset\n(\npairs\n:\nMapping\n[\nFullKey\n,\ntuple\n[\nValueT\n,\nint\n|\nNone\n]])\n->\nNone\nSet the cached values for the given keys and TTLs.\naset\nabstractmethod\nasync\n\u00b6\naset\n(\npairs\n:\nMapping\n[\nFullKey\n,\ntuple\n[\nValueT\n,\nint\n|\nNone\n]])\n->\nNone\nAsynchronously set the cached values for the given keys and TTLs.\nclear\nabstractmethod\n\u00b6\nclear\n(\nnamespaces\n:\nSequence\n[\nNamespace\n]\n|\nNone\n=\nNone\n)\n->\nNone\nDelete the cached values for the given namespaces.\nIf no namespaces are provided, clear all cached values.\naclear\nabstractmethod\nasync\n\u00b6\naclear\n(\nnamespaces\n:\nSequence\n[\nNamespace\n]\n|\nNone\n=\nNone\n)\n->\nNone\nAsynchronously delete the cached values for the given namespaces.\nIf no namespaces are provided, clear all cached values.\nmemory\n\u00b6\nInMemoryCache\n\u00b6\nBases:\nBaseCache\n[\nValueT\n]\nMETHOD\nDESCRIPTION\n__init__\nInitialize the cache with a serializer.\nget\nGet the cached values for the given keys.\naget\nAsynchronously get the cached values for the given keys.\nset\nSet the cached values for the given keys.\naset\nAsynchronously set the cached values for the given keys.\nclear\nDelete the cached values for the given namespaces.\naclear\nAsynchronously delete the cached values for the given namespaces.\n__init__\n\u00b6\n__init__\n(\n*\n,\nserde\n:\nSerializerProtocol\n|\nNone\n=\nNone\n)\nInitialize the cache with a serializer.\nget\n\u00b6\nget\n(\nkeys\n:\nSequence\n[\nFullKey\n])\n->\ndict\n[\nFullKey\n,\nValueT\n]\nGet the cached values for the given keys.\naget\nasync\n\u00b6\naget\n(\nkeys\n:\nSequence\n[\nFullKey\n])\n->\ndict\n[\nFullKey\n,\nValueT\n]\nAsynchronously get the cached values for the given keys.\nset\n\u00b6\nset\n(\nkeys\n:\nMapping\n[\nFullKey\n,\ntuple\n[\nValueT\n,\nint\n|\nNone\n]])\n->\nNone\nSet the cached values for the given keys.\naset\nasync\n\u00b6\naset\n(\nkeys\n:\nMapping\n[\nFullKey\n,\ntuple\n[\nValueT\n,\nint\n|\nNone\n]])\n->\nNone\nAsynchronously set the cached values for the given keys.\nclear\n\u00b6\nclear\n(\nnamespaces\n:\nSequence\n[\nNamespace\n]\n|\nNone\n=\nNone\n)\n->\nNone\nDelete the cached values for the given namespaces.\nIf no namespaces are provided, clear all cached values.\naclear\nasync\n\u00b6\naclear\n(\nnamespaces\n:\nSequence\n[\nNamespace\n]\n|\nNone\n=\nNone\n)\n->\nNone\nAsynchronously delete the cached values for the given namespaces.\nIf no namespaces are provided, clear all cached values.\nsqlite\n\u00b6\nSqliteCache\n\u00b6\nBases:\nBaseCache\n[\nValueT\n]\nFile-based cache using SQLite.\nMETHOD\nDESCRIPTION\n__init__\nInitialize the cache with a file path.\nget\nGet the cached values for the given keys.\naget\nAsynchronously get the cached values for the given keys.\nset\nSet the cached values for the given keys and TTLs.\naset\nAsynchronously set the cached values for the given keys and TTLs.\nclear\nDelete the cached values for the given namespaces.\naclear\nAsynchronously delete the cached values for the given namespaces.\n__init__\n\u00b6\n__init__\n(\n*\n,\npath\n:\nstr\n,\nserde\n:\nSerializerProtocol\n|\nNone\n=\nNone\n)\n->\nNone\nInitialize the cache with a file path.\nget\n\u00b6\nget\n(\nkeys\n:\nSequence\n[\nFullKey\n])\n->\ndict\n[\nFullKey\n,\nValueT\n]\nGet the cached values for the given keys.\naget\nasync\n\u00b6\naget\n(\nkeys\n:\nSequence\n[\nFullKey\n])\n->\ndict\n[\nFullKey\n,\nValueT\n]\nAsynchronously get the cached values for the given keys.\nset\n\u00b6\nset\n(\nmapping\n:\nMapping\n[\nFullKey\n,\ntuple\n[\nValueT\n,\nint\n|\nNone\n]])\n->\nNone\nSet the cached values for the given keys and TTLs.\naset\nasync\n\u00b6\naset\n(\nmapping\n:\nMapping\n[\nFullKey\n,\ntuple\n[\nValueT\n,\nint\n|\nNone\n]])\n->\nNone\nAsynchronously set the cached values for the given keys and TTLs.\nclear\n\u00b6\nclear\n(\nnamespaces\n:\nSequence\n[\nNamespace\n]\n|\nNone\n=\nNone\n)\n->\nNone\nDelete the cached values for the given namespaces.\nIf no namespaces are provided, clear all cached values.\naclear\nasync\n\u00b6\naclear\n(\nnamespaces\n:\nSequence\n[\nNamespace\n]\n|\nNone\n=\nNone\n)\n->\nNone\nAsynchronously delete the cached values for the given namespaces.\nIf no namespaces are provided, clear all cached values.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/cache/",
      "title": "Caching (LangGraph) | LangChain Reference",
      "heading": "Caching"
    }
  },
  {
    "page_content": "Types | LangChain Reference\nSkip to content\nLangChain Reference\nTypes\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nTypes\nTable of contents\ntypes\nAll\nStreamMode\nStreamWriter\nRetryPolicy\ninitial_interval\nbackoff_factor\nmax_interval\nmax_attempts\njitter\nretry_on\nCachePolicy\nkey_func\nttl\nInterrupt\nid\nvalue\nPregelTask\nStateSnapshot\nvalues\nnext\nconfig\nmetadata\ncreated_at\nparent_config\ntasks\ninterrupts\nSend\n__init__\nCommand\nOverwrite\nvalue\ninterrupt\nRuntime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\ntypes\nAll\nStreamMode\nStreamWriter\nRetryPolicy\ninitial_interval\nbackoff_factor\nmax_interval\nmax_attempts\njitter\nretry_on\nCachePolicy\nkey_func\nttl\nInterrupt\nid\nvalue\nPregelTask\nStateSnapshot\nvalues\nnext\nconfig\nmetadata\ncreated_at\nparent_config\ntasks\ninterrupts\nSend\n__init__\nCommand\nOverwrite\nvalue\ninterrupt\nTypes\ntypes\n\u00b6\nFUNCTION\nDESCRIPTION\ninterrupt\nInterrupt the graph with a resumable exception from within a node.\nAll\nmodule-attribute\n\u00b6\nAll\n=\nLiteral\n[\n'*'\n]\nSpecial value to indicate that graph should interrupt on all nodes.\nStreamMode\nmodule-attribute\n\u00b6\nStreamMode\n=\nLiteral\n[\n\"values\"\n,\n\"updates\"\n,\n\"checkpoints\"\n,\n\"tasks\"\n,\n\"debug\"\n,\n\"messages\"\n,\n\"custom\"\n]\nHow the stream method should emit outputs.\n\"values\"\n: Emit all values in the state after each step, including interrupts.\nWhen used with functional API, values are emitted once at the end of the workflow.\n\"updates\"\n: Emit only the node or task names and updates returned by the nodes or tasks after each step.\nIf multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n\"custom\"\n: Emit custom data using from inside nodes or tasks using\nStreamWriter\n.\n\"messages\"\n: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.\n\"checkpoints\"\n: Emit an event when a checkpoint is created, in the same format as returned by\nget_state()\n.\n\"tasks\"\n: Emit events when tasks start and finish, including their results and errors.\n\"debug\"\n: Emit\n\"checkpoints\"\nand\n\"tasks\"\nevents for debugging purposes.\nStreamWriter\nmodule-attribute\n\u00b6\nStreamWriter\n=\nCallable\n[[\nAny\n],\nNone\n]\nCallable\nthat accepts a single argument and writes it to the output stream.\nAlways injected into nodes if requested as a keyword argument, but it's a no-op\nwhen not using\nstream_mode=\"custom\"\n.\nRetryPolicy\n\u00b6\nBases:\nNamedTuple\nConfiguration for retrying nodes.\nAdded in version 0.2.24\ninitial_interval\nclass-attribute\ninstance-attribute\n\u00b6\ninitial_interval\n:\nfloat\n=\n0.5\nAmount of time that must elapse before the first retry occurs. In seconds.\nbackoff_factor\nclass-attribute\ninstance-attribute\n\u00b6\nbackoff_factor\n:\nfloat\n=\n2.0\nMultiplier by which the interval increases after each retry.\nmax_interval\nclass-attribute\ninstance-attribute\n\u00b6\nmax_interval\n:\nfloat\n=\n128.0\nMaximum amount of time that may elapse between retries. In seconds.\nmax_attempts\nclass-attribute\ninstance-attribute\n\u00b6\nmax_attempts\n:\nint\n=\n3\nMaximum number of attempts to make before giving up, including the first.\njitter\nclass-attribute\ninstance-attribute\n\u00b6\njitter\n:\nbool\n=\nTrue\nWhether to add random jitter to the interval between retries.\nretry_on\nclass-attribute\ninstance-attribute\n\u00b6\nretry_on\n:\ntype\n[\nException\n]\n|\nSequence\n[\ntype\n[\nException\n]]\n|\nCallable\n[[\nException\n],\nbool\n]\n=\n(\ndefault_retry_on\n)\nList of exception classes that should trigger a retry, or a callable that returns\nTrue\nfor exceptions that should trigger a retry.\nCachePolicy\ndataclass\n\u00b6\nBases:\nGeneric\n[\nKeyFuncT\n]\nConfiguration for caching nodes.\nkey_func\nclass-attribute\ninstance-attribute\n\u00b6\nkey_func\n:\nKeyFuncT\n=\ndefault_cache_key\nFunction to generate a cache key from the node's input.\nDefaults to hashing the input with pickle.\nttl\nclass-attribute\ninstance-attribute\n\u00b6\nttl\n:\nint\n|\nNone\n=\nNone\nTime to live for the cache entry in seconds. If\nNone\n, the entry never expires.\nInterrupt\ndataclass\n\u00b6\nInformation about an interrupt that occurred in a node.\nAdded in version 0.2.24\nChanged in version v0.4.0\ninterrupt_id\nwas introduced as a property\nChanged in version v0.6.0\nThe following attributes have been removed:\nns\nwhen\nresumable\ninterrupt_id\n, deprecated in favor of\nid\nid\ninstance-attribute\n\u00b6\nid\n:\nstr\nThe ID of the interrupt. Can be used to resume the interrupt directly.\nvalue\ninstance-attribute\n\u00b6\nvalue\n:\nAny\n=\nvalue\nThe value associated with the interrupt.\nPregelTask\n\u00b6\nBases:\nNamedTuple\nA Pregel task.\nStateSnapshot\n\u00b6\nBases:\nNamedTuple\nSnapshot of the state of the graph at the beginning of a step.\nvalues\ninstance-attribute\n\u00b6\nvalues\n:\ndict\n[\nstr\n,\nAny\n]\n|\nAny\nCurrent values of channels.\nnext\ninstance-attribute\n\u00b6\nnext\n:\ntuple\n[\nstr\n,\n...\n]\nThe name of the node to execute in each task for this step.\nconfig\ninstance-attribute\n\u00b6\nconfig\n:\nRunnableConfig\nConfig used to fetch this snapshot.\nmetadata\ninstance-attribute\n\u00b6\nmetadata\n:\nCheckpointMetadata\n|\nNone\nMetadata associated with this snapshot.\ncreated_at\ninstance-attribute\n\u00b6\ncreated_at\n:\nstr\n|\nNone\nTimestamp of snapshot creation.\nparent_config\ninstance-attribute\n\u00b6\nparent_config\n:\nRunnableConfig\n|\nNone\nConfig used to fetch the parent snapshot, if any.\ntasks\ninstance-attribute\n\u00b6\ntasks\n:\ntuple\n[\nPregelTask\n,\n...\n]\nTasks to execute in this step. If already attempted, may contain an error.\ninterrupts\ninstance-attribute\n\u00b6\ninterrupts\n:\ntuple\n[\nInterrupt\n,\n...\n]\nInterrupts that occurred in this step that are pending resolution.\nSend\n\u00b6\nA message or packet to send to a specific node in the graph.\nThe\nSend\nclass is used within a\nStateGraph\n's conditional edges to\ndynamically invoke a node with a custom state at the next step.\nImportantly, the sent state can differ from the core graph's state,\nallowing for flexible and dynamic workflow management.\nOne such example is a \"map-reduce\" workflow where your graph invokes\nthe same node multiple times in parallel with different states,\nbefore aggregating the results back into the main graph's state.\nATTRIBUTE\nDESCRIPTION\nnode\nThe name of the target node to send the message to.\nTYPE:\nstr\narg\nThe state or message to send to the target node.\nTYPE:\nAny\nExample\nfrom\ntyping\nimport\nAnnotated\nfrom\nlanggraph.types\nimport\nSend\nfrom\nlanggraph.graph\nimport\nEND\n,\nSTART\nfrom\nlanggraph.graph\nimport\nStateGraph\nimport\noperator\nclass\nOverallState\n(\nTypedDict\n):\nsubjects\n:\nlist\n[\nstr\n]\njokes\n:\nAnnotated\n[\nlist\n[\nstr\n],\noperator\n.\nadd\n]\ndef\ncontinue_to_jokes\n(\nstate\n:\nOverallState\n):\nreturn\n[\nSend\n(\n\"generate_joke\"\n,\n{\n\"subject\"\n:\ns\n})\nfor\ns\nin\nstate\n[\n\"subjects\"\n]]\nbuilder\n=\nStateGraph\n(\nOverallState\n)\nbuilder\n.\nadd_node\n(\n\"generate_joke\"\n,\nlambda\nstate\n:\n{\n\"jokes\"\n:\n[\nf\n\"Joke about\n{\nstate\n[\n'subject'\n]\n}\n\"\n]})\nbuilder\n.\nadd_conditional_edges\n(\nSTART\n,\ncontinue_to_jokes\n)\nbuilder\n.\nadd_edge\n(\n\"generate_joke\"\n,\nEND\n)\ngraph\n=\nbuilder\n.\ncompile\n()\n# Invoking with two subjects results in a generated joke for each\ngraph\n.\ninvoke\n({\n\"subjects\"\n:\n[\n\"cats\"\n,\n\"dogs\"\n]})\n# {'subjects': ['cats', 'dogs'], 'jokes': ['Joke about cats', 'Joke about dogs']}\nMETHOD\nDESCRIPTION\n__init__\nInitialize a new instance of the\nSend\nclass.\n__init__\n\u00b6\n__init__\n(\nnode\n:\nstr\n,\narg\n:\nAny\n)\n->\nNone\nInitialize a new instance of the\nSend\nclass.\nPARAMETER\nDESCRIPTION\nnode\nThe name of the target node to send the message to.\nTYPE:\nstr\narg\nThe state or message to send to the target node.\nTYPE:\nAny\nCommand\ndataclass\n\u00b6\nBases:\nGeneric\n[\nN\n]\n,\nToolOutputMixin\nOne or more commands to update the graph's state and send messages to nodes.\nPARAMETER\nDESCRIPTION\ngraph\nGraph to send the command to. Supported values are:\nNone\n: the current graph\nCommand.PARENT\n: closest parent graph\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nupdate\nUpdate to apply to the graph's state.\nTYPE:\nAny\n| None\nDEFAULT:\nNone\nresume\nValue to resume execution with. To be used together with\ninterrupt()\n.\nCan be one of the following:\nMapping of interrupt ids to resume values\nA single value with which to resume the next interrupt\nTYPE:\ndict\n[\nstr\n,\nAny\n] |\nAny\n| None\nDEFAULT:\nNone\ngoto\nCan be one of the following:\nName of the node to navigate to next (any node that belongs to the specified\ngraph\n)\nSequence of node names to navigate to next\nSend\nobject (to execute a node with the input provided)\nSequence of\nSend\nobjects\nTYPE:\nSend\n|\nSequence\n[\nSend\n|\nN\n] |\nN\nDEFAULT:\n()\nOverwrite\ndataclass\n\u00b6\nBypass a reducer and write the wrapped value directly to a\nBinaryOperatorAggregate\nchannel.\nReceiving multiple\nOverwrite\nvalues for the same channel in a single super-step\nwill raise an\nInvalidUpdateError\n.\nExample\nfrom\ntyping\nimport\nAnnotated\nimport\noperator\nfrom\nlanggraph.graph\nimport\nStateGraph\nfrom\nlanggraph.types\nimport\nOverwrite\nclass\nState\n(\nTypedDict\n):\nmessages\n:\nAnnotated\n[\nlist\n,\noperator\n.\nadd\n]\ndef\nnode_a\n(\nstate\n:\nTypedDict\n):\n# Normal update: uses the reducer (operator.add)\nreturn\n{\n\"messages\"\n:\n[\n\"a\"\n]}\ndef\nnode_b\n(\nstate\n:\nState\n):\n# Overwrite: bypasses the reducer and replaces the entire value\nreturn\n{\n\"messages\"\n:\nOverwrite\n(\nvalue\n=\n[\n\"b\"\n])}\nbuilder\n=\nStateGraph\n(\nState\n)\nbuilder\n.\nadd_node\n(\n\"node_a\"\n,\nnode_a\n)\nbuilder\n.\nadd_node\n(\n\"node_b\"\n,\nnode_b\n)\nbuilder\n.\nset_entry_point\n(\n\"node_a\"\n)\nbuilder\n.\nadd_edge\n(\n\"node_a\"\n,\n\"node_b\"\n)\ngraph\n=\nbuilder\n.\ncompile\n()\n# Without Overwrite in node_b, messages would be [\"START\", \"a\", \"b\"]\n# With Overwrite, messages is just [\"b\"]\nresult\n=\ngraph\n.\ninvoke\n({\n\"messages\"\n:\n[\n\"START\"\n]})\nassert\nresult\n==\n{\n\"messages\"\n:\n[\n\"b\"\n]}\nvalue\ninstance-attribute\n\u00b6\nvalue\n:\nAny\nThe value to write directly to the channel, bypassing any reducer.\ninterrupt\n\u00b6\ninterrupt\n(\nvalue\n:\nAny\n)\n->\nAny\nInterrupt the graph with a resumable exception from within a node.\nThe\ninterrupt\nfunction enables human-in-the-loop workflows by pausing graph\nexecution and surfacing a value to the client. This value can communicate context\nor request input required to resume execution.\nIn a given node, the first invocation of this function raises a\nGraphInterrupt\nexception, halting execution. The provided\nvalue\nis included with the exception\nand sent to the client executing the graph.\nA client resuming the graph must use the\nCommand\nprimitive to specify a value for the interrupt and continue execution.\nThe graph resumes from the start of the node,\nre-executing\nall logic.\nIf a node contains multiple\ninterrupt\ncalls, LangGraph matches resume values\nto interrupts based on their order in the node. This list of resume values\nis scoped to the specific task executing the node and is not shared across tasks.\nTo use an\ninterrupt\n, you must enable a checkpointer, as the feature relies\non persisting the graph state.\nExample\nimport\nuuid\nfrom\ntyping\nimport\nOptional\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.constants\nimport\nSTART\nfrom\nlanggraph.graph\nimport\nStateGraph\nfrom\nlanggraph.types\nimport\ninterrupt\n,\nCommand\nclass\nState\n(\nTypedDict\n):\n\"\"\"The graph state.\"\"\"\nfoo\n:\nstr\nhuman_value\n:\nOptional\n[\nstr\n]\n\"\"\"Human value will be updated using an interrupt.\"\"\"\ndef\nnode\n(\nstate\n:\nState\n):\nanswer\n=\ninterrupt\n(\n# This value will be sent to the client\n# as part of the interrupt information.\n\"what is your age?\"\n)\nprint\n(\nf\n\"> Received an input from the interrupt:\n{\nanswer\n}\n\"\n)\nreturn\n{\n\"human_value\"\n:\nanswer\n}\nbuilder\n=\nStateGraph\n(\nState\n)\nbuilder\n.\nadd_node\n(\n\"node\"\n,\nnode\n)\nbuilder\n.\nadd_edge\n(\nSTART\n,\n\"node\"\n)\n# A checkpointer must be enabled for interrupts to work!\ncheckpointer\n=\nInMemorySaver\n()\ngraph\n=\nbuilder\n.\ncompile\n(\ncheckpointer\n=\ncheckpointer\n)\nconfig\n=\n{\n\"configurable\"\n:\n{\n\"thread_id\"\n:\nuuid\n.\nuuid4\n(),\n}\n}\nfor\nchunk\nin\ngraph\n.\nstream\n({\n\"foo\"\n:\n\"abc\"\n},\nconfig\n):\nprint\n(\nchunk\n)\n# > {'__interrupt__': (Interrupt(value='what is your age?', id='45fda8478b2ef754419799e10992af06'),)}\ncommand\n=\nCommand\n(\nresume\n=\n\"some input from a human!!!\"\n)\nfor\nchunk\nin\ngraph\n.\nstream\n(\nCommand\n(\nresume\n=\n\"some input from a human!!!\"\n),\nconfig\n):\nprint\n(\nchunk\n)\n# > Received an input from the interrupt: some input from a human!!!\n# > {'node': {'human_value': 'some input from a human!!!'}}\nPARAMETER\nDESCRIPTION\nvalue\nThe value to surface to the client when the graph is interrupted.\nTYPE:\nAny\nRETURNS\nDESCRIPTION\nAny\nOn subsequent invocations within the same node (same task to be precise), returns the value provided during the first invocation\nTYPE:\nAny\nRAISES\nDESCRIPTION\nGraphInterrupt\nOn the first invocation within the node, halts execution and surfaces the provided value to the client.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/types/",
      "title": "Types | LangChain Reference",
      "heading": "Types"
    }
  },
  {
    "page_content": "Runtime | LangChain Reference\nSkip to content\nLangChain Reference\nRuntime\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nRuntime\nTable of contents\nRuntime\ncontext\nstore\nstream_writer\nprevious\nruntime\nget_runtime\nConfig\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nRuntime\ncontext\nstore\nstream_writer\nprevious\nruntime\nget_runtime\nRuntime\nRuntime\ndataclass\n\u00b6\nBases:\nGeneric\n[\nContextT\n]\nConvenience class that bundles run-scoped context and other runtime utilities.\nAdded in version v0.6.0\nExample:\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlanggraph.runtime\nimport\nRuntime\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n@dataclass\nclass\nContext\n:\n# (1)!\nuser_id\n:\nstr\nclass\nState\n(\nTypedDict\n,\ntotal\n=\nFalse\n):\nresponse\n:\nstr\nstore\n=\nInMemoryStore\n()\n# (2)!\nstore\n.\nput\n((\n\"users\"\n,),\n\"user_123\"\n,\n{\n\"name\"\n:\n\"Alice\"\n})\ndef\npersonalized_greeting\n(\nstate\n:\nState\n,\nruntime\n:\nRuntime\n[\nContext\n])\n->\nState\n:\n'''Generate personalized greeting using runtime context and store.'''\nuser_id\n=\nruntime\n.\ncontext\n.\nuser_id\n# (3)!\nname\n=\n\"unknown_user\"\nif\nruntime\n.\nstore\n:\nif\nmemory\n:=\nruntime\n.\nstore\n.\nget\n((\n\"users\"\n,),\nuser_id\n):\nname\n=\nmemory\n.\nvalue\n[\n\"name\"\n]\nresponse\n=\nf\n\"Hello\n{\nname\n}\n! Nice to see you again.\"\nreturn\n{\n\"response\"\n:\nresponse\n}\ngraph\n=\n(\nStateGraph\n(\nstate_schema\n=\nState\n,\ncontext_schema\n=\nContext\n)\n.\nadd_node\n(\n\"personalized_greeting\"\n,\npersonalized_greeting\n)\n.\nset_entry_point\n(\n\"personalized_greeting\"\n)\n.\nset_finish_point\n(\n\"personalized_greeting\"\n)\n.\ncompile\n(\nstore\n=\nstore\n)\n)\nresult\n=\ngraph\n.\ninvoke\n({},\ncontext\n=\nContext\n(\nuser_id\n=\n\"user_123\"\n))\nprint\n(\nresult\n)\n# > {'response': 'Hello Alice! Nice to see you again.'}\nDefine a schema for the runtime context.\nCreate a store to persist memories and other information.\nUse the runtime context to access the\nuser_id\n.\ncontext\nclass-attribute\ninstance-attribute\n\u00b6\ncontext\n:\nContextT\n=\nfield\n(\ndefault\n=\nNone\n)\nStatic context for the graph run, like\nuser_id\n,\ndb_conn\n, etc.\nCan also be thought of as 'run dependencies'.\nstore\nclass-attribute\ninstance-attribute\n\u00b6\nstore\n:\nBaseStore\n|\nNone\n=\nfield\n(\ndefault\n=\nNone\n)\nStore for the graph run, enabling persistence and memory.\nstream_writer\nclass-attribute\ninstance-attribute\n\u00b6\nstream_writer\n:\nStreamWriter\n=\nfield\n(\ndefault\n=\n_no_op_stream_writer\n)\nFunction that writes to the custom stream.\nprevious\nclass-attribute\ninstance-attribute\n\u00b6\nprevious\n:\nAny\n=\nfield\n(\ndefault\n=\nNone\n)\nThe previous return value for the given thread.\nOnly available with the functional API when a checkpointer is provided.\nruntime\n\u00b6\nFUNCTION\nDESCRIPTION\nget_runtime\nGet the runtime for the current graph run.\nget_runtime\n\u00b6\nget_runtime\n(\ncontext_schema\n:\ntype\n[\nContextT\n]\n|\nNone\n=\nNone\n)\n->\nRuntime\n[\nContextT\n]\nGet the runtime for the current graph run.\nPARAMETER\nDESCRIPTION\ncontext_schema\nOptional schema used for type hinting the return type of the runtime.\nTYPE:\ntype\n[\nContextT\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRuntime\n[\nContextT\n]\nThe runtime for the current graph run.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/runtime/",
      "title": "Runtime | LangChain Reference",
      "heading": "Runtime"
    }
  },
  {
    "page_content": "Config | LangChain Reference\nSkip to content\nLangChain Reference\nConfig\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nConfig\nTable of contents\nconfig\nget_store\nget_stream_writer\nErrors\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nconfig\nget_store\nget_stream_writer\nConfig\nconfig\n\u00b6\nFUNCTION\nDESCRIPTION\nget_store\nAccess LangGraph store from inside a graph node or entrypoint task at runtime.\nget_stream_writer\nAccess LangGraph\nStreamWriter\nfrom inside a graph node or entrypoint task at runtime.\nget_store\n\u00b6\nget_store\n()\n->\nBaseStore\nAccess LangGraph store from inside a graph node or entrypoint task at runtime.\nCan be called from inside any\nStateGraph\nnode or\nfunctional API\ntask\n, as long as the\nStateGraph\nor the\nentrypoint\nwas initialized with a store, e.g.:\n# with StateGraph\ngraph\n=\n(\nStateGraph\n(\n...\n)\n...\n.\ncompile\n(\nstore\n=\nstore\n)\n)\n# or with entrypoint\n@entrypoint\n(\nstore\n=\nstore\n)\ndef\nworkflow\n(\ninputs\n):\n...\nAsync with Python < 3.11\nIf you are using Python < 3.11 and are running LangGraph asynchronously,\nget_store()\nwon't work since it uses\ncontextvar\npropagation (only available in\nPython >= 3.11\n).\nUsing with\nStateGraph\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph\n,\nSTART\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nfrom\nlanggraph.config\nimport\nget_store\nstore\n=\nInMemoryStore\n()\nstore\n.\nput\n((\n\"values\"\n,),\n\"foo\"\n,\n{\n\"bar\"\n:\n2\n})\nclass\nState\n(\nTypedDict\n):\nfoo\n:\nint\ndef\nmy_node\n(\nstate\n:\nState\n):\nmy_store\n=\nget_store\n()\nstored_value\n=\nmy_store\n.\nget\n((\n\"values\"\n,),\n\"foo\"\n)\n.\nvalue\n[\n\"bar\"\n]\nreturn\n{\n\"foo\"\n:\nstored_value\n+\n1\n}\ngraph\n=\n(\nStateGraph\n(\nState\n)\n.\nadd_node\n(\nmy_node\n)\n.\nadd_edge\n(\nSTART\n,\n\"my_node\"\n)\n.\ncompile\n(\nstore\n=\nstore\n)\n)\ngraph\n.\ninvoke\n({\n\"foo\"\n:\n1\n})\n{\"foo\": 3}\nUsing with functional API\nfrom\nlanggraph.func\nimport\nentrypoint\n,\ntask\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nfrom\nlanggraph.config\nimport\nget_store\nstore\n=\nInMemoryStore\n()\nstore\n.\nput\n((\n\"values\"\n,),\n\"foo\"\n,\n{\n\"bar\"\n:\n2\n})\n@task\ndef\nmy_task\n(\nvalue\n:\nint\n):\nmy_store\n=\nget_store\n()\nstored_value\n=\nmy_store\n.\nget\n((\n\"values\"\n,),\n\"foo\"\n)\n.\nvalue\n[\n\"bar\"\n]\nreturn\nstored_value\n+\n1\n@entrypoint\n(\nstore\n=\nstore\n)\ndef\nworkflow\n(\nvalue\n:\nint\n):\nreturn\nmy_task\n(\nvalue\n)\n.\nresult\n()\nworkflow\n.\ninvoke\n(\n1\n)\n3\nget_stream_writer\n\u00b6\nget_stream_writer\n()\n->\nStreamWriter\nAccess LangGraph\nStreamWriter\nfrom inside a graph node or entrypoint task at runtime.\nCan be called from inside any\nStateGraph\nnode or\nfunctional API\ntask\n.\nAsync with Python < 3.11\nIf you are using Python < 3.11 and are running LangGraph asynchronously,\nget_stream_writer()\nwon't work since it uses\ncontextvar\npropagation (only available in\nPython >= 3.11\n).\nUsing with\nStateGraph\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph\n,\nSTART\nfrom\nlanggraph.config\nimport\nget_stream_writer\nclass\nState\n(\nTypedDict\n):\nfoo\n:\nint\ndef\nmy_node\n(\nstate\n:\nState\n):\nmy_stream_writer\n=\nget_stream_writer\n()\nmy_stream_writer\n({\n\"custom_data\"\n:\n\"Hello!\"\n})\nreturn\n{\n\"foo\"\n:\nstate\n[\n\"foo\"\n]\n+\n1\n}\ngraph\n=\n(\nStateGraph\n(\nState\n)\n.\nadd_node\n(\nmy_node\n)\n.\nadd_edge\n(\nSTART\n,\n\"my_node\"\n)\n.\ncompile\n(\nstore\n=\nstore\n)\n)\nfor\nchunk\nin\ngraph\n.\nstream\n({\n\"foo\"\n:\n1\n},\nstream_mode\n=\n\"custom\"\n):\nprint\n(\nchunk\n)\n{\"custom_data\": \"Hello!\"}\nUsing with functional API\nfrom\nlanggraph.func\nimport\nentrypoint\n,\ntask\nfrom\nlanggraph.config\nimport\nget_stream_writer\n@task\ndef\nmy_task\n(\nvalue\n:\nint\n):\nmy_stream_writer\n=\nget_stream_writer\n()\nmy_stream_writer\n({\n\"custom_data\"\n:\n\"Hello!\"\n})\nreturn\nvalue\n+\n1\n@entrypoint\n(\nstore\n=\nstore\n)\ndef\nworkflow\n(\nvalue\n:\nint\n):\nreturn\nmy_task\n(\nvalue\n)\n.\nresult\n()\nfor\nchunk\nin\nworkflow\n.\nstream\n(\n1\n,\nstream_mode\n=\n\"custom\"\n):\nprint\n(\nchunk\n)\n{\"custom_data\": \"Hello!\"}\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/config/",
      "title": "Config | LangChain Reference",
      "heading": "Config"
    }
  },
  {
    "page_content": "Errors | LangChain Reference\nSkip to content\nLangChain Reference\nErrors\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nErrors\nTable of contents\nerrors\nEmptyChannelError\nGraphRecursionError\nInvalidUpdateError\nGraphInterrupt\nNodeInterrupt\nEmptyInputError\nTaskNotFound\nConstants\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nerrors\nEmptyChannelError\nGraphRecursionError\nInvalidUpdateError\nGraphInterrupt\nNodeInterrupt\nEmptyInputError\nTaskNotFound\nErrors\nerrors\n\u00b6\nEmptyChannelError\n\u00b6\nBases:\nException\nRaised when attempting to get the value of a channel that hasn't been updated\nfor the first time yet.\nGraphRecursionError\n\u00b6\nBases:\nRecursionError\nRaised when the graph has exhausted the maximum number of steps.\nThis prevents infinite loops. To increase the maximum number of steps,\nrun your graph with a config specifying a higher\nrecursion_limit\n.\nTroubleshooting guides:\nGRAPH_RECURSION_LIMIT\nExamples:\ngraph = builder.compile()\ngraph.invoke(\n{\"messages\": [(\"user\", \"Hello, world!\")]},\n# The config is the second positional argument\n{\"recursion_limit\": 1000},\n)\nInvalidUpdateError\n\u00b6\nBases:\nException\nRaised when attempting to update a channel with an invalid set of updates.\nTroubleshooting guides:\nINVALID_CONCURRENT_GRAPH_UPDATE\nINVALID_GRAPH_NODE_RETURN_VALUE\nGraphInterrupt\n\u00b6\nBases:\nGraphBubbleUp\nRaised when a subgraph is interrupted, suppressed by the root graph.\nNever raised directly, or surfaced to the user.\nNodeInterrupt\ndeprecated\n\u00b6\nBases:\nGraphInterrupt\nDeprecated\nNodeInterrupt is deprecated. Please use\ninterrupt\ninstead.\nRaised by a node to interrupt execution.\nEmptyInputError\n\u00b6\nBases:\nException\nRaised when graph receives an empty input.\nTaskNotFound\n\u00b6\nBases:\nException\nRaised when the executor is unable to find a task (for distributed mode).\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/errors/",
      "title": "Errors | LangChain Reference",
      "heading": "Errors"
    }
  },
  {
    "page_content": "Constants | LangChain Reference\nSkip to content\nLangChain Reference\nConstants\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nConstants\nTable of contents\nconstants\nTAG_HIDDEN\nTAG_NOSTREAM\nSTART\nEND\nChannels\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nconstants\nTAG_HIDDEN\nTAG_NOSTREAM\nSTART\nEND\nConstants\nconstants\n\u00b6\nTAG_HIDDEN\nmodule-attribute\n\u00b6\nTAG_HIDDEN\n=\nintern\n(\n'langsmith:hidden'\n)\nTag to hide a node/edge from certain tracing/streaming environments.\nTAG_NOSTREAM\nmodule-attribute\n\u00b6\nTAG_NOSTREAM\n=\nintern\n(\n'nostream'\n)\nTag to disable streaming for a chat model.\nSTART\nmodule-attribute\n\u00b6\nSTART\n=\nintern\n(\n'__start__'\n)\nThe first (maybe virtual) node in graph-style Pregel.\nEND\nmodule-attribute\n\u00b6\nEND\n=\nintern\n(\n'__end__'\n)\nThe last (maybe virtual) node in graph-style Pregel.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/constants/",
      "title": "Constants | LangChain Reference",
      "heading": "Constants"
    }
  },
  {
    "page_content": "Channels | LangChain Reference\nSkip to content\nLangChain Reference\nChannels\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nLangGraph\nlanggraph\nlanggraph\nGraphs\nFunctional API\nPregel\nCheckpointing\nStorage\nCaching\nTypes\nRuntime\nConfig\nErrors\nConstants\nChannels\nChannels\nTable of contents\nbase\nBaseChannel\nValueType\nUpdateType\ncopy\ncheckpoint\nfrom_checkpoint\nget\nis_available\nupdate\nconsume\nfinish\nchannels\nTopic\nValueType\nUpdateType\nconsume\nfinish\ncopy\ncheckpoint\nfrom_checkpoint\nupdate\nget\nis_available\nLastValue\nValueType\nUpdateType\nconsume\nfinish\ncopy\nfrom_checkpoint\nupdate\nget\nis_available\ncheckpoint\nEphemeralValue\nValueType\nUpdateType\nconsume\nfinish\ncopy\nfrom_checkpoint\nupdate\nget\nis_available\ncheckpoint\nBinaryOperatorAggregate\nValueType\nUpdateType\nconsume\nfinish\ncopy\nfrom_checkpoint\nupdate\nget\nis_available\ncheckpoint\nAnyValue\nValueType\nUpdateType\nconsume\nfinish\ncopy\nfrom_checkpoint\nupdate\nget\nis_available\ncheckpoint\nPrebuilt\nPrebuilt\nAgents\nSupervisor\nSwarm\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nbase\nBaseChannel\nValueType\nUpdateType\ncopy\ncheckpoint\nfrom_checkpoint\nget\nis_available\nupdate\nconsume\nfinish\nchannels\nTopic\nValueType\nUpdateType\nconsume\nfinish\ncopy\ncheckpoint\nfrom_checkpoint\nupdate\nget\nis_available\nLastValue\nValueType\nUpdateType\nconsume\nfinish\ncopy\nfrom_checkpoint\nupdate\nget\nis_available\ncheckpoint\nEphemeralValue\nValueType\nUpdateType\nconsume\nfinish\ncopy\nfrom_checkpoint\nupdate\nget\nis_available\ncheckpoint\nBinaryOperatorAggregate\nValueType\nUpdateType\nconsume\nfinish\ncopy\nfrom_checkpoint\nupdate\nget\nis_available\ncheckpoint\nAnyValue\nValueType\nUpdateType\nconsume\nfinish\ncopy\nfrom_checkpoint\nupdate\nget\nis_available\ncheckpoint\nChannels\nbase\n\u00b6\nBaseChannel\n\u00b6\nBases:\nGeneric\n[\nValue\n,\nUpdate\n,\nCheckpoint\n]\n,\nABC\nBase class for all channels.\nMETHOD\nDESCRIPTION\ncopy\nReturn a copy of the channel.\ncheckpoint\nReturn a serializable representation of the channel's current state.\nfrom_checkpoint\nReturn a new identical channel, optionally initialized from a checkpoint.\nget\nReturn the current value of the channel.\nis_available\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\nupdate\nUpdate the channel's value with the given sequence of updates.\nconsume\nNotify the channel that a subscribed task ran.\nfinish\nNotify the channel that the Pregel run is finishing.\nValueType\nabstractmethod\nproperty\n\u00b6\nValueType\n:\nAny\nThe type of the value stored in the channel.\nUpdateType\nabstractmethod\nproperty\n\u00b6\nUpdateType\n:\nAny\nThe type of the update received by the channel.\ncopy\n\u00b6\ncopy\n()\n->\nSelf\nReturn a copy of the channel.\nBy default, delegates to\ncheckpoint()\nand\nfrom_checkpoint()\n.\nSubclasses can override this method with a more efficient implementation.\ncheckpoint\n\u00b6\ncheckpoint\n()\n->\nCheckpoint\n|\nAny\nReturn a serializable representation of the channel's current state.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet),\nor doesn't support checkpoints.\nfrom_checkpoint\nabstractmethod\n\u00b6\nfrom_checkpoint\n(\ncheckpoint\n:\nCheckpoint\n|\nAny\n)\n->\nSelf\nReturn a new identical channel, optionally initialized from a checkpoint.\nIf the checkpoint contains complex data structures, they should be copied.\nget\nabstractmethod\n\u00b6\nget\n()\n->\nValue\nReturn the current value of the channel.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet).\nis_available\n\u00b6\nis_available\n()\n->\nbool\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\nSubclasses should override this method to provide a more efficient\nimplementation than calling\nget()\nand catching\nEmptyChannelError\n.\nupdate\nabstractmethod\n\u00b6\nupdate\n(\nvalues\n:\nSequence\n[\nUpdate\n])\n->\nbool\nUpdate the channel's value with the given sequence of updates.\nThe order of the updates in the sequence is arbitrary.\nThis method is called by Pregel for all channels at the end of each step.\nIf there are no updates, it is called with an empty sequence.\nRaises\nInvalidUpdateError\nif the sequence of updates is invalid.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nconsume\n\u00b6\nconsume\n()\n->\nbool\nNotify the channel that a subscribed task ran.\nBy default, no-op.\nA channel can use this method to modify its state, preventing the value from being consumed again.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nfinish\n\u00b6\nfinish\n()\n->\nbool\nNotify the channel that the Pregel run is finishing.\nBy default, no-op.\nA channel can use this method to modify its state, preventing finish.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nchannels\n\u00b6\nTopic\n\u00b6\nBases:\nGeneric\n[\nValue\n]\n,\nBaseChannel\n[\nSequence\n[\nValue\n],\nValue\n|\nlist\n[\nValue\n],\nlist\n[\nValue\n]]\nA configurable PubSub Topic.\nPARAMETER\nDESCRIPTION\ntyp\nThe type of the value stored in the channel.\nTYPE:\ntype\n[\nValue\n]\naccumulate\nWhether to accumulate values across steps. If\nFalse\n, the channel will be emptied after each step.\nTYPE:\nbool\nDEFAULT:\nFalse\nMETHOD\nDESCRIPTION\nconsume\nNotify the channel that a subscribed task ran.\nfinish\nNotify the channel that the Pregel run is finishing.\ncopy\nReturn a copy of the channel.\ncheckpoint\nReturn a serializable representation of the channel's current state.\nfrom_checkpoint\nReturn a new identical channel, optionally initialized from a checkpoint.\nupdate\nUpdate the channel's value with the given sequence of updates.\nget\nReturn the current value of the channel.\nis_available\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\nValueType\nproperty\n\u00b6\nValueType\n:\nAny\nThe type of the value stored in the channel.\nUpdateType\nproperty\n\u00b6\nUpdateType\n:\nAny\nThe type of the update received by the channel.\nconsume\n\u00b6\nconsume\n()\n->\nbool\nNotify the channel that a subscribed task ran.\nBy default, no-op.\nA channel can use this method to modify its state, preventing the value from being consumed again.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nfinish\n\u00b6\nfinish\n()\n->\nbool\nNotify the channel that the Pregel run is finishing.\nBy default, no-op.\nA channel can use this method to modify its state, preventing finish.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\ncopy\n\u00b6\ncopy\n()\n->\nSelf\nReturn a copy of the channel.\ncheckpoint\n\u00b6\ncheckpoint\n()\n->\nlist\n[\nValue\n]\nReturn a serializable representation of the channel's current state.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet),\nor doesn't support checkpoints.\nfrom_checkpoint\n\u00b6\nfrom_checkpoint\n(\ncheckpoint\n:\nlist\n[\nValue\n])\n->\nSelf\nReturn a new identical channel, optionally initialized from a checkpoint.\nIf the checkpoint contains complex data structures, they should be copied.\nupdate\n\u00b6\nupdate\n(\nvalues\n:\nSequence\n[\nValue\n|\nlist\n[\nValue\n]])\n->\nbool\nUpdate the channel's value with the given sequence of updates.\nThe order of the updates in the sequence is arbitrary.\nThis method is called by Pregel for all channels at the end of each step.\nIf there are no updates, it is called with an empty sequence.\nRaises\nInvalidUpdateError\nif the sequence of updates is invalid.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nget\n\u00b6\nget\n()\n->\nSequence\n[\nValue\n]\nReturn the current value of the channel.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet).\nis_available\n\u00b6\nis_available\n()\n->\nbool\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\nSubclasses should override this method to provide a more efficient\nimplementation than calling\nget()\nand catching\nEmptyChannelError\n.\nLastValue\n\u00b6\nBases:\nGeneric\n[\nValue\n]\n,\nBaseChannel\n[\nValue\n,\nValue\n,\nValue\n]\nStores the last value received, can receive at most one value per step.\nMETHOD\nDESCRIPTION\nconsume\nNotify the channel that a subscribed task ran.\nfinish\nNotify the channel that the Pregel run is finishing.\ncopy\nReturn a copy of the channel.\nfrom_checkpoint\nReturn a new identical channel, optionally initialized from a checkpoint.\nupdate\nUpdate the channel's value with the given sequence of updates.\nget\nReturn the current value of the channel.\nis_available\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\ncheckpoint\nReturn a serializable representation of the channel's current state.\nValueType\nproperty\n\u00b6\nValueType\n:\ntype\n[\nValue\n]\nThe type of the value stored in the channel.\nUpdateType\nproperty\n\u00b6\nUpdateType\n:\ntype\n[\nValue\n]\nThe type of the update received by the channel.\nconsume\n\u00b6\nconsume\n()\n->\nbool\nNotify the channel that a subscribed task ran.\nBy default, no-op.\nA channel can use this method to modify its state, preventing the value from being consumed again.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nfinish\n\u00b6\nfinish\n()\n->\nbool\nNotify the channel that the Pregel run is finishing.\nBy default, no-op.\nA channel can use this method to modify its state, preventing finish.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\ncopy\n\u00b6\ncopy\n()\n->\nSelf\nReturn a copy of the channel.\nfrom_checkpoint\n\u00b6\nfrom_checkpoint\n(\ncheckpoint\n:\nValue\n)\n->\nSelf\nReturn a new identical channel, optionally initialized from a checkpoint.\nIf the checkpoint contains complex data structures, they should be copied.\nupdate\n\u00b6\nupdate\n(\nvalues\n:\nSequence\n[\nValue\n])\n->\nbool\nUpdate the channel's value with the given sequence of updates.\nThe order of the updates in the sequence is arbitrary.\nThis method is called by Pregel for all channels at the end of each step.\nIf there are no updates, it is called with an empty sequence.\nRaises\nInvalidUpdateError\nif the sequence of updates is invalid.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nget\n\u00b6\nget\n()\n->\nValue\nReturn the current value of the channel.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet).\nis_available\n\u00b6\nis_available\n()\n->\nbool\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\nSubclasses should override this method to provide a more efficient\nimplementation than calling\nget()\nand catching\nEmptyChannelError\n.\ncheckpoint\n\u00b6\ncheckpoint\n()\n->\nValue\nReturn a serializable representation of the channel's current state.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet),\nor doesn't support checkpoints.\nEphemeralValue\n\u00b6\nBases:\nGeneric\n[\nValue\n]\n,\nBaseChannel\n[\nValue\n,\nValue\n,\nValue\n]\nStores the value received in the step immediately preceding, clears after.\nMETHOD\nDESCRIPTION\nconsume\nNotify the channel that a subscribed task ran.\nfinish\nNotify the channel that the Pregel run is finishing.\ncopy\nReturn a copy of the channel.\nfrom_checkpoint\nReturn a new identical channel, optionally initialized from a checkpoint.\nupdate\nUpdate the channel's value with the given sequence of updates.\nget\nReturn the current value of the channel.\nis_available\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\ncheckpoint\nReturn a serializable representation of the channel's current state.\nValueType\nproperty\n\u00b6\nValueType\n:\ntype\n[\nValue\n]\nThe type of the value stored in the channel.\nUpdateType\nproperty\n\u00b6\nUpdateType\n:\ntype\n[\nValue\n]\nThe type of the update received by the channel.\nconsume\n\u00b6\nconsume\n()\n->\nbool\nNotify the channel that a subscribed task ran.\nBy default, no-op.\nA channel can use this method to modify its state, preventing the value from being consumed again.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nfinish\n\u00b6\nfinish\n()\n->\nbool\nNotify the channel that the Pregel run is finishing.\nBy default, no-op.\nA channel can use this method to modify its state, preventing finish.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\ncopy\n\u00b6\ncopy\n()\n->\nSelf\nReturn a copy of the channel.\nfrom_checkpoint\n\u00b6\nfrom_checkpoint\n(\ncheckpoint\n:\nValue\n)\n->\nSelf\nReturn a new identical channel, optionally initialized from a checkpoint.\nIf the checkpoint contains complex data structures, they should be copied.\nupdate\n\u00b6\nupdate\n(\nvalues\n:\nSequence\n[\nValue\n])\n->\nbool\nUpdate the channel's value with the given sequence of updates.\nThe order of the updates in the sequence is arbitrary.\nThis method is called by Pregel for all channels at the end of each step.\nIf there are no updates, it is called with an empty sequence.\nRaises\nInvalidUpdateError\nif the sequence of updates is invalid.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nget\n\u00b6\nget\n()\n->\nValue\nReturn the current value of the channel.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet).\nis_available\n\u00b6\nis_available\n()\n->\nbool\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\nSubclasses should override this method to provide a more efficient\nimplementation than calling\nget()\nand catching\nEmptyChannelError\n.\ncheckpoint\n\u00b6\ncheckpoint\n()\n->\nValue\nReturn a serializable representation of the channel's current state.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet),\nor doesn't support checkpoints.\nBinaryOperatorAggregate\n\u00b6\nBases:\nGeneric\n[\nValue\n]\n,\nBaseChannel\n[\nValue\n,\nValue\n,\nValue\n]\nStores the result of applying a binary operator to the current value and each new value.\nimport\noperator\ntotal\n=\nChannels\n.\nBinaryOperatorAggregate\n(\nint\n,\noperator\n.\nadd\n)\nMETHOD\nDESCRIPTION\nconsume\nNotify the channel that a subscribed task ran.\nfinish\nNotify the channel that the Pregel run is finishing.\ncopy\nReturn a copy of the channel.\nfrom_checkpoint\nReturn a new identical channel, optionally initialized from a checkpoint.\nupdate\nUpdate the channel's value with the given sequence of updates.\nget\nReturn the current value of the channel.\nis_available\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\ncheckpoint\nReturn a serializable representation of the channel's current state.\nValueType\nproperty\n\u00b6\nValueType\n:\ntype\n[\nValue\n]\nThe type of the value stored in the channel.\nUpdateType\nproperty\n\u00b6\nUpdateType\n:\ntype\n[\nValue\n]\nThe type of the update received by the channel.\nconsume\n\u00b6\nconsume\n()\n->\nbool\nNotify the channel that a subscribed task ran.\nBy default, no-op.\nA channel can use this method to modify its state, preventing the value from being consumed again.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nfinish\n\u00b6\nfinish\n()\n->\nbool\nNotify the channel that the Pregel run is finishing.\nBy default, no-op.\nA channel can use this method to modify its state, preventing finish.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\ncopy\n\u00b6\ncopy\n()\n->\nSelf\nReturn a copy of the channel.\nfrom_checkpoint\n\u00b6\nfrom_checkpoint\n(\ncheckpoint\n:\nValue\n)\n->\nSelf\nReturn a new identical channel, optionally initialized from a checkpoint.\nIf the checkpoint contains complex data structures, they should be copied.\nupdate\n\u00b6\nupdate\n(\nvalues\n:\nSequence\n[\nValue\n])\n->\nbool\nUpdate the channel's value with the given sequence of updates.\nThe order of the updates in the sequence is arbitrary.\nThis method is called by Pregel for all channels at the end of each step.\nIf there are no updates, it is called with an empty sequence.\nRaises\nInvalidUpdateError\nif the sequence of updates is invalid.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nget\n\u00b6\nget\n()\n->\nValue\nReturn the current value of the channel.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet).\nis_available\n\u00b6\nis_available\n()\n->\nbool\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\nSubclasses should override this method to provide a more efficient\nimplementation than calling\nget()\nand catching\nEmptyChannelError\n.\ncheckpoint\n\u00b6\ncheckpoint\n()\n->\nValue\nReturn a serializable representation of the channel's current state.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet),\nor doesn't support checkpoints.\nAnyValue\n\u00b6\nBases:\nGeneric\n[\nValue\n]\n,\nBaseChannel\n[\nValue\n,\nValue\n,\nValue\n]\nStores the last value received, assumes that if multiple values are\nreceived, they are all equal.\nMETHOD\nDESCRIPTION\nconsume\nNotify the channel that a subscribed task ran.\nfinish\nNotify the channel that the Pregel run is finishing.\ncopy\nReturn a copy of the channel.\nfrom_checkpoint\nReturn a new identical channel, optionally initialized from a checkpoint.\nupdate\nUpdate the channel's value with the given sequence of updates.\nget\nReturn the current value of the channel.\nis_available\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\ncheckpoint\nReturn a serializable representation of the channel's current state.\nValueType\nproperty\n\u00b6\nValueType\n:\ntype\n[\nValue\n]\nThe type of the value stored in the channel.\nUpdateType\nproperty\n\u00b6\nUpdateType\n:\ntype\n[\nValue\n]\nThe type of the update received by the channel.\nconsume\n\u00b6\nconsume\n()\n->\nbool\nNotify the channel that a subscribed task ran.\nBy default, no-op.\nA channel can use this method to modify its state, preventing the value from being consumed again.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nfinish\n\u00b6\nfinish\n()\n->\nbool\nNotify the channel that the Pregel run is finishing.\nBy default, no-op.\nA channel can use this method to modify its state, preventing finish.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\ncopy\n\u00b6\ncopy\n()\n->\nSelf\nReturn a copy of the channel.\nfrom_checkpoint\n\u00b6\nfrom_checkpoint\n(\ncheckpoint\n:\nValue\n)\n->\nSelf\nReturn a new identical channel, optionally initialized from a checkpoint.\nIf the checkpoint contains complex data structures, they should be copied.\nupdate\n\u00b6\nupdate\n(\nvalues\n:\nSequence\n[\nValue\n])\n->\nbool\nUpdate the channel's value with the given sequence of updates.\nThe order of the updates in the sequence is arbitrary.\nThis method is called by Pregel for all channels at the end of each step.\nIf there are no updates, it is called with an empty sequence.\nRaises\nInvalidUpdateError\nif the sequence of updates is invalid.\nReturns\nTrue\nif the channel was updated,\nFalse\notherwise.\nget\n\u00b6\nget\n()\n->\nValue\nReturn the current value of the channel.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet).\nis_available\n\u00b6\nis_available\n()\n->\nbool\nReturn\nTrue\nif the channel is available (not empty),\nFalse\notherwise.\nSubclasses should override this method to provide a more efficient\nimplementation than calling\nget()\nand catching\nEmptyChannelError\n.\ncheckpoint\n\u00b6\ncheckpoint\n()\n->\nValue\nReturn a serializable representation of the channel's current state.\nRaises\nEmptyChannelError\nif the channel is empty (never updated yet),\nor doesn't support checkpoints.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langgraph/channels/",
      "title": "Channels | LangChain Reference",
      "heading": "Channels"
    }
  },
  {
    "page_content": "Agents | LangChain Reference\nSkip to content\nLangChain Reference\nAgents\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nAgents\nTable of contents\nagents\ncreate_agent\nmodel\ntools\nsystem_prompt\nmiddleware\nresponse_format\nstate_schema\ncontext_schema\ncheckpointer\nstore\ninterrupt_before\ninterrupt_after\ndebug\nname\ncache\nAgentState\nMiddleware\nModels\nMessages\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nagents\ncreate_agent\nmodel\ntools\nsystem_prompt\nmiddleware\nresponse_format\nstate_schema\ncontext_schema\ncheckpointer\nstore\ninterrupt_before\ninterrupt_after\ndebug\nname\ncache\nAgentState\nAgents\nReference docs\nThis page contains\nreference documentation\nfor Agents. See\nthe docs\nfor conceptual guides, tutorials, and examples on using Agents.\nagents\n\u00b6\nEntrypoint to building\nAgents\nwith LangChain.\ncreate_agent\n\u00b6\ncreate_agent\n(\nmodel\n:\nstr\n|\nBaseChatModel\n,\ntools\n:\nSequence\n[\nBaseTool\n|\nCallable\n|\ndict\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\n*\n,\nsystem_prompt\n:\nstr\n|\nSystemMessage\n|\nNone\n=\nNone\n,\nmiddleware\n:\nSequence\n[\nAgentMiddleware\n[\nStateT_co\n,\nContextT\n]]\n=\n(),\nresponse_format\n:\nResponseFormat\n[\nResponseT\n]\n|\ntype\n[\nResponseT\n]\n|\nNone\n=\nNone\n,\nstate_schema\n:\ntype\n[\nAgentState\n[\nResponseT\n]]\n|\nNone\n=\nNone\n,\ncontext_schema\n:\ntype\n[\nContextT\n]\n|\nNone\n=\nNone\n,\ncheckpointer\n:\nCheckpointer\n|\nNone\n=\nNone\n,\nstore\n:\nBaseStore\n|\nNone\n=\nNone\n,\ninterrupt_before\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninterrupt_after\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ndebug\n:\nbool\n=\nFalse\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ncache\n:\nBaseCache\n|\nNone\n=\nNone\n,\n)\n->\nCompiledStateGraph\n[\nAgentState\n[\nResponseT\n],\nContextT\n,\n_InputAgentState\n,\n_OutputAgentState\n[\nResponseT\n]\n]\nCreates an agent graph that calls tools in a loop until a stopping condition is met.\nFor more details on using\ncreate_agent\n,\nvisit the\nAgents\ndocs.\nPARAMETER\nDESCRIPTION\nmodel\n\u00b6\nThe language model for the agent.\nCan be a string identifier (e.g.,\n\"openai:gpt-4\"\n) or a direct chat model\ninstance (e.g.,\nChatOpenAI\nor other another\nLangChain chat model\n).\nFor a full list of supported model strings, see\ninit_chat_model\n.\nSee the\nModels\ndocs for more information.\nTYPE:\nstr\n|\nBaseChatModel\ntools\n\u00b6\nA list of tools,\ndict\n, or\nCallable\n.\nIf\nNone\nor an empty list, the agent will consist of a model node without a\ntool calling loop.\nSee the\nTools\ndocs for more information.\nTYPE:\nSequence\n[\nBaseTool\n|\nCallable\n|\ndict\n[\nstr\n,\nAny\n]] | None\nDEFAULT:\nNone\nsystem_prompt\n\u00b6\nAn optional system prompt for the LLM.\nCan be a\nstr\n(which will be converted to a\nSystemMessage\n) or a\nSystemMessage\ninstance directly. The system message is added to the\nbeginning of the message list when calling the model.\nTYPE:\nstr\n|\nSystemMessage\n| None\nDEFAULT:\nNone\nmiddleware\n\u00b6\nA sequence of middleware instances to apply to the agent.\nMiddleware can intercept and modify agent behavior at various stages.\nSee the\nMiddleware\ndocs for more information.\nTYPE:\nSequence\n[\nAgentMiddleware\n[\nStateT_co\n,\nContextT\n]]\nDEFAULT:\n()\nresponse_format\n\u00b6\nAn optional configuration for structured responses.\nCan be a\nToolStrategy\n,\nProviderStrategy\n, or a Pydantic model class.\nIf provided, the agent will handle structured output during the\nconversation flow.\nRaw schemas will be wrapped in an appropriate strategy based on model\ncapabilities.\nSee the\nStructured output\ndocs for more information.\nTYPE:\nResponseFormat\n[\nResponseT\n] |\ntype\n[\nResponseT\n] | None\nDEFAULT:\nNone\nstate_schema\n\u00b6\nAn optional\nTypedDict\nschema that extends\nAgentState\n.\nWhen provided, this schema is used instead of\nAgentState\nas the base\nschema for merging with middleware state schemas. This allows users to\nadd custom state fields without needing to create custom middleware.\nGenerally, it's recommended to use\nstate_schema\nextensions via middleware\nto keep relevant extensions scoped to corresponding hooks / tools.\nTYPE:\ntype\n[\nAgentState\n[\nResponseT\n]] | None\nDEFAULT:\nNone\ncontext_schema\n\u00b6\nAn optional schema for runtime context.\nTYPE:\ntype\n[\nContextT\n] | None\nDEFAULT:\nNone\ncheckpointer\n\u00b6\nAn optional checkpoint saver object.\nUsed for persisting the state of the graph (e.g., as chat memory) for a\nsingle thread (e.g., a single conversation).\nTYPE:\nCheckpointer\n| None\nDEFAULT:\nNone\nstore\n\u00b6\nAn optional store object.\nUsed for persisting data across multiple threads (e.g., multiple\nconversations / users).\nTYPE:\nBaseStore\n| None\nDEFAULT:\nNone\ninterrupt_before\n\u00b6\nAn optional list of node names to interrupt before.\nUseful if you want to add a user confirmation or other interrupt\nbefore taking an action.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ninterrupt_after\n\u00b6\nAn optional list of node names to interrupt after.\nUseful if you want to return directly or run additional processing\non an output.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ndebug\n\u00b6\nWhether to enable verbose logging for graph execution.\nWhen enabled, prints detailed information about each node execution, state\nupdates, and transitions during agent runtime. Useful for debugging\nmiddleware behavior and understanding agent execution flow.\nTYPE:\nbool\nDEFAULT:\nFalse\nname\n\u00b6\nAn optional name for the\nCompiledStateGraph\n.\nThis name will be automatically used when adding the agent graph to\nanother graph as a subgraph node - particularly useful for building\nmulti-agent systems.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ncache\n\u00b6\nAn optional\nBaseCache\ninstance to enable caching of graph execution.\nTYPE:\nBaseCache\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCompiledStateGraph\n[\nAgentState\n[\nResponseT\n],\nContextT\n,\n_InputAgentState\n,\n_OutputAgentState\n[\nResponseT\n]]\nA compiled\nStateGraph\nthat can be used for chat interactions.\nThe agent node calls the language model with the messages list (after applying\nthe system prompt). If the resulting\nAIMessage\ncontains\ntool_calls\n, the graph will then call the tools. The tools node executes\nthe tools and adds the responses to the messages list as\nToolMessage\nobjects. The agent node then calls\nthe language model again. The process repeats until no more\ntool_calls\nare present\nin the response. The agent then returns the full list of messages.\nExample\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\ncheck_weather\n(\nlocation\n:\nstr\n)\n->\nstr\n:\n'''Return the weather forecast for the specified location.'''\nreturn\nf\n\"It's always sunny in\n{\nlocation\n}\n\"\ngraph\n=\ncreate_agent\n(\nmodel\n=\n\"anthropic:claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[\ncheck_weather\n],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\ninputs\n=\n{\n\"messages\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\nfor\nchunk\nin\ngraph\n.\nstream\n(\ninputs\n,\nstream_mode\n=\n\"updates\"\n):\nprint\n(\nchunk\n)\nAgentState\n\u00b6\nBases:\nTypedDict\n,\nGeneric\n[\nResponseT\n]\nState schema for the agent.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain/agents/",
      "title": "Agents | LangChain Reference",
      "heading": "Agents"
    }
  },
  {
    "page_content": "Middleware | LangChain Reference\nSkip to content\nLangChain Reference\nMiddleware\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nMiddleware\nTable of contents\nMiddleware classes\nDecorators\nTypes and utilities\nSummarizationMiddleware\nHumanInTheLoopMiddleware\nModelCallLimitMiddleware\nstate_schema\nToolCallLimitMiddleware\nstate_schema\nModelFallbackMiddleware\nPIIMiddleware\nTodoListMiddleware\nstate_schema\nLLMToolSelectorMiddleware\nToolRetryMiddleware\nLLMToolEmulator\nContextEditingMiddleware\nShellToolMiddleware\nFilesystemFileSearchMiddleware\nAgentMiddleware\nbefore_agent\nbefore_model\nafter_model\nafter_agent\nwrap_model_call\nwrap_tool_call\ndynamic_prompt\nhook_config\nAgentState\nModelRequest\nsystem_prompt\n__setattr__\noverride\nModelResponse\nresult\nstructured_response\nClearToolUsesEdit\ntrigger\nclear_at_least\nkeep\nclear_tool_inputs\nexclude_tools\nplaceholder\napply\nInterruptOnConfig\nallowed_decisions\ndescription\nargs_schema\nContextSize\nContextFraction\nContextTokens\nContextMessages\nModels\nMessages\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nMiddleware classes\nDecorators\nTypes and utilities\nSummarizationMiddleware\nHumanInTheLoopMiddleware\nModelCallLimitMiddleware\nstate_schema\nToolCallLimitMiddleware\nstate_schema\nModelFallbackMiddleware\nPIIMiddleware\nTodoListMiddleware\nstate_schema\nLLMToolSelectorMiddleware\nToolRetryMiddleware\nLLMToolEmulator\nContextEditingMiddleware\nShellToolMiddleware\nFilesystemFileSearchMiddleware\nAgentMiddleware\nbefore_agent\nbefore_model\nafter_model\nafter_agent\nwrap_model_call\nwrap_tool_call\ndynamic_prompt\nhook_config\nAgentState\nModelRequest\nsystem_prompt\n__setattr__\noverride\nModelResponse\nresult\nstructured_response\nClearToolUsesEdit\ntrigger\nclear_at_least\nkeep\nclear_tool_inputs\nexclude_tools\nplaceholder\napply\nInterruptOnConfig\nallowed_decisions\ndescription\nargs_schema\nContextSize\nContextFraction\nContextTokens\nContextMessages\nMiddleware\nReference docs\nThis page contains\nreference documentation\nfor Middleware. See\nthe docs\nfor conceptual guides, tutorials, and examples on using Middleware.\nMiddleware classes\n\u00b6\nLangChain provides prebuilt middleware for common agent use cases:\nCLASS\nDESCRIPTION\nSummarizationMiddleware\nAutomatically summarize conversation history when approaching token limits\nHumanInTheLoopMiddleware\nPause execution for human approval of tool calls\nModelCallLimitMiddleware\nLimit the number of model calls to prevent excessive costs\nToolCallLimitMiddleware\nControl tool execution by limiting call counts\nModelFallbackMiddleware\nAutomatically fallback to alternative models when primary fails\nPIIMiddleware\nDetect and handle Personally Identifiable Information\nTodoListMiddleware\nEquip agents with task planning and tracking capabilities\nLLMToolSelectorMiddleware\nUse an LLM to select relevant tools before calling main model\nToolRetryMiddleware\nAutomatically retry failed tool calls with exponential backoff\nLLMToolEmulator\nEmulate tool execution using LLM for testing purposes\nContextEditingMiddleware\nManage conversation context by trimming or clearing tool uses\nShellToolMiddleware\nExpose a persistent shell session to agents for command execution\nFilesystemFileSearchMiddleware\nProvide Glob and Grep search tools over filesystem files\nAgentMiddleware\nBase middleware class for creating custom middleware\nDecorators\n\u00b6\nCreate custom middleware using these decorators:\nDECORATOR\nDESCRIPTION\n@before_agent\nExecute logic before agent execution starts\n@before_model\nExecute logic before each model call\n@after_model\nExecute logic after each model receives a response\n@after_agent\nExecute logic after agent execution completes\n@wrap_model_call\nWrap and intercept model calls\n@wrap_tool_call\nWrap and intercept tool calls\n@dynamic_prompt\nGenerate dynamic system prompts based on request context\n@hook_config\nConfigure hook behavior (e.g., conditional routing)\nTypes and utilities\n\u00b6\nCore types for building middleware:\nTYPE\nDESCRIPTION\nAgentState\nState container for agent execution\nModelRequest\nRequest details passed to model calls\nModelResponse\nResponse details from model calls\nClearToolUsesEdit\nUtility for clearing tool usage history from context\nInterruptOnConfig\nConfiguration for human-in-the-loop interruptions\nSummarizationMiddleware\ntypes:\nTYPE\nDESCRIPTION\nContextSize\nUnion type\nContextFraction\nSummarize at fraction of total context\nContextTokens\nSummarize at token threshold\nContextMessages\nSummarize at message threshold\nSummarizationMiddleware\n\u00b6\nSummarizationMiddleware\n(\nmodel\n:\nstr\n|\nBaseChatModel\n,\n*\n,\ntrigger\n:\nContextSize\n|\nlist\n[\nContextSize\n]\n|\nNone\n=\nNone\n,\nkeep\n:\nContextSize\n=\n(\n\"messages\"\n,\n_DEFAULT_MESSAGES_TO_KEEP\n),\ntoken_counter\n:\nTokenCounter\n=\ncount_tokens_approximately\n,\nsummary_prompt\n:\nstr\n=\nDEFAULT_SUMMARY_PROMPT\n,\ntrim_tokens_to_summarize\n:\nint\n|\nNone\n=\n_DEFAULT_TRIM_TOKEN_LIMIT\n,\n**\ndeprecated_kwargs\n:\nAny\n,\n)\nBases:\nAgentMiddleware\nSummarizes conversation history when token limits are approached.\nThis middleware monitors message token counts and automatically summarizes older\nmessages when a threshold is reached, preserving recent messages and maintaining\ncontext continuity by ensuring AI/Tool message pairs remain together.\nInitialize summarization middleware.\nPARAMETER\nDESCRIPTION\nmodel\nThe language model to use for generating summaries.\nTYPE:\nstr\n|\nBaseChatModel\ntrigger\nOne or more thresholds that trigger summarization.\nProvide a single\nContextSize\ntuple or a list of tuples, in which case summarization runs when any\nthreshold is met.\nExample\n# Trigger summarization when 50 messages is reached\n(\n\"messages\"\n,\n50\n)\n# Trigger summarization when 3000 tokens is reached\n(\n\"tokens\"\n,\n3000\n)\n# Trigger summarization either when 80% of model's max input tokens\n# is reached or when 100 messages is reached (whichever comes first)\n[(\n\"fraction\"\n,\n0.8\n),\n(\n\"messages\"\n,\n100\n)]\nSee\nContextSize\nfor more details.\nTYPE:\nContextSize\n|\nlist\n[\nContextSize\n] | None\nDEFAULT:\nNone\nkeep\nContext retention policy applied after summarization.\nProvide a\nContextSize\ntuple to specify how much history to preserve.\nDefaults to keeping the most recent\n20\nmessages.\nDoes not support multiple values like\ntrigger\n.\nExample\n# Keep the most recent 20 messages\n(\n\"messages\"\n,\n20\n)\n# Keep the most recent 3000 tokens\n(\n\"tokens\"\n,\n3000\n)\n# Keep the most recent 30% of the model's max input tokens\n(\n\"fraction\"\n,\n0.3\n)\nTYPE:\nContextSize\nDEFAULT:\n('messages',\n_DEFAULT_MESSAGES_TO_KEEP\n)\ntoken_counter\nFunction to count tokens in messages.\nTYPE:\nTokenCounter\nDEFAULT:\ncount_tokens_approximately\nsummary_prompt\nPrompt template for generating summaries.\nTYPE:\nstr\nDEFAULT:\nDEFAULT_SUMMARY_PROMPT\ntrim_tokens_to_summarize\nMaximum tokens to keep when preparing messages for\nthe summarization call.\nPass\nNone\nto skip trimming entirely.\nTYPE:\nint\n| None\nDEFAULT:\n_DEFAULT_TRIM_TOKEN_LIMIT\nHumanInTheLoopMiddleware\n\u00b6\nHumanInTheLoopMiddleware\n(\ninterrupt_on\n:\ndict\n[\nstr\n,\nbool\n|\nInterruptOnConfig\n],\n*\n,\ndescription_prefix\n:\nstr\n=\n\"Tool execution requires approval\"\n,\n)\nBases:\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\nHuman in the loop middleware.\nInitialize the human in the loop middleware.\nPARAMETER\nDESCRIPTION\ninterrupt_on\nMapping of tool name to allowed actions.\nIf a tool doesn't have an entry, it's auto-approved by default.\nTrue\nindicates all decisions are allowed: approve, edit, and reject.\nFalse\nindicates that the tool is auto-approved.\nInterruptOnConfig\nindicates the specific decisions allowed for this\ntool.\nThe\nInterruptOnConfig\ncan include a\ndescription\nfield (\nstr\nor\nCallable\n) for custom formatting of the interrupt description.\nTYPE:\ndict\n[\nstr\n,\nbool\n|\nInterruptOnConfig\n]\ndescription_prefix\nThe prefix to use when constructing action requests.\nThis is used to provide context about the tool call and the action being\nrequested.\nNot used if a tool has a\ndescription\nin its\nInterruptOnConfig\n.\nTYPE:\nstr\nDEFAULT:\n'Tool execution requires approval'\nModelCallLimitMiddleware\n\u00b6\nModelCallLimitMiddleware\n(\n*\n,\nthread_limit\n:\nint\n|\nNone\n=\nNone\n,\nrun_limit\n:\nint\n|\nNone\n=\nNone\n,\nexit_behavior\n:\nLiteral\n[\n\"end\"\n,\n\"error\"\n]\n=\n\"end\"\n,\n)\nBases:\nAgentMiddleware\n[\nModelCallLimitState\n,\nAny\n]\nTracks model call counts and enforces limits.\nThis middleware monitors the number of model calls made during agent execution\nand can terminate the agent when specified limits are reached. It supports\nboth thread-level and run-level call counting with configurable exit behaviors.\nThread-level: The middleware tracks the number of model calls and persists\ncall count across multiple runs (invocations) of the agent.\nRun-level: The middleware tracks the number of model calls made during a single\nrun (invocation) of the agent.\nExample\nfrom\nlangchain.agents.middleware.call_tracking\nimport\nModelCallLimitMiddleware\nfrom\nlangchain.agents\nimport\ncreate_agent\n# Create middleware with limits\ncall_tracker\n=\nModelCallLimitMiddleware\n(\nthread_limit\n=\n10\n,\nrun_limit\n=\n5\n,\nexit_behavior\n=\n\"end\"\n)\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\nmiddleware\n=\n[\ncall_tracker\n])\n# Agent will automatically jump to end when limits are exceeded\nresult\n=\nawait\nagent\n.\ninvoke\n({\n\"messages\"\n:\n[\nHumanMessage\n(\n\"Help me with a task\"\n)]})\nInitialize the call tracking middleware.\nPARAMETER\nDESCRIPTION\nthread_limit\nMaximum number of model calls allowed per thread.\nNone\nmeans no limit.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nrun_limit\nMaximum number of model calls allowed per run.\nNone\nmeans no limit.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nexit_behavior\nWhat to do when limits are exceeded.\n'end'\n: Jump to the end of the agent execution and\ninject an artificial AI message indicating that the limit was\nexceeded.\n'error'\n: Raise a\nModelCallLimitExceededError\nTYPE:\nLiteral\n['end', 'error']\nDEFAULT:\n'end'\nRAISES\nDESCRIPTION\nValueError\nIf both limits are\nNone\nor if\nexit_behavior\nis invalid.\nstate_schema\nclass-attribute\ninstance-attribute\n\u00b6\nstate_schema\n=\nModelCallLimitState\nThe schema for state passed to the middleware nodes.\nToolCallLimitMiddleware\n\u00b6\nToolCallLimitMiddleware\n(\n*\n,\ntool_name\n:\nstr\n|\nNone\n=\nNone\n,\nthread_limit\n:\nint\n|\nNone\n=\nNone\n,\nrun_limit\n:\nint\n|\nNone\n=\nNone\n,\nexit_behavior\n:\nExitBehavior\n=\n\"continue\"\n,\n)\nBases:\nAgentMiddleware\n[\nToolCallLimitState\n[\nResponseT\n],\nContextT\n]\n,\nGeneric\n[\nResponseT\n,\nContextT\n]\nTrack tool call counts and enforces limits during agent execution.\nThis middleware monitors the number of tool calls made and can terminate or\nrestrict execution when limits are exceeded. It supports both thread-level\n(persistent across runs) and run-level (per invocation) call counting.\nConfiguration\nexit_behavior\n: How to handle when limits are exceeded\n'continue'\n: Block exceeded tools, let execution continue (default)\n'error'\n: Raise an exception\n'end'\n: Stop immediately with a\nToolMessage\n+ AI message for the single\ntool call that exceeded the limit (raises\nNotImplementedError\nif there\nare other pending tool calls (due to parallel tool calling).\nExamples:\nContinue execution with blocked tools (default)\nfrom\nlangchain.agents.middleware.tool_call_limit\nimport\nToolCallLimitMiddleware\nfrom\nlangchain.agents\nimport\ncreate_agent\n# Block exceeded tools but let other tools and model continue\nlimiter\n=\nToolCallLimitMiddleware\n(\nthread_limit\n=\n20\n,\nrun_limit\n=\n10\n,\nexit_behavior\n=\n\"continue\"\n,\n# default\n)\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\nmiddleware\n=\n[\nlimiter\n])\nStop immediately when limit exceeded\n# End execution immediately with an AI message\nlimiter\n=\nToolCallLimitMiddleware\n(\nrun_limit\n=\n5\n,\nexit_behavior\n=\n\"end\"\n)\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\nmiddleware\n=\n[\nlimiter\n])\nRaise exception on limit\n# Strict limit with exception handling\nlimiter\n=\nToolCallLimitMiddleware\n(\ntool_name\n=\n\"search\"\n,\nthread_limit\n=\n5\n,\nexit_behavior\n=\n\"error\"\n)\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\nmiddleware\n=\n[\nlimiter\n])\ntry\n:\nresult\n=\nawait\nagent\n.\ninvoke\n({\n\"messages\"\n:\n[\nHumanMessage\n(\n\"Task\"\n)]})\nexcept\nToolCallLimitExceededError\nas\ne\n:\nprint\n(\nf\n\"Search limit exceeded:\n{\ne\n}\n\"\n)\nInitialize the tool call limit middleware.\nPARAMETER\nDESCRIPTION\ntool_name\nName of the specific tool to limit. If\nNone\n, limits apply\nto all tools.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nthread_limit\nMaximum number of tool calls allowed per thread.\nNone\nmeans no limit.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nrun_limit\nMaximum number of tool calls allowed per run.\nNone\nmeans no limit.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nexit_behavior\nHow to handle when limits are exceeded.\n'continue'\n: Block exceeded tools with error messages, let other\ntools continue. Model decides when to end.\n'error'\n: Raise a\nToolCallLimitExceededError\nexception\n'end'\n: Stop execution immediately with a\nToolMessage\n+ AI message\nfor the single tool call that exceeded the limit. Raises\nNotImplementedError\nif there are multiple parallel tool\ncalls to other tools or multiple pending tool calls.\nTYPE:\nExitBehavior\nDEFAULT:\n'continue'\nRAISES\nDESCRIPTION\nValueError\nIf both limits are\nNone\n, if\nexit_behavior\nis invalid,\nor if\nrun_limit\nexceeds\nthread_limit\n.\nstate_schema\nclass-attribute\ninstance-attribute\n\u00b6\nstate_schema\n=\nToolCallLimitState\nThe schema for state passed to the middleware nodes.\nModelFallbackMiddleware\n\u00b6\nModelFallbackMiddleware\n(\nfirst_model\n:\nstr\n|\nBaseChatModel\n,\n*\nadditional_models\n:\nstr\n|\nBaseChatModel\n)\nBases:\nAgentMiddleware\nAutomatic fallback to alternative models on errors.\nRetries failed model calls with alternative models in sequence until\nsuccess or all models exhausted. Primary model specified in\ncreate_agent\n.\nExample\nfrom\nlangchain.agents.middleware.model_fallback\nimport\nModelFallbackMiddleware\nfrom\nlangchain.agents\nimport\ncreate_agent\nfallback\n=\nModelFallbackMiddleware\n(\n\"openai:gpt-5-mini\"\n,\n# Try first on error\n\"anthropic:claude-sonnet-4-5-20250929\"\n,\n# Then this\n)\nagent\n=\ncreate_agent\n(\nmodel\n=\n\"openai:gpt-5-mini\"\n,\n# Primary model\nmiddleware\n=\n[\nfallback\n],\n)\n# If primary fails: tries gpt-5-mini, then claude-sonnet-4-5-20250929\nresult\n=\nawait\nagent\n.\ninvoke\n({\n\"messages\"\n:\n[\nHumanMessage\n(\n\"Hello\"\n)]})\nInitialize model fallback middleware.\nPARAMETER\nDESCRIPTION\nfirst_model\nFirst fallback model (string name or instance).\nTYPE:\nstr\n|\nBaseChatModel\n*additional_models\nAdditional fallbacks in order.\nTYPE:\nstr\n|\nBaseChatModel\nDEFAULT:\n()\nPIIMiddleware\n\u00b6\nPIIMiddleware\n(\npii_type\n:\nLiteral\n[\n\"email\"\n,\n\"credit_card\"\n,\n\"ip\"\n,\n\"mac_address\"\n,\n\"url\"\n]\n|\nstr\n,\n*\n,\nstrategy\n:\nLiteral\n[\n\"block\"\n,\n\"redact\"\n,\n\"mask\"\n,\n\"hash\"\n]\n=\n\"redact\"\n,\ndetector\n:\nCallable\n[[\nstr\n],\nlist\n[\nPIIMatch\n]]\n|\nstr\n|\nNone\n=\nNone\n,\napply_to_input\n:\nbool\n=\nTrue\n,\napply_to_output\n:\nbool\n=\nFalse\n,\napply_to_tool_results\n:\nbool\n=\nFalse\n,\n)\nBases:\nAgentMiddleware\nDetect and handle Personally Identifiable Information (PII) in conversations.\nThis middleware detects common PII types and applies configurable strategies\nto handle them. It can detect emails, credit cards, IP addresses, MAC addresses, and\nURLs in both user input and agent output.\nBuilt-in PII types:\nemail\n: Email addresses\ncredit_card\n: Credit card numbers (validated with Luhn algorithm)\nip\n: IP addresses (validated with stdlib)\nmac_address\n: MAC addresses\nurl\n: URLs (both\nhttp\n/\nhttps\nand bare URLs)\nStrategies:\nblock\n: Raise an exception when PII is detected\nredact\n: Replace PII with\n[REDACTED_TYPE]\nplaceholders\nmask\n: Partially mask PII (e.g.,\n****-****-****-1234\nfor credit card)\nhash\n: Replace PII with deterministic hash (e.g.,\n<email_hash:a1b2c3d4>\n)\nStrategy Selection Guide:\nStrategy\nPreserves Identity?\nBest For\nblock\nN/A\nAvoid PII completely\nredact\nNo\nGeneral compliance, log sanitization\nmask\nNo\nHuman readability, customer service UIs\nhash\nYes (pseudonymous)\nAnalytics, debugging\nExample\nfrom\nlangchain.agents.middleware\nimport\nPIIMiddleware\nfrom\nlangchain.agents\nimport\ncreate_agent\n# Redact all emails in user input\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5\"\n,\nmiddleware\n=\n[\nPIIMiddleware\n(\n\"email\"\n,\nstrategy\n=\n\"redact\"\n),\n],\n)\n# Use different strategies for different PII types\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\nmiddleware\n=\n[\nPIIMiddleware\n(\n\"credit_card\"\n,\nstrategy\n=\n\"mask\"\n),\nPIIMiddleware\n(\n\"url\"\n,\nstrategy\n=\n\"redact\"\n),\nPIIMiddleware\n(\n\"ip\"\n,\nstrategy\n=\n\"hash\"\n),\n],\n)\n# Custom PII type with regex\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5\"\n,\nmiddleware\n=\n[\nPIIMiddleware\n(\n\"api_key\"\n,\ndetector\n=\nr\n\"sk-[a-zA-Z0-9]\n{32}\n\"\n,\nstrategy\n=\n\"block\"\n),\n],\n)\nInitialize the PII detection middleware.\nPARAMETER\nDESCRIPTION\npii_type\nType of PII to detect.\nCan be a built-in type (\nemail\n,\ncredit_card\n,\nip\n,\nmac_address\n,\nurl\n) or a custom type name.\nTYPE:\nLiteral\n['email', 'credit_card', 'ip', 'mac_address', 'url'] |\nstr\nstrategy\nHow to handle detected PII.\nOptions:\nblock\n: Raise\nPIIDetectionError\nwhen PII is detected\nredact\n: Replace with\n[REDACTED_TYPE]\nplaceholders\nmask\n: Partially mask PII (show last few characters)\nhash\n: Replace with deterministic hash (format:\n<type_hash:digest>\n)\nTYPE:\nLiteral\n['block', 'redact', 'mask', 'hash']\nDEFAULT:\n'redact'\ndetector\nCustom detector function or regex pattern.\nIf\nCallable\n: Function that takes content string and returns\nlist of\nPIIMatch\nobjects\nIf\nstr\n: Regex pattern to match PII\nIf\nNone\n: Uses built-in detector for the\npii_type\nTYPE:\nCallable\n[[\nstr\n],\nlist\n[\nPIIMatch\n]] |\nstr\n| None\nDEFAULT:\nNone\napply_to_input\nWhether to check user messages before model call.\nTYPE:\nbool\nDEFAULT:\nTrue\napply_to_output\nWhether to check AI messages after model call.\nTYPE:\nbool\nDEFAULT:\nFalse\napply_to_tool_results\nWhether to check tool result messages after tool execution.\nTYPE:\nbool\nDEFAULT:\nFalse\nRAISES\nDESCRIPTION\nValueError\nIf\npii_type\nis not built-in and no detector is provided.\nTodoListMiddleware\n\u00b6\nTodoListMiddleware\n(\n*\n,\nsystem_prompt\n:\nstr\n=\nWRITE_TODOS_SYSTEM_PROMPT\n,\ntool_description\n:\nstr\n=\nWRITE_TODOS_TOOL_DESCRIPTION\n,\n)\nBases:\nAgentMiddleware\nMiddleware that provides todo list management capabilities to agents.\nThis middleware adds a\nwrite_todos\ntool that allows agents to create and manage\nstructured task lists for complex multi-step operations. It's designed to help\nagents track progress, organize complex tasks, and provide users with visibility\ninto task completion status.\nThe middleware automatically injects system prompts that guide the agent on when\nand how to use the todo functionality effectively.\nExample\nfrom\nlangchain.agents.middleware.todo\nimport\nTodoListMiddleware\nfrom\nlangchain.agents\nimport\ncreate_agent\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\nmiddleware\n=\n[\nTodoListMiddleware\n()])\n# Agent now has access to write_todos tool and todo state tracking\nresult\n=\nawait\nagent\n.\ninvoke\n({\n\"messages\"\n:\n[\nHumanMessage\n(\n\"Help me refactor my codebase\"\n)]})\nprint\n(\nresult\n[\n\"todos\"\n])\n# Array of todo items with status tracking\nInitialize the\nTodoListMiddleware\nwith optional custom prompts.\nPARAMETER\nDESCRIPTION\nsystem_prompt\nCustom system prompt to guide the agent on using the todo\ntool.\nTYPE:\nstr\nDEFAULT:\nWRITE_TODOS_SYSTEM_PROMPT\ntool_description\nCustom description for the\nwrite_todos\ntool.\nTYPE:\nstr\nDEFAULT:\nWRITE_TODOS_TOOL_DESCRIPTION\nstate_schema\nclass-attribute\ninstance-attribute\n\u00b6\nstate_schema\n=\nPlanningState\nThe schema for state passed to the middleware nodes.\nLLMToolSelectorMiddleware\n\u00b6\nLLMToolSelectorMiddleware\n(\n*\n,\nmodel\n:\nstr\n|\nBaseChatModel\n|\nNone\n=\nNone\n,\nsystem_prompt\n:\nstr\n=\nDEFAULT_SYSTEM_PROMPT\n,\nmax_tools\n:\nint\n|\nNone\n=\nNone\n,\nalways_include\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n)\nBases:\nAgentMiddleware\nUses an LLM to select relevant tools before calling the main model.\nWhen an agent has many tools available, this middleware filters them down\nto only the most relevant ones for the user's query. This reduces token usage\nand helps the main model focus on the right tools.\nExamples:\nLimit to 3 tools\nfrom\nlangchain.agents.middleware\nimport\nLLMToolSelectorMiddleware\nmiddleware\n=\nLLMToolSelectorMiddleware\n(\nmax_tools\n=\n3\n)\nagent\n=\ncreate_agent\n(\nmodel\n=\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[\ntool1\n,\ntool2\n,\ntool3\n,\ntool4\n,\ntool5\n],\nmiddleware\n=\n[\nmiddleware\n],\n)\nUse a smaller model for selection\nmiddleware\n=\nLLMToolSelectorMiddleware\n(\nmodel\n=\n\"openai:gpt-5-mini\"\n,\nmax_tools\n=\n2\n)\nInitialize the tool selector.\nPARAMETER\nDESCRIPTION\nmodel\nModel to use for selection.\nIf not provided, uses the agent's main model.\nCan be a model identifier string or\nBaseChatModel\ninstance.\nTYPE:\nstr\n|\nBaseChatModel\n| None\nDEFAULT:\nNone\nsystem_prompt\nInstructions for the selection model.\nTYPE:\nstr\nDEFAULT:\nDEFAULT_SYSTEM_PROMPT\nmax_tools\nMaximum number of tools to select.\nIf the model selects more, only the first\nmax_tools\nwill be used.\nIf not specified, there is no limit.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nalways_include\nTool names to always include regardless of selection.\nThese do not count against the\nmax_tools\nlimit.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nToolRetryMiddleware\n\u00b6\nToolRetryMiddleware\n(\n*\n,\nmax_retries\n:\nint\n=\n2\n,\ntools\n:\nlist\n[\nBaseTool\n|\nstr\n]\n|\nNone\n=\nNone\n,\nretry_on\n:\nRetryOn\n=\n(\nException\n,),\non_failure\n:\nOnFailure\n=\n\"continue\"\n,\nbackoff_factor\n:\nfloat\n=\n2.0\n,\ninitial_delay\n:\nfloat\n=\n1.0\n,\nmax_delay\n:\nfloat\n=\n60.0\n,\njitter\n:\nbool\n=\nTrue\n,\n)\nBases:\nAgentMiddleware\nMiddleware that automatically retries failed tool calls with configurable backoff.\nSupports retrying on specific exceptions and exponential backoff.\nExamples:\nBasic usage with default settings (2 retries, exponential backoff)\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nToolRetryMiddleware\nagent\n=\ncreate_agent\n(\nmodel\n,\ntools\n=\n[\nsearch_tool\n],\nmiddleware\n=\n[\nToolRetryMiddleware\n()])\nRetry specific exceptions only\nfrom\nrequests.exceptions\nimport\nRequestException\n,\nTimeout\nretry\n=\nToolRetryMiddleware\n(\nmax_retries\n=\n4\n,\nretry_on\n=\n(\nRequestException\n,\nTimeout\n),\nbackoff_factor\n=\n1.5\n,\n)\nCustom exception filtering\nfrom\nrequests.exceptions\nimport\nHTTPError\ndef\nshould_retry\n(\nexc\n:\nException\n)\n->\nbool\n:\n# Only retry on 5xx errors\nif\nisinstance\n(\nexc\n,\nHTTPError\n):\nreturn\n500\n<=\nexc\n.\nstatus_code\n<\n600\nreturn\nFalse\nretry\n=\nToolRetryMiddleware\n(\nmax_retries\n=\n3\n,\nretry_on\n=\nshould_retry\n,\n)\nApply to specific tools with custom error handling\ndef\nformat_error\n(\nexc\n:\nException\n)\n->\nstr\n:\nreturn\n\"Database temporarily unavailable. Please try again later.\"\nretry\n=\nToolRetryMiddleware\n(\nmax_retries\n=\n4\n,\ntools\n=\n[\n\"search_database\"\n],\non_failure\n=\nformat_error\n,\n)\nApply to specific tools using\nBaseTool\ninstances\nfrom\nlangchain_core.tools\nimport\ntool\n@tool\ndef\nsearch_database\n(\nquery\n:\nstr\n)\n->\nstr\n:\n'''Search the database.'''\nreturn\nresults\nretry\n=\nToolRetryMiddleware\n(\nmax_retries\n=\n4\n,\ntools\n=\n[\nsearch_database\n],\n# Pass BaseTool instance\n)\nConstant backoff (no exponential growth)\nretry\n=\nToolRetryMiddleware\n(\nmax_retries\n=\n5\n,\nbackoff_factor\n=\n0.0\n,\n# No exponential growth\ninitial_delay\n=\n2.0\n,\n# Always wait 2 seconds\n)\nRaise exception on failure\nretry\n=\nToolRetryMiddleware\n(\nmax_retries\n=\n2\n,\non_failure\n=\n\"error\"\n,\n# Re-raise exception instead of returning message\n)\nInitialize\nToolRetryMiddleware\n.\nPARAMETER\nDESCRIPTION\nmax_retries\nMaximum number of retry attempts after the initial call.\nMust be\n>= 0\n.\nTYPE:\nint\nDEFAULT:\n2\ntools\nOptional list of tools or tool names to apply retry logic to.\nCan be a list of\nBaseTool\ninstances or tool name strings.\nIf\nNone\n, applies to all tools.\nTYPE:\nlist\n[\nBaseTool\n|\nstr\n] | None\nDEFAULT:\nNone\nretry_on\nEither a tuple of exception types to retry on, or a callable\nthat takes an exception and returns\nTrue\nif it should be retried.\nDefault is to retry on all exceptions.\nTYPE:\nRetryOn\nDEFAULT:\n(\nException\n,)\non_failure\nBehavior when all retries are exhausted.\nOptions:\n'continue'\n: Return a\nToolMessage\nwith error details,\nallowing the LLM to handle the failure and potentially recover.\n'error'\n: Re-raise the exception, stopping agent execution.\nCustom callable:\nFunction that takes the exception and returns a\nstring for the\nToolMessage\ncontent, allowing custom error\nformatting.\nDeprecated values\n(for backwards compatibility):\n'return_message'\n: Use\n'continue'\ninstead.\n'raise'\n: Use\n'error'\ninstead.\nTYPE:\nOnFailure\nDEFAULT:\n'continue'\nbackoff_factor\nMultiplier for exponential backoff.\nEach retry waits\ninitial_delay * (backoff_factor ** retry_number)\nseconds.\nSet to\n0.0\nfor constant delay.\nTYPE:\nfloat\nDEFAULT:\n2.0\ninitial_delay\nInitial delay in seconds before first retry.\nTYPE:\nfloat\nDEFAULT:\n1.0\nmax_delay\nMaximum delay in seconds between retries.\nCaps exponential backoff growth.\nTYPE:\nfloat\nDEFAULT:\n60.0\njitter\nWhether to add random jitter (\n\u00b125%\n) to delay to avoid thundering herd.\nTYPE:\nbool\nDEFAULT:\nTrue\nRAISES\nDESCRIPTION\nValueError\nIf\nmax_retries < 0\nor delays are negative.\nLLMToolEmulator\n\u00b6\nLLMToolEmulator\n(\n*\n,\ntools\n:\nlist\n[\nstr\n|\nBaseTool\n]\n|\nNone\n=\nNone\n,\nmodel\n:\nstr\n|\nBaseChatModel\n|\nNone\n=\nNone\n,\n)\nBases:\nAgentMiddleware\nEmulates specified tools using an LLM instead of executing them.\nThis middleware allows selective emulation of tools for testing purposes.\nBy default (when\ntools=None\n), all tools are emulated. You can specify which\ntools to emulate by passing a list of tool names or\nBaseTool\ninstances.\nExamples:\nEmulate all tools (default behavior)\nfrom\nlangchain.agents.middleware\nimport\nLLMToolEmulator\nmiddleware\n=\nLLMToolEmulator\n()\nagent\n=\ncreate_agent\n(\nmodel\n=\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[\nget_weather\n,\nget_user_location\n,\ncalculator\n],\nmiddleware\n=\n[\nmiddleware\n],\n)\nEmulate specific tools by name\nmiddleware\n=\nLLMToolEmulator\n(\ntools\n=\n[\n\"get_weather\"\n,\n\"get_user_location\"\n])\nUse a custom model for emulation\nmiddleware\n=\nLLMToolEmulator\n(\ntools\n=\n[\n\"get_weather\"\n],\nmodel\n=\n\"anthropic:claude-sonnet-4-5-20250929\"\n)\nEmulate specific tools by passing tool instances\nmiddleware\n=\nLLMToolEmulator\n(\ntools\n=\n[\nget_weather\n,\nget_user_location\n])\nInitialize the tool emulator.\nPARAMETER\nDESCRIPTION\ntools\nList of tool names (\nstr\n) or\nBaseTool\ninstances to emulate.\nIf\nNone\n, ALL tools will be emulated.\nIf empty list, no tools will be emulated.\nTYPE:\nlist\n[\nstr\n|\nBaseTool\n] | None\nDEFAULT:\nNone\nmodel\nModel to use for emulation.\nDefaults to\n'anthropic:claude-sonnet-4-5-20250929'\n.\nCan be a model identifier string or\nBaseChatModel\ninstance.\nTYPE:\nstr\n|\nBaseChatModel\n| None\nDEFAULT:\nNone\nContextEditingMiddleware\n\u00b6\nContextEditingMiddleware\n(\n*\n,\nedits\n:\nIterable\n[\nContextEdit\n]\n|\nNone\n=\nNone\n,\ntoken_count_method\n:\nLiteral\n[\n\"approximate\"\n,\n\"model\"\n]\n=\n\"approximate\"\n,\n)\nBases:\nAgentMiddleware\nAutomatically prune tool results to manage context size.\nThe middleware applies a sequence of edits when the total input token count exceeds\nconfigured thresholds.\nCurrently the\nClearToolUsesEdit\nstrategy is supported, aligning with Anthropic's\nclear_tool_uses_20250919\nbehavior\n(read more)\n.\nInitialize an instance of context editing middleware.\nPARAMETER\nDESCRIPTION\nedits\nSequence of edit strategies to apply.\nDefaults to a single\nClearToolUsesEdit\nmirroring Anthropic defaults.\nTYPE:\nIterable\n[\nContextEdit\n] | None\nDEFAULT:\nNone\ntoken_count_method\nWhether to use approximate token counting\n(faster, less accurate) or exact counting implemented by the\nchat model (potentially slower, more accurate).\nTYPE:\nLiteral\n['approximate', 'model']\nDEFAULT:\n'approximate'\nShellToolMiddleware\n\u00b6\nShellToolMiddleware\n(\nworkspace_root\n:\nstr\n|\nPath\n|\nNone\n=\nNone\n,\n*\n,\nstartup_commands\n:\ntuple\n[\nstr\n,\n...\n]\n|\nlist\n[\nstr\n]\n|\nstr\n|\nNone\n=\nNone\n,\nshutdown_commands\n:\ntuple\n[\nstr\n,\n...\n]\n|\nlist\n[\nstr\n]\n|\nstr\n|\nNone\n=\nNone\n,\nexecution_policy\n:\nBaseExecutionPolicy\n|\nNone\n=\nNone\n,\nredaction_rules\n:\ntuple\n[\nRedactionRule\n,\n...\n]\n|\nlist\n[\nRedactionRule\n]\n|\nNone\n=\nNone\n,\ntool_description\n:\nstr\n|\nNone\n=\nNone\n,\ntool_name\n:\nstr\n=\nSHELL_TOOL_NAME\n,\nshell_command\n:\nSequence\n[\nstr\n]\n|\nstr\n|\nNone\n=\nNone\n,\nenv\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n)\nBases:\nAgentMiddleware\n[\nShellToolState\n,\nAny\n]\nMiddleware that registers a persistent shell tool for agents.\nThe middleware exposes a single long-lived shell session. Use the execution policy\nto match your deployment's security posture:\nHostExecutionPolicy\n\u2013 full host access; best for trusted environments where the\nagent already runs inside a container or VM that provides isolation.\nCodexSandboxExecutionPolicy\n\u2013 reuses the Codex CLI sandbox for additional\nsyscall/filesystem restrictions when the CLI is available.\nDockerExecutionPolicy\n\u2013 launches a separate Docker container for each agent run,\nproviding harder isolation, optional read-only root filesystems, and user\nremapping.\nWhen no policy is provided the middleware defaults to\nHostExecutionPolicy\n.\nInitialize an instance of\nShellToolMiddleware\n.\nPARAMETER\nDESCRIPTION\nworkspace_root\nBase directory for the shell session.\nIf omitted, a temporary directory is created when the agent starts and\nremoved when it ends.\nTYPE:\nstr\n|\nPath\n| None\nDEFAULT:\nNone\nstartup_commands\nOptional commands executed sequentially after the session\nstarts.\nTYPE:\ntuple\n[\nstr\n, ...] |\nlist\n[\nstr\n] |\nstr\n| None\nDEFAULT:\nNone\nshutdown_commands\nOptional commands executed before the session shuts down.\nTYPE:\ntuple\n[\nstr\n, ...] |\nlist\n[\nstr\n] |\nstr\n| None\nDEFAULT:\nNone\nexecution_policy\nExecution policy controlling timeouts, output limits, and\nresource configuration.\nDefaults to\nHostExecutionPolicy\nfor native execution.\nTYPE:\nBaseExecutionPolicy\n| None\nDEFAULT:\nNone\nredaction_rules\nOptional redaction rules to sanitize command output before\nreturning it to the model.\nTYPE:\ntuple\n[\nRedactionRule\n, ...] |\nlist\n[\nRedactionRule\n] | None\nDEFAULT:\nNone\ntool_description\nOptional override for the registered shell tool\ndescription.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ntool_name\nName for the registered shell tool.\nDefaults to\n\"shell\"\n.\nTYPE:\nstr\nDEFAULT:\nSHELL_TOOL_NAME\nshell_command\nOptional shell executable (string) or argument sequence used\nto launch the persistent session.\nDefaults to an implementation-defined bash command.\nTYPE:\nSequence\n[\nstr\n] |\nstr\n| None\nDEFAULT:\nNone\nenv\nOptional environment variables to supply to the shell session.\nValues are coerced to strings before command execution. If omitted, the\nsession inherits the parent process environment.\nTYPE:\nMapping\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nFilesystemFileSearchMiddleware\n\u00b6\nFilesystemFileSearchMiddleware\n(\n*\n,\nroot_path\n:\nstr\n,\nuse_ripgrep\n:\nbool\n=\nTrue\n,\nmax_file_size_mb\n:\nint\n=\n10\n)\nBases:\nAgentMiddleware\nProvides Glob and Grep search over filesystem files.\nThis middleware adds two tools that search through local filesystem:\nGlob: Fast file pattern matching by file path\nGrep: Fast content search using ripgrep or Python fallback\nExample\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\n(\nFilesystemFileSearchMiddleware\n,\n)\nagent\n=\ncreate_agent\n(\nmodel\n=\nmodel\n,\ntools\n=\n[],\n# Add tools as needed\nmiddleware\n=\n[\nFilesystemFileSearchMiddleware\n(\nroot_path\n=\n\"/workspace\"\n),\n],\n)\nInitialize the search middleware.\nPARAMETER\nDESCRIPTION\nroot_path\nRoot directory to search.\nTYPE:\nstr\nuse_ripgrep\nWhether to use\nripgrep\nfor search.\nFalls back to Python if\nripgrep\nunavailable.\nTYPE:\nbool\nDEFAULT:\nTrue\nmax_file_size_mb\nMaximum file size to search in MB.\nTYPE:\nint\nDEFAULT:\n10\nAgentMiddleware\n\u00b6\nBases:\nGeneric\n[\nStateT\n,\nContextT\n]\nBase middleware class for an agent.\nSubclass this and implement any of the defined methods to customize agent behavior\nbetween steps in the main agent loop.\nbefore_agent\n\u00b6\nbefore_agent\n(\nfunc\n:\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]\n|\nNone\n=\nNone\n,\n*\n,\nstate_schema\n:\ntype\n[\nStateT\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nBaseTool\n]\n|\nNone\n=\nNone\n,\ncan_jump_to\n:\nlist\n[\nJumpTo\n]\n|\nNone\n=\nNone\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\n(\nCallable\n[\n[\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n],\n]\n|\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\n)\nDecorator used to dynamically create a middleware with the\nbefore_agent\nhook.\nPARAMETER\nDESCRIPTION\nfunc\nThe function to be decorated.\nMust accept:\nstate: StateT, runtime: Runtime[ContextT]\n- State and runtime\ncontext\nTYPE:\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n] | None\nDEFAULT:\nNone\nstate_schema\nOptional custom state schema type.\nIf not provided, uses the default\nAgentState\nschema.\nTYPE:\ntype\n[\nStateT\n] | None\nDEFAULT:\nNone\ntools\nOptional list of additional tools to register with this middleware.\nTYPE:\nlist\n[\nBaseTool\n] | None\nDEFAULT:\nNone\ncan_jump_to\nOptional list of valid jump destinations for conditional edges.\nValid values are:\n'tools'\n,\n'model'\n,\n'end'\nTYPE:\nlist\n[\nJumpTo\n] | None\nDEFAULT:\nNone\nname\nOptional name for the generated middleware class.\nIf not provided, uses the decorated function's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallable\n[[\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n]] |\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\nEither an\nAgentMiddleware\ninstance (if func is provided directly) or a\ndecorator function that can be applied to a function it is wrapping.\nThe decorated function should return:\ndict[str, Any]\n- State updates to merge into the agent state\nCommand\n- A command to control flow (e.g., jump to different node)\nNone\n- No state updates or flow control\nExamples:\nBasic usage\n@before_agent\ndef\nlog_before_agent\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\nNone\n:\nprint\n(\nf\n\"Starting agent with\n{\nlen\n(\nstate\n[\n'messages'\n])\n}\nmessages\"\n)\nWith conditional jumping\n@before_agent\n(\ncan_jump_to\n=\n[\n\"end\"\n])\ndef\nconditional_before_agent\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\nif\nsome_condition\n(\nstate\n):\nreturn\n{\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\nWith custom state schema\n@before_agent\n(\nstate_schema\n=\nMyCustomState\n)\ndef\ncustom_before_agent\n(\nstate\n:\nMyCustomState\n,\nruntime\n:\nRuntime\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"custom_field\"\n:\n\"initialized_value\"\n}\nStreaming custom events\nUse\nruntime.stream_writer\nto emit custom events during agent execution.\nEvents are received when streaming with\nstream_mode=\"custom\"\n.\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nbefore_agent\n,\nAgentState\nfrom\nlangchain.messages\nimport\nHumanMessage\nfrom\nlanggraph.runtime\nimport\nRuntime\n@before_agent\nasync\ndef\nnotify_start\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\nNone\n:\n'''Notify user that agent is starting.'''\nruntime\n.\nstream_writer\n(\n{\n\"type\"\n:\n\"status\"\n,\n\"message\"\n:\n\"Initializing agent session...\"\n,\n}\n)\n# Perform prerequisite tasks here\nruntime\n.\nstream_writer\n({\n\"type\"\n:\n\"status\"\n,\n\"message\"\n:\n\"Agent ready!\"\n})\nagent\n=\ncreate_agent\n(\nmodel\n=\n\"openai:gpt-5.2\"\n,\ntools\n=\n[\n...\n],\nmiddleware\n=\n[\nnotify_start\n],\n)\n# Consume with stream_mode=\"custom\" to receive events\nasync\nfor\nmode\n,\nevent\nin\nagent\n.\nastream\n(\n{\n\"messages\"\n:\n[\nHumanMessage\n(\n\"Hello\"\n)]},\nstream_mode\n=\n[\n\"updates\"\n,\n\"custom\"\n],\n):\nif\nmode\n==\n\"custom\"\n:\nprint\n(\nf\n\"Status:\n{\nevent\n}\n\"\n)\nbefore_model\n\u00b6\nbefore_model\n(\nfunc\n:\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]\n|\nNone\n=\nNone\n,\n*\n,\nstate_schema\n:\ntype\n[\nStateT\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nBaseTool\n]\n|\nNone\n=\nNone\n,\ncan_jump_to\n:\nlist\n[\nJumpTo\n]\n|\nNone\n=\nNone\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\n(\nCallable\n[\n[\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n],\n]\n|\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\n)\nDecorator used to dynamically create a middleware with the\nbefore_model\nhook.\nPARAMETER\nDESCRIPTION\nfunc\nThe function to be decorated.\nMust accept:\nstate: StateT, runtime: Runtime[ContextT]\n- State and runtime\ncontext\nTYPE:\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n] | None\nDEFAULT:\nNone\nstate_schema\nOptional custom state schema type.\nIf not provided, uses the default\nAgentState\nschema.\nTYPE:\ntype\n[\nStateT\n] | None\nDEFAULT:\nNone\ntools\nOptional list of additional tools to register with this middleware.\nTYPE:\nlist\n[\nBaseTool\n] | None\nDEFAULT:\nNone\ncan_jump_to\nOptional list of valid jump destinations for conditional edges.\nValid values are:\n'tools'\n,\n'model'\n,\n'end'\nTYPE:\nlist\n[\nJumpTo\n] | None\nDEFAULT:\nNone\nname\nOptional name for the generated middleware class.\nIf not provided, uses the decorated function's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallable\n[[\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n]] |\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\nEither an\nAgentMiddleware\ninstance (if func is provided directly) or a\ndecorator function that can be applied to a function it is wrapping.\nThe decorated function should return:\ndict[str, Any]\n- State updates to merge into the agent state\nCommand\n- A command to control flow (e.g., jump to different node)\nNone\n- No state updates or flow control\nExamples:\nBasic usage\n@before_model\ndef\nlog_before_model\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\nNone\n:\nprint\n(\nf\n\"About to call model with\n{\nlen\n(\nstate\n[\n'messages'\n])\n}\nmessages\"\n)\nWith conditional jumping\n@before_model\n(\ncan_jump_to\n=\n[\n\"end\"\n])\ndef\nconditional_before_model\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\nif\nsome_condition\n(\nstate\n):\nreturn\n{\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\nWith custom state schema\n@before_model\n(\nstate_schema\n=\nMyCustomState\n)\ndef\ncustom_before_model\n(\nstate\n:\nMyCustomState\n,\nruntime\n:\nRuntime\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"custom_field\"\n:\n\"updated_value\"\n}\nStreaming custom events before model call\nUse\nruntime.stream_writer\nto emit custom events before each model invocation.\nEvents are received when streaming with\nstream_mode=\"custom\"\n.\n@before_model\nasync\ndef\nnotify_model_call\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\nNone\n:\n'''Notify user before model is called.'''\nruntime\n.\nstream_writer\n(\n{\n\"type\"\n:\n\"status\"\n,\n\"message\"\n:\n\"Thinking...\"\n,\n}\n)\nafter_model\n\u00b6\nafter_model\n(\nfunc\n:\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]\n|\nNone\n=\nNone\n,\n*\n,\nstate_schema\n:\ntype\n[\nStateT\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nBaseTool\n]\n|\nNone\n=\nNone\n,\ncan_jump_to\n:\nlist\n[\nJumpTo\n]\n|\nNone\n=\nNone\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\n(\nCallable\n[\n[\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n],\n]\n|\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\n)\nDecorator used to dynamically create a middleware with the\nafter_model\nhook.\nPARAMETER\nDESCRIPTION\nfunc\nThe function to be decorated.\nMust accept:\nstate: StateT, runtime: Runtime[ContextT]\n- State and runtime\ncontext\nTYPE:\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n] | None\nDEFAULT:\nNone\nstate_schema\nOptional custom state schema type.\nIf not provided, uses the default\nAgentState\nschema.\nTYPE:\ntype\n[\nStateT\n] | None\nDEFAULT:\nNone\ntools\nOptional list of additional tools to register with this middleware.\nTYPE:\nlist\n[\nBaseTool\n] | None\nDEFAULT:\nNone\ncan_jump_to\nOptional list of valid jump destinations for conditional edges.\nValid values are:\n'tools'\n,\n'model'\n,\n'end'\nTYPE:\nlist\n[\nJumpTo\n] | None\nDEFAULT:\nNone\nname\nOptional name for the generated middleware class.\nIf not provided, uses the decorated function's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallable\n[[\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n]] |\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\nEither an\nAgentMiddleware\ninstance (if func is provided) or a decorator\nfunction that can be applied to a function.\nThe decorated function should return:\ndict[str, Any]\n- State updates to merge into the agent state\nCommand\n- A command to control flow (e.g., jump to different node)\nNone\n- No state updates or flow control\nExamples:\nBasic usage for logging model responses\n@after_model\ndef\nlog_latest_message\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\nNone\n:\nprint\n(\nstate\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n)\nWith custom state schema\n@after_model\n(\nstate_schema\n=\nMyCustomState\n,\nname\n=\n\"MyAfterModelMiddleware\"\n)\ndef\ncustom_after_model\n(\nstate\n:\nMyCustomState\n,\nruntime\n:\nRuntime\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"custom_field\"\n:\n\"updated_after_model\"\n}\nStreaming custom events after model call\nUse\nruntime.stream_writer\nto emit custom events after model responds.\nEvents are received when streaming with\nstream_mode=\"custom\"\n.\n@after_model\nasync\ndef\nnotify_model_response\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\nNone\n:\n'''Notify user after model has responded.'''\nlast_message\n=\nstate\n[\n\"messages\"\n][\n-\n1\n]\nhas_tool_calls\n=\nhasattr\n(\nlast_message\n,\n\"tool_calls\"\n)\nand\nlast_message\n.\ntool_calls\nruntime\n.\nstream_writer\n(\n{\n\"type\"\n:\n\"status\"\n,\n\"message\"\n:\n\"Using tools...\"\nif\nhas_tool_calls\nelse\n\"Response ready!\"\n,\n}\n)\nafter_agent\n\u00b6\nafter_agent\n(\nfunc\n:\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]\n|\nNone\n=\nNone\n,\n*\n,\nstate_schema\n:\ntype\n[\nStateT\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nBaseTool\n]\n|\nNone\n=\nNone\n,\ncan_jump_to\n:\nlist\n[\nJumpTo\n]\n|\nNone\n=\nNone\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\n(\nCallable\n[\n[\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n],\n]\n|\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\n)\nDecorator used to dynamically create a middleware with the\nafter_agent\nhook.\nAsync version is\naafter_agent\n.\nPARAMETER\nDESCRIPTION\nfunc\nThe function to be decorated.\nMust accept:\nstate: StateT, runtime: Runtime[ContextT]\n- State and runtime\ncontext\nTYPE:\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n] | None\nDEFAULT:\nNone\nstate_schema\nOptional custom state schema type.\nIf not provided, uses the default\nAgentState\nschema.\nTYPE:\ntype\n[\nStateT\n] | None\nDEFAULT:\nNone\ntools\nOptional list of additional tools to register with this middleware.\nTYPE:\nlist\n[\nBaseTool\n] | None\nDEFAULT:\nNone\ncan_jump_to\nOptional list of valid jump destinations for conditional edges.\nValid values are:\n'tools'\n,\n'model'\n,\n'end'\nTYPE:\nlist\n[\nJumpTo\n] | None\nDEFAULT:\nNone\nname\nOptional name for the generated middleware class.\nIf not provided, uses the decorated function's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallable\n[[\n_CallableWithStateAndRuntime\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n]] |\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\nEither an\nAgentMiddleware\ninstance (if func is provided) or a decorator\nfunction that can be applied to a function.\nThe decorated function should return:\ndict[str, Any]\n- State updates to merge into the agent state\nCommand\n- A command to control flow (e.g., jump to different node)\nNone\n- No state updates or flow control\nExamples:\nBasic usage for logging agent completion\n@after_agent\ndef\nlog_completion\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\nNone\n:\nprint\n(\nf\n\"Agent completed with\n{\nlen\n(\nstate\n[\n'messages'\n])\n}\nmessages\"\n)\nWith custom state schema\n@after_agent\n(\nstate_schema\n=\nMyCustomState\n,\nname\n=\n\"MyAfterAgentMiddleware\"\n)\ndef\ncustom_after_agent\n(\nstate\n:\nMyCustomState\n,\nruntime\n:\nRuntime\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"custom_field\"\n:\n\"finalized_value\"\n}\nStreaming custom events on completion\nUse\nruntime.stream_writer\nto emit custom events when agent completes.\nEvents are received when streaming with\nstream_mode=\"custom\"\n.\n@after_agent\nasync\ndef\nnotify_completion\n(\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n)\n->\nNone\n:\n'''Notify user that agent has completed.'''\nruntime\n.\nstream_writer\n(\n{\n\"type\"\n:\n\"status\"\n,\n\"message\"\n:\n\"Agent execution complete!\"\n,\n\"total_messages\"\n:\nlen\n(\nstate\n[\n\"messages\"\n]),\n}\n)\nwrap_model_call\n\u00b6\nwrap_model_call\n(\nfunc\n:\n_CallableReturningModelResponse\n[\nStateT\n,\nContextT\n]\n|\nNone\n=\nNone\n,\n*\n,\nstate_schema\n:\ntype\n[\nStateT\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nBaseTool\n]\n|\nNone\n=\nNone\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\n(\nCallable\n[\n[\n_CallableReturningModelResponse\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n],\n]\n|\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\n)\nCreate middleware with\nwrap_model_call\nhook from a function.\nConverts a function with handler callback into middleware that can intercept model\ncalls, implement retry logic, handle errors, and rewrite responses.\nPARAMETER\nDESCRIPTION\nfunc\nFunction accepting (request, handler) that calls handler(request)\nto execute the model and returns\nModelResponse\nor\nAIMessage\n.\nRequest contains state and runtime.\nTYPE:\n_CallableReturningModelResponse\n[\nStateT\n,\nContextT\n] | None\nDEFAULT:\nNone\nstate_schema\nCustom state schema.\nDefaults to\nAgentState\n.\nTYPE:\ntype\n[\nStateT\n] | None\nDEFAULT:\nNone\ntools\nAdditional tools to register with this middleware.\nTYPE:\nlist\n[\nBaseTool\n] | None\nDEFAULT:\nNone\nname\nMiddleware class name.\nDefaults to function name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallable\n[[\n_CallableReturningModelResponse\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n]] |\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\nAgentMiddleware\ninstance if func provided, otherwise a decorator.\nExamples:\nBasic retry logic\n@wrap_model_call\ndef\nretry_on_error\n(\nrequest\n,\nhandler\n):\nmax_retries\n=\n3\nfor\nattempt\nin\nrange\n(\nmax_retries\n):\ntry\n:\nreturn\nhandler\n(\nrequest\n)\nexcept\nException\n:\nif\nattempt\n==\nmax_retries\n-\n1\n:\nraise\nModel fallback\n@wrap_model_call\ndef\nfallback_model\n(\nrequest\n,\nhandler\n):\n# Try primary model\ntry\n:\nreturn\nhandler\n(\nrequest\n)\nexcept\nException\n:\npass\n# Try fallback model\nrequest\n=\nrequest\n.\noverride\n(\nmodel\n=\nfallback_model_instance\n)\nreturn\nhandler\n(\nrequest\n)\nRewrite response content (full\nModelResponse\n)\n@wrap_model_call\ndef\nuppercase_responses\n(\nrequest\n,\nhandler\n):\nresponse\n=\nhandler\n(\nrequest\n)\nai_msg\n=\nresponse\n.\nresult\n[\n0\n]\nreturn\nModelResponse\n(\nresult\n=\n[\nAIMessage\n(\ncontent\n=\nai_msg\n.\ncontent\n.\nupper\n())],\nstructured_response\n=\nresponse\n.\nstructured_response\n,\n)\nSimple\nAIMessage\nreturn (converted automatically)\n@wrap_model_call\ndef\nsimple_response\n(\nrequest\n,\nhandler\n):\n# AIMessage is automatically converted to ModelResponse\nreturn\nAIMessage\n(\ncontent\n=\n\"Simple response\"\n)\nwrap_tool_call\n\u00b6\nwrap_tool_call\n(\nfunc\n:\n_CallableReturningToolResponse\n|\nNone\n=\nNone\n,\n*\n,\ntools\n:\nlist\n[\nBaseTool\n]\n|\nNone\n=\nNone\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nCallable\n[[\n_CallableReturningToolResponse\n],\nAgentMiddleware\n]\n|\nAgentMiddleware\nCreate middleware with\nwrap_tool_call\nhook from a function.\nAsync version is\nawrap_tool_call\n.\nConverts a function with handler callback into middleware that can intercept\ntool calls, implement retry logic, monitor execution, and modify responses.\nPARAMETER\nDESCRIPTION\nfunc\nFunction accepting (request, handler) that calls\nhandler(request) to execute the tool and returns final\nToolMessage\nor\nCommand\n.\nCan be sync or async.\nTYPE:\n_CallableReturningToolResponse\n| None\nDEFAULT:\nNone\ntools\nAdditional tools to register with this middleware.\nTYPE:\nlist\n[\nBaseTool\n] | None\nDEFAULT:\nNone\nname\nMiddleware class name.\nDefaults to function name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallable\n[[\n_CallableReturningToolResponse\n],\nAgentMiddleware\n] |\nAgentMiddleware\nAgentMiddleware\ninstance if func provided, otherwise a decorator.\nExamples:\nRetry logic\n@wrap_tool_call\ndef\nretry_on_error\n(\nrequest\n,\nhandler\n):\nmax_retries\n=\n3\nfor\nattempt\nin\nrange\n(\nmax_retries\n):\ntry\n:\nreturn\nhandler\n(\nrequest\n)\nexcept\nException\n:\nif\nattempt\n==\nmax_retries\n-\n1\n:\nraise\nAsync retry logic\n@wrap_tool_call\nasync\ndef\nasync_retry\n(\nrequest\n,\nhandler\n):\nfor\nattempt\nin\nrange\n(\n3\n):\ntry\n:\nreturn\nawait\nhandler\n(\nrequest\n)\nexcept\nException\n:\nif\nattempt\n==\n2\n:\nraise\nModify request\n@wrap_tool_call\ndef\nmodify_args\n(\nrequest\n,\nhandler\n):\nmodified_call\n=\n{\n**\nrequest\n.\ntool_call\n,\n\"args\"\n:\n{\n**\nrequest\n.\ntool_call\n[\n\"args\"\n],\n\"value\"\n:\nrequest\n.\ntool_call\n[\n\"args\"\n][\n\"value\"\n]\n*\n2\n,\n},\n}\nrequest\n=\nrequest\n.\noverride\n(\ntool_call\n=\nmodified_call\n)\nreturn\nhandler\n(\nrequest\n)\nShort-circuit with cached result\n@wrap_tool_call\ndef\nwith_cache\n(\nrequest\n,\nhandler\n):\nif\ncached\n:=\nget_cache\n(\nrequest\n):\nreturn\nToolMessage\n(\ncontent\n=\ncached\n,\ntool_call_id\n=\nrequest\n.\ntool_call\n[\n\"id\"\n])\nresult\n=\nhandler\n(\nrequest\n)\nsave_cache\n(\nrequest\n,\nresult\n)\nreturn\nresult\ndynamic_prompt\n\u00b6\ndynamic_prompt\n(\nfunc\n:\n_CallableReturningSystemMessage\n[\nStateT\n,\nContextT\n]\n|\nNone\n=\nNone\n,\n)\n->\n(\nCallable\n[\n[\n_CallableReturningSystemMessage\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n],\n]\n|\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\n)\nDecorator used to dynamically generate system prompts for the model.\nThis is a convenience decorator that creates middleware using\nwrap_model_call\nspecifically for dynamic prompt generation. The decorated function should return\na string that will be set as the system prompt for the model request.\nPARAMETER\nDESCRIPTION\nfunc\nThe function to be decorated.\nMust accept:\nrequest: ModelRequest\n- Model request (contains state and\nruntime)\nTYPE:\n_CallableReturningSystemMessage\n[\nStateT\n,\nContextT\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallable\n[[\n_CallableReturningSystemMessage\n[\nStateT\n,\nContextT\n]],\nAgentMiddleware\n[\nStateT\n,\nContextT\n]] |\nAgentMiddleware\n[\nStateT\n,\nContextT\n]\nEither an\nAgentMiddleware\ninstance (if func is provided) or a decorator\nfunction that can be applied to a function.\nThe decorated function should return\nstr\n\u2013 The system prompt string to use for the model request\nSystemMessage\n\u2013 A complete system message to use for the model request\nExamples:\nBasic usage with dynamic content:\n@dynamic_prompt\ndef\nmy_prompt\n(\nrequest\n:\nModelRequest\n)\n->\nstr\n:\nuser_name\n=\nrequest\n.\nruntime\n.\ncontext\n.\nget\n(\n\"user_name\"\n,\n\"User\"\n)\nreturn\nf\n\"You are a helpful assistant helping\n{\nuser_name\n}\n.\"\nUsing state to customize the prompt:\n@dynamic_prompt\ndef\ncontext_aware_prompt\n(\nrequest\n:\nModelRequest\n)\n->\nstr\n:\nmsg_count\n=\nlen\n(\nrequest\n.\nstate\n[\n\"messages\"\n])\nif\nmsg_count\n>\n10\n:\nreturn\n\"You are in a long conversation. Be concise.\"\nreturn\n\"You are a helpful assistant.\"\nUsing with agent:\nagent\n=\ncreate_agent\n(\nmodel\n,\nmiddleware\n=\n[\nmy_prompt\n])\nhook_config\n\u00b6\nhook_config\n(\n*\n,\ncan_jump_to\n:\nlist\n[\nJumpTo\n]\n|\nNone\n=\nNone\n)\n->\nCallable\n[[\nCallableT\n],\nCallableT\n]\nDecorator to configure hook behavior in middleware methods.\nUse this decorator on\nbefore_model\nor\nafter_model\nmethods in middleware classes\nto configure their behavior. Currently supports specifying which destinations they\ncan jump to, which establishes conditional edges in the agent graph.\nPARAMETER\nDESCRIPTION\ncan_jump_to\nOptional list of valid jump destinations.\nCan be:\n'tools'\n: Jump to the tools node\n'model'\n: Jump back to the model node\n'end'\n: Jump to the end of the graph\nTYPE:\nlist\n[\nJumpTo\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallable\n[[\nCallableT\n],\nCallableT\n]\nDecorator function that marks the method with configuration metadata.\nExamples:\nUsing decorator on a class method\nclass\nMyMiddleware\n(\nAgentMiddleware\n):\n@hook_config\n(\ncan_jump_to\n=\n[\n\"end\"\n,\n\"model\"\n])\ndef\nbefore_model\n(\nself\n,\nstate\n:\nAgentState\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\nif\nsome_condition\n(\nstate\n):\nreturn\n{\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\nAlternative: Use the\ncan_jump_to\nparameter in\nbefore_model\n/\nafter_model\ndecorators:\n@before_model\n(\ncan_jump_to\n=\n[\n\"end\"\n])\ndef\nconditional_middleware\n(\nstate\n:\nAgentState\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\nif\nshould_exit\n(\nstate\n):\nreturn\n{\n\"jump_to\"\n:\n\"end\"\n}\nreturn\nNone\nAgentState\n\u00b6\nBases:\nTypedDict\n,\nGeneric\n[\nResponseT\n]\nState schema for the agent.\nModelRequest\ndataclass\n\u00b6\nModelRequest\n(\n*\n,\nmodel\n:\nBaseChatModel\n,\nmessages\n:\nlist\n[\nAnyMessage\n],\nsystem_message\n:\nSystemMessage\n|\nNone\n=\nNone\n,\nsystem_prompt\n:\nstr\n|\nNone\n=\nNone\n,\ntool_choice\n:\nAny\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nBaseTool\n|\ndict\n]\n|\nNone\n=\nNone\n,\nresponse_format\n:\nResponseFormat\n|\nNone\n=\nNone\n,\nstate\n:\nAgentState\n|\nNone\n=\nNone\n,\nruntime\n:\nRuntime\n[\nContextT\n]\n|\nNone\n=\nNone\n,\nmodel_settings\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n)\nModel request information for the agent.\nInitialize ModelRequest with backward compatibility for system_prompt.\nPARAMETER\nDESCRIPTION\nmodel\nThe chat model to use.\nTYPE:\nBaseChatModel\nmessages\nList of messages (excluding system prompt).\nTYPE:\nlist\n[\nAnyMessage\n]\ntool_choice\nTool choice configuration.\nTYPE:\nAny\n| None\nDEFAULT:\nNone\ntools\nList of available tools.\nTYPE:\nlist\n[\nBaseTool\n|\ndict\n] | None\nDEFAULT:\nNone\nresponse_format\nResponse format specification.\nTYPE:\nResponseFormat\n| None\nDEFAULT:\nNone\nstate\nAgent state.\nTYPE:\nAgentState\n| None\nDEFAULT:\nNone\nruntime\nRuntime context.\nTYPE:\nRuntime\n[\nContextT\n] | None\nDEFAULT:\nNone\nmodel_settings\nAdditional model settings.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nsystem_message\nSystem message instance (preferred).\nTYPE:\nSystemMessage\n| None\nDEFAULT:\nNone\nsystem_prompt\nSystem prompt string (deprecated, converted to SystemMessage).\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nMETHOD\nDESCRIPTION\n__setattr__\nSet an attribute with a deprecation warning.\noverride\nReplace the request with a new request with the given overrides.\nsystem_prompt\nproperty\n\u00b6\nsystem_prompt\n:\nstr\n|\nNone\nGet system prompt text from system_message.\nRETURNS\nDESCRIPTION\nstr\n| None\nThe content of the system message if present, otherwise\nNone\n.\n__setattr__\n\u00b6\n__setattr__\n(\nname\n:\nstr\n,\nvalue\n:\nAny\n)\n->\nNone\nSet an attribute with a deprecation warning.\nDirect attribute assignment on\nModelRequest\nis deprecated. Use the\noverride()\nmethod instead to create a new request with modified attributes.\nPARAMETER\nDESCRIPTION\nname\nAttribute name.\nTYPE:\nstr\nvalue\nAttribute value.\nTYPE:\nAny\noverride\n\u00b6\noverride\n(\n**\noverrides\n:\nUnpack\n[\n_ModelRequestOverrides\n])\n->\nModelRequest\nReplace the request with a new request with the given overrides.\nReturns a new\nModelRequest\ninstance with the specified attributes replaced.\nThis follows an immutable pattern, leaving the original request unchanged.\nPARAMETER\nDESCRIPTION\n**overrides\nKeyword arguments for attributes to override.\nSupported keys:\nmodel\n:\nBaseChatModel\ninstance\nsystem_prompt\n: deprecated, use\nsystem_message\ninstead\nsystem_message\n:\nSystemMessage\ninstance\nmessages\n:\nlist\nof messages\ntool_choice\n: Tool choice configuration\ntools\n:\nlist\nof available tools\nresponse_format\n: Response format specification\nmodel_settings\n: Additional model settings\nTYPE:\nUnpack\n[\n_ModelRequestOverrides\n]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nModelRequest\nNew\nModelRequest\ninstance with specified overrides applied.\nExamples:\nCreate a new request with different model\nnew_request\n=\nrequest\n.\noverride\n(\nmodel\n=\ndifferent_model\n)\nOverride system message (preferred)\nfrom\nlangchain_core.messages\nimport\nSystemMessage\nnew_request\n=\nrequest\n.\noverride\n(\nsystem_message\n=\nSystemMessage\n(\ncontent\n=\n\"New instructions\"\n)\n)\nOverride multiple attributes\nnew_request\n=\nrequest\n.\noverride\n(\nmodel\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-5-mini\"\n),\nsystem_message\n=\nSystemMessage\n(\ncontent\n=\n\"New instructions\"\n),\n)\nModelResponse\ndataclass\n\u00b6\nModelResponse\n(\nresult\n:\nlist\n[\nBaseMessage\n],\nstructured_response\n:\nAny\n=\nNone\n)\nResponse from model execution including messages and optional structured output.\nThe result will usually contain a single\nAIMessage\n, but may include an additional\nToolMessage\nif the model used a tool for structured output.\nresult\ninstance-attribute\n\u00b6\nresult\n:\nlist\n[\nBaseMessage\n]\nList of messages from model execution.\nstructured_response\nclass-attribute\ninstance-attribute\n\u00b6\nstructured_response\n:\nAny\n=\nNone\nParsed structured output if\nresponse_format\nwas specified,\nNone\notherwise.\nClearToolUsesEdit\ndataclass\n\u00b6\nClearToolUsesEdit\n(\ntrigger\n:\nint\n=\n100000\n,\nclear_at_least\n:\nint\n=\n0\n,\nkeep\n:\nint\n=\n3\n,\nclear_tool_inputs\n:\nbool\n=\nFalse\n,\nexclude_tools\n:\nSequence\n[\nstr\n]\n=\n(),\nplaceholder\n:\nstr\n=\nDEFAULT_TOOL_PLACEHOLDER\n,\n)\nBases:\nContextEdit\nConfiguration for clearing tool outputs when token limits are exceeded.\nMETHOD\nDESCRIPTION\napply\nApply the clear-tool-uses strategy.\ntrigger\nclass-attribute\ninstance-attribute\n\u00b6\ntrigger\n:\nint\n=\n100000\nToken count that triggers the edit.\nclear_at_least\nclass-attribute\ninstance-attribute\n\u00b6\nclear_at_least\n:\nint\n=\n0\nMinimum number of tokens to reclaim when the edit runs.\nkeep\nclass-attribute\ninstance-attribute\n\u00b6\nkeep\n:\nint\n=\n3\nNumber of most recent tool results that must be preserved.\nclear_tool_inputs\nclass-attribute\ninstance-attribute\n\u00b6\nclear_tool_inputs\n:\nbool\n=\nFalse\nWhether to clear the originating tool call parameters on the AI message.\nexclude_tools\nclass-attribute\ninstance-attribute\n\u00b6\nexclude_tools\n:\nSequence\n[\nstr\n]\n=\n()\nList of tool names to exclude from clearing.\nplaceholder\nclass-attribute\ninstance-attribute\n\u00b6\nplaceholder\n:\nstr\n=\nDEFAULT_TOOL_PLACEHOLDER\nPlaceholder text inserted for cleared tool outputs.\napply\n\u00b6\napply\n(\nmessages\n:\nlist\n[\nAnyMessage\n],\n*\n,\ncount_tokens\n:\nTokenCounter\n)\n->\nNone\nApply the clear-tool-uses strategy.\nInterruptOnConfig\n\u00b6\nBases:\nTypedDict\nConfiguration for an action requiring human in the loop.\nThis is the configuration format used in the\nHumanInTheLoopMiddleware.__init__\nmethod.\nallowed_decisions\ninstance-attribute\n\u00b6\nallowed_decisions\n:\nlist\n[\nDecisionType\n]\nThe decisions that are allowed for this action.\ndescription\ninstance-attribute\n\u00b6\ndescription\n:\nNotRequired\n[\nstr\n|\n_DescriptionFactory\n]\nThe description attached to the request for human input.\nCan be either:\nA static string describing the approval request\nA callable that dynamically generates the description based on agent state,\nruntime, and tool call information\nExample\n# Static string description\nconfig\n=\nToolConfig\n(\nallowed_decisions\n=\n[\n\"approve\"\n,\n\"reject\"\n],\ndescription\n=\n\"Please review this tool execution\"\n)\n# Dynamic callable description\ndef\nformat_tool_description\n(\ntool_call\n:\nToolCall\n,\nstate\n:\nAgentState\n,\nruntime\n:\nRuntime\n[\nContextT\n]\n)\n->\nstr\n:\nimport\njson\nreturn\n(\nf\n\"Tool:\n{\ntool_call\n[\n'name'\n]\n}\n\\n\n\"\nf\n\"Arguments:\n\\n\n{\njson\n.\ndumps\n(\ntool_call\n[\n'args'\n],\nindent\n=\n2\n)\n}\n\"\n)\nconfig\n=\nInterruptOnConfig\n(\nallowed_decisions\n=\n[\n\"approve\"\n,\n\"edit\"\n,\n\"reject\"\n],\ndescription\n=\nformat_tool_description\n)\nargs_schema\ninstance-attribute\n\u00b6\nargs_schema\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nJSON schema for the args associated with the action, if edits are allowed.\nContextSize\nmodule-attribute\n\u00b6\nContextSize\n=\nContextFraction\n|\nContextTokens\n|\nContextMessages\nUnion type for context size specifications.\nCan be either:\nContextFraction\n: A\nfraction of the model's maximum input tokens.\nContextTokens\n: An absolute\nnumber of tokens.\nContextMessages\n: An\nabsolute number of messages.\nDepending on use with\ntrigger\nor\nkeep\nparameters, this type indicates either\nwhen to trigger summarization or how much context to retain.\nExample\n# ContextFraction\ncontext_size\n:\nContextSize\n=\n(\n\"fraction\"\n,\n0.5\n)\n# ContextTokens\ncontext_size\n:\nContextSize\n=\n(\n\"tokens\"\n,\n3000\n)\n# ContextMessages\ncontext_size\n:\nContextSize\n=\n(\n\"messages\"\n,\n50\n)\nContextFraction\nmodule-attribute\n\u00b6\nContextFraction\n=\ntuple\n[\nLiteral\n[\n'fraction'\n],\nfloat\n]\nFraction of model's maximum input tokens.\nExample\nTo specify 50% of the model's max input tokens:\n(\n\"fraction\"\n,\n0.5\n)\nContextTokens\nmodule-attribute\n\u00b6\nContextTokens\n=\ntuple\n[\nLiteral\n[\n'tokens'\n],\nint\n]\nAbsolute number of tokens.\nExample\nTo specify 3000 tokens:\n(\n\"tokens\"\n,\n3000\n)\nContextMessages\nmodule-attribute\n\u00b6\nContextMessages\n=\ntuple\n[\nLiteral\n[\n'messages'\n],\nint\n]\nAbsolute number of messages.\nExample\nTo specify 50 messages:\n(\n\"messages\"\n,\n50\n)\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain/middleware/",
      "title": "Middleware | LangChain Reference",
      "heading": "Middleware"
    }
  },
  {
    "page_content": "Messages | LangChain Reference\nSkip to content\nLangChain Reference\nMessages\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nMessages\nTable of contents\nmessages\nAIMessage\ntool_calls\ninvalid_tool_calls\nusage_metadata\ntype\nlc_attributes\ncontent_blocks\npretty_repr\nAIMessageChunk\ntype\ntool_call_chunks\nchunk_position\nlc_attributes\ncontent_blocks\ninit_tool_calls\ninit_server_tool_calls\n__add__\nHumanMessage\ntype\nSystemMessage\ntype\nAnyMessage\nMessageLikeRepresentation\nToolMessage\ntool_call_id\ntype\nartifact\nstatus\nadditional_kwargs\nresponse_metadata\ncoerce_args\nToolCall\nname\nargs\nid\nInvalidToolCall\ntype\nid\nname\nargs\nerror\nindex\nextras\nToolCallChunk\nname\nargs\nid\nindex\nServerToolCall\ntype\nid\nname\nargs\nindex\nextras\nServerToolCallChunk\ntype\nname\nargs\nid\nindex\nextras\nServerToolResult\ntype\nid\ntool_call_id\nstatus\noutput\nindex\nextras\nContentBlock\nTextContentBlock\ntype\nid\ntext\nannotations\nindex\nextras\nAnnotation\nCitation\ntype\nid\nurl\ntitle\nstart_index\nend_index\ncited_text\nextras\nNonStandardAnnotation\ntype\nid\nvalue\nReasoningContentBlock\ntype\nid\nreasoning\nindex\nextras\nDataContentBlock\nImageContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\nextras\nVideoContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\nextras\nAudioContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\nextras\nPlainTextContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\ntext\ntitle\ncontext\nextras\nFileContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\nextras\nNonStandardContentBlock\ntype\nid\nvalue\nindex\ntrim_messages\nUsageMetadata\ninput_tokens\noutput_tokens\ntotal_tokens\ninput_token_details\noutput_token_details\nInputTokenDetails\naudio\ncache_creation\ncache_read\nOutputTokenDetails\naudio\nreasoning\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nmessages\nAIMessage\ntool_calls\ninvalid_tool_calls\nusage_metadata\ntype\nlc_attributes\ncontent_blocks\npretty_repr\nAIMessageChunk\ntype\ntool_call_chunks\nchunk_position\nlc_attributes\ncontent_blocks\ninit_tool_calls\ninit_server_tool_calls\n__add__\nHumanMessage\ntype\nSystemMessage\ntype\nAnyMessage\nMessageLikeRepresentation\nToolMessage\ntool_call_id\ntype\nartifact\nstatus\nadditional_kwargs\nresponse_metadata\ncoerce_args\nToolCall\nname\nargs\nid\nInvalidToolCall\ntype\nid\nname\nargs\nerror\nindex\nextras\nToolCallChunk\nname\nargs\nid\nindex\nServerToolCall\ntype\nid\nname\nargs\nindex\nextras\nServerToolCallChunk\ntype\nname\nargs\nid\nindex\nextras\nServerToolResult\ntype\nid\ntool_call_id\nstatus\noutput\nindex\nextras\nContentBlock\nTextContentBlock\ntype\nid\ntext\nannotations\nindex\nextras\nAnnotation\nCitation\ntype\nid\nurl\ntitle\nstart_index\nend_index\ncited_text\nextras\nNonStandardAnnotation\ntype\nid\nvalue\nReasoningContentBlock\ntype\nid\nreasoning\nindex\nextras\nDataContentBlock\nImageContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\nextras\nVideoContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\nextras\nAudioContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\nextras\nPlainTextContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\ntext\ntitle\ncontext\nextras\nFileContentBlock\ntype\nid\nfile_id\nmime_type\nindex\nurl\nbase64\nextras\nNonStandardContentBlock\ntype\nid\nvalue\nindex\ntrim_messages\nUsageMetadata\ninput_tokens\noutput_tokens\ntotal_tokens\ninput_token_details\noutput_token_details\nInputTokenDetails\naudio\ncache_creation\ncache_read\nOutputTokenDetails\naudio\nreasoning\nMessages\nReference docs\nThis page contains\nreference documentation\nfor Messages. See\nthe docs\nfor conceptual guides, tutorials, and examples on using Messages.\nmessages\n\u00b6\nMessage and message content types.\nIncludes message types for different roles (e.g., human, AI, system), as well as types\nfor message content blocks (e.g., text, image, audio) and tool calls.\nCLASS\nDESCRIPTION\nAIMessage\nMessage from an AI.\nAIMessageChunk\nMessage chunk from an AI (yielded when streaming).\nHumanMessage\nMessage from the user.\nSystemMessage\nMessage for priming AI behavior.\nToolMessage\nMessage for passing the result of executing a tool back to a model.\nToolCall\nRepresents an AI's request to call a tool.\nInvalidToolCall\nAllowance for errors made by LLM.\nToolCallChunk\nA chunk of a tool call (yielded when streaming).\nServerToolCall\nTool call that is executed server-side.\nServerToolCallChunk\nA chunk of a server-side tool call (yielded when streaming).\nServerToolResult\nResult of a server-side tool call.\nTextContentBlock\nText output from a LLM.\nCitation\nAnnotation for citing data from a document.\nNonStandardAnnotation\nProvider-specific annotation format.\nReasoningContentBlock\nReasoning output from a LLM.\nImageContentBlock\nImage data.\nVideoContentBlock\nVideo data.\nAudioContentBlock\nAudio data.\nPlainTextContentBlock\nPlaintext data (e.g., from a\n.txt\nor\n.md\ndocument).\nFileContentBlock\nFile data that doesn't fit into other multimodal block types.\nNonStandardContentBlock\nProvider-specific content data.\nUsageMetadata\nUsage metadata for a message, such as token counts.\nInputTokenDetails\nBreakdown of input token counts.\nOutputTokenDetails\nBreakdown of output token counts.\nFUNCTION\nDESCRIPTION\ntrim_messages\nTrim messages to be below a token count.\nATTRIBUTE\nDESCRIPTION\nAnyMessage\nA type representing any defined\nMessage\nor\nMessageChunk\ntype.\nMessageLikeRepresentation\nA type representing the various ways a message can be represented.\nContentBlock\nA union of all defined\nContentBlock\ntypes and aliases.\nAnnotation\nA union of all defined\nAnnotation\ntypes.\nDataContentBlock\nA union of all defined multimodal data\nContentBlock\ntypes.\nAIMessage\n\u00b6\nAIMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n],\n**\nkwargs\n:\nAny\n)\nAIMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nAIMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nBases:\nBaseMessage\nMessage from an AI.\nAn\nAIMessage\nis returned from a chat model as a response to a prompt.\nThis message represents the output of the model and consists of both\nthe raw output as returned by the model and standardized fields\n(e.g., tool calls, usage metadata) added by the LangChain framework.\nInitialize an\nAIMessage\n.\nSpecify\ncontent\nas positional arg or\ncontent_blocks\nfor typing.\nPARAMETER\nDESCRIPTION\ncontent\nThe content of the message.\nTYPE:\nstr\n|\nlist\n[\nstr\n|\ndict\n] | None\nDEFAULT:\nNone\ncontent_blocks\nTyped standard content.\nTYPE:\nlist\n[\nContentBlock\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional arguments to pass to the parent class.\nTYPE:\nAny\nDEFAULT:\n{}\nMETHOD\nDESCRIPTION\npretty_repr\nReturn a pretty representation of the message for display.\nATTRIBUTE\nDESCRIPTION\ntool_calls\nIf present, tool calls associated with the message.\nTYPE:\nlist\n[\nToolCall\n]\ninvalid_tool_calls\nIf present, tool calls with parsing errors associated with the message.\nTYPE:\nlist\n[\nInvalidToolCall\n]\nusage_metadata\nIf present, usage metadata for a message, such as token counts.\nTYPE:\nUsageMetadata\n| None\ntype\nThe type of the message (used for deserialization).\nTYPE:\nLiteral\n['ai']\nlc_attributes\nAttributes to be serialized.\nTYPE:\ndict\ncontent_blocks\nReturn standard, typed\nContentBlock\ndicts from the message.\nTYPE:\nlist\n[\nContentBlock\n]\ntool_calls\nclass-attribute\ninstance-attribute\n\u00b6\ntool_calls\n:\nlist\n[\nToolCall\n]\n=\nField\n(\ndefault_factory\n=\nlist\n)\nIf present, tool calls associated with the message.\ninvalid_tool_calls\nclass-attribute\ninstance-attribute\n\u00b6\ninvalid_tool_calls\n:\nlist\n[\nInvalidToolCall\n]\n=\nField\n(\ndefault_factory\n=\nlist\n)\nIf present, tool calls with parsing errors associated with the message.\nusage_metadata\nclass-attribute\ninstance-attribute\n\u00b6\nusage_metadata\n:\nUsageMetadata\n|\nNone\n=\nNone\nIf present, usage metadata for a message, such as token counts.\nThis is a standard representation of token usage that is consistent across models.\ntype\nclass-attribute\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'ai'\n]\n=\n'ai'\nThe type of the message (used for deserialization).\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nAttributes to be serialized.\nIncludes all attributes, even if they are derived from other initialization\narguments.\ncontent_blocks\nproperty\n\u00b6\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\nReturn standard, typed\nContentBlock\ndicts from the message.\nIf the message has a known model provider, use the provider-specific translator\nfirst before falling back to best-effort parsing. For details, see the property\non\nBaseMessage\n.\npretty_repr\n\u00b6\npretty_repr\n(\nhtml\n:\nbool\n=\nFalse\n)\n->\nstr\nReturn a pretty representation of the message for display.\nPARAMETER\nDESCRIPTION\nhtml\nWhether to return an HTML-formatted string.\nTYPE:\nbool\nDEFAULT:\nFalse\nRETURNS\nDESCRIPTION\nstr\nA pretty representation of the message.\nAIMessageChunk\n\u00b6\nAIMessageChunk\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n],\n**\nkwargs\n:\nAny\n)\nAIMessageChunk\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nAIMessageChunk\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nBases:\nAIMessage\n,\nBaseMessageChunk\nMessage chunk from an AI (yielded when streaming).\nMETHOD\nDESCRIPTION\ninit_tool_calls\nInitialize tool calls from tool call chunks.\ninit_server_tool_calls\nParse\nserver_tool_call_chunks\nfrom\nServerToolCallChunk\nobjects.\n__add__\nMessage chunks support concatenation with other message chunks.\nATTRIBUTE\nDESCRIPTION\ntype\nThe type of the message (used for deserialization).\nTYPE:\nLiteral\n['AIMessageChunk']\ntool_call_chunks\nIf provided, tool call chunks associated with the message.\nTYPE:\nlist\n[\nToolCallChunk\n]\nchunk_position\nOptional span represented by an aggregated\nAIMessageChunk\n.\nTYPE:\nLiteral\n['last'] | None\nlc_attributes\nAttributes to be serialized, even if they are derived from other initialization args.\nTYPE:\ndict\ncontent_blocks\nReturn standard, typed\nContentBlock\ndicts from the message.\nTYPE:\nlist\n[\nContentBlock\n]\ntype\nclass-attribute\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'AIMessageChunk'\n]\n=\n'AIMessageChunk'\nThe type of the message (used for deserialization).\ntool_call_chunks\nclass-attribute\ninstance-attribute\n\u00b6\ntool_call_chunks\n:\nlist\n[\nToolCallChunk\n]\n=\nField\n(\ndefault_factory\n=\nlist\n)\nIf provided, tool call chunks associated with the message.\nchunk_position\nclass-attribute\ninstance-attribute\n\u00b6\nchunk_position\n:\nLiteral\n[\n'last'\n]\n|\nNone\n=\nNone\nOptional span represented by an aggregated\nAIMessageChunk\n.\nIf a chunk with\nchunk_position=\"last\"\nis aggregated into a stream,\ntool_call_chunks\nin message content will be parsed into\ntool_calls\n.\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nAttributes to be serialized, even if they are derived from other initialization args.\ncontent_blocks\nproperty\n\u00b6\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\nReturn standard, typed\nContentBlock\ndicts from the message.\ninit_tool_calls\n\u00b6\ninit_tool_calls\n()\n->\nSelf\nInitialize tool calls from tool call chunks.\nRETURNS\nDESCRIPTION\nSelf\nThe values with tool calls initialized.\nRAISES\nDESCRIPTION\nValueError\nIf the tool call chunks are malformed.\ninit_server_tool_calls\n\u00b6\ninit_server_tool_calls\n()\n->\nSelf\nParse\nserver_tool_call_chunks\nfrom\nServerToolCallChunk\nobjects.\n__add__\n\u00b6\n__add__\n(\nother\n:\nAny\n)\n->\nBaseMessageChunk\nMessage chunks support concatenation with other message chunks.\nThis functionality is useful to combine message chunks yielded from\na streaming model into a complete message.\nPARAMETER\nDESCRIPTION\nother\nAnother message chunk to concatenate with this one.\nTYPE:\nAny\nRETURNS\nDESCRIPTION\nBaseMessageChunk\nA new message chunk that is the concatenation of this message chunk\nBaseMessageChunk\nand the other message chunk.\nRAISES\nDESCRIPTION\nTypeError\nIf the other object is not a message chunk.\nExample\nAIMessageChunk(content=\"Hello\", ...)\n+ AIMessageChunk(content=\" World\", ...)\n= AIMessageChunk(content=\"Hello World\", ...)\nHumanMessage\n\u00b6\nHumanMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n],\n**\nkwargs\n:\nAny\n)\nHumanMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nHumanMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nBases:\nBaseMessage\nMessage from the user.\nA\nHumanMessage\nis a message that is passed in from a user to the model.\nExample\nfrom\nlangchain_core.messages\nimport\nHumanMessage\n,\nSystemMessage\nmessages\n=\n[\nSystemMessage\n(\ncontent\n=\n\"You are a helpful assistant! Your name is Bob.\"\n),\nHumanMessage\n(\ncontent\n=\n\"What is your name?\"\n),\n]\n# Instantiate a chat model and invoke it with the messages\nmodel\n=\n...\nprint\n(\nmodel\n.\ninvoke\n(\nmessages\n))\nSpecify\ncontent\nas positional arg or\ncontent_blocks\nfor typing.\nATTRIBUTE\nDESCRIPTION\ntype\nThe type of the message (used for serialization).\nTYPE:\nLiteral\n['human']\ntype\nclass-attribute\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'human'\n]\n=\n'human'\nThe type of the message (used for serialization).\nSystemMessage\n\u00b6\nSystemMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n],\n**\nkwargs\n:\nAny\n)\nSystemMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nSystemMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nBases:\nBaseMessage\nMessage for priming AI behavior.\nThe system message is usually passed in as the first of a sequence\nof input messages.\nExample\nfrom\nlangchain_core.messages\nimport\nHumanMessage\n,\nSystemMessage\nmessages\n=\n[\nSystemMessage\n(\ncontent\n=\n\"You are a helpful assistant! Your name is Bob.\"\n),\nHumanMessage\n(\ncontent\n=\n\"What is your name?\"\n),\n]\n# Define a chat model and invoke it with the messages\nprint\n(\nmodel\n.\ninvoke\n(\nmessages\n))\nSpecify\ncontent\nas positional arg or\ncontent_blocks\nfor typing.\nATTRIBUTE\nDESCRIPTION\ntype\nThe type of the message (used for serialization).\nTYPE:\nLiteral\n['system']\ntype\nclass-attribute\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'system'\n]\n=\n'system'\nThe type of the message (used for serialization).\nAnyMessage\nmodule-attribute\n\u00b6\nAnyMessage\n=\nAnnotated\n[\nAnnotated\n[\nAIMessage\n,\nTag\n(\ntag\n=\n\"ai\"\n)]\n|\nAnnotated\n[\nHumanMessage\n,\nTag\n(\ntag\n=\n\"human\"\n)]\n|\nAnnotated\n[\nChatMessage\n,\nTag\n(\ntag\n=\n\"chat\"\n)]\n|\nAnnotated\n[\nSystemMessage\n,\nTag\n(\ntag\n=\n\"system\"\n)]\n|\nAnnotated\n[\nFunctionMessage\n,\nTag\n(\ntag\n=\n\"function\"\n)]\n|\nAnnotated\n[\nToolMessage\n,\nTag\n(\ntag\n=\n\"tool\"\n)]\n|\nAnnotated\n[\nAIMessageChunk\n,\nTag\n(\ntag\n=\n\"AIMessageChunk\"\n)]\n|\nAnnotated\n[\nHumanMessageChunk\n,\nTag\n(\ntag\n=\n\"HumanMessageChunk\"\n)]\n|\nAnnotated\n[\nChatMessageChunk\n,\nTag\n(\ntag\n=\n\"ChatMessageChunk\"\n)]\n|\nAnnotated\n[\nSystemMessageChunk\n,\nTag\n(\ntag\n=\n\"SystemMessageChunk\"\n)]\n|\nAnnotated\n[\nFunctionMessageChunk\n,\nTag\n(\ntag\n=\n\"FunctionMessageChunk\"\n)]\n|\nAnnotated\n[\nToolMessageChunk\n,\nTag\n(\ntag\n=\n\"ToolMessageChunk\"\n)],\nField\n(\ndiscriminator\n=\nDiscriminator\n(\n_get_type\n)),\n]\nA type representing any defined\nMessage\nor\nMessageChunk\ntype.\nMessageLikeRepresentation\nmodule-attribute\n\u00b6\nMessageLikeRepresentation\n=\n(\nBaseMessage\n|\nlist\n[\nstr\n]\n|\ntuple\n[\nstr\n,\nstr\n]\n|\nstr\n|\ndict\n[\nstr\n,\nAny\n]\n)\nA type representing the various ways a message can be represented.\nToolMessage\n\u00b6\nToolMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n],\n**\nkwargs\n:\nAny\n)\nToolMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nToolMessage\n(\ncontent\n:\nstr\n|\nlist\n[\nstr\n|\ndict\n]\n|\nNone\n=\nNone\n,\ncontent_blocks\n:\nlist\n[\nContentBlock\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\nBases:\nBaseMessage\n,\nToolOutputMixin\nMessage for passing the result of executing a tool back to a model.\nToolMessage\nobjects contain the result of a tool invocation. Typically, the result\nis encoded inside the\ncontent\nfield.\ntool_call_id\nis used to associate the tool call request with the tool call\nresponse. Useful in situations where a chat model is able to request multiple tool\ncalls in parallel.\nExample\nA\nToolMessage\nrepresenting a result of\n42\nfrom a tool call with id\nfrom\nlangchain_core.messages\nimport\nToolMessage\nToolMessage\n(\ncontent\n=\n\"42\"\n,\ntool_call_id\n=\n\"call_Jja7J89XsjrOLA5r!MEOW!SL\"\n)\nExample\nA\nToolMessage\nwhere only part of the tool output is sent to the model\nand the full output is passed in to artifact.\nfrom\nlangchain_core.messages\nimport\nToolMessage\ntool_output\n=\n{\n\"stdout\"\n:\n\"From the graph we can see that the correlation between \"\n\"x and y is ...\"\n,\n\"stderr\"\n:\nNone\n,\n\"artifacts\"\n:\n{\n\"type\"\n:\n\"image\"\n,\n\"base64_data\"\n:\n\"/9j/4gIcSU...\"\n},\n}\nToolMessage\n(\ncontent\n=\ntool_output\n[\n\"stdout\"\n],\nartifact\n=\ntool_output\n,\ntool_call_id\n=\n\"call_Jja7J89XsjrOLA5r!MEOW!SL\"\n,\n)\nInitialize a\nToolMessage\n.\nSpecify\ncontent\nas positional arg or\ncontent_blocks\nfor typing.\nPARAMETER\nDESCRIPTION\ncontent\nThe contents of the message.\nTYPE:\nstr\n|\nlist\n[\nstr\n|\ndict\n] | None\nDEFAULT:\nNone\ncontent_blocks\nTyped standard content.\nTYPE:\nlist\n[\nContentBlock\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional fields.\nTYPE:\nAny\nDEFAULT:\n{}\nMETHOD\nDESCRIPTION\ncoerce_args\nCoerce the model arguments to the correct types.\nATTRIBUTE\nDESCRIPTION\ntool_call_id\nTool call that this message is responding to.\nTYPE:\nstr\ntype\nThe type of the message (used for serialization).\nTYPE:\nLiteral\n['tool']\nartifact\nArtifact of the Tool execution which is not meant to be sent to the model.\nTYPE:\nAny\nstatus\nStatus of the tool invocation.\nTYPE:\nLiteral\n['success', 'error']\nadditional_kwargs\nCurrently inherited from\nBaseMessage\n, but not used.\nTYPE:\ndict\nresponse_metadata\nCurrently inherited from\nBaseMessage\n, but not used.\nTYPE:\ndict\ntool_call_id\ninstance-attribute\n\u00b6\ntool_call_id\n:\nstr\nTool call that this message is responding to.\ntype\nclass-attribute\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'tool'\n]\n=\n'tool'\nThe type of the message (used for serialization).\nartifact\nclass-attribute\ninstance-attribute\n\u00b6\nartifact\n:\nAny\n=\nNone\nArtifact of the Tool execution which is not meant to be sent to the model.\nShould only be specified if it is different from the message content, e.g. if only\na subset of the full tool output is being passed as message content but the full\noutput is needed in other parts of the code.\nstatus\nclass-attribute\ninstance-attribute\n\u00b6\nstatus\n:\nLiteral\n[\n'success'\n,\n'error'\n]\n=\n'success'\nStatus of the tool invocation.\nadditional_kwargs\nclass-attribute\ninstance-attribute\n\u00b6\nadditional_kwargs\n:\ndict\n=\nField\n(\ndefault_factory\n=\ndict\n,\nrepr\n=\nFalse\n)\nCurrently inherited from\nBaseMessage\n, but not used.\nresponse_metadata\nclass-attribute\ninstance-attribute\n\u00b6\nresponse_metadata\n:\ndict\n=\nField\n(\ndefault_factory\n=\ndict\n,\nrepr\n=\nFalse\n)\nCurrently inherited from\nBaseMessage\n, but not used.\ncoerce_args\nclassmethod\n\u00b6\ncoerce_args\n(\nvalues\n:\ndict\n)\n->\ndict\nCoerce the model arguments to the correct types.\nPARAMETER\nDESCRIPTION\nvalues\nThe model arguments.\nTYPE:\ndict\nToolCall\n\u00b6\nBases:\nTypedDict\nRepresents an AI's request to call a tool.\nExample\n{\n\"name\"\n:\n\"foo\"\n,\n\"args\"\n:\n{\n\"a\"\n:\n1\n},\n\"id\"\n:\n\"123\"\n}\nThis represents a request to call the tool named\n'foo'\nwith arguments\n{\"a\": 1}\nand an identifier of\n'123'\n.\nATTRIBUTE\nDESCRIPTION\nname\nThe name of the tool to be called.\nTYPE:\nstr\nargs\nThe arguments to the tool call.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nid\nAn identifier associated with the tool call.\nTYPE:\nstr\n| None\nname\ninstance-attribute\n\u00b6\nname\n:\nstr\nThe name of the tool to be called.\nargs\ninstance-attribute\n\u00b6\nargs\n:\ndict\n[\nstr\n,\nAny\n]\nThe arguments to the tool call.\nid\ninstance-attribute\n\u00b6\nid\n:\nstr\n|\nNone\nAn identifier associated with the tool call.\nAn identifier is needed to associate a tool call request with a tool\ncall result in events when multiple concurrent tool calls are made.\nInvalidToolCall\n\u00b6\nBases:\nTypedDict\nAllowance for errors made by LLM.\nHere we add an\nerror\nkey to surface errors made during generation\n(e.g., invalid JSON arguments.)\nATTRIBUTE\nDESCRIPTION\ntype\nUsed for discrimination.\nTYPE:\nLiteral\n['invalid_tool_call']\nid\nAn identifier associated with the tool call.\nTYPE:\nstr\n| None\nname\nThe name of the tool to be called.\nTYPE:\nstr\n| None\nargs\nThe arguments to the tool call.\nTYPE:\nstr\n| None\nerror\nAn error message associated with the tool call.\nTYPE:\nstr\n| None\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nextras\nProvider-specific metadata.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'invalid_tool_call'\n]\nUsed for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nstr\n|\nNone\nAn identifier associated with the tool call.\nAn identifier is needed to associate a tool call request with a tool\ncall result in events when multiple concurrent tool calls are made.\nname\ninstance-attribute\n\u00b6\nname\n:\nstr\n|\nNone\nThe name of the tool to be called.\nargs\ninstance-attribute\n\u00b6\nargs\n:\nstr\n|\nNone\nThe arguments to the tool call.\nerror\ninstance-attribute\n\u00b6\nerror\n:\nstr\n|\nNone\nAn error message associated with the tool call.\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata.\nToolCallChunk\n\u00b6\nBases:\nTypedDict\nA chunk of a tool call (yielded when streaming).\nWhen merging\nToolCallChunk\ns (e.g., via\nAIMessageChunk.__add__\n),\nall string attributes are concatenated. Chunks are only merged if their\nvalues of\nindex\nare equal and not None.\nExample:\nleft_chunks\n=\n[\nToolCallChunk\n(\nname\n=\n\"foo\"\n,\nargs\n=\n'{\"a\":'\n,\nindex\n=\n0\n)]\nright_chunks\n=\n[\nToolCallChunk\n(\nname\n=\nNone\n,\nargs\n=\n\"1}\"\n,\nindex\n=\n0\n)]\n(\nAIMessageChunk\n(\ncontent\n=\n\"\"\n,\ntool_call_chunks\n=\nleft_chunks\n)\n+\nAIMessageChunk\n(\ncontent\n=\n\"\"\n,\ntool_call_chunks\n=\nright_chunks\n)\n)\n.\ntool_call_chunks\n==\n[\nToolCallChunk\n(\nname\n=\n\"foo\"\n,\nargs\n=\n'{\"a\":1}'\n,\nindex\n=\n0\n)]\nATTRIBUTE\nDESCRIPTION\nname\nThe name of the tool to be called.\nTYPE:\nstr\n| None\nargs\nThe arguments to the tool call.\nTYPE:\nstr\n| None\nid\nAn identifier associated with the tool call.\nTYPE:\nstr\n| None\nindex\nThe index of the tool call in a sequence.\nTYPE:\nint\n| None\nname\ninstance-attribute\n\u00b6\nname\n:\nstr\n|\nNone\nThe name of the tool to be called.\nargs\ninstance-attribute\n\u00b6\nargs\n:\nstr\n|\nNone\nThe arguments to the tool call.\nid\ninstance-attribute\n\u00b6\nid\n:\nstr\n|\nNone\nAn identifier associated with the tool call.\nindex\ninstance-attribute\n\u00b6\nindex\n:\nint\n|\nNone\nThe index of the tool call in a sequence.\nServerToolCall\n\u00b6\nBases:\nTypedDict\nTool call that is executed server-side.\nFor example: code execution, web search, etc.\nATTRIBUTE\nDESCRIPTION\ntype\nUsed for discrimination.\nTYPE:\nLiteral\n['server_tool_call']\nid\nAn identifier associated with the tool call.\nTYPE:\nstr\nname\nThe name of the tool to be called.\nTYPE:\nstr\nargs\nThe arguments to the tool call.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nextras\nProvider-specific metadata.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'server_tool_call'\n]\nUsed for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nstr\nAn identifier associated with the tool call.\nname\ninstance-attribute\n\u00b6\nname\n:\nstr\nThe name of the tool to be called.\nargs\ninstance-attribute\n\u00b6\nargs\n:\ndict\n[\nstr\n,\nAny\n]\nThe arguments to the tool call.\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata.\nServerToolCallChunk\n\u00b6\nBases:\nTypedDict\nA chunk of a server-side tool call (yielded when streaming).\nATTRIBUTE\nDESCRIPTION\ntype\nUsed for discrimination.\nTYPE:\nLiteral\n['server_tool_call_chunk']\nname\nThe name of the tool to be called.\nTYPE:\nNotRequired\n[\nstr\n]\nargs\nJSON substring of the arguments to the tool call.\nTYPE:\nNotRequired\n[\nstr\n]\nid\nAn identifier associated with the tool call.\nTYPE:\nNotRequired\n[\nstr\n]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nextras\nProvider-specific metadata.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'server_tool_call_chunk'\n]\nUsed for discrimination.\nname\ninstance-attribute\n\u00b6\nname\n:\nNotRequired\n[\nstr\n]\nThe name of the tool to be called.\nargs\ninstance-attribute\n\u00b6\nargs\n:\nNotRequired\n[\nstr\n]\nJSON substring of the arguments to the tool call.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nAn identifier associated with the tool call.\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata.\nServerToolResult\n\u00b6\nBases:\nTypedDict\nResult of a server-side tool call.\nATTRIBUTE\nDESCRIPTION\ntype\nUsed for discrimination.\nTYPE:\nLiteral\n['server_tool_result']\nid\nAn identifier associated with the server tool result.\nTYPE:\nNotRequired\n[\nstr\n]\ntool_call_id\nID of the corresponding server tool call.\nTYPE:\nstr\nstatus\nExecution status of the server-side tool.\nTYPE:\nLiteral\n['success', 'error']\noutput\nOutput of the executed tool.\nTYPE:\nNotRequired\n[\nAny\n]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nextras\nProvider-specific metadata.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'server_tool_result'\n]\nUsed for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nAn identifier associated with the server tool result.\ntool_call_id\ninstance-attribute\n\u00b6\ntool_call_id\n:\nstr\nID of the corresponding server tool call.\nstatus\ninstance-attribute\n\u00b6\nstatus\n:\nLiteral\n[\n'success'\n,\n'error'\n]\nExecution status of the server-side tool.\noutput\ninstance-attribute\n\u00b6\noutput\n:\nNotRequired\n[\nAny\n]\nOutput of the executed tool.\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata.\nContentBlock\nmodule-attribute\n\u00b6\nContentBlock\n=\n(\nTextContentBlock\n|\nInvalidToolCall\n|\nReasoningContentBlock\n|\nNonStandardContentBlock\n|\nDataContentBlock\n|\nToolContentBlock\n)\nA union of all defined\nContentBlock\ntypes and aliases.\nTextContentBlock\n\u00b6\nBases:\nTypedDict\nText output from a LLM.\nThis typically represents the main text content of a message, such as the response\nfrom a language model or the text of a user message.\nFactory function\ncreate_text_block\nmay also be used as a factory to create a\nTextContentBlock\n. Benefits include:\nAutomatic ID generation (when not provided)\nRequired arguments strictly validated at creation time\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['text']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\ntext\nBlock text.\nTYPE:\nstr\nannotations\nCitation\ns and other annotations.\nTYPE:\nNotRequired\n[\nlist\n[\nAnnotation\n]]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nextras\nProvider-specific metadata.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'text'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\ntext\ninstance-attribute\n\u00b6\ntext\n:\nstr\nBlock text.\nannotations\ninstance-attribute\n\u00b6\nannotations\n:\nNotRequired\n[\nlist\n[\nAnnotation\n]]\nCitation\ns and other annotations.\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata.\nAnnotation\nmodule-attribute\n\u00b6\nAnnotation\n=\nCitation\n|\nNonStandardAnnotation\nA union of all defined\nAnnotation\ntypes.\nCitation\n\u00b6\nBases:\nTypedDict\nAnnotation for citing data from a document.\nNote\nstart\n/\nend\nindices refer to the\nresponse text\n,\nnot the source text. This means that the indices are relative to the model's\nresponse, not the original document (as specified in the\nurl\n).\nFactory function\ncreate_citation\nmay also be used as a factory to create a\nCitation\n.\nBenefits include:\nAutomatic ID generation (when not provided)\nRequired arguments strictly validated at creation time\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['citation']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\nurl\nURL of the document source.\nTYPE:\nNotRequired\n[\nstr\n]\ntitle\nSource document title.\nTYPE:\nNotRequired\n[\nstr\n]\nstart_index\nStart index of the\nresponse text\n(\nTextContentBlock.text\n).\nTYPE:\nNotRequired\n[\nint\n]\nend_index\nEnd index of the\nresponse text\n(\nTextContentBlock.text\n)\nTYPE:\nNotRequired\n[\nint\n]\ncited_text\nExcerpt of source text being cited.\nTYPE:\nNotRequired\n[\nstr\n]\nextras\nProvider-specific metadata.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'citation'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\nurl\ninstance-attribute\n\u00b6\nurl\n:\nNotRequired\n[\nstr\n]\nURL of the document source.\ntitle\ninstance-attribute\n\u00b6\ntitle\n:\nNotRequired\n[\nstr\n]\nSource document title.\nFor example, the page title for a web page or the title of a paper.\nstart_index\ninstance-attribute\n\u00b6\nstart_index\n:\nNotRequired\n[\nint\n]\nStart index of the\nresponse text\n(\nTextContentBlock.text\n).\nend_index\ninstance-attribute\n\u00b6\nend_index\n:\nNotRequired\n[\nint\n]\nEnd index of the\nresponse text\n(\nTextContentBlock.text\n)\ncited_text\ninstance-attribute\n\u00b6\ncited_text\n:\nNotRequired\n[\nstr\n]\nExcerpt of source text being cited.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata.\nNonStandardAnnotation\n\u00b6\nBases:\nTypedDict\nProvider-specific annotation format.\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['non_standard_annotation']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\nvalue\nProvider-specific annotation data.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'non_standard_annotation'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\nvalue\ninstance-attribute\n\u00b6\nvalue\n:\ndict\n[\nstr\n,\nAny\n]\nProvider-specific annotation data.\nReasoningContentBlock\n\u00b6\nBases:\nTypedDict\nReasoning output from a LLM.\nFactory function\ncreate_reasoning_block\nmay also be used as a factory to create a\nReasoningContentBlock\n. Benefits include:\nAutomatic ID generation (when not provided)\nRequired arguments strictly validated at creation time\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['reasoning']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\nreasoning\nReasoning text.\nTYPE:\nNotRequired\n[\nstr\n]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nextras\nProvider-specific metadata.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'reasoning'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\nreasoning\ninstance-attribute\n\u00b6\nreasoning\n:\nNotRequired\n[\nstr\n]\nReasoning text.\nEither the thought summary or the raw reasoning text itself. This is often parsed\nfrom\n<think>\ntags in the model's response.\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata.\nDataContentBlock\nmodule-attribute\n\u00b6\nDataContentBlock\n=\n(\nImageContentBlock\n|\nVideoContentBlock\n|\nAudioContentBlock\n|\nPlainTextContentBlock\n|\nFileContentBlock\n)\nA union of all defined multimodal data\nContentBlock\ntypes.\nImageContentBlock\n\u00b6\nBases:\nTypedDict\nImage data.\nFactory function\ncreate_image_block\nmay also be used as a factory to create an\nImageContentBlock\n. Benefits include:\nAutomatic ID generation (when not provided)\nRequired arguments strictly validated at creation time\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['image']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\nfile_id\nID of the image file, e.g., from a file storage system.\nTYPE:\nNotRequired\n[\nstr\n]\nmime_type\nMIME type of the image. Required for base64.\nTYPE:\nNotRequired\n[\nstr\n]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nurl\nURL of the image.\nTYPE:\nNotRequired\n[\nstr\n]\nbase64\nData as a base64 string.\nTYPE:\nNotRequired\n[\nstr\n]\nextras\nProvider-specific metadata. This shouldn't be used for the image data itself.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'image'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\nfile_id\ninstance-attribute\n\u00b6\nfile_id\n:\nNotRequired\n[\nstr\n]\nID of the image file, e.g., from a file storage system.\nmime_type\ninstance-attribute\n\u00b6\nmime_type\n:\nNotRequired\n[\nstr\n]\nMIME type of the image. Required for base64.\nExamples from IANA\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nurl\ninstance-attribute\n\u00b6\nurl\n:\nNotRequired\n[\nstr\n]\nURL of the image.\nbase64\ninstance-attribute\n\u00b6\nbase64\n:\nNotRequired\n[\nstr\n]\nData as a base64 string.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata. This shouldn't be used for the image data itself.\nVideoContentBlock\n\u00b6\nBases:\nTypedDict\nVideo data.\nFactory function\ncreate_video_block\nmay also be used as a factory to create a\nVideoContentBlock\n. Benefits include:\nAutomatic ID generation (when not provided)\nRequired arguments strictly validated at creation time\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['video']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\nfile_id\nID of the video file, e.g., from a file storage system.\nTYPE:\nNotRequired\n[\nstr\n]\nmime_type\nMIME type of the video. Required for base64.\nTYPE:\nNotRequired\n[\nstr\n]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nurl\nURL of the video.\nTYPE:\nNotRequired\n[\nstr\n]\nbase64\nData as a base64 string.\nTYPE:\nNotRequired\n[\nstr\n]\nextras\nProvider-specific metadata. This shouldn't be used for the video data itself.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'video'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\nfile_id\ninstance-attribute\n\u00b6\nfile_id\n:\nNotRequired\n[\nstr\n]\nID of the video file, e.g., from a file storage system.\nmime_type\ninstance-attribute\n\u00b6\nmime_type\n:\nNotRequired\n[\nstr\n]\nMIME type of the video. Required for base64.\nExamples from IANA\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nurl\ninstance-attribute\n\u00b6\nurl\n:\nNotRequired\n[\nstr\n]\nURL of the video.\nbase64\ninstance-attribute\n\u00b6\nbase64\n:\nNotRequired\n[\nstr\n]\nData as a base64 string.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata. This shouldn't be used for the video data itself.\nAudioContentBlock\n\u00b6\nBases:\nTypedDict\nAudio data.\nFactory function\ncreate_audio_block\nmay also be used as a factory to create an\nAudioContentBlock\n. Benefits include:\n* Automatic ID generation (when not provided)\n* Required arguments strictly validated at creation time\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['audio']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\nfile_id\nID of the audio file, e.g., from a file storage system.\nTYPE:\nNotRequired\n[\nstr\n]\nmime_type\nMIME type of the audio. Required for base64.\nTYPE:\nNotRequired\n[\nstr\n]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nurl\nURL of the audio.\nTYPE:\nNotRequired\n[\nstr\n]\nbase64\nData as a base64 string.\nTYPE:\nNotRequired\n[\nstr\n]\nextras\nProvider-specific metadata. This shouldn't be used for the audio data itself.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'audio'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\nfile_id\ninstance-attribute\n\u00b6\nfile_id\n:\nNotRequired\n[\nstr\n]\nID of the audio file, e.g., from a file storage system.\nmime_type\ninstance-attribute\n\u00b6\nmime_type\n:\nNotRequired\n[\nstr\n]\nMIME type of the audio. Required for base64.\nExamples from IANA\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nurl\ninstance-attribute\n\u00b6\nurl\n:\nNotRequired\n[\nstr\n]\nURL of the audio.\nbase64\ninstance-attribute\n\u00b6\nbase64\n:\nNotRequired\n[\nstr\n]\nData as a base64 string.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata. This shouldn't be used for the audio data itself.\nPlainTextContentBlock\n\u00b6\nBases:\nTypedDict\nPlaintext data (e.g., from a\n.txt\nor\n.md\ndocument).\nNote\nA\nPlainTextContentBlock\nexisted in\nlangchain-core<1.0.0\n. Although the\nname has carried over, the structure has changed significantly. The only shared\nkeys between the old and new versions are\ntype\nand\ntext\n, though the\ntype\nvalue has changed from\n'text'\nto\n'text-plain'\n.\nNote\nTitle and context are optional fields that may be passed to the model. See\nAnthropic\nexample\n.\nFactory function\ncreate_plaintext_block\nmay also be used as a factory to create a\nPlainTextContentBlock\n. Benefits include:\nAutomatic ID generation (when not provided)\nRequired arguments strictly validated at creation time\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['text-plain']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\nfile_id\nID of the plaintext file, e.g., from a file storage system.\nTYPE:\nNotRequired\n[\nstr\n]\nmime_type\nMIME type of the file. Required for base64.\nTYPE:\nLiteral\n['text/plain']\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nurl\nURL of the plaintext.\nTYPE:\nNotRequired\n[\nstr\n]\nbase64\nData as a base64 string.\nTYPE:\nNotRequired\n[\nstr\n]\ntext\nPlaintext content. This is optional if the data is provided as base64.\nTYPE:\nNotRequired\n[\nstr\n]\ntitle\nTitle of the text data, e.g., the title of a document.\nTYPE:\nNotRequired\n[\nstr\n]\ncontext\nContext for the text, e.g., a description or summary of the text's content.\nTYPE:\nNotRequired\n[\nstr\n]\nextras\nProvider-specific metadata. This shouldn't be used for the data itself.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'text-plain'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\nfile_id\ninstance-attribute\n\u00b6\nfile_id\n:\nNotRequired\n[\nstr\n]\nID of the plaintext file, e.g., from a file storage system.\nmime_type\ninstance-attribute\n\u00b6\nmime_type\n:\nLiteral\n[\n'text/plain'\n]\nMIME type of the file. Required for base64.\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nurl\ninstance-attribute\n\u00b6\nurl\n:\nNotRequired\n[\nstr\n]\nURL of the plaintext.\nbase64\ninstance-attribute\n\u00b6\nbase64\n:\nNotRequired\n[\nstr\n]\nData as a base64 string.\ntext\ninstance-attribute\n\u00b6\ntext\n:\nNotRequired\n[\nstr\n]\nPlaintext content. This is optional if the data is provided as base64.\ntitle\ninstance-attribute\n\u00b6\ntitle\n:\nNotRequired\n[\nstr\n]\nTitle of the text data, e.g., the title of a document.\ncontext\ninstance-attribute\n\u00b6\ncontext\n:\nNotRequired\n[\nstr\n]\nContext for the text, e.g., a description or summary of the text's content.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata. This shouldn't be used for the data itself.\nFileContentBlock\n\u00b6\nBases:\nTypedDict\nFile data that doesn't fit into other multimodal block types.\nThis block is intended for files that are not images, audio, or plaintext. For\nexample, it can be used for PDFs, Word documents, etc.\nIf the file is an image, audio, or plaintext, you should use the corresponding\ncontent block type (e.g.,\nImageContentBlock\n,\nAudioContentBlock\n,\nPlainTextContentBlock\n).\nFactory function\ncreate_file_block\nmay also be used as a factory to create a\nFileContentBlock\n. Benefits include:\nAutomatic ID generation (when not provided)\nRequired arguments strictly validated at creation time\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['file']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\nfile_id\nID of the file, e.g., from a file storage system.\nTYPE:\nNotRequired\n[\nstr\n]\nmime_type\nMIME type of the file. Required for base64.\nTYPE:\nNotRequired\n[\nstr\n]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\nurl\nURL of the file.\nTYPE:\nNotRequired\n[\nstr\n]\nbase64\nData as a base64 string.\nTYPE:\nNotRequired\n[\nstr\n]\nextras\nProvider-specific metadata. This shouldn't be used for the file data itself.\nTYPE:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'file'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\nfile_id\ninstance-attribute\n\u00b6\nfile_id\n:\nNotRequired\n[\nstr\n]\nID of the file, e.g., from a file storage system.\nmime_type\ninstance-attribute\n\u00b6\nmime_type\n:\nNotRequired\n[\nstr\n]\nMIME type of the file. Required for base64.\nExamples from IANA\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\nurl\ninstance-attribute\n\u00b6\nurl\n:\nNotRequired\n[\nstr\n]\nURL of the file.\nbase64\ninstance-attribute\n\u00b6\nbase64\n:\nNotRequired\n[\nstr\n]\nData as a base64 string.\nextras\ninstance-attribute\n\u00b6\nextras\n:\nNotRequired\n[\ndict\n[\nstr\n,\nAny\n]]\nProvider-specific metadata. This shouldn't be used for the file data itself.\nNonStandardContentBlock\n\u00b6\nBases:\nTypedDict\nProvider-specific content data.\nThis block contains data for which there is not yet a standard type.\nThe purpose of this block should be to simply hold a provider-specific payload.\nIf a provider's non-standard output includes reasoning and tool calls, it should be\nthe adapter's job to parse that payload and emit the corresponding standard\nReasoningContentBlock\nand\nToolCalls\n.\nHas no\nextras\nfield, as provider-specific data should be included in the\nvalue\nfield.\nFactory function\ncreate_non_standard_block\nmay also be used as a factory to create a\nNonStandardContentBlock\n. Benefits include:\nAutomatic ID generation (when not provided)\nRequired arguments strictly validated at creation time\nATTRIBUTE\nDESCRIPTION\ntype\nType of the content block. Used for discrimination.\nTYPE:\nLiteral\n['non_standard']\nid\nContent block identifier.\nTYPE:\nNotRequired\n[\nstr\n]\nvalue\nProvider-specific content data.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nindex\nIndex of block in aggregate response. Used during streaming.\nTYPE:\nNotRequired\n[\nint\n|\nstr\n]\ntype\ninstance-attribute\n\u00b6\ntype\n:\nLiteral\n[\n'non_standard'\n]\nType of the content block. Used for discrimination.\nid\ninstance-attribute\n\u00b6\nid\n:\nNotRequired\n[\nstr\n]\nContent block identifier.\nEither:\nGenerated by the provider (e.g., OpenAI's file ID)\nGenerated by LangChain upon creation (\nUUID4\nprefixed with\n'lc_'\n))\nvalue\ninstance-attribute\n\u00b6\nvalue\n:\ndict\n[\nstr\n,\nAny\n]\nProvider-specific content data.\nindex\ninstance-attribute\n\u00b6\nindex\n:\nNotRequired\n[\nint\n|\nstr\n]\nIndex of block in aggregate response. Used during streaming.\ntrim_messages\n\u00b6\ntrim_messages\n(\nmessages\n:\nIterable\n[\nMessageLikeRepresentation\n]\n|\nPromptValue\n,\n*\n,\nmax_tokens\n:\nint\n,\ntoken_counter\n:\nCallable\n[[\nlist\n[\nBaseMessage\n]],\nint\n]\n|\nCallable\n[[\nBaseMessage\n],\nint\n]\n|\nBaseLanguageModel\n|\nLiteral\n[\n\"approximate\"\n],\nstrategy\n:\nLiteral\n[\n\"first\"\n,\n\"last\"\n]\n=\n\"last\"\n,\nallow_partial\n:\nbool\n=\nFalse\n,\nend_on\n:\nstr\n|\ntype\n[\nBaseMessage\n]\n|\nSequence\n[\nstr\n|\ntype\n[\nBaseMessage\n]]\n|\nNone\n=\nNone\n,\nstart_on\n:\nstr\n|\ntype\n[\nBaseMessage\n]\n|\nSequence\n[\nstr\n|\ntype\n[\nBaseMessage\n]]\n|\nNone\n=\nNone\n,\ninclude_system\n:\nbool\n=\nFalse\n,\ntext_splitter\n:\nCallable\n[[\nstr\n],\nlist\n[\nstr\n]]\n|\nTextSplitter\n|\nNone\n=\nNone\n,\n)\n->\nlist\n[\nBaseMessage\n]\nTrim messages to be below a token count.\ntrim_messages\ncan be used to reduce the size of a chat history to a specified\ntoken or message count.\nIn either case, if passing the trimmed chat history back into a chat model\ndirectly, the resulting chat history should usually satisfy the following\nproperties:\nThe resulting chat history should be valid. Most chat models expect that chat\nhistory starts with either (1) a\nHumanMessage\nor (2) a\nSystemMessage\nfollowed by a\nHumanMessage\n. To achieve this, set\nstart_on='human'\n.\nIn addition, generally a\nToolMessage\ncan only appear after an\nAIMessage\nthat involved a tool call.\nIt includes recent messages and drops old messages in the chat history.\nTo achieve this set the\nstrategy='last'\n.\nUsually, the new chat history should include the\nSystemMessage\nif it\nwas present in the original chat history since the\nSystemMessage\nincludes\nspecial instructions to the chat model. The\nSystemMessage\nis almost always\nthe first message in the history if present. To achieve this set the\ninclude_system=True\n.\nNote\nThe examples below show how to configure\ntrim_messages\nto achieve a behavior\nconsistent with the above properties.\nPARAMETER\nDESCRIPTION\nmessages\nSequence of Message-like objects to trim.\nTYPE:\nIterable\n[\nMessageLikeRepresentation\n] |\nPromptValue\nmax_tokens\nMax token count of trimmed messages.\nTYPE:\nint\ntoken_counter\nFunction or llm for counting tokens in a\nBaseMessage\nor a\nlist of\nBaseMessage\n.\nIf a\nBaseLanguageModel\nis passed in then\nBaseLanguageModel.get_num_tokens_from_messages()\nwill be used. Set to\nlen\nto count the number of\nmessages\nin the chat history.\nYou can also use string shortcuts for convenience:\n'approximate'\n: Uses\ncount_tokens_approximately\nfor fast, approximate\ntoken counts.\nNote\ncount_tokens_approximately\n(or the shortcut\n'approximate'\n) is\nrecommended for using\ntrim_messages\non the hot path, where exact token\ncounting is not necessary.\nTYPE:\nCallable\n[[\nlist\n[\nBaseMessage\n]],\nint\n] |\nCallable\n[[\nBaseMessage\n],\nint\n] |\nBaseLanguageModel\n|\nLiteral\n['approximate']\nstrategy\nStrategy for trimming.\n'first'\n: Keep the first\n<= n_count\ntokens of the messages.\n'last'\n: Keep the last\n<= n_count\ntokens of the messages.\nTYPE:\nLiteral\n['first', 'last']\nDEFAULT:\n'last'\nallow_partial\nWhether to split a message if only part of the message can be\nincluded.\nIf\nstrategy='last'\nthen the last partial contents of a message are\nincluded. If\nstrategy='first'\nthen the first partial contents of a\nmessage are included.\nTYPE:\nbool\nDEFAULT:\nFalse\nend_on\nThe message type to end on.\nIf specified then every message after the last occurrence of this type is\nignored. If\nstrategy='last'\nthen this is done before we attempt to get the\nlast\nmax_tokens\n. If\nstrategy='first'\nthen this is done after we get the\nfirst\nmax_tokens\n. Can be specified as string names (e.g.\n'system'\n,\n'human'\n,\n'ai'\n, ...) or as\nBaseMessage\nclasses (e.g.\nSystemMessage\n,\nHumanMessage\n,\nAIMessage\n, ...). Can be a single type or a list of types.\nTYPE:\nstr\n|\ntype\n[\nBaseMessage\n] |\nSequence\n[\nstr\n|\ntype\n[\nBaseMessage\n]] | None\nDEFAULT:\nNone\nstart_on\nThe message type to start on.\nShould only be specified if\nstrategy='last'\n. If specified then every\nmessage before the first occurrence of this type is ignored. This is done\nafter we trim the initial messages to the last\nmax_tokens\n. Does not apply\nto a\nSystemMessage\nat index 0 if\ninclude_system=True\n. Can be specified\nas string names (e.g.\n'system'\n,\n'human'\n,\n'ai'\n, ...) or as\nBaseMessage\nclasses (e.g.\nSystemMessage\n,\nHumanMessage\n,\nAIMessage\n,\n...). Can be a single type or a list of types.\nTYPE:\nstr\n|\ntype\n[\nBaseMessage\n] |\nSequence\n[\nstr\n|\ntype\n[\nBaseMessage\n]] | None\nDEFAULT:\nNone\ninclude_system\nWhether to keep the\nSystemMessage\nif there is one at index\n0\n.\nShould only be specified if\nstrategy=\"last\"\n.\nTYPE:\nbool\nDEFAULT:\nFalse\ntext_splitter\nFunction or\nlangchain_text_splitters.TextSplitter\nfor\nsplitting the string contents of a message.\nOnly used if\nallow_partial=True\n. If\nstrategy='last'\nthen the last split\ntokens from a partial message will be included. if\nstrategy='first'\nthen\nthe first split tokens from a partial message will be included. Token\nsplitter assumes that separators are kept, so that split contents can be\ndirectly concatenated to recreate the original text. Defaults to splitting\non newlines.\nTYPE:\nCallable\n[[\nstr\n],\nlist\n[\nstr\n]] |\nTextSplitter\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nlist\n[\nBaseMessage\n]\nList of trimmed\nBaseMessage\n.\nRAISES\nDESCRIPTION\nValueError\nif two incompatible arguments are specified or an unrecognized\nstrategy\nis specified.\nExample\nTrim chat history based on token count, keeping the\nSystemMessage\nif\npresent, and ensuring that the chat history starts with a\nHumanMessage\n(or a\nSystemMessage\nfollowed by a\nHumanMessage\n).\nfrom\nlangchain_core.messages\nimport\n(\nAIMessage\n,\nHumanMessage\n,\nBaseMessage\n,\nSystemMessage\n,\ntrim_messages\n,\n)\nmessages\n=\n[\nSystemMessage\n(\n\"you're a good assistant, you always respond with a joke.\"\n),\nHumanMessage\n(\n\"i wonder why it's called langchain\"\n),\nAIMessage\n(\n'Well, I guess they thought \"WordRope\" and \"SentenceString\" just '\n\"didn't have the same ring to it!\"\n),\nHumanMessage\n(\n\"and who is harrison chasing anyways\"\n),\nAIMessage\n(\n\"Hmmm let me think.\n\\n\\n\nWhy, he's probably chasing after the last \"\n\"cup of coffee in the office!\"\n),\nHumanMessage\n(\n\"what do you call a speechless parrot\"\n),\n]\ntrim_messages\n(\nmessages\n,\nmax_tokens\n=\n45\n,\nstrategy\n=\n\"last\"\n,\ntoken_counter\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-5-mini\"\n),\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\nstart_on\n=\n\"human\"\n,\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system\n=\nTrue\n,\nallow_partial\n=\nFalse\n,\n)\n[\nSystemMessage\n(\ncontent\n=\n\"you're a good assistant, you always respond with a joke.\"\n),\nHumanMessage\n(\ncontent\n=\n\"what do you call a speechless parrot\"\n),\n]\nTrim chat history using approximate token counting with\n'approximate'\n:\ntrim_messages\n(\nmessages\n,\nmax_tokens\n=\n45\n,\nstrategy\n=\n\"last\"\n,\n# Using the \"approximate\" shortcut for fast token counting\ntoken_counter\n=\n\"approximate\"\n,\nstart_on\n=\n\"human\"\n,\ninclude_system\n=\nTrue\n,\n)\n# This is equivalent to using `count_tokens_approximately` directly\nfrom\nlangchain_core.messages.utils\nimport\ncount_tokens_approximately\ntrim_messages\n(\nmessages\n,\nmax_tokens\n=\n45\n,\nstrategy\n=\n\"last\"\n,\ntoken_counter\n=\ncount_tokens_approximately\n,\nstart_on\n=\n\"human\"\n,\ninclude_system\n=\nTrue\n,\n)\nTrim chat history based on the message count, keeping the\nSystemMessage\nif\npresent, and ensuring that the chat history starts with a HumanMessage (\nor a\nSystemMessage\nfollowed by a\nHumanMessage\n).\ntrim_messages(\nmessages,\n# When `len` is passed in as the token counter function,\n# max_tokens will count the number of messages in the chat history.\nmax_tokens=4,\nstrategy=\"last\",\n# Passing in `len` as a token counter function will\n# count the number of messages in the chat history.\ntoken_counter=len,\n# Most chat models expect that chat history starts with either:\n# (1) a HumanMessage or\n# (2) a SystemMessage followed by a HumanMessage\nstart_on=\"human\",\n# Usually, we want to keep the SystemMessage\n# if it's present in the original history.\n# The SystemMessage has special instructions for the model.\ninclude_system=True,\nallow_partial=False,\n)\n[\nSystemMessage\n(\ncontent\n=\n\"you're a good assistant, you always respond with a joke.\"\n),\nHumanMessage\n(\ncontent\n=\n\"and who is harrison chasing anyways\"\n),\nAIMessage\n(\ncontent\n=\n\"Hmmm let me think.\n\\n\\n\nWhy, he's probably chasing after \"\n\"the last cup of coffee in the office!\"\n),\nHumanMessage\n(\ncontent\n=\n\"what do you call a speechless parrot\"\n),\n]\nTrim chat history using a custom token counter function that counts the\nnumber of tokens in each message.\nmessages\n=\n[\nSystemMessage\n(\n\"This is a 4 token text. The full message is 10 tokens.\"\n),\nHumanMessage\n(\n\"This is a 4 token text. The full message is 10 tokens.\"\n,\nid\n=\n\"first\"\n),\nAIMessage\n(\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"This is the FIRST 4 token block.\"\n},\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"This is the SECOND 4 token block.\"\n},\n],\nid\n=\n\"second\"\n,\n),\nHumanMessage\n(\n\"This is a 4 token text. The full message is 10 tokens.\"\n,\nid\n=\n\"third\"\n),\nAIMessage\n(\n\"This is a 4 token text. The full message is 10 tokens.\"\n,\nid\n=\n\"fourth\"\n,\n),\n]\ndef\ndummy_token_counter\n(\nmessages\n:\nlist\n[\nBaseMessage\n])\n->\nint\n:\n# treat each message like it adds 3 default tokens at the beginning\n# of the message and at the end of the message. 3 + 4 + 3 = 10 tokens\n# per message.\ndefault_content_len\n=\n4\ndefault_msg_prefix_len\n=\n3\ndefault_msg_suffix_len\n=\n3\ncount\n=\n0\nfor\nmsg\nin\nmessages\n:\nif\nisinstance\n(\nmsg\n.\ncontent\n,\nstr\n):\ncount\n+=\n(\ndefault_msg_prefix_len\n+\ndefault_content_len\n+\ndefault_msg_suffix_len\n)\nif\nisinstance\n(\nmsg\n.\ncontent\n,\nlist\n):\ncount\n+=\n(\ndefault_msg_prefix_len\n+\nlen\n(\nmsg\n.\ncontent\n)\n*\ndefault_content_len\n+\ndefault_msg_suffix_len\n)\nreturn\ncount\nFirst 30 tokens, allowing partial messages:\ntrim_messages\n(\nmessages\n,\nmax_tokens\n=\n30\n,\ntoken_counter\n=\ndummy_token_counter\n,\nstrategy\n=\n\"first\"\n,\nallow_partial\n=\nTrue\n,\n)\n[\nSystemMessage\n(\n\"This is a 4 token text. The full message is 10 tokens.\"\n),\nHumanMessage\n(\n\"This is a 4 token text. The full message is 10 tokens.\"\n,\nid\n=\n\"first\"\n,\n),\nAIMessage\n(\n[{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"This is the FIRST 4 token block.\"\n}],\nid\n=\n\"second\"\n,\n),\n]\nUsageMetadata\n\u00b6\nBases:\nTypedDict\nUsage metadata for a message, such as token counts.\nThis is a standard representation of token usage that is consistent across models.\nExample\n{\n\"input_tokens\"\n:\n350\n,\n\"output_tokens\"\n:\n240\n,\n\"total_tokens\"\n:\n590\n,\n\"input_token_details\"\n:\n{\n\"audio\"\n:\n10\n,\n\"cache_creation\"\n:\n200\n,\n\"cache_read\"\n:\n100\n,\n},\n\"output_token_details\"\n:\n{\n\"audio\"\n:\n10\n,\n\"reasoning\"\n:\n200\n,\n},\n}\nBehavior changed in\nlangchain-core\n0.3.9\nAdded\ninput_token_details\nand\noutput_token_details\n.\nLangSmith SDK\nThe LangSmith SDK also has a\nUsageMetadata\nclass. While the two share fields,\nLangSmith's\nUsageMetadata\nhas additional fields to capture cost information\nused by the LangSmith platform.\nATTRIBUTE\nDESCRIPTION\ninput_tokens\nCount of input (or prompt) tokens. Sum of all input token types.\nTYPE:\nint\noutput_tokens\nCount of output (or completion) tokens. Sum of all output token types.\nTYPE:\nint\ntotal_tokens\nTotal token count. Sum of\ninput_tokens\n+\noutput_tokens\n.\nTYPE:\nint\ninput_token_details\nBreakdown of input token counts.\nTYPE:\nNotRequired\n[\nInputTokenDetails\n]\noutput_token_details\nBreakdown of output token counts.\nTYPE:\nNotRequired\n[\nOutputTokenDetails\n]\ninput_tokens\ninstance-attribute\n\u00b6\ninput_tokens\n:\nint\nCount of input (or prompt) tokens. Sum of all input token types.\noutput_tokens\ninstance-attribute\n\u00b6\noutput_tokens\n:\nint\nCount of output (or completion) tokens. Sum of all output token types.\ntotal_tokens\ninstance-attribute\n\u00b6\ntotal_tokens\n:\nint\nTotal token count. Sum of\ninput_tokens\n+\noutput_tokens\n.\ninput_token_details\ninstance-attribute\n\u00b6\ninput_token_details\n:\nNotRequired\n[\nInputTokenDetails\n]\nBreakdown of input token counts.\nDoes\nnot\nneed to sum to full input token count. Does\nnot\nneed to have all keys.\noutput_token_details\ninstance-attribute\n\u00b6\noutput_token_details\n:\nNotRequired\n[\nOutputTokenDetails\n]\nBreakdown of output token counts.\nDoes\nnot\nneed to sum to full output token count. Does\nnot\nneed to have all keys.\nInputTokenDetails\n\u00b6\nBases:\nTypedDict\nBreakdown of input token counts.\nDoes\nnot\nneed to sum to full input token count. Does\nnot\nneed to have all keys.\nExample\n{\n\"audio\"\n:\n10\n,\n\"cache_creation\"\n:\n200\n,\n\"cache_read\"\n:\n100\n,\n}\nMay also hold extra provider-specific keys.\nAdded in\nlangchain-core\n0.3.9\nATTRIBUTE\nDESCRIPTION\naudio\nAudio input tokens.\nTYPE:\nint\ncache_creation\nInput tokens that were cached and there was a cache miss.\nTYPE:\nint\ncache_read\nInput tokens that were cached and there was a cache hit.\nTYPE:\nint\naudio\ninstance-attribute\n\u00b6\naudio\n:\nint\nAudio input tokens.\ncache_creation\ninstance-attribute\n\u00b6\ncache_creation\n:\nint\nInput tokens that were cached and there was a cache miss.\nSince there was a cache miss, the cache was created from these tokens.\ncache_read\ninstance-attribute\n\u00b6\ncache_read\n:\nint\nInput tokens that were cached and there was a cache hit.\nSince there was a cache hit, the tokens were read from the cache. More precisely,\nthe model state given these tokens was read from the cache.\nOutputTokenDetails\n\u00b6\nBases:\nTypedDict\nBreakdown of output token counts.\nDoes\nnot\nneed to sum to full output token count. Does\nnot\nneed to have all keys.\nExample\n{\n\"audio\"\n:\n10\n,\n\"reasoning\"\n:\n200\n,\n}\nMay also hold extra provider-specific keys.\nAdded in\nlangchain-core\n0.3.9\nATTRIBUTE\nDESCRIPTION\naudio\nAudio output tokens.\nTYPE:\nint\nreasoning\nReasoning output tokens.\nTYPE:\nint\naudio\ninstance-attribute\n\u00b6\naudio\n:\nint\nAudio output tokens.\nreasoning\ninstance-attribute\n\u00b6\nreasoning\n:\nint\nReasoning output tokens.\nTokens generated by the model in a chain of thought process (i.e. by OpenAI's o1\nmodels) that are not returned as part of model output.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain/messages/",
      "title": "Messages | LangChain Reference",
      "heading": "Messages"
    }
  },
  {
    "page_content": "Tools | LangChain Reference\nSkip to content\nLangChain Reference\nTools\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nTools\nTools\nTable of contents\ntool\nBaseTool\nname\ndescription\nresponse_format\nargs_schema\nreturn_direct\nextras\ninvoke\nainvoke\nget_input_schema\nget_output_schema\nInjectedState\n__init__\nInjectedStore\nInjectedToolArg\nInjectedToolCallId\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\ntool\nBaseTool\nname\ndescription\nresponse_format\nargs_schema\nreturn_direct\nextras\ninvoke\nainvoke\nget_input_schema\nget_output_schema\nInjectedState\n__init__\nInjectedStore\nInjectedToolArg\nInjectedToolCallId\nTools\nReference docs\nThis page contains\nreference documentation\nfor Tools. See\nthe docs\nfor conceptual guides, tutorials, and examples on using Tools.\ntool\n\u00b6\ntool\n(\nname_or_callable\n:\nstr\n|\nCallable\n|\nNone\n=\nNone\n,\nrunnable\n:\nRunnable\n|\nNone\n=\nNone\n,\n*\nargs\n:\nAny\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\nreturn_direct\n:\nbool\n=\nFalse\n,\nargs_schema\n:\nArgsSchema\n|\nNone\n=\nNone\n,\ninfer_schema\n:\nbool\n=\nTrue\n,\nresponse_format\n:\nLiteral\n[\n\"content\"\n,\n\"content_and_artifact\"\n]\n=\n\"content\"\n,\nparse_docstring\n:\nbool\n=\nFalse\n,\nerror_on_invalid_docstring\n:\nbool\n=\nTrue\n,\nextras\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\n|\nCallable\n[[\nCallable\n|\nRunnable\n],\nBaseTool\n]\nConvert Python functions and\nRunnables\nto LangChain tools.\nCan be used as a decorator with or without arguments to create tools from functions.\nFunctions can have any signature - the tool will automatically infer input schemas\nunless disabled.\nRequirements\nFunctions must have type hints for proper schema inference\nWhen\ninfer_schema=False\n, functions must be\n(str) -> str\nand have\ndocstrings\nWhen using with\nRunnable\n, a string name must be provided\nPARAMETER\nDESCRIPTION\nname_or_callable\nOptional name of the tool or the\nCallable\nto be\nconverted to a tool. Overrides the function's name.\nMust be provided as a positional argument.\nTYPE:\nstr\n|\nCallable\n| None\nDEFAULT:\nNone\nrunnable\nOptional\nRunnable\nto convert to a tool.\nMust be provided as a positional argument.\nTYPE:\nRunnable\n| None\nDEFAULT:\nNone\ndescription\nOptional description for the tool.\nPrecedence for the tool description value is as follows:\nThis\ndescription\nargument\n(used even if docstring and/or\nargs_schema\nare provided)\nTool function docstring\n(used even if\nargs_schema\nis provided)\nargs_schema\ndescription\n(used only if\ndescription\nand docstring are not provided)\nTYPE:\nstr\n| None\nDEFAULT:\nNone\n*args\nExtra positional arguments. Must be empty.\nTYPE:\nAny\nDEFAULT:\n()\nreturn_direct\nWhether to return directly from the tool rather than continuing\nthe agent loop.\nTYPE:\nbool\nDEFAULT:\nFalse\nargs_schema\nOptional argument schema for user to specify.\nTYPE:\nArgsSchema\n| None\nDEFAULT:\nNone\ninfer_schema\nWhether to infer the schema of the arguments from the function's\nsignature. This also makes the resultant tool accept a dictionary input to\nits\nrun()\nfunction.\nTYPE:\nbool\nDEFAULT:\nTrue\nresponse_format\nThe tool response format.\nIf\n'content'\n, then the output of the tool is interpreted as the contents\nof a\nToolMessage\n.\nIf\n'content_and_artifact'\n, then the output is expected to be a two-tuple\ncorresponding to the\n(content, artifact)\nof a\nToolMessage\n.\nTYPE:\nLiteral\n['content', 'content_and_artifact']\nDEFAULT:\n'content'\nparse_docstring\nIf\ninfer_schema\nand\nparse_docstring\n, will attempt to\nparse parameter descriptions from Google Style function docstrings.\nTYPE:\nbool\nDEFAULT:\nFalse\nerror_on_invalid_docstring\nIf\nparse_docstring\nis provided, configure\nwhether to raise\nValueError\non invalid Google Style docstrings.\nTYPE:\nbool\nDEFAULT:\nTrue\nextras\nOptional provider-specific extra fields for the tool.\nUsed to pass configuration that doesn't fit into standard tool fields.\nChat models should process known extras when constructing model payloads.\nExample\nFor example, Anthropic-specific fields like\ncache_control\n,\ndefer_loading\n, or\ninput_examples\n.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nRAISES\nDESCRIPTION\nValueError\nIf too many positional arguments are provided (e.g. violating the\n*args\nconstraint).\nValueError\nIf a\nRunnable\nis provided without a string name. When using\ntool\nwith a\nRunnable\n, a\nstr\nname must be provided as the\nname_or_callable\n.\nValueError\nIf the first argument is not a string or callable with\na\n__name__\nattribute.\nValueError\nIf the function does not have a docstring and description\nis not provided and\ninfer_schema\nis\nFalse\n.\nValueError\nIf\nparse_docstring\nis\nTrue\nand the function has an invalid\nGoogle-style docstring and\nerror_on_invalid_docstring\nis True.\nValueError\nIf a\nRunnable\nis provided that does not have an object schema.\nRETURNS\nDESCRIPTION\nBaseTool\n|\nCallable\n[[\nCallable\n|\nRunnable\n],\nBaseTool\n]\nThe tool.\nExamples:\n@tool\ndef\nsearch_api\n(\nquery\n:\nstr\n)\n->\nstr\n:\n# Searches the API for the query.\nreturn\n@tool\n(\n\"search\"\n,\nreturn_direct\n=\nTrue\n)\ndef\nsearch_api\n(\nquery\n:\nstr\n)\n->\nstr\n:\n# Searches the API for the query.\nreturn\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nsearch_api\n(\nquery\n:\nstr\n)\n->\ntuple\n[\nstr\n,\ndict\n]:\nreturn\n\"partial json of results\"\n,\n{\n\"full\"\n:\n\"object of results\"\n}\nParse Google-style docstrings:\n@tool\n(\nparse_docstring\n=\nTrue\n)\ndef\nfoo\n(\nbar\n:\nstr\n,\nbaz\n:\nint\n)\n->\nstr\n:\n\"\"\"The foo.\nArgs:\nbar: The bar.\nbaz: The baz.\n\"\"\"\nreturn\nbar\nfoo\n.\nargs_schema\n.\nmodel_json_schema\n()\n{\n\"title\"\n:\n\"foo\"\n,\n\"description\"\n:\n\"The foo.\"\n,\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"bar\"\n:\n{\n\"title\"\n:\n\"Bar\"\n,\n\"description\"\n:\n\"The bar.\"\n,\n\"type\"\n:\n\"string\"\n,\n},\n\"baz\"\n:\n{\n\"title\"\n:\n\"Baz\"\n,\n\"description\"\n:\n\"The baz.\"\n,\n\"type\"\n:\n\"integer\"\n,\n},\n},\n\"required\"\n:\n[\n\"bar\"\n,\n\"baz\"\n],\n}\nNote that parsing by default will raise\nValueError\nif the docstring\nis considered invalid. A docstring is considered invalid if it contains\narguments not in the function signature, or is unable to be parsed into\na summary and\n\"Args:\"\nblocks. Examples below:\n# No args section\ndef\ninvalid_docstring_1\n(\nbar\n:\nstr\n,\nbaz\n:\nint\n)\n->\nstr\n:\n\"\"\"The foo.\"\"\"\nreturn\nbar\n# Improper whitespace between summary and args section\ndef\ninvalid_docstring_2\n(\nbar\n:\nstr\n,\nbaz\n:\nint\n)\n->\nstr\n:\n\"\"\"The foo.\nArgs:\nbar: The bar.\nbaz: The baz.\n\"\"\"\nreturn\nbar\n# Documented args absent from function signature\ndef\ninvalid_docstring_3\n(\nbar\n:\nstr\n,\nbaz\n:\nint\n)\n->\nstr\n:\n\"\"\"The foo.\nArgs:\nbanana: The bar.\nmonkey: The baz.\n\"\"\"\nreturn\nbar\nBaseTool\n\u00b6\nBases:\nRunnableSerializable\n[\nstr\n|\ndict\n|\nToolCall\n,\nAny\n]\nBase class for all LangChain tools.\nThis abstract class defines the interface that all LangChain tools must implement.\nTools are components that can be called by agents to perform specific actions.\nMETHOD\nDESCRIPTION\ninvoke\nTransform a single input into an output.\nainvoke\nTransform a single input into an output.\nget_input_schema\nThe tool's input schema.\nget_output_schema\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nname\ninstance-attribute\n\u00b6\nname\n:\nstr\nThe unique name of the tool that clearly communicates its purpose.\ndescription\ninstance-attribute\n\u00b6\ndescription\n:\nstr\nUsed to tell the model how/when/why to use the tool.\nYou can provide few-shot examples as a part of the description.\nresponse_format\nclass-attribute\ninstance-attribute\n\u00b6\nresponse_format\n:\nLiteral\n[\n'content'\n,\n'content_and_artifact'\n]\n=\n'content'\nThe tool response format.\nIf\n'content'\nthen the output of the tool is interpreted as the contents of a\nToolMessage\n. If\n'content_and_artifact'\nthen the output is expected to be a\ntwo-tuple corresponding to the\n(content, artifact)\nof a\nToolMessage\n.\nargs_schema\nclass-attribute\ninstance-attribute\n\u00b6\nargs_schema\n:\nAnnotated\n[\nArgsSchema\n|\nNone\n,\nSkipValidation\n()]\n=\nField\n(\ndefault\n=\nNone\n,\ndescription\n=\n\"The tool schema.\"\n)\nPydantic model class to validate and parse the tool's input arguments.\nArgs schema should be either:\nA subclass of\npydantic.BaseModel\n.\nA subclass of\npydantic.v1.BaseModel\nif accessing v1 namespace in pydantic 2\nA JSON schema dict\nreturn_direct\nclass-attribute\ninstance-attribute\n\u00b6\nreturn_direct\n:\nbool\n=\nFalse\nWhether to return the tool's output directly.\nSetting this to\nTrue\nmeans that after the tool is called, the\nAgentExecutor\nwill\nstop looping.\nextras\nclass-attribute\ninstance-attribute\n\u00b6\nextras\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\nOptional provider-specific extra fields for the tool.\nThis is used to pass provider-specific configuration that doesn't fit into\nstandard tool fields.\nExample\nAnthropic-specific fields like\ncache_control\n,\ndefer_loading\n,\nor\ninput_examples\n.\n@tool\n(\nextras\n=\n{\n\"defer_loading\"\n:\nTrue\n,\n\"cache_control\"\n:\n{\n\"type\"\n:\n\"ephemeral\"\n}})\ndef\nmy_tool\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\nstr\n|\ndict\n|\nToolCall\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAny\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nstr\n|\ndict\n|\nToolCall\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAny\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe tool's input schema.\nPARAMETER\nDESCRIPTION\nconfig\nThe configuration for the tool.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nThe input schema for the tool.\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic output schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an output schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate output.\nInjectedState\n\u00b6\nBases:\nInjectedToolArg\nAnnotation for injecting graph state into tool arguments.\nThis annotation enables tools to access graph state without exposing state\nmanagement details to the language model. Tools annotated with\nInjectedState\nreceive state data automatically during execution while remaining invisible\nto the model's tool-calling interface.\nPARAMETER\nDESCRIPTION\nfield\nOptional key to extract from the state dictionary. If\nNone\n, the entire\nstate is injected. If specified, only that field's value is injected.\nThis allows tools to request specific state components rather than\nprocessing the full state structure.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nExample\nfrom\ntyping\nimport\nList\nfrom\ntyping_extensions\nimport\nAnnotated\n,\nTypedDict\nfrom\nlangchain_core.messages\nimport\nBaseMessage\n,\nAIMessage\nfrom\nlangchain.tools\nimport\nInjectedState\n,\nToolNode\n,\ntool\nclass\nAgentState\n(\nTypedDict\n):\nmessages\n:\nList\n[\nBaseMessage\n]\nfoo\n:\nstr\n@tool\ndef\nstate_tool\n(\nx\n:\nint\n,\nstate\n:\nAnnotated\n[\ndict\n,\nInjectedState\n])\n->\nstr\n:\n'''Do something with state.'''\nif\nlen\n(\nstate\n[\n\"messages\"\n])\n>\n2\n:\nreturn\nstate\n[\n\"foo\"\n]\n+\nstr\n(\nx\n)\nelse\n:\nreturn\n\"not enough messages\"\n@tool\ndef\nfoo_tool\n(\nx\n:\nint\n,\nfoo\n:\nAnnotated\n[\nstr\n,\nInjectedState\n(\n\"foo\"\n)])\n->\nstr\n:\n'''Do something else with state.'''\nreturn\nfoo\n+\nstr\n(\nx\n+\n1\n)\nnode\n=\nToolNode\n([\nstate_tool\n,\nfoo_tool\n])\ntool_call1\n=\n{\n\"name\"\n:\n\"state_tool\"\n,\n\"args\"\n:\n{\n\"x\"\n:\n1\n},\n\"id\"\n:\n\"1\"\n,\n\"type\"\n:\n\"tool_call\"\n}\ntool_call2\n=\n{\n\"name\"\n:\n\"foo_tool\"\n,\n\"args\"\n:\n{\n\"x\"\n:\n1\n},\n\"id\"\n:\n\"2\"\n,\n\"type\"\n:\n\"tool_call\"\n}\nstate\n=\n{\n\"messages\"\n:\n[\nAIMessage\n(\n\"\"\n,\ntool_calls\n=\n[\ntool_call1\n,\ntool_call2\n])],\n\"foo\"\n:\n\"bar\"\n,\n}\nnode\n.\ninvoke\n(\nstate\n)\n[\nToolMessage\n(\ncontent\n=\n\"not enough messages\"\n,\nname\n=\n\"state_tool\"\n,\ntool_call_id\n=\n\"1\"\n),\nToolMessage\n(\ncontent\n=\n\"bar2\"\n,\nname\n=\n\"foo_tool\"\n,\ntool_call_id\n=\n\"2\"\n),\n]\nNote\nInjectedState\narguments are automatically excluded from tool schemas\npresented to language models\nToolNode\nhandles the injection process during execution\nTools can mix regular arguments (controlled by the model) with injected\narguments (controlled by the system)\nState injection occurs after the model generates tool calls but before\ntool execution\nMETHOD\nDESCRIPTION\n__init__\nInitialize the\nInjectedState\nannotation.\n__init__\n\u00b6\n__init__\n(\nfield\n:\nstr\n|\nNone\n=\nNone\n)\n->\nNone\nInitialize the\nInjectedState\nannotation.\nInjectedStore\n\u00b6\nBases:\nInjectedToolArg\nAnnotation for injecting persistent store into tool arguments.\nThis annotation enables tools to access LangGraph's persistent storage system\nwithout exposing storage details to the language model. Tools annotated with\nInjectedStore\nreceive the store instance automatically during execution while\nremaining invisible to the model's tool-calling interface.\nThe store provides persistent, cross-session data storage that tools can use\nfor maintaining context, user preferences, or any other data that needs to\npersist beyond individual workflow executions.\nWarning\nInjectedStore\nannotation requires\nlangchain-core >= 0.3.8\nExample\nfrom\ntyping_extensions\nimport\nAnnotated\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nfrom\nlangchain.tools\nimport\nInjectedStore\n,\nToolNode\n,\ntool\n@tool\ndef\nsave_preference\n(\nkey\n:\nstr\n,\nvalue\n:\nstr\n,\nstore\n:\nAnnotated\n[\nAny\n,\nInjectedStore\n()]\n)\n->\nstr\n:\n\"\"\"Save user preference to persistent storage.\"\"\"\nstore\n.\nput\n((\n\"preferences\"\n,),\nkey\n,\nvalue\n)\nreturn\nf\n\"Saved\n{\nkey\n}\n=\n{\nvalue\n}\n\"\n@tool\ndef\nget_preference\n(\nkey\n:\nstr\n,\nstore\n:\nAnnotated\n[\nAny\n,\nInjectedStore\n()]\n)\n->\nstr\n:\n\"\"\"Retrieve user preference from persistent storage.\"\"\"\nresult\n=\nstore\n.\nget\n((\n\"preferences\"\n,),\nkey\n)\nreturn\nresult\n.\nvalue\nif\nresult\nelse\n\"Not found\"\nUsage with\nToolNode\nand graph compilation:\nfrom\nlanggraph.graph\nimport\nStateGraph\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nstore\n=\nInMemoryStore\n()\ntool_node\n=\nToolNode\n([\nsave_preference\n,\nget_preference\n])\ngraph\n=\nStateGraph\n(\nState\n)\ngraph\n.\nadd_node\n(\n\"tools\"\n,\ntool_node\n)\ncompiled_graph\n=\ngraph\n.\ncompile\n(\nstore\n=\nstore\n)\n# Store is injected automatically\nCross-session persistence:\n# First session\nresult1\n=\ngraph\n.\ninvoke\n({\n\"messages\"\n:\n[\nHumanMessage\n(\n\"Save my favorite color as blue\"\n)]})\n# Later session - data persists\nresult2\n=\ngraph\n.\ninvoke\n({\n\"messages\"\n:\n[\nHumanMessage\n(\n\"What's my favorite color?\"\n)]})\nNote\nInjectedStore\narguments are automatically excluded from tool schemas\npresented to language models\nThe store instance is automatically injected by\nToolNode\nduring execution\nTools can access namespaced storage using the store's get/put methods\nStore injection requires the graph to be compiled with a store instance\nMultiple tools can share the same store instance for data consistency\nInjectedToolArg\n\u00b6\nAnnotation for tool arguments that are injected at runtime.\nTool arguments annotated with this class are not included in the tool\nschema sent to language models and are instead injected during execution.\nInjectedToolCallId\n\u00b6\nBases:\nInjectedToolArg\nAnnotation for injecting the tool call ID.\nThis annotation is used to mark a tool parameter that should receive\nthe tool call ID at runtime.\nfrom\ntyping\nimport\nAnnotated\nfrom\nlangchain_core.messages\nimport\nToolMessage\nfrom\nlangchain_core.tools\nimport\ntool\n,\nInjectedToolCallId\n@tool\ndef\nfoo\n(\nx\n:\nint\n,\ntool_call_id\n:\nAnnotated\n[\nstr\n,\nInjectedToolCallId\n]\n)\n->\nToolMessage\n:\n\"\"\"Return x.\"\"\"\nreturn\nToolMessage\n(\nstr\n(\nx\n),\nartifact\n=\nx\n,\nname\n=\n\"foo\"\n,\ntool_call_id\n=\ntool_call_id\n)\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain/tools/",
      "title": "Tools | LangChain Reference",
      "heading": "Tools"
    }
  },
  {
    "page_content": "Embeddings | LangChain Reference\nSkip to content\nLangChain Reference\nEmbeddings\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nTools\nEmbeddings\nEmbeddings\nTable of contents\nembeddings\ninit_embeddings\nEmbeddings\nembed_documents\nembed_query\naembed_documents\naembed_query\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nembeddings\ninit_embeddings\nEmbeddings\nembed_documents\nembed_query\naembed_documents\naembed_query\nEmbeddings\nReference docs\nThis page contains\nreference documentation\nfor Embeddings. See\nthe docs\nfor conceptual guides, tutorials, and examples on using Embeddings.\nembeddings\n\u00b6\nEmbeddings models.\nModules moved\nWith the release of\nlangchain 1.0.0\n, several embeddings modules were moved to\nlangchain-classic\n, such as\nCacheBackedEmbeddings\nand all community\nembeddings. See\nlist\nof moved modules to inform your migration.\ninit_embeddings\n\u00b6\ninit_embeddings\n(\nmodel\n:\nstr\n,\n*\n,\nprovider\n:\nstr\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nEmbeddings\nInitialize an embedding model from a model name and optional provider.\nNote\nRequires the integration package for the chosen model provider to be installed.\nSee the\nmodel_provider\nparameter below for specific package names\n(e.g.,\npip install langchain-openai\n).\nRefer to the\nprovider integration's API reference\nfor supported model parameters to use as\n**kwargs\n.\nPARAMETER\nDESCRIPTION\nmodel\nThe name of the model, e.g.\n'openai:text-embedding-3-small'\n.\nYou can also specify model and model provider in a single argument using\n'{model_provider}:{model}'\nformat, e.g.\n'openai:text-embedding-3-small'\n.\nTYPE:\nstr\nprovider\nThe model provider if not specified as part of the model arg\n(see above).\nSupported\nprovider\nvalues and the corresponding integration package\nare:\nopenai\n->\nlangchain-openai\nazure_openai\n->\nlangchain-openai\nbedrock\n->\nlangchain-aws\ncohere\n->\nlangchain-cohere\ngoogle_vertexai\n->\nlangchain-google-vertexai\nhuggingface\n->\nlangchain-huggingface\nmistralai\n->\nlangchain-mistralai\nollama\n->\nlangchain-ollama\nTYPE:\nstr\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional model-specific parameters passed to the embedding model.\nThese vary by provider. Refer to the specific model provider's\nintegration reference\nfor all available parameters.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nEmbeddings\nAn\nEmbeddings\ninstance that can generate embeddings for text.\nRAISES\nDESCRIPTION\nValueError\nIf the model provider is not supported or cannot be determined\nImportError\nIf the required provider package is not installed\nExample\n# pip install langchain langchain-openai\n# Using a model string\nmodel\n=\ninit_embeddings\n(\n\"openai:text-embedding-3-small\"\n)\nmodel\n.\nembed_query\n(\n\"Hello, world!\"\n)\n# Using explicit provider\nmodel\n=\ninit_embeddings\n(\nmodel\n=\n\"text-embedding-3-small\"\n,\nprovider\n=\n\"openai\"\n)\nmodel\n.\nembed_documents\n([\n\"Hello, world!\"\n,\n\"Goodbye, world!\"\n])\n# With additional parameters\nmodel\n=\ninit_embeddings\n(\n\"openai:text-embedding-3-small\"\n,\napi_key\n=\n\"sk-...\"\n)\nAdded in\nlangchain\n0.3.9\nEmbeddings\n\u00b6\nBases:\nABC\nInterface for embedding models.\nThis is an interface meant for implementing text embedding models.\nText embedding models are used to map text to a vector (a point in n-dimensional\nspace).\nTexts that are similar will usually be mapped to points that are close to each\nother in this space. The exact details of what's considered \"similar\" and how\n\"distance\" is measured in this space are dependent on the specific embedding model.\nThis abstraction contains a method for embedding a list of documents and a method\nfor embedding a query text. The embedding of a query text is expected to be a single\nvector, while the embedding of a list of documents is expected to be a list of\nvectors.\nUsually the query embedding is identical to the document embedding, but the\nabstraction allows treating them independently.\nIn addition to the synchronous methods, this interface also provides asynchronous\nversions of the methods.\nBy default, the asynchronous methods are implemented using the synchronous methods;\nhowever, implementations may choose to override the asynchronous methods with\nan async native implementation for performance reasons.\nembed_documents\nabstractmethod\n\u00b6\nembed_documents\n(\ntexts\n:\nlist\n[\nstr\n])\n->\nlist\n[\nlist\n[\nfloat\n]]\nEmbed search docs.\nPARAMETER\nDESCRIPTION\ntexts\nList of text to embed.\nTYPE:\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nlist\n[\nlist\n[\nfloat\n]]\nList of embeddings.\nembed_query\nabstractmethod\n\u00b6\nembed_query\n(\ntext\n:\nstr\n)\n->\nlist\n[\nfloat\n]\nEmbed query text.\nPARAMETER\nDESCRIPTION\ntext\nText to embed.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nlist\n[\nfloat\n]\nEmbedding.\naembed_documents\nasync\n\u00b6\naembed_documents\n(\ntexts\n:\nlist\n[\nstr\n])\n->\nlist\n[\nlist\n[\nfloat\n]]\nAsynchronous Embed search docs.\nPARAMETER\nDESCRIPTION\ntexts\nList of text to embed.\nTYPE:\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nlist\n[\nlist\n[\nfloat\n]]\nList of embeddings.\naembed_query\nasync\n\u00b6\naembed_query\n(\ntext\n:\nstr\n)\n->\nlist\n[\nfloat\n]\nAsynchronous Embed query text.\nPARAMETER\nDESCRIPTION\ntext\nText to embed.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nlist\n[\nfloat\n]\nEmbedding.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain/embeddings/",
      "title": "Embeddings | LangChain Reference",
      "heading": "Embeddings"
    }
  },
  {
    "page_content": "Caches | LangChain Reference\nSkip to content\nLangChain Reference\nCaches\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCaches\nTable of contents\ncaches\nInMemoryCache\n__init__\nlookup\nupdate\nclear\nalookup\naupdate\naclear\nBaseCache\nlookup\nupdate\nclear\nalookup\naupdate\naclear\nCallbacks\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\ncaches\nInMemoryCache\n__init__\nlookup\nupdate\nclear\nalookup\naupdate\naclear\nBaseCache\nlookup\nupdate\nclear\nalookup\naupdate\naclear\nCaches\ncaches\n\u00b6\nOptional caching layer for language models.\nDistinct from provider-based\nprompt caching\n.\nBeta feature\nThis is a beta feature. Please be wary of deploying experimental code to production\nunless you've taken appropriate precautions.\nA cache is useful for two reasons:\nIt can save you money by reducing the number of API calls you make to the LLM\nprovider if you're often requesting the same completion multiple times.\nIt can speed up your application by reducing the number of API calls you make to the\nLLM provider.\nInMemoryCache\n\u00b6\nBases:\nBaseCache\nCache that stores things in memory.\nMETHOD\nDESCRIPTION\n__init__\nInitialize with empty cache.\nlookup\nLook up based on\nprompt\nand\nllm_string\n.\nupdate\nUpdate cache based on\nprompt\nand\nllm_string\n.\nclear\nClear cache.\nalookup\nAsync look up based on\nprompt\nand\nllm_string\n.\naupdate\nAsync update cache based on\nprompt\nand\nllm_string\n.\naclear\nAsync clear cache.\n__init__\n\u00b6\n__init__\n(\n*\n,\nmaxsize\n:\nint\n|\nNone\n=\nNone\n)\n->\nNone\nInitialize with empty cache.\nPARAMETER\nDESCRIPTION\nmaxsize\nThe maximum number of items to store in the cache.\nIf\nNone\n, the cache has no maximum size.\nIf the cache exceeds the maximum size, the oldest items are removed.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nRAISES\nDESCRIPTION\nValueError\nIf\nmaxsize\nis less than or equal to\n0\n.\nlookup\n\u00b6\nlookup\n(\nprompt\n:\nstr\n,\nllm_string\n:\nstr\n)\n->\nRETURN_VAL_TYPE\n|\nNone\nLook up based on\nprompt\nand\nllm_string\n.\nPARAMETER\nDESCRIPTION\nprompt\nA string representation of the prompt.\nIn the case of a chat model, the prompt is a non-trivial\nserialization of the prompt into the language model.\nTYPE:\nstr\nllm_string\nA string representation of the LLM configuration.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nRETURN_VAL_TYPE\n| None\nOn a cache miss, return\nNone\n. On a cache hit, return the cached value.\nupdate\n\u00b6\nupdate\n(\nprompt\n:\nstr\n,\nllm_string\n:\nstr\n,\nreturn_val\n:\nRETURN_VAL_TYPE\n)\n->\nNone\nUpdate cache based on\nprompt\nand\nllm_string\n.\nPARAMETER\nDESCRIPTION\nprompt\nA string representation of the prompt.\nIn the case of a chat model, the prompt is a non-trivial\nserialization of the prompt into the language model.\nTYPE:\nstr\nllm_string\nA string representation of the LLM configuration.\nTYPE:\nstr\nreturn_val\nThe value to be cached. The value is a list of\nGeneration\n(or subclasses).\nTYPE:\nRETURN_VAL_TYPE\nclear\n\u00b6\nclear\n(\n**\nkwargs\n:\nAny\n)\n->\nNone\nClear cache.\nalookup\nasync\n\u00b6\nalookup\n(\nprompt\n:\nstr\n,\nllm_string\n:\nstr\n)\n->\nRETURN_VAL_TYPE\n|\nNone\nAsync look up based on\nprompt\nand\nllm_string\n.\nPARAMETER\nDESCRIPTION\nprompt\nA string representation of the prompt.\nIn the case of a chat model, the prompt is a non-trivial\nserialization of the prompt into the language model.\nTYPE:\nstr\nllm_string\nA string representation of the LLM configuration.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nRETURN_VAL_TYPE\n| None\nOn a cache miss, return\nNone\n. On a cache hit, return the cached value.\naupdate\nasync\n\u00b6\naupdate\n(\nprompt\n:\nstr\n,\nllm_string\n:\nstr\n,\nreturn_val\n:\nRETURN_VAL_TYPE\n)\n->\nNone\nAsync update cache based on\nprompt\nand\nllm_string\n.\nPARAMETER\nDESCRIPTION\nprompt\nA string representation of the prompt.\nIn the case of a chat model, the prompt is a non-trivial\nserialization of the prompt into the language model.\nTYPE:\nstr\nllm_string\nA string representation of the LLM configuration.\nTYPE:\nstr\nreturn_val\nThe value to be cached. The value is a list of\nGeneration\n(or subclasses).\nTYPE:\nRETURN_VAL_TYPE\naclear\nasync\n\u00b6\naclear\n(\n**\nkwargs\n:\nAny\n)\n->\nNone\nAsync clear cache.\nBaseCache\n\u00b6\nBases:\nABC\nInterface for a caching layer for LLMs and Chat models.\nThe cache interface consists of the following methods:\nlookup: Look up a value based on a prompt and\nllm_string\n.\nupdate: Update the cache based on a prompt and\nllm_string\n.\nclear: Clear the cache.\nIn addition, the cache interface provides an async version of each method.\nThe default implementation of the async methods is to run the synchronous\nmethod in an executor. It's recommended to override the async methods\nand provide async implementations to avoid unnecessary overhead.\nMETHOD\nDESCRIPTION\nlookup\nLook up based on\nprompt\nand\nllm_string\n.\nupdate\nUpdate cache based on\nprompt\nand\nllm_string\n.\nclear\nClear cache that can take additional keyword arguments.\nalookup\nAsync look up based on\nprompt\nand\nllm_string\n.\naupdate\nAsync update cache based on\nprompt\nand\nllm_string\n.\naclear\nAsync clear cache that can take additional keyword arguments.\nlookup\nabstractmethod\n\u00b6\nlookup\n(\nprompt\n:\nstr\n,\nllm_string\n:\nstr\n)\n->\nRETURN_VAL_TYPE\n|\nNone\nLook up based on\nprompt\nand\nllm_string\n.\nA cache implementation is expected to generate a key from the 2-tuple\nof\nprompt\nand\nllm_string\n(e.g., by concatenating them with a delimiter).\nPARAMETER\nDESCRIPTION\nprompt\nA string representation of the prompt.\nIn the case of a chat model, the prompt is a non-trivial\nserialization of the prompt into the language model.\nTYPE:\nstr\nllm_string\nA string representation of the LLM configuration.\nThis is used to capture the invocation parameters of the LLM\n(e.g., model name, temperature, stop tokens, max tokens, etc.).\nThese invocation parameters are serialized into a string representation.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nRETURN_VAL_TYPE\n| None\nOn a cache miss, return\nNone\n. On a cache hit, return the cached value.\nRETURN_VAL_TYPE\n| None\nThe cached value is a list of\nGeneration\n(or subclasses).\nupdate\nabstractmethod\n\u00b6\nupdate\n(\nprompt\n:\nstr\n,\nllm_string\n:\nstr\n,\nreturn_val\n:\nRETURN_VAL_TYPE\n)\n->\nNone\nUpdate cache based on\nprompt\nand\nllm_string\n.\nThe prompt and llm_string are used to generate a key for the cache.\nThe key should match that of the lookup method.\nPARAMETER\nDESCRIPTION\nprompt\nA string representation of the prompt.\nIn the case of a chat model, the prompt is a non-trivial\nserialization of the prompt into the language model.\nTYPE:\nstr\nllm_string\nA string representation of the LLM configuration.\nThis is used to capture the invocation parameters of the LLM\n(e.g., model name, temperature, stop tokens, max tokens, etc.).\nThese invocation parameters are serialized into a string\nrepresentation.\nTYPE:\nstr\nreturn_val\nThe value to be cached. The value is a list of\nGeneration\n(or subclasses).\nTYPE:\nRETURN_VAL_TYPE\nclear\nabstractmethod\n\u00b6\nclear\n(\n**\nkwargs\n:\nAny\n)\n->\nNone\nClear cache that can take additional keyword arguments.\nalookup\nasync\n\u00b6\nalookup\n(\nprompt\n:\nstr\n,\nllm_string\n:\nstr\n)\n->\nRETURN_VAL_TYPE\n|\nNone\nAsync look up based on\nprompt\nand\nllm_string\n.\nA cache implementation is expected to generate a key from the 2-tuple\nof\nprompt\nand\nllm_string\n(e.g., by concatenating them with a delimiter).\nPARAMETER\nDESCRIPTION\nprompt\nA string representation of the prompt.\nIn the case of a chat model, the prompt is a non-trivial\nserialization of the prompt into the language model.\nTYPE:\nstr\nllm_string\nA string representation of the LLM configuration.\nThis is used to capture the invocation parameters of the LLM\n(e.g., model name, temperature, stop tokens, max tokens, etc.).\nThese invocation parameters are serialized into a string\nrepresentation.\nTYPE:\nstr\nRETURNS\nDESCRIPTION\nRETURN_VAL_TYPE\n| None\nOn a cache miss, return\nNone\n. On a cache hit, return the cached value.\nRETURN_VAL_TYPE\n| None\nThe cached value is a list of\nGeneration\n(or subclasses).\naupdate\nasync\n\u00b6\naupdate\n(\nprompt\n:\nstr\n,\nllm_string\n:\nstr\n,\nreturn_val\n:\nRETURN_VAL_TYPE\n)\n->\nNone\nAsync update cache based on\nprompt\nand\nllm_string\n.\nThe prompt and llm_string are used to generate a key for the cache.\nThe key should match that of the look up method.\nPARAMETER\nDESCRIPTION\nprompt\nA string representation of the prompt.\nIn the case of a chat model, the prompt is a non-trivial\nserialization of the prompt into the language model.\nTYPE:\nstr\nllm_string\nA string representation of the LLM configuration.\nThis is used to capture the invocation parameters of the LLM\n(e.g., model name, temperature, stop tokens, max tokens, etc.).\nThese invocation parameters are serialized into a string\nrepresentation.\nTYPE:\nstr\nreturn_val\nThe value to be cached. The value is a list of\nGeneration\n(or subclasses).\nTYPE:\nRETURN_VAL_TYPE\naclear\nasync\n\u00b6\naclear\n(\n**\nkwargs\n:\nAny\n)\n->\nNone\nAsync clear cache that can take additional keyword arguments.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain_core/caches/",
      "title": "Caches | LangChain Reference",
      "heading": "Caches"
    }
  },
  {
    "page_content": "Callbacks | LangChain Reference\nSkip to content\nLangChain Reference\nCallbacks\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nCallbacks\nTable of contents\nBaseCallbackHandler\nraise_error\nrun_inline\nignore_llm\nignore_retry\nignore_chain\nignore_agent\nignore_retriever\nignore_chat_model\nignore_custom_event\non_text\non_retry\non_custom_event\non_llm_start\non_chat_model_start\non_retriever_start\non_chain_start\non_tool_start\non_retriever_error\non_retriever_end\non_tool_end\non_tool_error\non_chain_end\non_chain_error\non_agent_action\non_agent_finish\non_llm_new_token\non_llm_end\non_llm_error\nAsyncCallbackHandler\nraise_error\nrun_inline\nignore_llm\nignore_retry\nignore_chain\nignore_agent\nignore_retriever\nignore_chat_model\nignore_custom_event\non_llm_start\non_chat_model_start\non_llm_new_token\non_llm_end\non_llm_error\non_chain_start\non_chain_end\non_chain_error\non_tool_start\non_tool_end\non_tool_error\non_text\non_retry\non_agent_action\non_agent_finish\non_retriever_start\non_retriever_end\non_retriever_error\non_custom_event\nBaseCallbackManager\nis_async\non_llm_start\non_chat_model_start\non_retriever_start\non_chain_start\non_tool_start\n__init__\ncopy\nmerge\nadd_handler\nremove_handler\nset_handlers\nset_handler\nadd_tags\nremove_tags\nadd_metadata\nremove_metadata\nCallbackManager\nis_async\non_llm_start\non_chat_model_start\non_chain_start\non_tool_start\non_retriever_start\non_custom_event\nconfigure\n__init__\ncopy\nmerge\nadd_handler\nremove_handler\nset_handlers\nset_handler\nadd_tags\nremove_tags\nadd_metadata\nremove_metadata\nAsyncCallbackManager\nis_async\non_llm_start\non_chat_model_start\non_chain_start\non_tool_start\non_custom_event\non_retriever_start\nconfigure\n__init__\ncopy\nmerge\nadd_handler\nremove_handler\nset_handlers\nset_handler\nadd_tags\nremove_tags\nadd_metadata\nremove_metadata\nUsageMetadataCallbackHandler\nraise_error\nrun_inline\nignore_llm\nignore_retry\nignore_chain\nignore_agent\nignore_retriever\nignore_chat_model\nignore_custom_event\n__init__\non_llm_end\non_text\non_retry\non_custom_event\non_llm_start\non_chat_model_start\non_retriever_start\non_chain_start\non_tool_start\non_retriever_error\non_retriever_end\non_tool_end\non_tool_error\non_chain_end\non_chain_error\non_agent_action\non_agent_finish\non_llm_new_token\non_llm_error\nget_usage_metadata_callback\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nBaseCallbackHandler\nraise_error\nrun_inline\nignore_llm\nignore_retry\nignore_chain\nignore_agent\nignore_retriever\nignore_chat_model\nignore_custom_event\non_text\non_retry\non_custom_event\non_llm_start\non_chat_model_start\non_retriever_start\non_chain_start\non_tool_start\non_retriever_error\non_retriever_end\non_tool_end\non_tool_error\non_chain_end\non_chain_error\non_agent_action\non_agent_finish\non_llm_new_token\non_llm_end\non_llm_error\nAsyncCallbackHandler\nraise_error\nrun_inline\nignore_llm\nignore_retry\nignore_chain\nignore_agent\nignore_retriever\nignore_chat_model\nignore_custom_event\non_llm_start\non_chat_model_start\non_llm_new_token\non_llm_end\non_llm_error\non_chain_start\non_chain_end\non_chain_error\non_tool_start\non_tool_end\non_tool_error\non_text\non_retry\non_agent_action\non_agent_finish\non_retriever_start\non_retriever_end\non_retriever_error\non_custom_event\nBaseCallbackManager\nis_async\non_llm_start\non_chat_model_start\non_retriever_start\non_chain_start\non_tool_start\n__init__\ncopy\nmerge\nadd_handler\nremove_handler\nset_handlers\nset_handler\nadd_tags\nremove_tags\nadd_metadata\nremove_metadata\nCallbackManager\nis_async\non_llm_start\non_chat_model_start\non_chain_start\non_tool_start\non_retriever_start\non_custom_event\nconfigure\n__init__\ncopy\nmerge\nadd_handler\nremove_handler\nset_handlers\nset_handler\nadd_tags\nremove_tags\nadd_metadata\nremove_metadata\nAsyncCallbackManager\nis_async\non_llm_start\non_chat_model_start\non_chain_start\non_tool_start\non_custom_event\non_retriever_start\nconfigure\n__init__\ncopy\nmerge\nadd_handler\nremove_handler\nset_handlers\nset_handler\nadd_tags\nremove_tags\nadd_metadata\nremove_metadata\nUsageMetadataCallbackHandler\nraise_error\nrun_inline\nignore_llm\nignore_retry\nignore_chain\nignore_agent\nignore_retriever\nignore_chat_model\nignore_custom_event\n__init__\non_llm_end\non_text\non_retry\non_custom_event\non_llm_start\non_chat_model_start\non_retriever_start\non_chain_start\non_tool_start\non_retriever_error\non_retriever_end\non_tool_end\non_tool_error\non_chain_end\non_chain_error\non_agent_action\non_agent_finish\non_llm_new_token\non_llm_error\nget_usage_metadata_callback\nCallbacks\nBaseCallbackHandler\n\u00b6\nBases:\nLLMManagerMixin\n,\nChainManagerMixin\n,\nToolManagerMixin\n,\nRetrieverManagerMixin\n,\nCallbackManagerMixin\n,\nRunManagerMixin\nflowchart TD\nlangchain_core.callbacks.base.BaseCallbackHandler[BaseCallbackHandler]\nlangchain_core.callbacks.base.LLMManagerMixin[LLMManagerMixin]\nlangchain_core.callbacks.base.ChainManagerMixin[ChainManagerMixin]\nlangchain_core.callbacks.base.ToolManagerMixin[ToolManagerMixin]\nlangchain_core.callbacks.base.RetrieverManagerMixin[RetrieverManagerMixin]\nlangchain_core.callbacks.base.CallbackManagerMixin[CallbackManagerMixin]\nlangchain_core.callbacks.base.RunManagerMixin[RunManagerMixin]\nlangchain_core.callbacks.base.LLMManagerMixin --> langchain_core.callbacks.base.BaseCallbackHandler\nlangchain_core.callbacks.base.ChainManagerMixin --> langchain_core.callbacks.base.BaseCallbackHandler\nlangchain_core.callbacks.base.ToolManagerMixin --> langchain_core.callbacks.base.BaseCallbackHandler\nlangchain_core.callbacks.base.RetrieverManagerMixin --> langchain_core.callbacks.base.BaseCallbackHandler\nlangchain_core.callbacks.base.CallbackManagerMixin --> langchain_core.callbacks.base.BaseCallbackHandler\nlangchain_core.callbacks.base.RunManagerMixin --> langchain_core.callbacks.base.BaseCallbackHandler\nclick langchain_core.callbacks.base.BaseCallbackHandler href \"\" \"langchain_core.callbacks.base.BaseCallbackHandler\"\nclick langchain_core.callbacks.base.LLMManagerMixin href \"\" \"langchain_core.callbacks.base.LLMManagerMixin\"\nclick langchain_core.callbacks.base.ChainManagerMixin href \"\" \"langchain_core.callbacks.base.ChainManagerMixin\"\nclick langchain_core.callbacks.base.ToolManagerMixin href \"\" \"langchain_core.callbacks.base.ToolManagerMixin\"\nclick langchain_core.callbacks.base.RetrieverManagerMixin href \"\" \"langchain_core.callbacks.base.RetrieverManagerMixin\"\nclick langchain_core.callbacks.base.CallbackManagerMixin href \"\" \"langchain_core.callbacks.base.CallbackManagerMixin\"\nclick langchain_core.callbacks.base.RunManagerMixin href \"\" \"langchain_core.callbacks.base.RunManagerMixin\"\nBase callback handler for LangChain.\nMETHOD\nDESCRIPTION\non_text\nRun on an arbitrary text.\non_retry\nRun on a retry event.\non_custom_event\nOverride to define a handler for a custom event.\non_llm_start\nRun when LLM starts running.\non_chat_model_start\nRun when a chat model starts running.\non_retriever_start\nRun when the Retriever starts running.\non_chain_start\nRun when a chain starts running.\non_tool_start\nRun when the tool starts running.\non_retriever_error\nRun when Retriever errors.\non_retriever_end\nRun when Retriever ends running.\non_tool_end\nRun when the tool ends running.\non_tool_error\nRun when tool errors.\non_chain_end\nRun when chain ends running.\non_chain_error\nRun when chain errors.\non_agent_action\nRun on agent action.\non_agent_finish\nRun on the agent end.\non_llm_new_token\nRun on new output token. Only available when streaming is enabled.\non_llm_end\nRun when LLM ends running.\non_llm_error\nRun when LLM errors.\nraise_error\nclass-attribute\ninstance-attribute\n\u00b6\nraise_error\n:\nbool\n=\nFalse\nWhether to raise an error if an exception occurs.\nrun_inline\nclass-attribute\ninstance-attribute\n\u00b6\nrun_inline\n:\nbool\n=\nFalse\nWhether to run the callback inline.\nignore_llm\nproperty\n\u00b6\nignore_llm\n:\nbool\nWhether to ignore LLM callbacks.\nignore_retry\nproperty\n\u00b6\nignore_retry\n:\nbool\nWhether to ignore retry callbacks.\nignore_chain\nproperty\n\u00b6\nignore_chain\n:\nbool\nWhether to ignore chain callbacks.\nignore_agent\nproperty\n\u00b6\nignore_agent\n:\nbool\nWhether to ignore agent callbacks.\nignore_retriever\nproperty\n\u00b6\nignore_retriever\n:\nbool\nWhether to ignore retriever callbacks.\nignore_chat_model\nproperty\n\u00b6\nignore_chat_model\n:\nbool\nWhether to ignore chat model callbacks.\nignore_custom_event\nproperty\n\u00b6\nignore_custom_event\n:\nbool\nIgnore custom event.\non_text\n\u00b6\non_text\n(\ntext\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAny\nRun on an arbitrary text.\nPARAMETER\nDESCRIPTION\ntext\nThe text.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retry\n\u00b6\non_retry\n(\nretry_state\n:\nRetryCallState\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun on a retry event.\nPARAMETER\nDESCRIPTION\nretry_state\nThe retry state.\nTYPE:\nRetryCallState\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_custom_event\n\u00b6\non_custom_event\n(\nname\n:\nstr\n,\ndata\n:\nAny\n,\n*\n,\nrun_id\n:\nUUID\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nOverride to define a handler for a custom event.\nPARAMETER\nDESCRIPTION\nname\nThe name of the custom event.\nTYPE:\nstr\ndata\nThe data for the custom event. Format will match\nthe format specified by the user.\nTYPE:\nAny\nrun_id\nThe ID of the run.\nTYPE:\nUUID\ntags\nThe tags associated with the custom event\n(includes inherited tags).\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata associated with the custom event\n(includes inherited metadata).\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\non_llm_start\n\u00b6\non_llm_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nprompts\n:\nlist\n[\nstr\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when LLM starts running.\nWarning\nThis method is called for non-chat models (regular LLMs). If you're\nimplementing a handler for a chat model, you should use\non_chat_model_start\ninstead.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized LLM.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nprompts\nThe prompts.\nTYPE:\nlist\n[\nstr\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chat_model_start\n\u00b6\non_chat_model_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nmessages\n:\nlist\n[\nlist\n[\nBaseMessage\n]],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when a chat model starts running.\nWarning\nThis method is called for chat models. If you're implementing a handler for\na non-chat model, you should use\non_llm_start\ninstead.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chat model.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nmessages\nThe messages.\nTYPE:\nlist\n[\nlist\n[\nBaseMessage\n]]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_start\n\u00b6\non_retriever_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nquery\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when the Retriever starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized Retriever.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nquery\nThe query.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_start\n\u00b6\non_chain_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\ninputs\n:\ndict\n[\nstr\n,\nAny\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when a chain starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninputs\nThe inputs.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_start\n\u00b6\non_tool_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\ninput_str\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ninputs\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when the tool starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninput_str\nThe input string.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ninputs\nThe inputs.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_error\n\u00b6\non_retriever_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when Retriever errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_end\n\u00b6\non_retriever_end\n(\ndocuments\n:\nSequence\n[\nDocument\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when Retriever ends running.\nPARAMETER\nDESCRIPTION\ndocuments\nThe documents retrieved.\nTYPE:\nSequence\n[\nDocument\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_end\n\u00b6\non_tool_end\n(\noutput\n:\nAny\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAny\nRun when the tool ends running.\nPARAMETER\nDESCRIPTION\noutput\nThe output of the tool.\nTYPE:\nAny\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_error\n\u00b6\non_tool_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when tool errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_end\n\u00b6\non_chain_end\n(\noutputs\n:\ndict\n[\nstr\n,\nAny\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when chain ends running.\nPARAMETER\nDESCRIPTION\noutputs\nThe outputs of the chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_error\n\u00b6\non_chain_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when chain errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_agent_action\n\u00b6\non_agent_action\n(\naction\n:\nAgentAction\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun on agent action.\nPARAMETER\nDESCRIPTION\naction\nThe agent action.\nTYPE:\nAgentAction\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_agent_finish\n\u00b6\non_agent_finish\n(\nfinish\n:\nAgentFinish\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun on the agent end.\nPARAMETER\nDESCRIPTION\nfinish\nThe agent finish.\nTYPE:\nAgentFinish\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_llm_new_token\n\u00b6\non_llm_new_token\n(\ntoken\n:\nstr\n,\n*\n,\nchunk\n:\nGenerationChunk\n|\nChatGenerationChunk\n|\nNone\n=\nNone\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun on new output token. Only available when streaming is enabled.\nFor both chat models and non-chat models (legacy LLMs).\nPARAMETER\nDESCRIPTION\ntoken\nThe new token.\nTYPE:\nstr\nchunk\nThe new generated chunk, containing content and other information.\nTYPE:\nGenerationChunk\n|\nChatGenerationChunk\n| None\nDEFAULT:\nNone\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_llm_end\n\u00b6\non_llm_end\n(\nresponse\n:\nLLMResult\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when LLM ends running.\nPARAMETER\nDESCRIPTION\nresponse\nThe response which was generated.\nTYPE:\nLLMResult\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_llm_error\n\u00b6\non_llm_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when LLM errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nAsyncCallbackHandler\n\u00b6\nBases:\nBaseCallbackHandler\nAsync callback handler for LangChain.\nMETHOD\nDESCRIPTION\non_llm_start\nRun when the model starts running.\non_chat_model_start\nRun when a chat model starts running.\non_llm_new_token\nRun on new output token. Only available when streaming is enabled.\non_llm_end\nRun when the model ends running.\non_llm_error\nRun when LLM errors.\non_chain_start\nRun when a chain starts running.\non_chain_end\nRun when a chain ends running.\non_chain_error\nRun when chain errors.\non_tool_start\nRun when the tool starts running.\non_tool_end\nRun when the tool ends running.\non_tool_error\nRun when tool errors.\non_text\nRun on an arbitrary text.\non_retry\nRun on a retry event.\non_agent_action\nRun on agent action.\non_agent_finish\nRun on the agent end.\non_retriever_start\nRun on the retriever start.\non_retriever_end\nRun on the retriever end.\non_retriever_error\nRun on retriever error.\non_custom_event\nOverride to define a handler for custom events.\nraise_error\nclass-attribute\ninstance-attribute\n\u00b6\nraise_error\n:\nbool\n=\nFalse\nWhether to raise an error if an exception occurs.\nrun_inline\nclass-attribute\ninstance-attribute\n\u00b6\nrun_inline\n:\nbool\n=\nFalse\nWhether to run the callback inline.\nignore_llm\nproperty\n\u00b6\nignore_llm\n:\nbool\nWhether to ignore LLM callbacks.\nignore_retry\nproperty\n\u00b6\nignore_retry\n:\nbool\nWhether to ignore retry callbacks.\nignore_chain\nproperty\n\u00b6\nignore_chain\n:\nbool\nWhether to ignore chain callbacks.\nignore_agent\nproperty\n\u00b6\nignore_agent\n:\nbool\nWhether to ignore agent callbacks.\nignore_retriever\nproperty\n\u00b6\nignore_retriever\n:\nbool\nWhether to ignore retriever callbacks.\nignore_chat_model\nproperty\n\u00b6\nignore_chat_model\n:\nbool\nWhether to ignore chat model callbacks.\nignore_custom_event\nproperty\n\u00b6\nignore_custom_event\n:\nbool\nIgnore custom event.\non_llm_start\nasync\n\u00b6\non_llm_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nprompts\n:\nlist\n[\nstr\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun when the model starts running.\nWarning\nThis method is called for non-chat models (regular LLMs). If you're\nimplementing a handler for a chat model, you should use\non_chat_model_start\ninstead.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized LLM.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nprompts\nThe prompts.\nTYPE:\nlist\n[\nstr\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chat_model_start\nasync\n\u00b6\non_chat_model_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nmessages\n:\nlist\n[\nlist\n[\nBaseMessage\n]],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when a chat model starts running.\nWarning\nThis method is called for chat models. If you're implementing a handler for\na non-chat model, you should use\non_llm_start\ninstead.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chat model.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nmessages\nThe messages.\nTYPE:\nlist\n[\nlist\n[\nBaseMessage\n]]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_llm_new_token\nasync\n\u00b6\non_llm_new_token\n(\ntoken\n:\nstr\n,\n*\n,\nchunk\n:\nGenerationChunk\n|\nChatGenerationChunk\n|\nNone\n=\nNone\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun on new output token. Only available when streaming is enabled.\nFor both chat models and non-chat models (legacy LLMs).\nPARAMETER\nDESCRIPTION\ntoken\nThe new token.\nTYPE:\nstr\nchunk\nThe new generated chunk, containing content and other information.\nTYPE:\nGenerationChunk\n|\nChatGenerationChunk\n| None\nDEFAULT:\nNone\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_llm_end\nasync\n\u00b6\non_llm_end\n(\nresponse\n:\nLLMResult\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun when the model ends running.\nPARAMETER\nDESCRIPTION\nresponse\nThe response which was generated.\nTYPE:\nLLMResult\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_llm_error\nasync\n\u00b6\non_llm_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun when LLM errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\n- response (LLMResult): The response which was generated before\nthe error occurred.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_start\nasync\n\u00b6\non_chain_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\ninputs\n:\ndict\n[\nstr\n,\nAny\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun when a chain starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninputs\nThe inputs.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_end\nasync\n\u00b6\non_chain_end\n(\noutputs\n:\ndict\n[\nstr\n,\nAny\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun when a chain ends running.\nPARAMETER\nDESCRIPTION\noutputs\nThe outputs of the chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_error\nasync\n\u00b6\non_chain_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun when chain errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_start\nasync\n\u00b6\non_tool_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\ninput_str\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ninputs\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun when the tool starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized tool.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninput_str\nThe input string.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ninputs\nThe inputs.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_end\nasync\n\u00b6\non_tool_end\n(\noutput\n:\nAny\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun when the tool ends running.\nPARAMETER\nDESCRIPTION\noutput\nThe output of the tool.\nTYPE:\nAny\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_error\nasync\n\u00b6\non_tool_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun when tool errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_text\nasync\n\u00b6\non_text\n(\ntext\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun on an arbitrary text.\nPARAMETER\nDESCRIPTION\ntext\nThe text.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retry\nasync\n\u00b6\non_retry\n(\nretry_state\n:\nRetryCallState\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun on a retry event.\nPARAMETER\nDESCRIPTION\nretry_state\nThe retry state.\nTYPE:\nRetryCallState\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_agent_action\nasync\n\u00b6\non_agent_action\n(\naction\n:\nAgentAction\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun on agent action.\nPARAMETER\nDESCRIPTION\naction\nThe agent action.\nTYPE:\nAgentAction\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_agent_finish\nasync\n\u00b6\non_agent_finish\n(\nfinish\n:\nAgentFinish\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun on the agent end.\nPARAMETER\nDESCRIPTION\nfinish\nThe agent finish.\nTYPE:\nAgentFinish\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_start\nasync\n\u00b6\non_retriever_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nquery\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun on the retriever start.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized retriever.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nquery\nThe query.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_end\nasync\n\u00b6\non_retriever_end\n(\ndocuments\n:\nSequence\n[\nDocument\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun on the retriever end.\nPARAMETER\nDESCRIPTION\ndocuments\nThe documents retrieved.\nTYPE:\nSequence\n[\nDocument\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_error\nasync\n\u00b6\non_retriever_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nRun on retriever error.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_custom_event\nasync\n\u00b6\non_custom_event\n(\nname\n:\nstr\n,\ndata\n:\nAny\n,\n*\n,\nrun_id\n:\nUUID\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nOverride to define a handler for custom events.\nPARAMETER\nDESCRIPTION\nname\nThe name of the custom event.\nTYPE:\nstr\ndata\nThe data for the custom event. Format will match\nthe format specified by the user.\nTYPE:\nAny\nrun_id\nThe ID of the run.\nTYPE:\nUUID\ntags\nThe tags associated with the custom event\n(includes inherited tags).\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata associated with the custom event\n(includes inherited metadata).\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nBaseCallbackManager\n\u00b6\nBases:\nCallbackManagerMixin\nBase callback manager for LangChain.\nMETHOD\nDESCRIPTION\non_llm_start\nRun when LLM starts running.\non_chat_model_start\nRun when a chat model starts running.\non_retriever_start\nRun when the Retriever starts running.\non_chain_start\nRun when a chain starts running.\non_tool_start\nRun when the tool starts running.\n__init__\nInitialize callback manager.\ncopy\nReturn a copy of the callback manager.\nmerge\nMerge the callback manager with another callback manager.\nadd_handler\nAdd a handler to the callback manager.\nremove_handler\nRemove a handler from the callback manager.\nset_handlers\nSet handlers as the only handlers on the callback manager.\nset_handler\nSet handler as the only handler on the callback manager.\nadd_tags\nAdd tags to the callback manager.\nremove_tags\nRemove tags from the callback manager.\nadd_metadata\nAdd metadata to the callback manager.\nremove_metadata\nRemove metadata from the callback manager.\nis_async\nproperty\n\u00b6\nis_async\n:\nbool\nWhether the callback manager is async.\non_llm_start\n\u00b6\non_llm_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nprompts\n:\nlist\n[\nstr\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when LLM starts running.\nWarning\nThis method is called for non-chat models (regular LLMs). If you're\nimplementing a handler for a chat model, you should use\non_chat_model_start\ninstead.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized LLM.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nprompts\nThe prompts.\nTYPE:\nlist\n[\nstr\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chat_model_start\n\u00b6\non_chat_model_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nmessages\n:\nlist\n[\nlist\n[\nBaseMessage\n]],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when a chat model starts running.\nWarning\nThis method is called for chat models. If you're implementing a handler for\na non-chat model, you should use\non_llm_start\ninstead.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chat model.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nmessages\nThe messages.\nTYPE:\nlist\n[\nlist\n[\nBaseMessage\n]]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_start\n\u00b6\non_retriever_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nquery\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when the Retriever starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized Retriever.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nquery\nThe query.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_start\n\u00b6\non_chain_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\ninputs\n:\ndict\n[\nstr\n,\nAny\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when a chain starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninputs\nThe inputs.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_start\n\u00b6\non_tool_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\ninput_str\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ninputs\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when the tool starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninput_str\nThe input string.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ninputs\nThe inputs.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\n__init__\n\u00b6\n__init__\n(\nhandlers\n:\nlist\n[\nBaseCallbackHandler\n],\ninheritable_handlers\n:\nlist\n[\nBaseCallbackHandler\n]\n|\nNone\n=\nNone\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n*\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninheritable_tags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ninheritable_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n)\n->\nNone\nInitialize callback manager.\nPARAMETER\nDESCRIPTION\nhandlers\nThe handlers.\nTYPE:\nlist\n[\nBaseCallbackHandler\n]\ninheritable_handlers\nThe inheritable handlers.\nTYPE:\nlist\n[\nBaseCallbackHandler\n] | None\nDEFAULT:\nNone\nparent_run_id\nThe parent run ID.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ninheritable_tags\nThe inheritable tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ninheritable_metadata\nThe inheritable metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ncopy\n\u00b6\ncopy\n()\n->\nSelf\nReturn a copy of the callback manager.\nmerge\n\u00b6\nmerge\n(\nother\n:\nBaseCallbackManager\n)\n->\nSelf\nMerge the callback manager with another callback manager.\nMay be overwritten in subclasses. Primarily used internally\nwithin merge_configs.\nRETURNS\nDESCRIPTION\nSelf\nThe merged callback manager of the same type as the current object.\nExample: Merging two callback managers.\n```python\nfrom langchain_core.callbacks.manager import (\nCallbackManager,\ntrace_as_chain_group,\n)\nfrom langchain_core.callbacks.stdout import StdOutCallbackHandler\nmanager = CallbackManager(handlers=[StdOutCallbackHandler()], tags=[\"tag2\"])\nwith trace_as_chain_group(\"My Group Name\", tags=[\"tag1\"]) as group_manager:\nmerged_manager = group_manager.merge(manager)\nprint(merged_manager.handlers)\n# [\n#    <langchain_core.callbacks.stdout.StdOutCallbackHandler object at ...>,\n#    <langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler object at ...>,\n# ]\nprint(merged_manager.tags)\n#    ['tag2', 'tag1']\n```\nadd_handler\n\u00b6\nadd_handler\n(\nhandler\n:\nBaseCallbackHandler\n,\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nAdd a handler to the callback manager.\nPARAMETER\nDESCRIPTION\nhandler\nThe handler to add.\nTYPE:\nBaseCallbackHandler\ninherit\nWhether to inherit the handler.\nTYPE:\nbool\nDEFAULT:\nTrue\nremove_handler\n\u00b6\nremove_handler\n(\nhandler\n:\nBaseCallbackHandler\n)\n->\nNone\nRemove a handler from the callback manager.\nPARAMETER\nDESCRIPTION\nhandler\nThe handler to remove.\nTYPE:\nBaseCallbackHandler\nset_handlers\n\u00b6\nset_handlers\n(\nhandlers\n:\nlist\n[\nBaseCallbackHandler\n],\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nSet handlers as the only handlers on the callback manager.\nPARAMETER\nDESCRIPTION\nhandlers\nThe handlers to set.\nTYPE:\nlist\n[\nBaseCallbackHandler\n]\ninherit\nWhether to inherit the handlers.\nTYPE:\nbool\nDEFAULT:\nTrue\nset_handler\n\u00b6\nset_handler\n(\nhandler\n:\nBaseCallbackHandler\n,\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nSet handler as the only handler on the callback manager.\nPARAMETER\nDESCRIPTION\nhandler\nThe handler to set.\nTYPE:\nBaseCallbackHandler\ninherit\nWhether to inherit the handler.\nTYPE:\nbool\nDEFAULT:\nTrue\nadd_tags\n\u00b6\nadd_tags\n(\ntags\n:\nlist\n[\nstr\n],\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nAdd tags to the callback manager.\nPARAMETER\nDESCRIPTION\ntags\nThe tags to add.\nTYPE:\nlist\n[\nstr\n]\ninherit\nWhether to inherit the tags.\nTYPE:\nbool\nDEFAULT:\nTrue\nremove_tags\n\u00b6\nremove_tags\n(\ntags\n:\nlist\n[\nstr\n])\n->\nNone\nRemove tags from the callback manager.\nPARAMETER\nDESCRIPTION\ntags\nThe tags to remove.\nTYPE:\nlist\n[\nstr\n]\nadd_metadata\n\u00b6\nadd_metadata\n(\nmetadata\n:\ndict\n[\nstr\n,\nAny\n],\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nAdd metadata to the callback manager.\nPARAMETER\nDESCRIPTION\nmetadata\nThe metadata to add.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninherit\nWhether to inherit the metadata.\nTYPE:\nbool\nDEFAULT:\nTrue\nremove_metadata\n\u00b6\nremove_metadata\n(\nkeys\n:\nlist\n[\nstr\n])\n->\nNone\nRemove metadata from the callback manager.\nPARAMETER\nDESCRIPTION\nkeys\nThe keys to remove.\nTYPE:\nlist\n[\nstr\n]\nCallbackManager\n\u00b6\nBases:\nBaseCallbackManager\nCallback manager for LangChain.\nMETHOD\nDESCRIPTION\non_llm_start\nRun when LLM starts running.\non_chat_model_start\nRun when chat model starts running.\non_chain_start\nRun when chain starts running.\non_tool_start\nRun when tool starts running.\non_retriever_start\nRun when the retriever starts running.\non_custom_event\nDispatch an adhoc event to the handlers (async version).\nconfigure\nConfigure the callback manager.\n__init__\nInitialize callback manager.\ncopy\nReturn a copy of the callback manager.\nmerge\nMerge the callback manager with another callback manager.\nadd_handler\nAdd a handler to the callback manager.\nremove_handler\nRemove a handler from the callback manager.\nset_handlers\nSet handlers as the only handlers on the callback manager.\nset_handler\nSet handler as the only handler on the callback manager.\nadd_tags\nAdd tags to the callback manager.\nremove_tags\nRemove tags from the callback manager.\nadd_metadata\nAdd metadata to the callback manager.\nremove_metadata\nRemove metadata from the callback manager.\nis_async\nproperty\n\u00b6\nis_async\n:\nbool\nWhether the callback manager is async.\non_llm_start\n\u00b6\non_llm_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nprompts\n:\nlist\n[\nstr\n],\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nCallbackManagerForLLMRun\n]\nRun when LLM starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized LLM.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nprompts\nThe list of prompts.\nTYPE:\nlist\n[\nstr\n]\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nCallbackManagerForLLMRun\n]\nA callback manager for each prompt as an LLM run.\non_chat_model_start\n\u00b6\non_chat_model_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nmessages\n:\nlist\n[\nlist\n[\nBaseMessage\n]],\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nCallbackManagerForLLMRun\n]\nRun when chat model starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized LLM.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nmessages\nThe list of messages.\nTYPE:\nlist\n[\nlist\n[\nBaseMessage\n]]\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nCallbackManagerForLLMRun\n]\nA callback manager for each list of messages as an LLM run.\non_chain_start\n\u00b6\non_chain_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n,\ninputs\n:\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n,\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nCallbackManagerForChainRun\nRun when chain starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\ninputs\nThe inputs to the chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n] |\nAny\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nCallbackManagerForChainRun\nThe callback manager for the chain run.\non_tool_start\n\u00b6\non_tool_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n,\ninput_str\n:\nstr\n,\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ninputs\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nCallbackManagerForToolRun\nRun when tool starts running.\nPARAMETER\nDESCRIPTION\nserialized\nSerialized representation of the tool.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\ninput_str\nThe  input to the tool as a string.\nNon-string inputs are cast to strings.\nTYPE:\nstr\nrun_id\nID for the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\nparent_run_id\nThe ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ninputs\nThe original input to the tool if provided.\nRecommended for usage instead of input_str when the original\ninput is needed.\nIf provided, the inputs are expected to be formatted as a dict.\nThe keys will correspond to the named-arguments in the tool.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nThe keyword arguments to pass to the event handler\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nCallbackManagerForToolRun\nThe callback manager for the tool run.\non_retriever_start\n\u00b6\non_retriever_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n,\nquery\n:\nstr\n,\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nCallbackManagerForRetrieverRun\nRun when the retriever starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized retriever.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nquery\nThe query.\nTYPE:\nstr\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\nparent_run_id\nThe ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nCallbackManagerForRetrieverRun\nThe callback manager for the retriever run.\non_custom_event\n\u00b6\non_custom_event\n(\nname\n:\nstr\n,\ndata\n:\nAny\n,\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nDispatch an adhoc event to the handlers (async version).\nThis event should NOT be used in any internal LangChain code. The event\nis meant specifically for users of the library to dispatch custom\nevents that are tailored to their application.\nPARAMETER\nDESCRIPTION\nname\nThe name of the adhoc event.\nTYPE:\nstr\ndata\nThe data for the adhoc event.\nTYPE:\nAny\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\nRAISES\nDESCRIPTION\nValueError\nIf additional keyword arguments are passed.\nconfigure\nclassmethod\n\u00b6\nconfigure\n(\ninheritable_callbacks\n:\nCallbacks\n=\nNone\n,\nlocal_callbacks\n:\nCallbacks\n=\nNone\n,\nverbose\n:\nbool\n=\nFalse\n,\ninheritable_tags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nlocal_tags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninheritable_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nlocal_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n)\n->\nCallbackManager\nConfigure the callback manager.\nPARAMETER\nDESCRIPTION\ninheritable_callbacks\nThe inheritable callbacks.\nTYPE:\nCallbacks\nDEFAULT:\nNone\nlocal_callbacks\nThe local callbacks.\nTYPE:\nCallbacks\nDEFAULT:\nNone\nverbose\nWhether to enable verbose mode.\nTYPE:\nbool\nDEFAULT:\nFalse\ninheritable_tags\nThe inheritable tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nlocal_tags\nThe local tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ninheritable_metadata\nThe inheritable metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nlocal_metadata\nThe local metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCallbackManager\nThe configured callback manager.\n__init__\n\u00b6\n__init__\n(\nhandlers\n:\nlist\n[\nBaseCallbackHandler\n],\ninheritable_handlers\n:\nlist\n[\nBaseCallbackHandler\n]\n|\nNone\n=\nNone\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n*\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninheritable_tags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ninheritable_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n)\n->\nNone\nInitialize callback manager.\nPARAMETER\nDESCRIPTION\nhandlers\nThe handlers.\nTYPE:\nlist\n[\nBaseCallbackHandler\n]\ninheritable_handlers\nThe inheritable handlers.\nTYPE:\nlist\n[\nBaseCallbackHandler\n] | None\nDEFAULT:\nNone\nparent_run_id\nThe parent run ID.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ninheritable_tags\nThe inheritable tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ninheritable_metadata\nThe inheritable metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ncopy\n\u00b6\ncopy\n()\n->\nSelf\nReturn a copy of the callback manager.\nmerge\n\u00b6\nmerge\n(\nother\n:\nBaseCallbackManager\n)\n->\nSelf\nMerge the callback manager with another callback manager.\nMay be overwritten in subclasses. Primarily used internally\nwithin merge_configs.\nRETURNS\nDESCRIPTION\nSelf\nThe merged callback manager of the same type as the current object.\nExample: Merging two callback managers.\n```python\nfrom langchain_core.callbacks.manager import (\nCallbackManager,\ntrace_as_chain_group,\n)\nfrom langchain_core.callbacks.stdout import StdOutCallbackHandler\nmanager = CallbackManager(handlers=[StdOutCallbackHandler()], tags=[\"tag2\"])\nwith trace_as_chain_group(\"My Group Name\", tags=[\"tag1\"]) as group_manager:\nmerged_manager = group_manager.merge(manager)\nprint(merged_manager.handlers)\n# [\n#    <langchain_core.callbacks.stdout.StdOutCallbackHandler object at ...>,\n#    <langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler object at ...>,\n# ]\nprint(merged_manager.tags)\n#    ['tag2', 'tag1']\n```\nadd_handler\n\u00b6\nadd_handler\n(\nhandler\n:\nBaseCallbackHandler\n,\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nAdd a handler to the callback manager.\nPARAMETER\nDESCRIPTION\nhandler\nThe handler to add.\nTYPE:\nBaseCallbackHandler\ninherit\nWhether to inherit the handler.\nTYPE:\nbool\nDEFAULT:\nTrue\nremove_handler\n\u00b6\nremove_handler\n(\nhandler\n:\nBaseCallbackHandler\n)\n->\nNone\nRemove a handler from the callback manager.\nPARAMETER\nDESCRIPTION\nhandler\nThe handler to remove.\nTYPE:\nBaseCallbackHandler\nset_handlers\n\u00b6\nset_handlers\n(\nhandlers\n:\nlist\n[\nBaseCallbackHandler\n],\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nSet handlers as the only handlers on the callback manager.\nPARAMETER\nDESCRIPTION\nhandlers\nThe handlers to set.\nTYPE:\nlist\n[\nBaseCallbackHandler\n]\ninherit\nWhether to inherit the handlers.\nTYPE:\nbool\nDEFAULT:\nTrue\nset_handler\n\u00b6\nset_handler\n(\nhandler\n:\nBaseCallbackHandler\n,\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nSet handler as the only handler on the callback manager.\nPARAMETER\nDESCRIPTION\nhandler\nThe handler to set.\nTYPE:\nBaseCallbackHandler\ninherit\nWhether to inherit the handler.\nTYPE:\nbool\nDEFAULT:\nTrue\nadd_tags\n\u00b6\nadd_tags\n(\ntags\n:\nlist\n[\nstr\n],\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nAdd tags to the callback manager.\nPARAMETER\nDESCRIPTION\ntags\nThe tags to add.\nTYPE:\nlist\n[\nstr\n]\ninherit\nWhether to inherit the tags.\nTYPE:\nbool\nDEFAULT:\nTrue\nremove_tags\n\u00b6\nremove_tags\n(\ntags\n:\nlist\n[\nstr\n])\n->\nNone\nRemove tags from the callback manager.\nPARAMETER\nDESCRIPTION\ntags\nThe tags to remove.\nTYPE:\nlist\n[\nstr\n]\nadd_metadata\n\u00b6\nadd_metadata\n(\nmetadata\n:\ndict\n[\nstr\n,\nAny\n],\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nAdd metadata to the callback manager.\nPARAMETER\nDESCRIPTION\nmetadata\nThe metadata to add.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninherit\nWhether to inherit the metadata.\nTYPE:\nbool\nDEFAULT:\nTrue\nremove_metadata\n\u00b6\nremove_metadata\n(\nkeys\n:\nlist\n[\nstr\n])\n->\nNone\nRemove metadata from the callback manager.\nPARAMETER\nDESCRIPTION\nkeys\nThe keys to remove.\nTYPE:\nlist\n[\nstr\n]\nAsyncCallbackManager\n\u00b6\nBases:\nBaseCallbackManager\nAsync callback manager that handles callbacks from LangChain.\nMETHOD\nDESCRIPTION\non_llm_start\nRun when LLM starts running.\non_chat_model_start\nAsync run when LLM starts running.\non_chain_start\nAsync run when chain starts running.\non_tool_start\nRun when the tool starts running.\non_custom_event\nDispatch an adhoc event to the handlers (async version).\non_retriever_start\nRun when the retriever starts running.\nconfigure\nConfigure the async callback manager.\n__init__\nInitialize callback manager.\ncopy\nReturn a copy of the callback manager.\nmerge\nMerge the callback manager with another callback manager.\nadd_handler\nAdd a handler to the callback manager.\nremove_handler\nRemove a handler from the callback manager.\nset_handlers\nSet handlers as the only handlers on the callback manager.\nset_handler\nSet handler as the only handler on the callback manager.\nadd_tags\nAdd tags to the callback manager.\nremove_tags\nRemove tags from the callback manager.\nadd_metadata\nAdd metadata to the callback manager.\nremove_metadata\nRemove metadata from the callback manager.\nis_async\nproperty\n\u00b6\nis_async\n:\nbool\nReturn whether the handler is async.\non_llm_start\nasync\n\u00b6\non_llm_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nprompts\n:\nlist\n[\nstr\n],\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nAsyncCallbackManagerForLLMRun\n]\nRun when LLM starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized LLM.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nprompts\nThe list of prompts.\nTYPE:\nlist\n[\nstr\n]\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nAsyncCallbackManagerForLLMRun\n]\nThe list of async callback managers, one for each LLM Run corresponding to\nlist\n[\nAsyncCallbackManagerForLLMRun\n]\neach prompt.\non_chat_model_start\nasync\n\u00b6\non_chat_model_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nmessages\n:\nlist\n[\nlist\n[\nBaseMessage\n]],\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nAsyncCallbackManagerForLLMRun\n]\nAsync run when LLM starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized LLM.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nmessages\nThe list of messages.\nTYPE:\nlist\n[\nlist\n[\nBaseMessage\n]]\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nAsyncCallbackManagerForLLMRun\n]\nThe list of async callback managers, one for each LLM Run corresponding to\nlist\n[\nAsyncCallbackManagerForLLMRun\n]\neach inner  message list.\non_chain_start\nasync\n\u00b6\non_chain_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n,\ninputs\n:\ndict\n[\nstr\n,\nAny\n]\n|\nAny\n,\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncCallbackManagerForChainRun\nAsync run when chain starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\ninputs\nThe inputs to the chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n] |\nAny\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nAsyncCallbackManagerForChainRun\nThe async callback manager for the chain run.\non_tool_start\nasync\n\u00b6\non_tool_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n,\ninput_str\n:\nstr\n,\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncCallbackManagerForToolRun\nRun when the tool starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized tool.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\ninput_str\nThe input to the tool.\nTYPE:\nstr\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\nparent_run_id\nThe ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nAsyncCallbackManagerForToolRun\nThe async callback manager for the tool run.\non_custom_event\nasync\n\u00b6\non_custom_event\n(\nname\n:\nstr\n,\ndata\n:\nAny\n,\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nDispatch an adhoc event to the handlers (async version).\nThis event should NOT be used in any internal LangChain code. The event\nis meant specifically for users of the library to dispatch custom\nevents that are tailored to their application.\nPARAMETER\nDESCRIPTION\nname\nThe name of the adhoc event.\nTYPE:\nstr\ndata\nThe data for the adhoc event.\nTYPE:\nAny\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\nRAISES\nDESCRIPTION\nValueError\nIf additional keyword arguments are passed.\non_retriever_start\nasync\n\u00b6\non_retriever_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n,\nquery\n:\nstr\n,\nrun_id\n:\nUUID\n|\nNone\n=\nNone\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncCallbackManagerForRetrieverRun\nRun when the retriever starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized retriever.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nquery\nThe query.\nTYPE:\nstr\nrun_id\nThe ID of the run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\nparent_run_id\nThe ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nAsyncCallbackManagerForRetrieverRun\nThe async callback manager for the retriever run.\nconfigure\nclassmethod\n\u00b6\nconfigure\n(\ninheritable_callbacks\n:\nCallbacks\n=\nNone\n,\nlocal_callbacks\n:\nCallbacks\n=\nNone\n,\nverbose\n:\nbool\n=\nFalse\n,\ninheritable_tags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nlocal_tags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninheritable_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nlocal_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n)\n->\nAsyncCallbackManager\nConfigure the async callback manager.\nPARAMETER\nDESCRIPTION\ninheritable_callbacks\nThe inheritable callbacks.\nTYPE:\nCallbacks\nDEFAULT:\nNone\nlocal_callbacks\nThe local callbacks.\nTYPE:\nCallbacks\nDEFAULT:\nNone\nverbose\nWhether to enable verbose mode.\nTYPE:\nbool\nDEFAULT:\nFalse\ninheritable_tags\nThe inheritable tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nlocal_tags\nThe local tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ninheritable_metadata\nThe inheritable metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nlocal_metadata\nThe local metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nAsyncCallbackManager\nThe configured async callback manager.\n__init__\n\u00b6\n__init__\n(\nhandlers\n:\nlist\n[\nBaseCallbackHandler\n],\ninheritable_handlers\n:\nlist\n[\nBaseCallbackHandler\n]\n|\nNone\n=\nNone\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n*\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninheritable_tags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ninheritable_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n)\n->\nNone\nInitialize callback manager.\nPARAMETER\nDESCRIPTION\nhandlers\nThe handlers.\nTYPE:\nlist\n[\nBaseCallbackHandler\n]\ninheritable_handlers\nThe inheritable handlers.\nTYPE:\nlist\n[\nBaseCallbackHandler\n] | None\nDEFAULT:\nNone\nparent_run_id\nThe parent run ID.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\ninheritable_tags\nThe inheritable tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ninheritable_metadata\nThe inheritable metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ncopy\n\u00b6\ncopy\n()\n->\nSelf\nReturn a copy of the callback manager.\nmerge\n\u00b6\nmerge\n(\nother\n:\nBaseCallbackManager\n)\n->\nSelf\nMerge the callback manager with another callback manager.\nMay be overwritten in subclasses. Primarily used internally\nwithin merge_configs.\nRETURNS\nDESCRIPTION\nSelf\nThe merged callback manager of the same type as the current object.\nExample: Merging two callback managers.\n```python\nfrom langchain_core.callbacks.manager import (\nCallbackManager,\ntrace_as_chain_group,\n)\nfrom langchain_core.callbacks.stdout import StdOutCallbackHandler\nmanager = CallbackManager(handlers=[StdOutCallbackHandler()], tags=[\"tag2\"])\nwith trace_as_chain_group(\"My Group Name\", tags=[\"tag1\"]) as group_manager:\nmerged_manager = group_manager.merge(manager)\nprint(merged_manager.handlers)\n# [\n#    <langchain_core.callbacks.stdout.StdOutCallbackHandler object at ...>,\n#    <langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler object at ...>,\n# ]\nprint(merged_manager.tags)\n#    ['tag2', 'tag1']\n```\nadd_handler\n\u00b6\nadd_handler\n(\nhandler\n:\nBaseCallbackHandler\n,\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nAdd a handler to the callback manager.\nPARAMETER\nDESCRIPTION\nhandler\nThe handler to add.\nTYPE:\nBaseCallbackHandler\ninherit\nWhether to inherit the handler.\nTYPE:\nbool\nDEFAULT:\nTrue\nremove_handler\n\u00b6\nremove_handler\n(\nhandler\n:\nBaseCallbackHandler\n)\n->\nNone\nRemove a handler from the callback manager.\nPARAMETER\nDESCRIPTION\nhandler\nThe handler to remove.\nTYPE:\nBaseCallbackHandler\nset_handlers\n\u00b6\nset_handlers\n(\nhandlers\n:\nlist\n[\nBaseCallbackHandler\n],\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nSet handlers as the only handlers on the callback manager.\nPARAMETER\nDESCRIPTION\nhandlers\nThe handlers to set.\nTYPE:\nlist\n[\nBaseCallbackHandler\n]\ninherit\nWhether to inherit the handlers.\nTYPE:\nbool\nDEFAULT:\nTrue\nset_handler\n\u00b6\nset_handler\n(\nhandler\n:\nBaseCallbackHandler\n,\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nSet handler as the only handler on the callback manager.\nPARAMETER\nDESCRIPTION\nhandler\nThe handler to set.\nTYPE:\nBaseCallbackHandler\ninherit\nWhether to inherit the handler.\nTYPE:\nbool\nDEFAULT:\nTrue\nadd_tags\n\u00b6\nadd_tags\n(\ntags\n:\nlist\n[\nstr\n],\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nAdd tags to the callback manager.\nPARAMETER\nDESCRIPTION\ntags\nThe tags to add.\nTYPE:\nlist\n[\nstr\n]\ninherit\nWhether to inherit the tags.\nTYPE:\nbool\nDEFAULT:\nTrue\nremove_tags\n\u00b6\nremove_tags\n(\ntags\n:\nlist\n[\nstr\n])\n->\nNone\nRemove tags from the callback manager.\nPARAMETER\nDESCRIPTION\ntags\nThe tags to remove.\nTYPE:\nlist\n[\nstr\n]\nadd_metadata\n\u00b6\nadd_metadata\n(\nmetadata\n:\ndict\n[\nstr\n,\nAny\n],\ninherit\n:\nbool\n=\nTrue\n)\n->\nNone\nAdd metadata to the callback manager.\nPARAMETER\nDESCRIPTION\nmetadata\nThe metadata to add.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninherit\nWhether to inherit the metadata.\nTYPE:\nbool\nDEFAULT:\nTrue\nremove_metadata\n\u00b6\nremove_metadata\n(\nkeys\n:\nlist\n[\nstr\n])\n->\nNone\nRemove metadata from the callback manager.\nPARAMETER\nDESCRIPTION\nkeys\nThe keys to remove.\nTYPE:\nlist\n[\nstr\n]\nUsageMetadataCallbackHandler\n\u00b6\nBases:\nBaseCallbackHandler\nCallback Handler that tracks AIMessage.usage_metadata.\nExample\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain_core.callbacks\nimport\nUsageMetadataCallbackHandler\nllm_1\n=\ninit_chat_model\n(\nmodel\n=\n\"openai:gpt-5-mini\"\n)\nllm_2\n=\ninit_chat_model\n(\nmodel\n=\n\"anthropic:claude-3-5-haiku-20241022\"\n)\ncallback\n=\nUsageMetadataCallbackHandler\n()\nresult_1\n=\nllm_1\n.\ninvoke\n(\n\"Hello\"\n,\nconfig\n=\n{\n\"callbacks\"\n:\n[\ncallback\n]})\nresult_2\n=\nllm_2\n.\ninvoke\n(\n\"Hello\"\n,\nconfig\n=\n{\n\"callbacks\"\n:\n[\ncallback\n]})\ncallback\n.\nusage_metadata\n{'gpt-5-mini-2024-07-18': {'input_tokens': 8,\n'output_tokens': 10,\n'total_tokens': 18,\n'input_token_details': {'audio': 0, 'cache_read': 0},\n'output_token_details': {'audio': 0, 'reasoning': 0}},\n'claude-3-5-haiku-20241022': {'input_tokens': 8,\n'output_tokens': 21,\n'total_tokens': 29,\n'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}\nAdded in\nlangchain-core\n0.3.49\nMETHOD\nDESCRIPTION\n__init__\nInitialize the UsageMetadataCallbackHandler.\non_llm_end\nCollect token usage.\non_text\nRun on an arbitrary text.\non_retry\nRun on a retry event.\non_custom_event\nOverride to define a handler for a custom event.\non_llm_start\nRun when LLM starts running.\non_chat_model_start\nRun when a chat model starts running.\non_retriever_start\nRun when the Retriever starts running.\non_chain_start\nRun when a chain starts running.\non_tool_start\nRun when the tool starts running.\non_retriever_error\nRun when Retriever errors.\non_retriever_end\nRun when Retriever ends running.\non_tool_end\nRun when the tool ends running.\non_tool_error\nRun when tool errors.\non_chain_end\nRun when chain ends running.\non_chain_error\nRun when chain errors.\non_agent_action\nRun on agent action.\non_agent_finish\nRun on the agent end.\non_llm_new_token\nRun on new output token. Only available when streaming is enabled.\non_llm_error\nRun when LLM errors.\nraise_error\nclass-attribute\ninstance-attribute\n\u00b6\nraise_error\n:\nbool\n=\nFalse\nWhether to raise an error if an exception occurs.\nrun_inline\nclass-attribute\ninstance-attribute\n\u00b6\nrun_inline\n:\nbool\n=\nFalse\nWhether to run the callback inline.\nignore_llm\nproperty\n\u00b6\nignore_llm\n:\nbool\nWhether to ignore LLM callbacks.\nignore_retry\nproperty\n\u00b6\nignore_retry\n:\nbool\nWhether to ignore retry callbacks.\nignore_chain\nproperty\n\u00b6\nignore_chain\n:\nbool\nWhether to ignore chain callbacks.\nignore_agent\nproperty\n\u00b6\nignore_agent\n:\nbool\nWhether to ignore agent callbacks.\nignore_retriever\nproperty\n\u00b6\nignore_retriever\n:\nbool\nWhether to ignore retriever callbacks.\nignore_chat_model\nproperty\n\u00b6\nignore_chat_model\n:\nbool\nWhether to ignore chat model callbacks.\nignore_custom_event\nproperty\n\u00b6\nignore_custom_event\n:\nbool\nIgnore custom event.\n__init__\n\u00b6\n__init__\n()\n->\nNone\nInitialize the UsageMetadataCallbackHandler.\non_llm_end\n\u00b6\non_llm_end\n(\nresponse\n:\nLLMResult\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nCollect token usage.\non_text\n\u00b6\non_text\n(\ntext\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAny\nRun on an arbitrary text.\nPARAMETER\nDESCRIPTION\ntext\nThe text.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retry\n\u00b6\non_retry\n(\nretry_state\n:\nRetryCallState\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun on a retry event.\nPARAMETER\nDESCRIPTION\nretry_state\nThe retry state.\nTYPE:\nRetryCallState\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_custom_event\n\u00b6\non_custom_event\n(\nname\n:\nstr\n,\ndata\n:\nAny\n,\n*\n,\nrun_id\n:\nUUID\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nOverride to define a handler for a custom event.\nPARAMETER\nDESCRIPTION\nname\nThe name of the custom event.\nTYPE:\nstr\ndata\nThe data for the custom event. Format will match\nthe format specified by the user.\nTYPE:\nAny\nrun_id\nThe ID of the run.\nTYPE:\nUUID\ntags\nThe tags associated with the custom event\n(includes inherited tags).\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata associated with the custom event\n(includes inherited metadata).\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\non_llm_start\n\u00b6\non_llm_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nprompts\n:\nlist\n[\nstr\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when LLM starts running.\nWarning\nThis method is called for non-chat models (regular LLMs). If you're\nimplementing a handler for a chat model, you should use\non_chat_model_start\ninstead.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized LLM.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nprompts\nThe prompts.\nTYPE:\nlist\n[\nstr\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chat_model_start\n\u00b6\non_chat_model_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nmessages\n:\nlist\n[\nlist\n[\nBaseMessage\n]],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when a chat model starts running.\nWarning\nThis method is called for chat models. If you're implementing a handler for\na non-chat model, you should use\non_llm_start\ninstead.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chat model.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nmessages\nThe messages.\nTYPE:\nlist\n[\nlist\n[\nBaseMessage\n]]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_start\n\u00b6\non_retriever_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\nquery\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when the Retriever starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized Retriever.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nquery\nThe query.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_start\n\u00b6\non_chain_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\ninputs\n:\ndict\n[\nstr\n,\nAny\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when a chain starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninputs\nThe inputs.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_start\n\u00b6\non_tool_start\n(\nserialized\n:\ndict\n[\nstr\n,\nAny\n],\ninput_str\n:\nstr\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ninputs\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when the tool starts running.\nPARAMETER\nDESCRIPTION\nserialized\nThe serialized chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\ninput_str\nThe input string.\nTYPE:\nstr\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\ntags\nThe tags.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\nmetadata\nThe metadata.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\ninputs\nThe inputs.\nTYPE:\ndict\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_error\n\u00b6\non_retriever_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when Retriever errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_retriever_end\n\u00b6\non_retriever_end\n(\ndocuments\n:\nSequence\n[\nDocument\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when Retriever ends running.\nPARAMETER\nDESCRIPTION\ndocuments\nThe documents retrieved.\nTYPE:\nSequence\n[\nDocument\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_end\n\u00b6\non_tool_end\n(\noutput\n:\nAny\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAny\nRun when the tool ends running.\nPARAMETER\nDESCRIPTION\noutput\nThe output of the tool.\nTYPE:\nAny\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_tool_error\n\u00b6\non_tool_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when tool errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_end\n\u00b6\non_chain_end\n(\noutputs\n:\ndict\n[\nstr\n,\nAny\n],\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when chain ends running.\nPARAMETER\nDESCRIPTION\noutputs\nThe outputs of the chain.\nTYPE:\ndict\n[\nstr\n,\nAny\n]\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_chain_error\n\u00b6\non_chain_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when chain errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_agent_action\n\u00b6\non_agent_action\n(\naction\n:\nAgentAction\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun on agent action.\nPARAMETER\nDESCRIPTION\naction\nThe agent action.\nTYPE:\nAgentAction\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_agent_finish\n\u00b6\non_agent_finish\n(\nfinish\n:\nAgentFinish\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun on the agent end.\nPARAMETER\nDESCRIPTION\nfinish\nThe agent finish.\nTYPE:\nAgentFinish\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_llm_new_token\n\u00b6\non_llm_new_token\n(\ntoken\n:\nstr\n,\n*\n,\nchunk\n:\nGenerationChunk\n|\nChatGenerationChunk\n|\nNone\n=\nNone\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun on new output token. Only available when streaming is enabled.\nFor both chat models and non-chat models (legacy LLMs).\nPARAMETER\nDESCRIPTION\ntoken\nThe new token.\nTYPE:\nstr\nchunk\nThe new generated chunk, containing content and other information.\nTYPE:\nGenerationChunk\n|\nChatGenerationChunk\n| None\nDEFAULT:\nNone\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\non_llm_error\n\u00b6\non_llm_error\n(\nerror\n:\nBaseException\n,\n*\n,\nrun_id\n:\nUUID\n,\nparent_run_id\n:\nUUID\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAny\nRun when LLM errors.\nPARAMETER\nDESCRIPTION\nerror\nThe error that occurred.\nTYPE:\nBaseException\nrun_id\nThe run ID. This is the ID of the current run.\nTYPE:\nUUID\nparent_run_id\nThe parent run ID. This is the ID of the parent run.\nTYPE:\nUUID\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nget_usage_metadata_callback\n\u00b6\nget_usage_metadata_callback\n(\nname\n:\nstr\n=\n\"usage_metadata_callback\"\n,\n)\n->\nGenerator\n[\nUsageMetadataCallbackHandler\n,\nNone\n,\nNone\n]\nGet usage metadata callback.\nGet context manager for tracking usage metadata across chat model calls using\nAIMessage.usage_metadata\n.\nPARAMETER\nDESCRIPTION\nname\nThe name of the context variable.\nTYPE:\nstr\nDEFAULT:\n'usage_metadata_callback'\nYIELDS\nDESCRIPTION\nUsageMetadataCallbackHandler\nThe usage metadata callback.\nExample\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain_core.callbacks\nimport\nget_usage_metadata_callback\nllm_1\n=\ninit_chat_model\n(\nmodel\n=\n\"openai:gpt-5-mini\"\n)\nllm_2\n=\ninit_chat_model\n(\nmodel\n=\n\"anthropic:claude-3-5-haiku-20241022\"\n)\nwith\nget_usage_metadata_callback\n()\nas\ncb\n:\nllm_1\n.\ninvoke\n(\n\"Hello\"\n)\nllm_2\n.\ninvoke\n(\n\"Hello\"\n)\nprint\n(\ncb\n.\nusage_metadata\n)\n{\n\"gpt-5-mini-2024-07-18\": {\n\"input_tokens\": 8,\n\"output_tokens\": 10,\n\"total_tokens\": 18,\n\"input_token_details\": {\"audio\": 0, \"cache_read\": 0},\n\"output_token_details\": {\"audio\": 0, \"reasoning\": 0},\n},\n\"claude-3-5-haiku-20241022\": {\n\"input_tokens\": 8,\n\"output_tokens\": 21,\n\"total_tokens\": 29,\n\"input_token_details\": {\"cache_read\": 0, \"cache_creation\": 0},\n},\n}\nAdded in\nlangchain-core\n0.3.49\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain_core/callbacks/",
      "title": "Callbacks | LangChain Reference",
      "heading": "Callbacks"
    }
  },
  {
    "page_content": "Documents | LangChain Reference\nSkip to content\nLangChain Reference\nDocuments\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocuments\nTable of contents\ndocuments\nDocument\npage_content\nlc_secrets\nlc_attributes\nid\nmetadata\n__init__\nis_lc_serializable\nget_lc_namespace\n__str__\nlc_id\nto_json\nto_json_not_implemented\nBlob\ndata\nmimetype\nencoding\npath\nsource\nlc_secrets\nlc_attributes\nid\nmetadata\ncheck_blob_is_valid\nas_string\nas_bytes\nas_bytes_io\nfrom_path\nfrom_data\n__repr__\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json\nto_json_not_implemented\nBaseMedia\nid\nmetadata\nlc_secrets\nlc_attributes\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json\nto_json_not_implemented\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\ndocuments\nDocument\npage_content\nlc_secrets\nlc_attributes\nid\nmetadata\n__init__\nis_lc_serializable\nget_lc_namespace\n__str__\nlc_id\nto_json\nto_json_not_implemented\nBlob\ndata\nmimetype\nencoding\npath\nsource\nlc_secrets\nlc_attributes\nid\nmetadata\ncheck_blob_is_valid\nas_string\nas_bytes\nas_bytes_io\nfrom_path\nfrom_data\n__repr__\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json\nto_json_not_implemented\nBaseMedia\nid\nmetadata\nlc_secrets\nlc_attributes\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json\nto_json_not_implemented\nDocuments\ndocuments\n\u00b6\nDocuments module for data retrieval and processing workflows.\nThis module provides core abstractions for handling data in retrieval-augmented\ngeneration (RAG) pipelines, vector stores, and document processing workflows.\nDocuments vs. message content\nThis module is distinct from\nlangchain_core.messages.content\n, which provides\nmultimodal content blocks for\nLLM chat I/O\n(text, images, audio, etc. within\nmessages).\nKey distinction:\nDocuments\n(this module): For\ndata retrieval and processing workflows\nVector stores, retrievers, RAG pipelines\nText chunking, embedding, and semantic search\nExample: Chunks of a PDF stored in a vector database\nContent Blocks\n(\nmessages.content\n): For\nLLM conversational I/O\nMultimodal message content sent to/from models\nTool calls, reasoning, citations within chat\nExample: An image sent to a vision model in a chat message (via\nImageContentBlock\n)\nWhile both can represent similar data types (text, files), they serve different\narchitectural purposes in LangChain applications.\nDocument\n\u00b6\nBases:\nBaseMedia\nClass for storing a piece of text and associated metadata.\nNote\nDocument\nis for\nretrieval workflows\n, not chat I/O. For sending text\nto an LLM in a conversation, use message types from\nlangchain.messages\n.\nExample\nfrom\nlangchain_core.documents\nimport\nDocument\ndocument\n=\nDocument\n(\npage_content\n=\n\"Hello, world!\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"https://example.com\"\n}\n)\nMETHOD\nDESCRIPTION\n__init__\nPass page_content in as positional or named arg.\nis_lc_serializable\nReturn\nTrue\nas this class is serializable.\nget_lc_namespace\nGet the namespace of the LangChain object.\n__str__\nOverride\n__str__\nto restrict it to page_content and metadata.\nlc_id\nReturn a unique identifier for this class for serialization purposes.\nto_json\nSerialize the object to JSON.\nto_json_not_implemented\nSerialize a \"not implemented\" object.\npage_content\ninstance-attribute\n\u00b6\npage_content\n:\nstr\nString text.\nlc_secrets\nproperty\n\u00b6\nlc_secrets\n:\ndict\n[\nstr\n,\nstr\n]\nA map of constructor argument names to secret ids.\nFor example,\n{\"openai_api_key\": \"OPENAI_API_KEY\"}\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nList of attribute names that should be included in the serialized kwargs.\nThese attributes must be accepted by the constructor.\nDefault is an empty dictionary.\nid\nclass-attribute\ninstance-attribute\n\u00b6\nid\n:\nstr\n|\nNone\n=\nField\n(\ndefault\n=\nNone\n,\ncoerce_numbers_to_str\n=\nTrue\n)\nAn optional identifier for the document.\nIdeally this should be unique across the document collection and formatted\nas a UUID, but this will not be enforced.\nmetadata\nclass-attribute\ninstance-attribute\n\u00b6\nmetadata\n:\ndict\n=\nField\n(\ndefault_factory\n=\ndict\n)\nArbitrary metadata associated with the content.\n__init__\n\u00b6\n__init__\n(\npage_content\n:\nstr\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nPass page_content in as positional or named arg.\nis_lc_serializable\nclassmethod\n\u00b6\nis_lc_serializable\n()\n->\nbool\nReturn\nTrue\nas this class is serializable.\nget_lc_namespace\nclassmethod\n\u00b6\nget_lc_namespace\n()\n->\nlist\n[\nstr\n]\nGet the namespace of the LangChain object.\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\n[\"langchain\", \"schema\", \"document\"]\n__str__\n\u00b6\n__str__\n()\n->\nstr\nOverride\n__str__\nto restrict it to page_content and metadata.\nRETURNS\nDESCRIPTION\nstr\nA string representation of the\nDocument\n.\nlc_id\nclassmethod\n\u00b6\nlc_id\n()\n->\nlist\n[\nstr\n]\nReturn a unique identifier for this class for serialization purposes.\nThe unique identifier is a list of strings that describes the path\nto the object.\nFor example, for the class\nlangchain.llms.openai.OpenAI\n, the id is\n[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]\n.\nto_json\n\u00b6\nto_json\n()\n->\nSerializedConstructor\n|\nSerializedNotImplemented\nSerialize the object to JSON.\nRAISES\nDESCRIPTION\nValueError\nIf the class has deprecated attributes.\nRETURNS\nDESCRIPTION\nSerializedConstructor\n|\nSerializedNotImplemented\nA JSON serializable object or a\nSerializedNotImplemented\nobject.\nto_json_not_implemented\n\u00b6\nto_json_not_implemented\n()\n->\nSerializedNotImplemented\nSerialize a \"not implemented\" object.\nRETURNS\nDESCRIPTION\nSerializedNotImplemented\nSerializedNotImplemented\n.\nBlob\n\u00b6\nBases:\nBaseMedia\nRaw data abstraction for document loading and file processing.\nRepresents raw bytes or text, either in-memory or by file reference. Used\nprimarily by document loaders to decouple data loading from parsing.\nInspired by\nMozilla's\nBlob\nInitialize a blob from in-memory data\nfrom\nlangchain_core.documents\nimport\nBlob\nblob\n=\nBlob\n.\nfrom_data\n(\n\"Hello, world!\"\n)\n# Read the blob as a string\nprint\n(\nblob\n.\nas_string\n())\n# Read the blob as bytes\nprint\n(\nblob\n.\nas_bytes\n())\n# Read the blob as a byte stream\nwith\nblob\n.\nas_bytes_io\n()\nas\nf\n:\nprint\n(\nf\n.\nread\n())\nLoad from memory and specify MIME type and metadata\nfrom\nlangchain_core.documents\nimport\nBlob\nblob\n=\nBlob\n.\nfrom_data\n(\ndata\n=\n\"Hello, world!\"\n,\nmime_type\n=\n\"text/plain\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"https://example.com\"\n},\n)\nLoad the blob from a file\nfrom\nlangchain_core.documents\nimport\nBlob\nblob\n=\nBlob\n.\nfrom_path\n(\n\"path/to/file.txt\"\n)\n# Read the blob as a string\nprint\n(\nblob\n.\nas_string\n())\n# Read the blob as bytes\nprint\n(\nblob\n.\nas_bytes\n())\n# Read the blob as a byte stream\nwith\nblob\n.\nas_bytes_io\n()\nas\nf\n:\nprint\n(\nf\n.\nread\n())\nMETHOD\nDESCRIPTION\ncheck_blob_is_valid\nVerify that either data or path is provided.\nas_string\nRead data as a string.\nas_bytes\nRead data as bytes.\nas_bytes_io\nRead data as a byte stream.\nfrom_path\nLoad the blob from a path like object.\nfrom_data\nInitialize the\nBlob\nfrom in-memory data.\n__repr__\nReturn the blob representation.\n__init__\nis_lc_serializable\nIs this class serializable?\nget_lc_namespace\nGet the namespace of the LangChain object.\nlc_id\nReturn a unique identifier for this class for serialization purposes.\nto_json\nSerialize the object to JSON.\nto_json_not_implemented\nSerialize a \"not implemented\" object.\ndata\nclass-attribute\ninstance-attribute\n\u00b6\ndata\n:\nbytes\n|\nstr\n|\nNone\n=\nNone\nRaw data associated with the\nBlob\n.\nmimetype\nclass-attribute\ninstance-attribute\n\u00b6\nmimetype\n:\nstr\n|\nNone\n=\nNone\nMIME type, not to be confused with a file extension.\nencoding\nclass-attribute\ninstance-attribute\n\u00b6\nencoding\n:\nstr\n=\n'utf-8'\nEncoding to use if decoding the bytes into a string.\nUses\nutf-8\nas default encoding if decoding to string.\npath\nclass-attribute\ninstance-attribute\n\u00b6\npath\n:\nPathLike\n|\nNone\n=\nNone\nLocation where the original content was found.\nsource\nproperty\n\u00b6\nsource\n:\nstr\n|\nNone\nThe source location of the blob as string if known otherwise none.\nIf a path is associated with the\nBlob\n, it will default to the path location.\nUnless explicitly set via a metadata field called\n'source'\n, in which\ncase that value will be used instead.\nlc_secrets\nproperty\n\u00b6\nlc_secrets\n:\ndict\n[\nstr\n,\nstr\n]\nA map of constructor argument names to secret ids.\nFor example,\n{\"openai_api_key\": \"OPENAI_API_KEY\"}\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nList of attribute names that should be included in the serialized kwargs.\nThese attributes must be accepted by the constructor.\nDefault is an empty dictionary.\nid\nclass-attribute\ninstance-attribute\n\u00b6\nid\n:\nstr\n|\nNone\n=\nField\n(\ndefault\n=\nNone\n,\ncoerce_numbers_to_str\n=\nTrue\n)\nAn optional identifier for the document.\nIdeally this should be unique across the document collection and formatted\nas a UUID, but this will not be enforced.\nmetadata\nclass-attribute\ninstance-attribute\n\u00b6\nmetadata\n:\ndict\n=\nField\n(\ndefault_factory\n=\ndict\n)\nArbitrary metadata associated with the content.\ncheck_blob_is_valid\nclassmethod\n\u00b6\ncheck_blob_is_valid\n(\nvalues\n:\ndict\n[\nstr\n,\nAny\n])\n->\nAny\nVerify that either data or path is provided.\nas_string\n\u00b6\nas_string\n()\n->\nstr\nRead data as a string.\nRAISES\nDESCRIPTION\nValueError\nIf the blob cannot be represented as a string.\nRETURNS\nDESCRIPTION\nstr\nThe data as a string.\nas_bytes\n\u00b6\nas_bytes\n()\n->\nbytes\nRead data as bytes.\nRAISES\nDESCRIPTION\nValueError\nIf the blob cannot be represented as bytes.\nRETURNS\nDESCRIPTION\nbytes\nThe data as bytes.\nas_bytes_io\n\u00b6\nas_bytes_io\n()\n->\nGenerator\n[\nBytesIO\n|\nBufferedReader\n,\nNone\n,\nNone\n]\nRead data as a byte stream.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the blob cannot be represented as a byte stream.\nYIELDS\nDESCRIPTION\nBytesIO\n|\nBufferedReader\nThe data as a byte stream.\nfrom_path\nclassmethod\n\u00b6\nfrom_path\n(\npath\n:\nPathLike\n,\n*\n,\nencoding\n:\nstr\n=\n\"utf-8\"\n,\nmime_type\n:\nstr\n|\nNone\n=\nNone\n,\nguess_type\n:\nbool\n=\nTrue\n,\nmetadata\n:\ndict\n|\nNone\n=\nNone\n,\n)\n->\nBlob\nLoad the blob from a path like object.\nPARAMETER\nDESCRIPTION\npath\nPath-like object to file to be read\nTYPE:\nPathLike\nencoding\nEncoding to use if decoding the bytes into a string\nTYPE:\nstr\nDEFAULT:\n'utf-8'\nmime_type\nIf provided, will be set as the MIME type of the data\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nguess_type\nIf\nTrue\n, the MIME type will be guessed from the file\nextension, if a MIME type was not provided\nTYPE:\nbool\nDEFAULT:\nTrue\nmetadata\nMetadata to associate with the\nBlob\nTYPE:\ndict\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBlob\nBlob\ninstance\nfrom_data\nclassmethod\n\u00b6\nfrom_data\n(\ndata\n:\nstr\n|\nbytes\n,\n*\n,\nencoding\n:\nstr\n=\n\"utf-8\"\n,\nmime_type\n:\nstr\n|\nNone\n=\nNone\n,\npath\n:\nstr\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n|\nNone\n=\nNone\n,\n)\n->\nBlob\nInitialize the\nBlob\nfrom in-memory data.\nPARAMETER\nDESCRIPTION\ndata\nThe in-memory data associated with the\nBlob\nTYPE:\nstr\n|\nbytes\nencoding\nEncoding to use if decoding the bytes into a string\nTYPE:\nstr\nDEFAULT:\n'utf-8'\nmime_type\nIf provided, will be set as the MIME type of the data\nTYPE:\nstr\n| None\nDEFAULT:\nNone\npath\nIf provided, will be set as the source from which the data came\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nmetadata\nMetadata to associate with the\nBlob\nTYPE:\ndict\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBlob\nBlob\ninstance\n__repr__\n\u00b6\n__repr__\n()\n->\nstr\nReturn the blob representation.\n__init__\n\u00b6\n__init__\n(\n*\nargs\n:\nAny\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nis_lc_serializable\nclassmethod\n\u00b6\nis_lc_serializable\n()\n->\nbool\nIs this class serializable?\nBy design, even if a class inherits from\nSerializable\n, it is not serializable\nby default. This is to prevent accidental serialization of objects that should\nnot be serialized.\nRETURNS\nDESCRIPTION\nbool\nWhether the class is serializable. Default is\nFalse\n.\nget_lc_namespace\nclassmethod\n\u00b6\nget_lc_namespace\n()\n->\nlist\n[\nstr\n]\nGet the namespace of the LangChain object.\nFor example, if the class is\nlangchain.llms.openai.OpenAI\n,\nthen the namespace is\n[\"langchain\", \"llms\", \"openai\"]\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nThe namespace.\nlc_id\nclassmethod\n\u00b6\nlc_id\n()\n->\nlist\n[\nstr\n]\nReturn a unique identifier for this class for serialization purposes.\nThe unique identifier is a list of strings that describes the path\nto the object.\nFor example, for the class\nlangchain.llms.openai.OpenAI\n, the id is\n[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]\n.\nto_json\n\u00b6\nto_json\n()\n->\nSerializedConstructor\n|\nSerializedNotImplemented\nSerialize the object to JSON.\nRAISES\nDESCRIPTION\nValueError\nIf the class has deprecated attributes.\nRETURNS\nDESCRIPTION\nSerializedConstructor\n|\nSerializedNotImplemented\nA JSON serializable object or a\nSerializedNotImplemented\nobject.\nto_json_not_implemented\n\u00b6\nto_json_not_implemented\n()\n->\nSerializedNotImplemented\nSerialize a \"not implemented\" object.\nRETURNS\nDESCRIPTION\nSerializedNotImplemented\nSerializedNotImplemented\n.\nBaseMedia\n\u00b6\nBases:\nSerializable\nBase class for content used in retrieval and data processing workflows.\nProvides common fields for content that needs to be stored, indexed, or searched.\nNote\nFor multimodal content in\nchat messages\n(images, audio sent to/from LLMs),\nuse\nlangchain.messages\ncontent blocks instead.\nMETHOD\nDESCRIPTION\n__init__\nis_lc_serializable\nIs this class serializable?\nget_lc_namespace\nGet the namespace of the LangChain object.\nlc_id\nReturn a unique identifier for this class for serialization purposes.\nto_json\nSerialize the object to JSON.\nto_json_not_implemented\nSerialize a \"not implemented\" object.\nid\nclass-attribute\ninstance-attribute\n\u00b6\nid\n:\nstr\n|\nNone\n=\nField\n(\ndefault\n=\nNone\n,\ncoerce_numbers_to_str\n=\nTrue\n)\nAn optional identifier for the document.\nIdeally this should be unique across the document collection and formatted\nas a UUID, but this will not be enforced.\nmetadata\nclass-attribute\ninstance-attribute\n\u00b6\nmetadata\n:\ndict\n=\nField\n(\ndefault_factory\n=\ndict\n)\nArbitrary metadata associated with the content.\nlc_secrets\nproperty\n\u00b6\nlc_secrets\n:\ndict\n[\nstr\n,\nstr\n]\nA map of constructor argument names to secret ids.\nFor example,\n{\"openai_api_key\": \"OPENAI_API_KEY\"}\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nList of attribute names that should be included in the serialized kwargs.\nThese attributes must be accepted by the constructor.\nDefault is an empty dictionary.\n__init__\n\u00b6\n__init__\n(\n*\nargs\n:\nAny\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nis_lc_serializable\nclassmethod\n\u00b6\nis_lc_serializable\n()\n->\nbool\nIs this class serializable?\nBy design, even if a class inherits from\nSerializable\n, it is not serializable\nby default. This is to prevent accidental serialization of objects that should\nnot be serialized.\nRETURNS\nDESCRIPTION\nbool\nWhether the class is serializable. Default is\nFalse\n.\nget_lc_namespace\nclassmethod\n\u00b6\nget_lc_namespace\n()\n->\nlist\n[\nstr\n]\nGet the namespace of the LangChain object.\nFor example, if the class is\nlangchain.llms.openai.OpenAI\n,\nthen the namespace is\n[\"langchain\", \"llms\", \"openai\"]\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nThe namespace.\nlc_id\nclassmethod\n\u00b6\nlc_id\n()\n->\nlist\n[\nstr\n]\nReturn a unique identifier for this class for serialization purposes.\nThe unique identifier is a list of strings that describes the path\nto the object.\nFor example, for the class\nlangchain.llms.openai.OpenAI\n, the id is\n[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]\n.\nto_json\n\u00b6\nto_json\n()\n->\nSerializedConstructor\n|\nSerializedNotImplemented\nSerialize the object to JSON.\nRAISES\nDESCRIPTION\nValueError\nIf the class has deprecated attributes.\nRETURNS\nDESCRIPTION\nSerializedConstructor\n|\nSerializedNotImplemented\nA JSON serializable object or a\nSerializedNotImplemented\nobject.\nto_json_not_implemented\n\u00b6\nto_json_not_implemented\n()\n->\nSerializedNotImplemented\nSerialize a \"not implemented\" object.\nRETURNS\nDESCRIPTION\nSerializedNotImplemented\nSerializedNotImplemented\n.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain_core/documents/",
      "title": "Documents | LangChain Reference",
      "heading": "Documents"
    }
  },
  {
    "page_content": "Document loaders | LangChain Reference\nSkip to content\nLangChain Reference\nDocument loaders\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocument loaders\nDocument loaders\nTable of contents\ndocument_loaders\nBaseLoader\nload\naload\nload_and_split\nlazy_load\nalazy_load\nBaseBlobParser\nlazy_parse\nparse\nBlobLoader\nyield_blobs\nLangSmithLoader\nload\naload\nload_and_split\nalazy_load\n__init__\nlazy_load\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\ndocument_loaders\nBaseLoader\nload\naload\nload_and_split\nlazy_load\nalazy_load\nBaseBlobParser\nlazy_parse\nparse\nBlobLoader\nyield_blobs\nLangSmithLoader\nload\naload\nload_and_split\nalazy_load\n__init__\nlazy_load\nDocument loaders\ndocument_loaders\n\u00b6\nDocument loaders.\nBaseLoader\n\u00b6\nBases:\nABC\nInterface for Document Loader.\nImplementations should implement the lazy-loading method using generators\nto avoid loading all documents into memory at once.\nload\nis provided just for user convenience and should not be overridden.\nMETHOD\nDESCRIPTION\nload\nLoad data into\nDocument\nobjects.\naload\nLoad data into\nDocument\nobjects.\nload_and_split\nLoad\nDocument\nand split into chunks. Chunks are returned as\nDocument\n.\nlazy_load\nA lazy loader for\nDocument\n.\nalazy_load\nA lazy loader for\nDocument\n.\nload\n\u00b6\nload\n()\n->\nlist\n[\nDocument\n]\nLoad data into\nDocument\nobjects.\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nThe documents.\naload\nasync\n\u00b6\naload\n()\n->\nlist\n[\nDocument\n]\nLoad data into\nDocument\nobjects.\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nThe documents.\nload_and_split\n\u00b6\nload_and_split\n(\ntext_splitter\n:\nTextSplitter\n|\nNone\n=\nNone\n)\n->\nlist\n[\nDocument\n]\nLoad\nDocument\nand split into chunks. Chunks are returned as\nDocument\n.\nDanger\nDo not override this method. It should be considered to be deprecated!\nPARAMETER\nDESCRIPTION\ntext_splitter\nTextSplitter\ninstance to use for splitting documents.\nDefaults to\nRecursiveCharacterTextSplitter\n.\nTYPE:\nTextSplitter\n| None\nDEFAULT:\nNone\nRAISES\nDESCRIPTION\nImportError\nIf\nlangchain-text-splitters\nis not installed\nand no\ntext_splitter\nis provided.\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\n.\nlazy_load\n\u00b6\nlazy_load\n()\n->\nIterator\n[\nDocument\n]\nA lazy loader for\nDocument\n.\nYIELDS\nDESCRIPTION\nDocument\nThe\nDocument\nobjects.\nalazy_load\nasync\n\u00b6\nalazy_load\n()\n->\nAsyncIterator\n[\nDocument\n]\nA lazy loader for\nDocument\n.\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nDocument\n]\nThe\nDocument\nobjects.\nBaseBlobParser\n\u00b6\nBases:\nABC\nAbstract interface for blob parsers.\nA blob parser provides a way to parse raw data stored in a blob into one\nor more\nDocument\nobjects.\nThe parser can be composed with blob loaders, making it easy to reuse\na parser independent of how the blob was originally loaded.\nMETHOD\nDESCRIPTION\nlazy_parse\nLazy parsing interface.\nparse\nEagerly parse the blob into a\nDocument\nor list of\nDocument\nobjects.\nlazy_parse\nabstractmethod\n\u00b6\nlazy_parse\n(\nblob\n:\nBlob\n)\n->\nIterator\n[\nDocument\n]\nLazy parsing interface.\nSubclasses are required to implement this method.\nPARAMETER\nDESCRIPTION\nblob\nBlob\ninstance\nTYPE:\nBlob\nRETURNS\nDESCRIPTION\nIterator\n[\nDocument\n]\nGenerator of\nDocument\nobjects\nparse\n\u00b6\nparse\n(\nblob\n:\nBlob\n)\n->\nlist\n[\nDocument\n]\nEagerly parse the blob into a\nDocument\nor list of\nDocument\nobjects.\nThis is a convenience method for interactive development environment.\nProduction applications should favor the\nlazy_parse\nmethod instead.\nSubclasses should generally not over-ride this parse method.\nPARAMETER\nDESCRIPTION\nblob\nBlob\ninstance\nTYPE:\nBlob\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects\nBlobLoader\n\u00b6\nBases:\nABC\nAbstract interface for blob loaders implementation.\nImplementer should be able to load raw content from a storage system according\nto some criteria and return the raw content lazily as a stream of blobs.\nMETHOD\nDESCRIPTION\nyield_blobs\nA lazy loader for raw data represented by LangChain's\nBlob\nobject.\nyield_blobs\nabstractmethod\n\u00b6\nyield_blobs\n()\n->\nIterable\n[\nBlob\n]\nA lazy loader for raw data represented by LangChain's\nBlob\nobject.\nRETURNS\nDESCRIPTION\nIterable\n[\nBlob\n]\nA generator over blobs\nLangSmithLoader\n\u00b6\nBases:\nBaseLoader\nLoad LangSmith Dataset examples as\nDocument\nobjects.\nLoads the example inputs as the\nDocument\npage content and places the entire\nexample into the\nDocument\nmetadata. This allows you to easily create few-shot\nexample retrievers from the loaded documents.\nLazy loading example\nfrom\nlangchain_core.document_loaders\nimport\nLangSmithLoader\nloader\n=\nLangSmithLoader\n(\ndataset_id\n=\n\"...\"\n,\nlimit\n=\n100\n)\ndocs\n=\n[]\nfor\ndoc\nin\nloader\n.\nlazy_load\n():\ndocs\n.\nappend\n(\ndoc\n)\n# -> [Document(\"...\", metadata={\"inputs\": {...}, \"outputs\": {...}, ...}), ...]\nMETHOD\nDESCRIPTION\nload\nLoad data into\nDocument\nobjects.\naload\nLoad data into\nDocument\nobjects.\nload_and_split\nLoad\nDocument\nand split into chunks. Chunks are returned as\nDocument\n.\nalazy_load\nA lazy loader for\nDocument\n.\n__init__\nCreate a LangSmith loader.\nlazy_load\nA lazy loader for\nDocument\n.\nload\n\u00b6\nload\n()\n->\nlist\n[\nDocument\n]\nLoad data into\nDocument\nobjects.\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nThe documents.\naload\nasync\n\u00b6\naload\n()\n->\nlist\n[\nDocument\n]\nLoad data into\nDocument\nobjects.\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nThe documents.\nload_and_split\n\u00b6\nload_and_split\n(\ntext_splitter\n:\nTextSplitter\n|\nNone\n=\nNone\n)\n->\nlist\n[\nDocument\n]\nLoad\nDocument\nand split into chunks. Chunks are returned as\nDocument\n.\nDanger\nDo not override this method. It should be considered to be deprecated!\nPARAMETER\nDESCRIPTION\ntext_splitter\nTextSplitter\ninstance to use for splitting documents.\nDefaults to\nRecursiveCharacterTextSplitter\n.\nTYPE:\nTextSplitter\n| None\nDEFAULT:\nNone\nRAISES\nDESCRIPTION\nImportError\nIf\nlangchain-text-splitters\nis not installed\nand no\ntext_splitter\nis provided.\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\n.\nalazy_load\nasync\n\u00b6\nalazy_load\n()\n->\nAsyncIterator\n[\nDocument\n]\nA lazy loader for\nDocument\n.\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nDocument\n]\nThe\nDocument\nobjects.\n__init__\n\u00b6\n__init__\n(\n*\n,\ndataset_id\n:\nUUID\n|\nstr\n|\nNone\n=\nNone\n,\ndataset_name\n:\nstr\n|\nNone\n=\nNone\n,\nexample_ids\n:\nSequence\n[\nUUID\n|\nstr\n]\n|\nNone\n=\nNone\n,\nas_of\n:\ndatetime\n|\nstr\n|\nNone\n=\nNone\n,\nsplits\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninline_s3_urls\n:\nbool\n=\nTrue\n,\noffset\n:\nint\n=\n0\n,\nlimit\n:\nint\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n|\nNone\n=\nNone\n,\nfilter\n:\nstr\n|\nNone\n=\nNone\n,\ncontent_key\n:\nstr\n=\n\"\"\n,\nformat_content\n:\nCallable\n[\n...\n,\nstr\n]\n|\nNone\n=\nNone\n,\nclient\n:\nClient\n|\nNone\n=\nNone\n,\n**\nclient_kwargs\n:\nAny\n,\n)\n->\nNone\nCreate a LangSmith loader.\nPARAMETER\nDESCRIPTION\ndataset_id\nThe ID of the dataset to filter by.\nTYPE:\nUUID\n|\nstr\n| None\nDEFAULT:\nNone\ndataset_name\nThe name of the dataset to filter by.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ncontent_key\nThe inputs key to set as Document page content.\n'.'\ncharacters\nare interpreted as nested keys. E.g.\ncontent_key=\"first.second\"\nwill\nresult in\nDocument(page_content=format_content(example.inputs[\"first\"][\"second\"]))\nTYPE:\nstr\nDEFAULT:\n''\nformat_content\nFunction for converting the content extracted from the example\ninputs into a string. Defaults to JSON-encoding the contents.\nTYPE:\nCallable\n[...,\nstr\n] | None\nDEFAULT:\nNone\nexample_ids\nThe IDs of the examples to filter by.\nTYPE:\nSequence\n[\nUUID\n|\nstr\n] | None\nDEFAULT:\nNone\nas_of\nThe dataset version tag or timestamp to retrieve the examples as of.\nResponse examples will only be those that were present at the time of\nthe tagged (or timestamped) version.\nTYPE:\ndatetime\n|\nstr\n| None\nDEFAULT:\nNone\nsplits\nA list of dataset splits, which are\ndivisions of your dataset such as\ntrain\n,\ntest\n, or\nvalidation\n.\nReturns examples only from the specified splits.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninline_s3_urls\nWhether to inline S3 URLs.\nTYPE:\nbool\nDEFAULT:\nTrue\noffset\nThe offset to start from.\nTYPE:\nint\nDEFAULT:\n0\nlimit\nThe maximum number of examples to return.\nTYPE:\nint\n| None\nDEFAULT:\nNone\nmetadata\nMetadata to filter by.\nTYPE:\ndict\n| None\nDEFAULT:\nNone\nfilter\nA structured filter string to apply to the examples.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nclient\nLangSmith Client. If not provided will be initialized from below args.\nTYPE:\nClient\n| None\nDEFAULT:\nNone\nclient_kwargs\nKeyword args to pass to LangSmith client init. Should only be\nspecified if\nclient\nisn't.\nTYPE:\nAny\nDEFAULT:\n{}\nRAISES\nDESCRIPTION\nValueError\nIf both\nclient\nand\nclient_kwargs\nare provided.\nlazy_load\n\u00b6\nlazy_load\n()\n->\nIterator\n[\nDocument\n]\nA lazy loader for\nDocument\n.\nYIELDS\nDESCRIPTION\nDocument\nThe\nDocument\nobjects.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain_core/document_loaders/",
      "title": "Document loaders | LangChain Reference",
      "heading": "Document loaders"
    }
  },
  {
    "page_content": "Prompts | LangChain Reference\nSkip to content\nLangChain Reference\nPrompts\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nPrompts\nTable of contents\nChatPromptTemplate\nmessages\nvalidate_template\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nlc_secrets\nlc_attributes\ninput_variables\noptional_variables\ninput_types\noutput_parser\npartial_variables\nmetadata\ntags\n__init__\nget_lc_namespace\n__add__\nvalidate_input_variables\nfrom_template\nfrom_messages\nformat_messages\naformat_messages\npartial\nappend\nextend\n__getitem__\n__len__\nsave\npretty_repr\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nis_lc_serializable\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nvalidate_variable_names\nformat_prompt\naformat_prompt\nformat\naformat\ndict\npretty_print\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nChatPromptTemplate\nmessages\nvalidate_template\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nlc_secrets\nlc_attributes\ninput_variables\noptional_variables\ninput_types\noutput_parser\npartial_variables\nmetadata\ntags\n__init__\nget_lc_namespace\n__add__\nvalidate_input_variables\nfrom_template\nfrom_messages\nformat_messages\naformat_messages\npartial\nappend\nextend\n__getitem__\n__len__\nsave\npretty_repr\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nis_lc_serializable\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nvalidate_variable_names\nformat_prompt\naformat_prompt\nformat\naformat\ndict\npretty_print\nPrompts\nChatPromptTemplate\n\u00b6\nBases:\nBaseChatPromptTemplate\nPrompt template for chat models.\nUse to create flexible templated prompts for chat models.\nfrom\nlangchain_core.prompts\nimport\nChatPromptTemplate\ntemplate\n=\nChatPromptTemplate\n(\n[\n(\n\"system\"\n,\n\"You are a helpful AI bot. Your name is\n{name}\n.\"\n),\n(\n\"human\"\n,\n\"Hello, how are you doing?\"\n),\n(\n\"ai\"\n,\n\"I'm doing well, thanks!\"\n),\n(\n\"human\"\n,\n\"\n{user_input}\n\"\n),\n]\n)\nprompt_value\n=\ntemplate\n.\ninvoke\n(\n{\n\"name\"\n:\n\"Bob\"\n,\n\"user_input\"\n:\n\"What is your name?\"\n,\n}\n)\n# Output:\n# ChatPromptValue(\n#    messages=[\n#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n#        HumanMessage(content='Hello, how are you doing?'),\n#        AIMessage(content=\"I'm doing well, thanks!\"),\n#        HumanMessage(content='What is your name?')\n#    ]\n# )\nMessages Placeholder\n# In addition to Human/AI/Tool/Function messages,\n# you can initialize the template with a MessagesPlaceholder\n# either using the class directly or with the shorthand tuple syntax:\ntemplate\n=\nChatPromptTemplate\n(\n[\n(\n\"system\"\n,\n\"You are a helpful AI bot.\"\n),\n# Means the template will receive an optional list of messages under\n# the \"conversation\" key\n(\n\"placeholder\"\n,\n\"\n{conversation}\n\"\n),\n# Equivalently:\n# MessagesPlaceholder(variable_name=\"conversation\", optional=True)\n]\n)\nprompt_value\n=\ntemplate\n.\ninvoke\n(\n{\n\"conversation\"\n:\n[\n(\n\"human\"\n,\n\"Hi!\"\n),\n(\n\"ai\"\n,\n\"How can I assist you today?\"\n),\n(\n\"human\"\n,\n\"Can you make me an ice cream sundae?\"\n),\n(\n\"ai\"\n,\n\"No.\"\n),\n]\n}\n)\n# Output:\n# ChatPromptValue(\n#    messages=[\n#        SystemMessage(content='You are a helpful AI bot.'),\n#        HumanMessage(content='Hi!'),\n#        AIMessage(content='How can I assist you today?'),\n#        HumanMessage(content='Can you make me an ice cream sundae?'),\n#        AIMessage(content='No.'),\n#    ]\n# )\nSingle-variable template\nIf your prompt has only a single input variable (i.e., 1 instance of \"{variable_nams}\"),\nand you invoke the template with a non-dict object, the prompt template will\ninject the provided argument into that variable location.\nfrom\nlangchain_core.prompts\nimport\nChatPromptTemplate\ntemplate\n=\nChatPromptTemplate\n(\n[\n(\n\"system\"\n,\n\"You are a helpful AI bot. Your name is Carl.\"\n),\n(\n\"human\"\n,\n\"\n{user_input}\n\"\n),\n]\n)\nprompt_value\n=\ntemplate\n.\ninvoke\n(\n\"Hello, there!\"\n)\n# Equivalent to\n# prompt_value = template.invoke({\"user_input\": \"Hello, there!\"})\n# Output:\n#  ChatPromptValue(\n#     messages=[\n#         SystemMessage(content='You are a helpful AI bot. Your name is Carl.'),\n#         HumanMessage(content='Hello, there!'),\n#     ]\n# )\nMETHOD\nDESCRIPTION\n__init__\nCreate a chat prompt template from a variety of message formats.\nget_lc_namespace\nGet the namespace of the LangChain object.\n__add__\nCombine two prompt templates.\nvalidate_input_variables\nValidate input variables.\nfrom_template\nCreate a chat prompt template from a template string.\nfrom_messages\nCreate a chat prompt template from a variety of message formats.\nformat_messages\nFormat the chat template into a list of finalized messages.\naformat_messages\nAsync format the chat template into a list of finalized messages.\npartial\nGet a new ChatPromptTemplate with some input variables already filled in.\nappend\nAppend a message to the end of the chat template.\nextend\nExtend the chat template with a sequence of messages.\n__getitem__\nUse to index into the chat template.\n__len__\nReturn the length of the chat template.\nsave\nSave prompt to file.\npretty_repr\nHuman-readable representation.\nget_name\nGet the name of the\nRunnable\n.\nget_input_schema\nGet the input schema for the prompt.\nget_input_jsonschema\nGet a JSON schema that represents the input to the\nRunnable\n.\nget_output_schema\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nget_output_jsonschema\nGet a JSON schema that represents the output of the\nRunnable\n.\nconfig_schema\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nget_config_jsonschema\nGet a JSON schema that represents the config of the\nRunnable\n.\nget_graph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\nReturn a list of prompts used by this\nRunnable\n.\n__or__\nRunnable \"or\" operator.\n__ror__\nRunnable \"reverse-or\" operator.\npipe\nPipe\nRunnable\nobjects.\npick\nPick keys from the output\ndict\nof this\nRunnable\n.\nassign\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\ninvoke\nInvoke the prompt.\nainvoke\nAsync invoke the prompt.\nbatch\nDefault implementation runs invoke in parallel using a thread pool executor.\nbatch_as_completed\nRun\ninvoke\nin parallel on a list of inputs.\nabatch\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nabatch_as_completed\nRun\nainvoke\nin parallel on a list of inputs.\nstream\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nastream\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nastream_log\nStream all output from a\nRunnable\n, as reported to the callback system.\nastream_events\nGenerate a stream of events.\ntransform\nTransform inputs to outputs.\natransform\nTransform inputs to outputs.\nbind\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nwith_config\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nwith_listeners\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nwith_alisteners\nBind async lifecycle listeners to a\nRunnable\n.\nwith_types\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nwith_retry\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nmap\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nwith_fallbacks\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nas_tool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nis_lc_serializable\nReturn\nTrue\nas this class is serializable.\nlc_id\nReturn a unique identifier for this class for serialization purposes.\nto_json\nSerialize the\nRunnable\nto JSON.\nto_json_not_implemented\nSerialize a \"not implemented\" object.\nconfigurable_fields\nConfigure particular\nRunnable\nfields at runtime.\nconfigurable_alternatives\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nvalidate_variable_names\nValidate variable names do not include restricted names.\nformat_prompt\nFormat prompt. Should return a ChatPromptValue.\naformat_prompt\nAsync format prompt. Should return a ChatPromptValue.\nformat\nFormat the chat template into a string.\naformat\nAsync format the chat template into a string.\ndict\nReturn dictionary representation of prompt.\npretty_print\nPrint a human-readable representation.\nmessages\ninstance-attribute\n\u00b6\nmessages\n:\nAnnotated\n[\nlist\n[\nMessageLike\n],\nSkipValidation\n()]\nList of messages consisting of either message prompt templates or messages.\nvalidate_template\nclass-attribute\ninstance-attribute\n\u00b6\nvalidate_template\n:\nbool\n=\nFalse\nWhether or not to try validating the template.\nname\nclass-attribute\ninstance-attribute\n\u00b6\nname\n:\nstr\n|\nNone\n=\nNone\nThe name of the\nRunnable\n. Used for debugging and tracing.\nInputType\nproperty\n\u00b6\nInputType\n:\ntype\n[\nInput\n]\nInput type.\nThe type of input this\nRunnable\naccepts specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the input type cannot be inferred.\nOutputType\nproperty\n\u00b6\nOutputType\n:\nAny\nReturn the output type of the prompt.\ninput_schema\nproperty\n\u00b6\ninput_schema\n:\ntype\n[\nBaseModel\n]\nThe type of input this\nRunnable\naccepts specified as a Pydantic model.\noutput_schema\nproperty\n\u00b6\noutput_schema\n:\ntype\n[\nBaseModel\n]\nOutput schema.\nThe type of output this\nRunnable\nproduces specified as a Pydantic model.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\n[\nConfigurableFieldSpec\n]\nList configurable fields for this\nRunnable\n.\nlc_secrets\nproperty\n\u00b6\nlc_secrets\n:\ndict\n[\nstr\n,\nstr\n]\nA map of constructor argument names to secret ids.\nFor example,\n{\"openai_api_key\": \"OPENAI_API_KEY\"}\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nList of attribute names that should be included in the serialized kwargs.\nThese attributes must be accepted by the constructor.\nDefault is an empty dictionary.\ninput_variables\ninstance-attribute\n\u00b6\ninput_variables\n:\nlist\n[\nstr\n]\nA list of the names of the variables whose values are required as inputs to the\nprompt.\noptional_variables\nclass-attribute\ninstance-attribute\n\u00b6\noptional_variables\n:\nlist\n[\nstr\n]\n=\nField\n(\ndefault\n=\n[])\nA list of the names of the variables for placeholder or\nMessagePlaceholder\nthat\nare optional.\nThese variables are auto inferred from the prompt and user need not provide them.\ninput_types\nclass-attribute\ninstance-attribute\n\u00b6\ninput_types\n:\ndict\n[\nstr\n,\nAny\n]\n=\nField\n(\ndefault_factory\n=\ndict\n,\nexclude\n=\nTrue\n)\nA dictionary of the types of the variables the prompt template expects.\nIf not provided, all variables are assumed to be strings.\noutput_parser\nclass-attribute\ninstance-attribute\n\u00b6\noutput_parser\n:\nBaseOutputParser\n|\nNone\n=\nNone\nHow to parse the output of calling an LLM on this formatted prompt.\npartial_variables\nclass-attribute\ninstance-attribute\n\u00b6\npartial_variables\n:\nMapping\n[\nstr\n,\nAny\n]\n=\nField\n(\ndefault_factory\n=\ndict\n)\nA dictionary of the partial variables the prompt template carries.\nPartial variables populate the template so that you don't need to pass them in every\ntime you call the prompt.\nmetadata\nclass-attribute\ninstance-attribute\n\u00b6\nmetadata\n:\nDict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\nMetadata to be used for tracing.\ntags\nclass-attribute\ninstance-attribute\n\u00b6\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\nTags to be used for tracing.\n__init__\n\u00b6\n__init__\n(\nmessages\n:\nSequence\n[\nMessageLikeRepresentation\n],\n*\n,\ntemplate_format\n:\nPromptTemplateFormat\n=\n\"f-string\"\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nNone\nCreate a chat prompt template from a variety of message formats.\nPARAMETER\nDESCRIPTION\nmessages\nSequence of message representations.\nA message can be represented using the following formats:\nBaseMessagePromptTemplate\nBaseMessage\n2-tuple of\n(message type, template)\n; e.g.,\n(\"human\", \"{user_input}\")\n2-tuple of\n(message class, template)\nA string which is shorthand for\n(\"human\", template)\n; e.g.,\n\"{user_input}\"\nTYPE:\nSequence\n[\nMessageLikeRepresentation\n]\ntemplate_format\nFormat of the template.\nTYPE:\nPromptTemplateFormat\nDEFAULT:\n'f-string'\n**kwargs\nAdditional keyword arguments passed to\nBasePromptTemplate\n,\nincluding (but not limited to):\ninput_variables\n: A list of the names of the variables whose values\nare required as inputs to the prompt.\noptional_variables\n: A list of the names of the variables for\nplaceholder or\nMessagePlaceholder\nthat are optional.\nThese variables are auto inferred from the prompt and user need not\nprovide them.\npartial_variables\n: A dictionary of the partial variables the prompt\ntemplate carries.\nPartial variables populate the template so that you don't need to\npass them in every time you call the prompt.\nvalidate_template\n: Whether to validate the template.\ninput_types\n: A dictionary of the types of the variables the prompt\ntemplate expects.\nIf not provided, all variables are assumed to be strings.\nTYPE:\nAny\nDEFAULT:\n{}\nExamples:\nInstantiation from a list of message templates:\ntemplate\n=\nChatPromptTemplate\n(\n[\n(\n\"human\"\n,\n\"Hello, how are you?\"\n),\n(\n\"ai\"\n,\n\"I'm doing well, thanks!\"\n),\n(\n\"human\"\n,\n\"That's good to hear.\"\n),\n]\n)\nInstantiation from mixed message formats:\ntemplate\n=\nChatPromptTemplate\n(\n[\nSystemMessage\n(\ncontent\n=\n\"hello\"\n),\n(\n\"human\"\n,\n\"Hello, how are you?\"\n),\n]\n)\nget_lc_namespace\nclassmethod\n\u00b6\nget_lc_namespace\n()\n->\nlist\n[\nstr\n]\nGet the namespace of the LangChain object.\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\n[\"langchain\", \"prompts\", \"chat\"]\n__add__\n\u00b6\n__add__\n(\nother\n:\nAny\n)\n->\nChatPromptTemplate\nCombine two prompt templates.\nPARAMETER\nDESCRIPTION\nother\nAnother prompt template.\nTYPE:\nAny\nRETURNS\nDESCRIPTION\nChatPromptTemplate\nCombined prompt template.\nvalidate_input_variables\nclassmethod\n\u00b6\nvalidate_input_variables\n(\nvalues\n:\ndict\n)\n->\nAny\nValidate input variables.\nIf input_variables is not set, it will be set to the union of\nall input variables in the messages.\nPARAMETER\nDESCRIPTION\nvalues\nvalues to validate.\nTYPE:\ndict\nRETURNS\nDESCRIPTION\nAny\nValidated values.\nRAISES\nDESCRIPTION\nValueError\nIf input variables do not match.\nfrom_template\nclassmethod\n\u00b6\nfrom_template\n(\ntemplate\n:\nstr\n,\n**\nkwargs\n:\nAny\n)\n->\nChatPromptTemplate\nCreate a chat prompt template from a template string.\nCreates a chat template consisting of a single message assumed to be from\nthe human.\nPARAMETER\nDESCRIPTION\ntemplate\ntemplate string\nTYPE:\nstr\n**kwargs\nkeyword arguments to pass to the constructor.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nChatPromptTemplate\nA new instance of this class.\nfrom_messages\nclassmethod\n\u00b6\nfrom_messages\n(\nmessages\n:\nSequence\n[\nMessageLikeRepresentation\n],\ntemplate_format\n:\nPromptTemplateFormat\n=\n\"f-string\"\n,\n)\n->\nChatPromptTemplate\nCreate a chat prompt template from a variety of message formats.\nExamples:\nInstantiation from a list of message templates:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"human\"\n,\n\"Hello, how are you?\"\n),\n(\n\"ai\"\n,\n\"I'm doing well, thanks!\"\n),\n(\n\"human\"\n,\n\"That's good to hear.\"\n),\n]\n)\nInstantiation from mixed message formats:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\nSystemMessage\n(\ncontent\n=\n\"hello\"\n),\n(\n\"human\"\n,\n\"Hello, how are you?\"\n),\n]\n)\nArgs:\nmessages: Sequence of message representations.\nA message can be represented using the following formats:\n1. `BaseMessagePromptTemplate`\n2. `BaseMessage`\n3. 2-tuple of `(message type, template)`; e.g.,\n`(\"human\", \"{user_input}\")`\n4. 2-tuple of `(message class, template)`\n5. A string which is shorthand for `(\"human\", template)`; e.g.,\n`\"{user_input}\"`\ntemplate_format: format of the template.\nRETURNS\nDESCRIPTION\nChatPromptTemplate\na chat prompt template.\nformat_messages\n\u00b6\nformat_messages\n(\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nBaseMessage\n]\nFormat the chat template into a list of finalized messages.\nPARAMETER\nDESCRIPTION\n**kwargs\nkeyword arguments to use for filling in template variables\nin all the template messages in this chat template.\nTYPE:\nAny\nDEFAULT:\n{}\nRAISES\nDESCRIPTION\nValueError\nif messages are of unexpected types.\nRETURNS\nDESCRIPTION\nlist\n[\nBaseMessage\n]\nlist of formatted messages.\naformat_messages\nasync\n\u00b6\naformat_messages\n(\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nBaseMessage\n]\nAsync format the chat template into a list of finalized messages.\nPARAMETER\nDESCRIPTION\n**kwargs\nkeyword arguments to use for filling in template variables\nin all the template messages in this chat template.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nBaseMessage\n]\nlist of formatted messages.\nRAISES\nDESCRIPTION\nValueError\nIf unexpected input.\npartial\n\u00b6\npartial\n(\n**\nkwargs\n:\nAny\n)\n->\nChatPromptTemplate\nGet a new ChatPromptTemplate with some input variables already filled in.\nPARAMETER\nDESCRIPTION\n**kwargs\nkeyword arguments to use for filling in template variables. Ought\nto be a subset of the input variables.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nChatPromptTemplate\nA new ChatPromptTemplate.\nExample\nfrom\nlangchain_core.prompts\nimport\nChatPromptTemplate\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are an AI assistant named\n{name}\n.\"\n),\n(\n\"human\"\n,\n\"Hi I'm\n{user}\n\"\n),\n(\n\"ai\"\n,\n\"Hi there,\n{user}\n, I'm\n{name}\n.\"\n),\n(\n\"human\"\n,\n\"\n{input}\n\"\n),\n]\n)\ntemplate2\n=\ntemplate\n.\npartial\n(\nuser\n=\n\"Lucy\"\n,\nname\n=\n\"R2D2\"\n)\ntemplate2\n.\nformat_messages\n(\ninput\n=\n\"hello\"\n)\nappend\n\u00b6\nappend\n(\nmessage\n:\nMessageLikeRepresentation\n)\n->\nNone\nAppend a message to the end of the chat template.\nPARAMETER\nDESCRIPTION\nmessage\nrepresentation of a message to append.\nTYPE:\nMessageLikeRepresentation\nextend\n\u00b6\nextend\n(\nmessages\n:\nSequence\n[\nMessageLikeRepresentation\n])\n->\nNone\nExtend the chat template with a sequence of messages.\nPARAMETER\nDESCRIPTION\nmessages\nSequence of message representations to append.\nTYPE:\nSequence\n[\nMessageLikeRepresentation\n]\n__getitem__\n\u00b6\n__getitem__\n(\nindex\n:\nint\n|\nslice\n)\n->\nMessageLike\n|\nChatPromptTemplate\nUse to index into the chat template.\nRETURNS\nDESCRIPTION\nMessageLike\n|\nChatPromptTemplate\nIf index is an int, returns the message at that index.\nMessageLike\n|\nChatPromptTemplate\nIf index is a slice, returns a new\nChatPromptTemplate\nMessageLike\n|\nChatPromptTemplate\ncontaining the messages in that slice.\n__len__\n\u00b6\n__len__\n()\n->\nint\nReturn the length of the chat template.\nsave\n\u00b6\nsave\n(\nfile_path\n:\nPath\n|\nstr\n)\n->\nNone\nSave prompt to file.\nPARAMETER\nDESCRIPTION\nfile_path\npath to file.\nTYPE:\nPath\n|\nstr\npretty_repr\n\u00b6\npretty_repr\n(\nhtml\n:\nbool\n=\nFalse\n)\n->\nstr\nHuman-readable representation.\nPARAMETER\nDESCRIPTION\nhtml\nWhether to format as HTML.\nTYPE:\nbool\nDEFAULT:\nFalse\nRETURNS\nDESCRIPTION\nstr\nHuman-readable representation.\nget_name\n\u00b6\nget_name\n(\nsuffix\n:\nstr\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nstr\nGet the name of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nsuffix\nAn optional suffix to append to the name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nname\nAn optional name to use instead of the\nRunnable\n's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nstr\nThe name of the\nRunnable\n.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet the input schema for the prompt.\nPARAMETER\nDESCRIPTION\nconfig\nConfiguration for the prompt.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nThe input schema for the prompt.\nget_input_jsonschema\n\u00b6\nget_input_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the input to the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the input to the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_input_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic output schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an output schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate output.\nget_output_jsonschema\n\u00b6\nget_output_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the output of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the output of the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_output_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nconfig_schema\n\u00b6\nconfig_schema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nTo mark a field as configurable, see the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate config.\nget_config_jsonschema\n\u00b6\nget_config_jsonschema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the config of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the config of the\nRunnable\n.\nAdded in\nlangchain-core\n0.3.0\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nGraph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\n\u00b6\nget_prompts\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nlist\n[\nBasePromptTemplate\n]\nReturn a list of prompts used by this\nRunnable\n.\n__or__\n\u00b6\n__or__\n(\nother\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nRunnable \"or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]] |\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]] |\nCallable\n[[\nAny\n],\nOther\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\n__ror__\n\u00b6\n__ror__\n(\nother\n:\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nRunnable \"reverse-or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]] |\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]] |\nCallable\n[[\nOther\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nOther\n],\nAny\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nA new\nRunnable\n.\npipe\n\u00b6\npipe\n(\n*\nothers\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n],\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nPipe\nRunnable\nobjects.\nCompose this\nRunnable\nwith\nRunnable\n-like objects to make a\nRunnableSequence\n.\nEquivalent to\nRunnableSequence(self, *others)\nor\nself | others[0] | ...\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nmul_two\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n*\n2\nrunnable_1\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable_2\n=\nRunnableLambda\n(\nmul_two\n)\nsequence\n=\nrunnable_1\n.\npipe\n(\nrunnable_2\n)\n# Or equivalently:\n# sequence = runnable_1 | runnable_2\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence\n.\ninvoke\n(\n1\n)\nawait\nsequence\n.\nainvoke\n(\n1\n)\n# -> 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\nawait\nsequence\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# -> [4, 6, 8]\nPARAMETER\nDESCRIPTION\n*others\nOther\nRunnable\nor\nRunnable\n-like objects to compose\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n]\nDEFAULT:\n()\nname\nAn optional name for the resulting\nRunnableSequence\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\npick\n\u00b6\npick\n(\nkeys\n:\nstr\n|\nlist\n[\nstr\n])\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nPick keys from the output\ndict\nof this\nRunnable\n.\nPick a single key\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\njson_only_chain\n=\nchain\n.\npick\n(\n\"json\"\n)\njson_only_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> [1, 2, 3]\nPick a list of keys\nfrom\ntyping\nimport\nAny\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\ndef\nas_bytes\n(\nx\n:\nAny\n)\n->\nbytes\n:\nreturn\nbytes\n(\nx\n,\n\"utf-8\"\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n,\nbytes\n=\nRunnableLambda\n(\nas_bytes\n)\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\njson_and_bytes_chain\n=\nchain\n.\npick\n([\n\"json\"\n,\n\"bytes\"\n])\njson_and_bytes_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\nPARAMETER\nDESCRIPTION\nkeys\nA key or list of keys to pick from the output dict.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\na new\nRunnable\n.\nassign\n\u00b6\nassign\n(\n**\nkwargs\n:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]],\n)\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nfrom\nlangchain_core.language_models.fake\nimport\nFakeStreamingListLLM\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core.prompts\nimport\nSystemMessagePromptTemplate\nfrom\nlangchain_core.runnables\nimport\nRunnable\nfrom\noperator\nimport\nitemgetter\nprompt\n=\n(\nSystemMessagePromptTemplate\n.\nfrom_template\n(\n\"You are a nice assistant.\"\n)\n+\n\"\n{question}\n\"\n)\nmodel\n=\nFakeStreamingListLLM\n(\nresponses\n=\n[\n\"foo-lish\"\n])\nchain\n:\nRunnable\n=\nprompt\n|\nmodel\n|\n{\n\"str\"\n:\nStrOutputParser\n()}\nchain_with_assign\n=\nchain\n.\nassign\n(\nhello\n=\nitemgetter\n(\n\"str\"\n)\n|\nmodel\n)\nprint\n(\nchain_with_assign\n.\ninput_schema\n.\nmodel_json_schema\n())\n# {'title': 'PromptInput', 'type': 'object', 'properties':\n{\n'question'\n:\n{\n'title'\n:\n'Question'\n,\n'type'\n:\n'string'\n}}}\nprint\n(\nchain_with_assign\n.\noutput_schema\n.\nmodel_json_schema\n())\n# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n{\n'str'\n:\n{\n'title'\n:\n'Str'\n,\n'type'\n:\n'string'\n},\n'hello'\n:\n{\n'title'\n:\n'Hello'\n,\n'type'\n:\n'string'\n}}}\nPARAMETER\nDESCRIPTION\n**kwargs\nA mapping of keys to\nRunnable\nor\nRunnable\n-like objects\nthat will be invoked with the entire output dict of this\nRunnable\n.\nTYPE:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\nA new\nRunnable\n.\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\ndict\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nPromptValue\nInvoke the prompt.\nPARAMETER\nDESCRIPTION\ninput\nInput to the prompt.\nTYPE:\ndict\nconfig\nConfiguration for the prompt.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nPromptValue\nThe output of the prompt.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\ndict\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nPromptValue\nAsync invoke the prompt.\nPARAMETER\nDESCRIPTION\ninput\nInput to the prompt.\nTYPE:\ndict\nconfig\nConfiguration for the prompt.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nPromptValue\nThe output of the prompt.\nbatch\n\u00b6\nbatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n. The config supports\nstandard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work\nto do in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nbatch_as_completed\n\u00b6\nbatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\ninvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\ntuple\n[\nint\n,\nOutput\n|\nException\n]\nTuples of the index of the input and the output from the\nRunnable\n.\nabatch\nasync\n\u00b6\nabatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nThe default implementation of\nbatch\nworks well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nabatch_as_completed\nasync\n\u00b6\nabatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\nainvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nA tuple of the index of the input and the output from the\nRunnable\n.\nstream\n\u00b6\nstream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nAsyncIterator\n[\nOutput\n]\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nastream_log\nasync\n\u00b6\nastream_log\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ndiff\n:\nbool\n=\nTrue\n,\nwith_streamed_output_list\n:\nbool\n=\nTrue\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nRunLogPatch\n]\n|\nAsyncIterator\n[\nRunLog\n]\nStream all output from a\nRunnable\n, as reported to the callback system.\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\nOutput is streamed as Log objects, which include a list of\nJsonpatch ops that describe how the state of the run has changed in each\nstep, and the final state of the run.\nThe Jsonpatch ops can be applied in order to construct state.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ndiff\nWhether to yield diffs between each step or the current state.\nTYPE:\nbool\nDEFAULT:\nTrue\nwith_streamed_output_list\nWhether to yield the\nstreamed_output\nlist.\nTYPE:\nbool\nDEFAULT:\nTrue\ninclude_names\nOnly include logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nRunLogPatch\n] |\nAsyncIterator\n[\nRunLog\n]\nA\nRunLogPatch\nor\nRunLog\nobject.\nastream_events\nasync\n\u00b6\nastream_events\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nversion\n:\nLiteral\n[\n\"v1\"\n,\n\"v2\"\n]\n=\n\"v2\"\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nStreamEvent\n]\nGenerate a stream of events.\nUse to create an iterator over\nStreamEvent\nthat provide real-time information\nabout the progress of the\nRunnable\n, including\nStreamEvent\nfrom intermediate\nresults.\nA\nStreamEvent\nis a dictionary with the following schema:\nevent\n: Event names are of the format:\non_[runnable_type]_(start|stream|end)\n.\nname\n: The name of the\nRunnable\nthat generated the event.\nrun_id\n: Randomly generated ID associated with the given execution of the\nRunnable\nthat emitted the event. A child\nRunnable\nthat gets invoked as\npart of the execution of a parent\nRunnable\nis assigned its own unique ID.\nparent_ids\n: The IDs of the parent runnables that generated the event. The\nroot\nRunnable\nwill have an empty list. The order of the parent IDs is from\nthe root to the immediate parent. Only available for v2 version of the API.\nThe v1 version of the API will return an empty list.\ntags\n: The tags of the\nRunnable\nthat generated the event.\nmetadata\n: The metadata of the\nRunnable\nthat generated the event.\ndata\n: The data associated with the event. The contents of this field\ndepend on the type of event. See the table below for more details.\nBelow is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table.\nNote\nThis reference table is for the v2 version of the schema.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n'[model name]'\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n'[model name]'\n{'input': 'hello'}\non_llm_stream\n'[model name]'\n'Hello'\non_llm_end\n'[model name]'\n'Hello human!'\non_chain_start\n'format_docs'\non_chain_stream\n'format_docs'\n'hello world!, goodbye world!'\non_chain_end\n'format_docs'\n[Document(...)]\n'hello world!, goodbye world!'\non_tool_start\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n'[retriever name]'\n{\"query\": \"hello\"}\non_retriever_end\n'[retriever name]'\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n'[template_name]'\n{\"question\": \"hello\"}\non_prompt_end\n'[template_name]'\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nIn addition to the standard events, users can also dispatch custom events (see example below).\nCustom events will be only be surfaced with in the v2 version of the API!\nA custom event has following format:\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nHere are declarations associated with the standard events shown above:\nformat_docs\n:\ndef\nformat_docs\n(\ndocs\n:\nlist\n[\nDocument\n])\n->\nstr\n:\n'''Format the docs.'''\nreturn\n\", \"\n.\njoin\n([\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n])\nformat_docs\n=\nRunnableLambda\n(\nformat_docs\n)\nsome_tool\n:\n@tool\ndef\nsome_tool\n(\nx\n:\nint\n,\ny\n:\nstr\n)\n->\ndict\n:\n'''Some_tool.'''\nreturn\n{\n\"x\"\n:\nx\n,\n\"y\"\n:\ny\n}\nprompt\n:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are Cat Agent 007\"\n),\n(\n\"human\"\n,\n\"\n{question}\n\"\n),\n]\n)\n.\nwith_config\n({\n\"run_name\"\n:\n\"my_template\"\n,\n\"tags\"\n:\n[\n\"my_template\"\n]})\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nasync\ndef\nreverse\n(\ns\n:\nstr\n)\n->\nstr\n:\nreturn\ns\n[::\n-\n1\n]\nchain\n=\nRunnableLambda\n(\nfunc\n=\nreverse\n)\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"hello\"\n,\nversion\n=\n\"v2\"\n)\n]\n# Will produce the following events\n# (run_id, and parent_ids has been omitted for brevity):\n[\n{\n\"data\"\n:\n{\n\"input\"\n:\n\"hello\"\n},\n\"event\"\n:\n\"on_chain_start\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"chunk\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_stream\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"output\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_end\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n]\nDispatch custom event\nfrom\nlangchain_core.callbacks.manager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableConfig\nimport\nasyncio\nasync\ndef\nslow_thing\n(\nsome_input\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n->\nstr\n:\n\"\"\"Do something that takes a long time.\"\"\"\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 1 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 2 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nreturn\n\"Done\"\nslow_thing\n=\nRunnableLambda\n(\nslow_thing\n)\nasync\nfor\nevent\nin\nslow_thing\n.\nastream_events\n(\n\"some_input\"\n,\nversion\n=\n\"v2\"\n):\nprint\n(\nevent\n)\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nversion\nThe version of the schema to use, either\n'v2'\nor\n'v1'\n.\nUsers should use\n'v2'\n.\n'v1'\nis for backwards compatibility and will be deprecated\nin\n0.4.0\n.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in\n'v2'\n.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\ninclude_names\nOnly include events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nThese will be passed to\nastream_log\nas this implementation\nof\nastream_events\nis built on top of\nastream_log\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nStreamEvent\n]\nAn async stream of\nStreamEvent\n.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the version is not\n'v1'\nor\n'v2'\n.\ntransform\n\u00b6\ntransform\n(\ninput\n:\nIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of transform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn iterator of inputs to the\nRunnable\n.\nTYPE:\nIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\natransform\nasync\n\u00b6\natransform\n(\ninput\n:\nAsyncIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of atransform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn async iterator of inputs to the\nRunnable\n.\nTYPE:\nAsyncIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nbind\n\u00b6\nbind\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nUseful when a\nRunnable\nin a chain requires an argument that is not\nin the output of the previous\nRunnable\nor included in the user input.\nPARAMETER\nDESCRIPTION\n**kwargs\nThe arguments to bind to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the arguments bound.\nExample\nfrom\nlangchain_ollama\nimport\nChatOllama\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nmodel\n=\nChatOllama\n(\nmodel\n=\n\"llama3.1\"\n)\n# Without bind\nchain\n=\nmodel\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two three four five.'\n# With bind\nchain\n=\nmodel\n.\nbind\n(\nstop\n=\n[\n\"three\"\n])\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two'\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to bind to the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the config bound.\nwith_listeners\n\u00b6\nwith_listeners\n(\n*\n,\non_start\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_end\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_error\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled before the\nRunnable\nstarts running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_end\nCalled after the\nRunnable\nfinishes running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_error\nCalled if the\nRunnable\nthrows an error, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nfrom\nlangchain_core.tracers.schemas\nimport\nRun\nimport\ntime\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\ntime\n.\nsleep\n(\ntime_to_sleep\n)\ndef\nfn_start\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"start_time:\"\n,\nrun_obj\n.\nstart_time\n)\ndef\nfn_end\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"end_time:\"\n,\nrun_obj\n.\nend_time\n)\nchain\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_listeners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nchain\n.\ninvoke\n(\n2\n)\nwith_alisteners\n\u00b6\nwith_alisteners\n(\n*\n,\non_start\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_end\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_error\n:\nAsyncListener\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind async lifecycle listeners to a\nRunnable\n.\nReturns a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled asynchronously before the\nRunnable\nstarts running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_end\nCalled asynchronously after the\nRunnable\nfinishes running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_error\nCalled asynchronously if the\nRunnable\nthrows an error,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnable\nfrom\ndatetime\nimport\ndatetime\n,\ntimezone\nimport\ntime\nimport\nasyncio\ndef\nformat_t\n(\ntimestamp\n:\nfloat\n)\n->\nstr\n:\nreturn\ndatetime\n.\nfromtimestamp\n(\ntimestamp\n,\ntz\n=\ntimezone\n.\nutc\n)\n.\nisoformat\n()\nasync\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\ntime_to_sleep\n)\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_start\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on start callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n3\n)\nprint\n(\nf\n\"on start callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_end\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on end callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n2\n)\nprint\n(\nf\n\"on end callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nrunnable\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_alisteners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nasync\ndef\nconcurrent_runs\n():\nawait\nasyncio\n.\ngather\n(\nrunnable\n.\nainvoke\n(\n2\n),\nrunnable\n.\nainvoke\n(\n3\n))\nasyncio\n.\nrun\n(\nconcurrent_runs\n())\n# Result:\n# on start callback starts at 2025-03-01T07:05:22.875378+00:00\n# on start callback starts at 2025-03-01T07:05:22.875495+00:00\n# on start callback ends at 2025-03-01T07:05:25.878862+00:00\n# on start callback ends at 2025-03-01T07:05:25.878947+00:00\n# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n# on end callback starts at 2025-03-01T07:05:27.882360+00:00\n# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n# on end callback starts at 2025-03-01T07:05:28.882428+00:00\n# on end callback ends at 2025-03-01T07:05:29.883893+00:00\n# on end callback ends at 2025-03-01T07:05:30.884831+00:00\nwith_types\n\u00b6\nwith_types\n(\n*\n,\ninput_type\n:\ntype\n[\nInput\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\ntype\n[\nOutput\n]\n|\nNone\n=\nNone\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninput_type\nThe input type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nInput\n] | None\nDEFAULT:\nNone\noutput_type\nThe output type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nOutput\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the types bound.\nwith_retry\n\u00b6\nwith_retry\n(\n*\n,\nretry_if_exception_type\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nwait_exponential_jitter\n:\nbool\n=\nTrue\n,\nexponential_jitter_params\n:\nExponentialJitterParams\n|\nNone\n=\nNone\n,\nstop_after_attempt\n:\nint\n=\n3\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nPARAMETER\nDESCRIPTION\nretry_if_exception_type\nA tuple of exception types to retry on.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nwait_exponential_jitter\nWhether to add jitter to the wait\ntime between retries.\nTYPE:\nbool\nDEFAULT:\nTrue\nstop_after_attempt\nThe maximum number of attempts to make before\ngiving up.\nTYPE:\nint\nDEFAULT:\n3\nexponential_jitter_params\nParameters for\ntenacity.wait_exponential_jitter\n. Namely:\ninitial\n,\nmax\n,\nexp_base\n, and\njitter\n(all\nfloat\nvalues).\nTYPE:\nExponentialJitterParams\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ncount\n=\n0\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nNone\n:\nglobal\ncount\ncount\n=\ncount\n+\n1\nif\nx\n==\n1\n:\nraise\nValueError\n(\n\"x is 1\"\n)\nelse\n:\npass\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\ntry\n:\nrunnable\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n,\nretry_if_exception_type\n=\n(\nValueError\n,),\n)\n.\ninvoke\n(\n1\n)\nexcept\nValueError\n:\npass\nassert\ncount\n==\n2\nmap\n\u00b6\nmap\n()\n->\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nCalls\ninvoke\nwith each input.\nRETURNS\nDESCRIPTION\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nA new\nRunnable\nthat maps a list of inputs to a list of outputs.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\nprint\n(\nrunnable\n.\nmap\n()\n.\ninvoke\n([\n1\n,\n2\n,\n3\n]))\n# [2, 3, 4]\nwith_fallbacks\n\u00b6\nwith_fallbacks\n(\nfallbacks\n:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]],\n*\n,\nexceptions_to_handle\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nexception_key\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nThe new\nRunnable\nwill try the original\nRunnable\n, and then each fallback\nin order, upon failures.\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nExample\nfrom\ntyping\nimport\nIterator\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\ndef\n_generate_immediate_error\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nraise\nValueError\n()\nyield\n\"\"\ndef\n_generate\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nyield from\n\"foo bar\"\nrunnable\n=\nRunnableGenerator\n(\n_generate_immediate_error\n)\n.\nwith_fallbacks\n(\n[\nRunnableGenerator\n(\n_generate\n)]\n)\nprint\n(\n\"\"\n.\njoin\n(\nrunnable\n.\nstream\n({})))\n# foo bar\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nas_tool\n\u00b6\nas_tool\n(\nargs_schema\n:\ntype\n[\nBaseModel\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\narg_types\n:\ndict\n[\nstr\n,\ntype\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nas_tool\nwill instantiate a\nBaseTool\nwith a name, description, and\nargs_schema\nfrom a\nRunnable\n. Where possible, schemas are inferred\nfrom\nrunnable.get_input_schema\n.\nAlternatively (e.g., if the\nRunnable\ntakes a dict as input and the specific\ndict\nkeys are not typed), the schema can be specified directly with\nargs_schema\n.\nYou can also pass\narg_types\nto just specify the required arguments and their\ntypes.\nPARAMETER\nDESCRIPTION\nargs_schema\nThe schema for the tool.\nTYPE:\ntype\n[\nBaseModel\n] | None\nDEFAULT:\nNone\nname\nThe name of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nThe description of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\narg_types\nA dictionary of argument names to types.\nTYPE:\ndict\n[\nstr\n,\ntype\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBaseTool\nA\nBaseTool\ninstance.\nTypedDict\ninput\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nclass\nArgs\n(\nTypedDict\n):\na\n:\nint\nb\n:\nlist\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\nargs_schema\nfrom\ntyping\nimport\nAny\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nclass\nFSchema\n(\nBaseModel\n):\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n...\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nlist\n[\nint\n]\n=\nField\n(\n...\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nFSchema\n)\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\narg_types\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nlist\n[\nint\n]})\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\nstr\ninput\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n(\n\"b\"\n)\nis_lc_serializable\nclassmethod\n\u00b6\nis_lc_serializable\n()\n->\nbool\nReturn\nTrue\nas this class is serializable.\nlc_id\nclassmethod\n\u00b6\nlc_id\n()\n->\nlist\n[\nstr\n]\nReturn a unique identifier for this class for serialization purposes.\nThe unique identifier is a list of strings that describes the path\nto the object.\nFor example, for the class\nlangchain.llms.openai.OpenAI\n, the id is\n[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]\n.\nto_json\n\u00b6\nto_json\n()\n->\nSerializedConstructor\n|\nSerializedNotImplemented\nSerialize the\nRunnable\nto JSON.\nRETURNS\nDESCRIPTION\nSerializedConstructor\n|\nSerializedNotImplemented\nA JSON-serializable representation of the\nRunnable\n.\nto_json_not_implemented\n\u00b6\nto_json_not_implemented\n()\n->\nSerializedNotImplemented\nSerialize a \"not implemented\" object.\nRETURNS\nDESCRIPTION\nSerializedNotImplemented\nSerializedNotImplemented\n.\nconfigurable_fields\n\u00b6\nconfigurable_fields\n(\n**\nkwargs\n:\nAnyConfigurableField\n,\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure particular\nRunnable\nfields at runtime.\nPARAMETER\nDESCRIPTION\n**kwargs\nA dictionary of\nConfigurableField\ninstances to configure.\nTYPE:\nAnyConfigurableField\nDEFAULT:\n{}\nRAISES\nDESCRIPTION\nValueError\nIf a configuration key is not found in the\nRunnable\n.\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the fields configured.\nExample\nfrom\nlangchain_core.runnables\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmax_tokens\n=\n20\n)\n.\nconfigurable_fields\n(\nmax_tokens\n=\nConfigurableField\n(\nid\n=\n\"output_token_number\"\n,\nname\n=\n\"Max tokens in the output\"\n,\ndescription\n=\n\"The maximum number of tokens in the output\"\n,\n)\n)\n# max_tokens = 20\nprint\n(\n\"max_tokens_20: \"\n,\nmodel\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n)\n# max_tokens = 200\nprint\n(\n\"max_tokens_200: \"\n,\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"output_token_number\"\n:\n200\n})\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n,\n)\nconfigurable_alternatives\n\u00b6\nconfigurable_alternatives\n(\nwhich\n:\nConfigurableField\n,\n*\n,\ndefault_key\n:\nstr\n=\n\"default\"\n,\nprefix_keys\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nRunnable\n[\nInput\n,\nOutput\n]\n|\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nPARAMETER\nDESCRIPTION\nwhich\nThe\nConfigurableField\ninstance that will be used to select the\nalternative.\nTYPE:\nConfigurableField\ndefault_key\nThe default key to use if no alternative is selected.\nTYPE:\nstr\nDEFAULT:\n'default'\nprefix_keys\nWhether to prefix the keys with the\nConfigurableField\nid.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nA dictionary of keys to\nRunnable\ninstances or callables that\nreturn\nRunnable\ninstances.\nTYPE:\nRunnable\n[\nInput\n,\nOutput\n] |\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the alternatives configured.\nExample\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core.runnables.utils\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatAnthropic\n(\nmodel_name\n=\n\"claude-sonnet-4-5-20250929\"\n)\n.\nconfigurable_alternatives\n(\nConfigurableField\n(\nid\n=\n\"llm\"\n),\ndefault_key\n=\n\"anthropic\"\n,\nopenai\n=\nChatOpenAI\n(),\n)\n# uses the default model ChatAnthropic\nprint\n(\nmodel\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\n# uses ChatOpenAI\nprint\n(\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"openai\"\n})\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\nvalidate_variable_names\n\u00b6\nvalidate_variable_names\n()\n->\nSelf\nValidate variable names do not include restricted names.\nformat_prompt\n\u00b6\nformat_prompt\n(\n**\nkwargs\n:\nAny\n)\n->\nChatPromptValue\nFormat prompt. Should return a ChatPromptValue.\nPARAMETER\nDESCRIPTION\n**kwargs\nKeyword arguments to use for formatting.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nChatPromptValue\nChatPromptValue.\naformat_prompt\nasync\n\u00b6\naformat_prompt\n(\n**\nkwargs\n:\nAny\n)\n->\nChatPromptValue\nAsync format prompt. Should return a ChatPromptValue.\nPARAMETER\nDESCRIPTION\n**kwargs\nKeyword arguments to use for formatting.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nChatPromptValue\nPromptValue.\nformat\n\u00b6\nformat\n(\n**\nkwargs\n:\nAny\n)\n->\nstr\nFormat the chat template into a string.\nPARAMETER\nDESCRIPTION\n**kwargs\nkeyword arguments to use for filling in template variables\nin all the template messages in this chat template.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nstr\nformatted string.\naformat\nasync\n\u00b6\naformat\n(\n**\nkwargs\n:\nAny\n)\n->\nstr\nAsync format the chat template into a string.\nPARAMETER\nDESCRIPTION\n**kwargs\nkeyword arguments to use for filling in template variables\nin all the template messages in this chat template.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nstr\nformatted string.\ndict\n\u00b6\ndict\n(\n**\nkwargs\n:\nAny\n)\n->\ndict\nReturn dictionary representation of prompt.\nPARAMETER\nDESCRIPTION\n**kwargs\nAny additional arguments to pass to the dictionary.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\ndict\nDictionary representation of the prompt.\npretty_print\n\u00b6\npretty_print\n()\n->\nNone\nPrint a human-readable representation.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain_core/prompts/",
      "title": "Prompts | LangChain Reference",
      "heading": "Prompts"
    }
  },
  {
    "page_content": "Runnables | LangChain Reference\nSkip to content\nLangChain Reference\nRunnables\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nRunnables\nTable of contents\nRunnable\nKey Methods\nComposition\nStandard Methods\nDebugging and tracing\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nRunnableBinding\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nlc_secrets\nlc_attributes\nbound\nkwargs\nconfig\nconfig_factories\ncustom_input_type\ncustom_output_type\nbind\nwith_config\nwith_listeners\nwith_types\nwith_retry\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nwith_alisteners\nmap\nwith_fallbacks\nas_tool\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nRunnableGenerator\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\n__init__\nget_input_schema\nget_output_schema\ntransform\nstream\ninvoke\natransform\nastream\nainvoke\nget_name\nget_input_jsonschema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nastream_log\nastream_events\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nRunnableLambda\nname\nInputType\nOutputType\ndeps\nconfig_specs\ninput_schema\noutput_schema\n__init__\nget_input_schema\nget_output_schema\nget_graph\n__repr__\ninvoke\nainvoke\ntransform\nstream\natransform\nastream\nget_name\nget_input_jsonschema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nastream_log\nastream_events\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nRunnableParallel\nInputType\nconfig_specs\nname\nOutputType\ninput_schema\noutput_schema\nlc_secrets\nlc_attributes\n__init__\nis_lc_serializable\nget_lc_namespace\nget_name\nget_input_schema\nget_output_schema\nget_graph\ninvoke\nainvoke\ntransform\nstream\natransform\nastream\nget_input_jsonschema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nastream_log\nastream_events\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nRunnableSequence\nfirst\nmiddle\nlast\nsteps\nInputType\nOutputType\nconfig_specs\nname\ninput_schema\noutput_schema\nlc_secrets\nlc_attributes\n__init__\nget_lc_namespace\nis_lc_serializable\nget_input_schema\nget_output_schema\nget_graph\n__or__\n__ror__\ninvoke\nainvoke\nbatch\nabatch\ntransform\nstream\natransform\nastream\nget_name\nget_input_jsonschema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_prompts\npipe\npick\nassign\nbatch_as_completed\nabatch_as_completed\nastream_log\nastream_events\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nRunnableSerializable\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nlc_secrets\nlc_attributes\nto_json\nconfigurable_fields\nconfigurable_alternatives\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json_not_implemented\nRunnableConfig\ntags\nmetadata\ncallbacks\nrun_name\nmax_concurrency\nrecursion_limit\nconfigurable\nrun_id\nUtilities\nVector stores\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nRunnable\nKey Methods\nComposition\nStandard Methods\nDebugging and tracing\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nRunnableBinding\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nlc_secrets\nlc_attributes\nbound\nkwargs\nconfig\nconfig_factories\ncustom_input_type\ncustom_output_type\nbind\nwith_config\nwith_listeners\nwith_types\nwith_retry\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nwith_alisteners\nmap\nwith_fallbacks\nas_tool\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nRunnableGenerator\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\n__init__\nget_input_schema\nget_output_schema\ntransform\nstream\ninvoke\natransform\nastream\nainvoke\nget_name\nget_input_jsonschema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nastream_log\nastream_events\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nRunnableLambda\nname\nInputType\nOutputType\ndeps\nconfig_specs\ninput_schema\noutput_schema\n__init__\nget_input_schema\nget_output_schema\nget_graph\n__repr__\ninvoke\nainvoke\ntransform\nstream\natransform\nastream\nget_name\nget_input_jsonschema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nastream_log\nastream_events\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nRunnableParallel\nInputType\nconfig_specs\nname\nOutputType\ninput_schema\noutput_schema\nlc_secrets\nlc_attributes\n__init__\nis_lc_serializable\nget_lc_namespace\nget_name\nget_input_schema\nget_output_schema\nget_graph\ninvoke\nainvoke\ntransform\nstream\natransform\nastream\nget_input_jsonschema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nastream_log\nastream_events\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nRunnableSequence\nfirst\nmiddle\nlast\nsteps\nInputType\nOutputType\nconfig_specs\nname\ninput_schema\noutput_schema\nlc_secrets\nlc_attributes\n__init__\nget_lc_namespace\nis_lc_serializable\nget_input_schema\nget_output_schema\nget_graph\n__or__\n__ror__\ninvoke\nainvoke\nbatch\nabatch\ntransform\nstream\natransform\nastream\nget_name\nget_input_jsonschema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_prompts\npipe\npick\nassign\nbatch_as_completed\nabatch_as_completed\nastream_log\nastream_events\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nRunnableSerializable\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nlc_secrets\nlc_attributes\nto_json\nconfigurable_fields\nconfigurable_alternatives\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json_not_implemented\nRunnableConfig\ntags\nmetadata\ncallbacks\nrun_name\nmax_concurrency\nrecursion_limit\nconfigurable\nrun_id\nRunnables\nRunnable\n\u00b6\nBases:\nABC\n,\nGeneric\n[\nInput\n,\nOutput\n]\nA unit of work that can be invoked, batched, streamed, transformed and composed.\nKey Methods\n\u00b6\ninvoke\n/\nainvoke\n: Transforms a single input into an output.\nbatch\n/\nabatch\n: Efficiently transforms multiple inputs into outputs.\nstream\n/\nastream\n: Streams output from a single input as it's produced.\nastream_log\n: Streams output and selected intermediate results from an\ninput.\nBuilt-in optimizations:\nBatch\n: By default, batch runs invoke() in parallel using a thread pool\nexecutor. Override to optimize batching.\nAsync\n: Methods with\n'a'\nsuffix are asynchronous. By default, they execute\nthe sync counterpart using asyncio's thread pool.\nOverride for native async.\nAll methods accept an optional config argument, which can be used to configure\nexecution, add tags and metadata for tracing and debugging etc.\nRunnables expose schematic information about their input, output and config via\nthe\ninput_schema\nproperty, the\noutput_schema\nproperty and\nconfig_schema\nmethod.\nComposition\n\u00b6\nRunnable objects can be composed together to create chains in a declarative way.\nAny chain constructed this way will automatically have sync, async, batch, and\nstreaming support.\nThe main composition primitives are\nRunnableSequence\nand\nRunnableParallel\n.\nRunnableSequence\ninvokes a series of runnables sequentially, with\none Runnable's output serving as the next's input. Construct using\nthe\n|\noperator or by passing a list of runnables to\nRunnableSequence\n.\nRunnableParallel\ninvokes runnables concurrently, providing the same input\nto each. Construct it using a dict literal within a sequence or by passing a\ndict to\nRunnableParallel\n.\nFor example,\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n# A RunnableSequence constructed using the `|` operator\nsequence\n=\nRunnableLambda\n(\nlambda\nx\n:\nx\n+\n1\n)\n|\nRunnableLambda\n(\nlambda\nx\n:\nx\n*\n2\n)\nsequence\n.\ninvoke\n(\n1\n)\n# 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\n# [4, 6, 8]\n# A sequence that contains a RunnableParallel constructed using a dict literal\nsequence\n=\nRunnableLambda\n(\nlambda\nx\n:\nx\n+\n1\n)\n|\n{\n\"mul_2\"\n:\nRunnableLambda\n(\nlambda\nx\n:\nx\n*\n2\n),\n\"mul_5\"\n:\nRunnableLambda\n(\nlambda\nx\n:\nx\n*\n5\n),\n}\nsequence\n.\ninvoke\n(\n1\n)\n# {'mul_2': 4, 'mul_5': 10}\nStandard Methods\n\u00b6\nAll\nRunnable\ns expose additional methods that can be used to modify their\nbehavior (e.g., add a retry policy, add lifecycle listeners, make them\nconfigurable, etc.).\nThese methods will work on any\nRunnable\n, including\nRunnable\nchains\nconstructed by composing other\nRunnable\ns.\nSee the individual methods for details.\nFor example,\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nimport\nrandom\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nbuggy_double\n(\ny\n:\nint\n)\n->\nint\n:\n\"\"\"Buggy code that will fail 70% of the time\"\"\"\nif\nrandom\n.\nrandom\n()\n>\n0.3\n:\nprint\n(\n'This code failed, and will probably be retried!'\n)\n# noqa: T201\nraise\nValueError\n(\n'Triggered buggy code'\n)\nreturn\ny\n*\n2\nsequence\n=\n(\nRunnableLambda\n(\nadd_one\n)\n|\nRunnableLambda\n(\nbuggy_double\n)\n.\nwith_retry\n(\n# Retry on failure\nstop_after_attempt\n=\n10\n,\nwait_exponential_jitter\n=\nFalse\n)\n)\nprint\n(\nsequence\n.\ninput_schema\n.\nmodel_json_schema\n())\n# Show inferred input schema\nprint\n(\nsequence\n.\noutput_schema\n.\nmodel_json_schema\n())\n# Show inferred output schema\nprint\n(\nsequence\n.\ninvoke\n(\n2\n))\n# invoke the sequence (note the retry above!!)\nDebugging and tracing\n\u00b6\nAs the chains get longer, it can be useful to be able to see intermediate results\nto debug and trace the chain.\nYou can set the global debug flag to True to enable debug output for all chains:\nfrom\nlangchain_core.globals\nimport\nset_debug\nset_debug\n(\nTrue\n)\nAlternatively, you can pass existing or custom callbacks to any given chain:\nfrom\nlangchain_core.tracers\nimport\nConsoleCallbackHandler\nchain\n.\ninvoke\n(\n...\n,\nconfig\n=\n{\n\"callbacks\"\n:\n[\nConsoleCallbackHandler\n()]})\nFor a UI (and much more) checkout\nLangSmith\n.\nMETHOD\nDESCRIPTION\nget_name\nGet the name of the\nRunnable\n.\nget_input_schema\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nget_input_jsonschema\nGet a JSON schema that represents the input to the\nRunnable\n.\nget_output_schema\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nget_output_jsonschema\nGet a JSON schema that represents the output of the\nRunnable\n.\nconfig_schema\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nget_config_jsonschema\nGet a JSON schema that represents the config of the\nRunnable\n.\nget_graph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\nReturn a list of prompts used by this\nRunnable\n.\n__or__\nRunnable \"or\" operator.\n__ror__\nRunnable \"reverse-or\" operator.\npipe\nPipe\nRunnable\nobjects.\npick\nPick keys from the output\ndict\nof this\nRunnable\n.\nassign\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\ninvoke\nTransform a single input into an output.\nainvoke\nTransform a single input into an output.\nbatch\nDefault implementation runs invoke in parallel using a thread pool executor.\nbatch_as_completed\nRun\ninvoke\nin parallel on a list of inputs.\nabatch\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nabatch_as_completed\nRun\nainvoke\nin parallel on a list of inputs.\nstream\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nastream\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nastream_log\nStream all output from a\nRunnable\n, as reported to the callback system.\nastream_events\nGenerate a stream of events.\ntransform\nTransform inputs to outputs.\natransform\nTransform inputs to outputs.\nbind\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nwith_config\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nwith_listeners\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nwith_alisteners\nBind async lifecycle listeners to a\nRunnable\n.\nwith_types\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nwith_retry\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nmap\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nwith_fallbacks\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nas_tool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nname\ninstance-attribute\n\u00b6\nname\n:\nstr\n|\nNone\nThe name of the\nRunnable\n. Used for debugging and tracing.\nInputType\nproperty\n\u00b6\nInputType\n:\ntype\n[\nInput\n]\nInput type.\nThe type of input this\nRunnable\naccepts specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the input type cannot be inferred.\nOutputType\nproperty\n\u00b6\nOutputType\n:\ntype\n[\nOutput\n]\nOutput Type.\nThe type of output this\nRunnable\nproduces specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the output type cannot be inferred.\ninput_schema\nproperty\n\u00b6\ninput_schema\n:\ntype\n[\nBaseModel\n]\nThe type of input this\nRunnable\naccepts specified as a Pydantic model.\noutput_schema\nproperty\n\u00b6\noutput_schema\n:\ntype\n[\nBaseModel\n]\nOutput schema.\nThe type of output this\nRunnable\nproduces specified as a Pydantic model.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\n[\nConfigurableFieldSpec\n]\nList configurable fields for this\nRunnable\n.\nget_name\n\u00b6\nget_name\n(\nsuffix\n:\nstr\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nstr\nGet the name of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nsuffix\nAn optional suffix to append to the name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nname\nAn optional name to use instead of the\nRunnable\n's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nstr\nThe name of the\nRunnable\n.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic input schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an input schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate input.\nget_input_jsonschema\n\u00b6\nget_input_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the input to the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the input to the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_input_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic output schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an output schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate output.\nget_output_jsonschema\n\u00b6\nget_output_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the output of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the output of the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_output_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nconfig_schema\n\u00b6\nconfig_schema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nTo mark a field as configurable, see the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate config.\nget_config_jsonschema\n\u00b6\nget_config_jsonschema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the config of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the config of the\nRunnable\n.\nAdded in\nlangchain-core\n0.3.0\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nGraph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\n\u00b6\nget_prompts\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nlist\n[\nBasePromptTemplate\n]\nReturn a list of prompts used by this\nRunnable\n.\n__or__\n\u00b6\n__or__\n(\nother\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nRunnable \"or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]] |\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]] |\nCallable\n[[\nAny\n],\nOther\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\n__ror__\n\u00b6\n__ror__\n(\nother\n:\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nRunnable \"reverse-or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]] |\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]] |\nCallable\n[[\nOther\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nOther\n],\nAny\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nA new\nRunnable\n.\npipe\n\u00b6\npipe\n(\n*\nothers\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n],\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nPipe\nRunnable\nobjects.\nCompose this\nRunnable\nwith\nRunnable\n-like objects to make a\nRunnableSequence\n.\nEquivalent to\nRunnableSequence(self, *others)\nor\nself | others[0] | ...\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nmul_two\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n*\n2\nrunnable_1\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable_2\n=\nRunnableLambda\n(\nmul_two\n)\nsequence\n=\nrunnable_1\n.\npipe\n(\nrunnable_2\n)\n# Or equivalently:\n# sequence = runnable_1 | runnable_2\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence\n.\ninvoke\n(\n1\n)\nawait\nsequence\n.\nainvoke\n(\n1\n)\n# -> 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\nawait\nsequence\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# -> [4, 6, 8]\nPARAMETER\nDESCRIPTION\n*others\nOther\nRunnable\nor\nRunnable\n-like objects to compose\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n]\nDEFAULT:\n()\nname\nAn optional name for the resulting\nRunnableSequence\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\npick\n\u00b6\npick\n(\nkeys\n:\nstr\n|\nlist\n[\nstr\n])\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nPick keys from the output\ndict\nof this\nRunnable\n.\nPick a single key\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\njson_only_chain\n=\nchain\n.\npick\n(\n\"json\"\n)\njson_only_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> [1, 2, 3]\nPick a list of keys\nfrom\ntyping\nimport\nAny\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\ndef\nas_bytes\n(\nx\n:\nAny\n)\n->\nbytes\n:\nreturn\nbytes\n(\nx\n,\n\"utf-8\"\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n,\nbytes\n=\nRunnableLambda\n(\nas_bytes\n)\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\njson_and_bytes_chain\n=\nchain\n.\npick\n([\n\"json\"\n,\n\"bytes\"\n])\njson_and_bytes_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\nPARAMETER\nDESCRIPTION\nkeys\nA key or list of keys to pick from the output dict.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\na new\nRunnable\n.\nassign\n\u00b6\nassign\n(\n**\nkwargs\n:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]],\n)\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nfrom\nlangchain_core.language_models.fake\nimport\nFakeStreamingListLLM\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core.prompts\nimport\nSystemMessagePromptTemplate\nfrom\nlangchain_core.runnables\nimport\nRunnable\nfrom\noperator\nimport\nitemgetter\nprompt\n=\n(\nSystemMessagePromptTemplate\n.\nfrom_template\n(\n\"You are a nice assistant.\"\n)\n+\n\"\n{question}\n\"\n)\nmodel\n=\nFakeStreamingListLLM\n(\nresponses\n=\n[\n\"foo-lish\"\n])\nchain\n:\nRunnable\n=\nprompt\n|\nmodel\n|\n{\n\"str\"\n:\nStrOutputParser\n()}\nchain_with_assign\n=\nchain\n.\nassign\n(\nhello\n=\nitemgetter\n(\n\"str\"\n)\n|\nmodel\n)\nprint\n(\nchain_with_assign\n.\ninput_schema\n.\nmodel_json_schema\n())\n# {'title': 'PromptInput', 'type': 'object', 'properties':\n{\n'question'\n:\n{\n'title'\n:\n'Question'\n,\n'type'\n:\n'string'\n}}}\nprint\n(\nchain_with_assign\n.\noutput_schema\n.\nmodel_json_schema\n())\n# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n{\n'str'\n:\n{\n'title'\n:\n'Str'\n,\n'type'\n:\n'string'\n},\n'hello'\n:\n{\n'title'\n:\n'Hello'\n,\n'type'\n:\n'string'\n}}}\nPARAMETER\nDESCRIPTION\n**kwargs\nA mapping of keys to\nRunnable\nor\nRunnable\n-like objects\nthat will be invoked with the entire output dict of this\nRunnable\n.\nTYPE:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\nA new\nRunnable\n.\ninvoke\nabstractmethod\n\u00b6\ninvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nbatch\n\u00b6\nbatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n. The config supports\nstandard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work\nto do in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nbatch_as_completed\n\u00b6\nbatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\ninvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\ntuple\n[\nint\n,\nOutput\n|\nException\n]\nTuples of the index of the input and the output from the\nRunnable\n.\nabatch\nasync\n\u00b6\nabatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nThe default implementation of\nbatch\nworks well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nabatch_as_completed\nasync\n\u00b6\nabatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\nainvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nA tuple of the index of the input and the output from the\nRunnable\n.\nstream\n\u00b6\nstream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nAsyncIterator\n[\nOutput\n]\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nastream_log\nasync\n\u00b6\nastream_log\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ndiff\n:\nbool\n=\nTrue\n,\nwith_streamed_output_list\n:\nbool\n=\nTrue\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nRunLogPatch\n]\n|\nAsyncIterator\n[\nRunLog\n]\nStream all output from a\nRunnable\n, as reported to the callback system.\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\nOutput is streamed as Log objects, which include a list of\nJsonpatch ops that describe how the state of the run has changed in each\nstep, and the final state of the run.\nThe Jsonpatch ops can be applied in order to construct state.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ndiff\nWhether to yield diffs between each step or the current state.\nTYPE:\nbool\nDEFAULT:\nTrue\nwith_streamed_output_list\nWhether to yield the\nstreamed_output\nlist.\nTYPE:\nbool\nDEFAULT:\nTrue\ninclude_names\nOnly include logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nRunLogPatch\n] |\nAsyncIterator\n[\nRunLog\n]\nA\nRunLogPatch\nor\nRunLog\nobject.\nastream_events\nasync\n\u00b6\nastream_events\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nversion\n:\nLiteral\n[\n\"v1\"\n,\n\"v2\"\n]\n=\n\"v2\"\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nStreamEvent\n]\nGenerate a stream of events.\nUse to create an iterator over\nStreamEvent\nthat provide real-time information\nabout the progress of the\nRunnable\n, including\nStreamEvent\nfrom intermediate\nresults.\nA\nStreamEvent\nis a dictionary with the following schema:\nevent\n: Event names are of the format:\non_[runnable_type]_(start|stream|end)\n.\nname\n: The name of the\nRunnable\nthat generated the event.\nrun_id\n: Randomly generated ID associated with the given execution of the\nRunnable\nthat emitted the event. A child\nRunnable\nthat gets invoked as\npart of the execution of a parent\nRunnable\nis assigned its own unique ID.\nparent_ids\n: The IDs of the parent runnables that generated the event. The\nroot\nRunnable\nwill have an empty list. The order of the parent IDs is from\nthe root to the immediate parent. Only available for v2 version of the API.\nThe v1 version of the API will return an empty list.\ntags\n: The tags of the\nRunnable\nthat generated the event.\nmetadata\n: The metadata of the\nRunnable\nthat generated the event.\ndata\n: The data associated with the event. The contents of this field\ndepend on the type of event. See the table below for more details.\nBelow is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table.\nNote\nThis reference table is for the v2 version of the schema.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n'[model name]'\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n'[model name]'\n{'input': 'hello'}\non_llm_stream\n'[model name]'\n'Hello'\non_llm_end\n'[model name]'\n'Hello human!'\non_chain_start\n'format_docs'\non_chain_stream\n'format_docs'\n'hello world!, goodbye world!'\non_chain_end\n'format_docs'\n[Document(...)]\n'hello world!, goodbye world!'\non_tool_start\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n'[retriever name]'\n{\"query\": \"hello\"}\non_retriever_end\n'[retriever name]'\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n'[template_name]'\n{\"question\": \"hello\"}\non_prompt_end\n'[template_name]'\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nIn addition to the standard events, users can also dispatch custom events (see example below).\nCustom events will be only be surfaced with in the v2 version of the API!\nA custom event has following format:\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nHere are declarations associated with the standard events shown above:\nformat_docs\n:\ndef\nformat_docs\n(\ndocs\n:\nlist\n[\nDocument\n])\n->\nstr\n:\n'''Format the docs.'''\nreturn\n\", \"\n.\njoin\n([\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n])\nformat_docs\n=\nRunnableLambda\n(\nformat_docs\n)\nsome_tool\n:\n@tool\ndef\nsome_tool\n(\nx\n:\nint\n,\ny\n:\nstr\n)\n->\ndict\n:\n'''Some_tool.'''\nreturn\n{\n\"x\"\n:\nx\n,\n\"y\"\n:\ny\n}\nprompt\n:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are Cat Agent 007\"\n),\n(\n\"human\"\n,\n\"\n{question}\n\"\n),\n]\n)\n.\nwith_config\n({\n\"run_name\"\n:\n\"my_template\"\n,\n\"tags\"\n:\n[\n\"my_template\"\n]})\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nasync\ndef\nreverse\n(\ns\n:\nstr\n)\n->\nstr\n:\nreturn\ns\n[::\n-\n1\n]\nchain\n=\nRunnableLambda\n(\nfunc\n=\nreverse\n)\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"hello\"\n,\nversion\n=\n\"v2\"\n)\n]\n# Will produce the following events\n# (run_id, and parent_ids has been omitted for brevity):\n[\n{\n\"data\"\n:\n{\n\"input\"\n:\n\"hello\"\n},\n\"event\"\n:\n\"on_chain_start\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"chunk\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_stream\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"output\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_end\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n]\nDispatch custom event\nfrom\nlangchain_core.callbacks.manager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableConfig\nimport\nasyncio\nasync\ndef\nslow_thing\n(\nsome_input\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n->\nstr\n:\n\"\"\"Do something that takes a long time.\"\"\"\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 1 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 2 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nreturn\n\"Done\"\nslow_thing\n=\nRunnableLambda\n(\nslow_thing\n)\nasync\nfor\nevent\nin\nslow_thing\n.\nastream_events\n(\n\"some_input\"\n,\nversion\n=\n\"v2\"\n):\nprint\n(\nevent\n)\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nversion\nThe version of the schema to use, either\n'v2'\nor\n'v1'\n.\nUsers should use\n'v2'\n.\n'v1'\nis for backwards compatibility and will be deprecated\nin\n0.4.0\n.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in\n'v2'\n.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\ninclude_names\nOnly include events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nThese will be passed to\nastream_log\nas this implementation\nof\nastream_events\nis built on top of\nastream_log\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nStreamEvent\n]\nAn async stream of\nStreamEvent\n.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the version is not\n'v1'\nor\n'v2'\n.\ntransform\n\u00b6\ntransform\n(\ninput\n:\nIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of transform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn iterator of inputs to the\nRunnable\n.\nTYPE:\nIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\natransform\nasync\n\u00b6\natransform\n(\ninput\n:\nAsyncIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of atransform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn async iterator of inputs to the\nRunnable\n.\nTYPE:\nAsyncIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nbind\n\u00b6\nbind\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nUseful when a\nRunnable\nin a chain requires an argument that is not\nin the output of the previous\nRunnable\nor included in the user input.\nPARAMETER\nDESCRIPTION\n**kwargs\nThe arguments to bind to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the arguments bound.\nExample\nfrom\nlangchain_ollama\nimport\nChatOllama\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nmodel\n=\nChatOllama\n(\nmodel\n=\n\"llama3.1\"\n)\n# Without bind\nchain\n=\nmodel\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two three four five.'\n# With bind\nchain\n=\nmodel\n.\nbind\n(\nstop\n=\n[\n\"three\"\n])\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two'\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to bind to the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the config bound.\nwith_listeners\n\u00b6\nwith_listeners\n(\n*\n,\non_start\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_end\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_error\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled before the\nRunnable\nstarts running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_end\nCalled after the\nRunnable\nfinishes running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_error\nCalled if the\nRunnable\nthrows an error, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nfrom\nlangchain_core.tracers.schemas\nimport\nRun\nimport\ntime\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\ntime\n.\nsleep\n(\ntime_to_sleep\n)\ndef\nfn_start\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"start_time:\"\n,\nrun_obj\n.\nstart_time\n)\ndef\nfn_end\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"end_time:\"\n,\nrun_obj\n.\nend_time\n)\nchain\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_listeners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nchain\n.\ninvoke\n(\n2\n)\nwith_alisteners\n\u00b6\nwith_alisteners\n(\n*\n,\non_start\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_end\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_error\n:\nAsyncListener\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind async lifecycle listeners to a\nRunnable\n.\nReturns a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled asynchronously before the\nRunnable\nstarts running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_end\nCalled asynchronously after the\nRunnable\nfinishes running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_error\nCalled asynchronously if the\nRunnable\nthrows an error,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnable\nfrom\ndatetime\nimport\ndatetime\n,\ntimezone\nimport\ntime\nimport\nasyncio\ndef\nformat_t\n(\ntimestamp\n:\nfloat\n)\n->\nstr\n:\nreturn\ndatetime\n.\nfromtimestamp\n(\ntimestamp\n,\ntz\n=\ntimezone\n.\nutc\n)\n.\nisoformat\n()\nasync\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\ntime_to_sleep\n)\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_start\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on start callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n3\n)\nprint\n(\nf\n\"on start callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_end\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on end callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n2\n)\nprint\n(\nf\n\"on end callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nrunnable\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_alisteners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nasync\ndef\nconcurrent_runs\n():\nawait\nasyncio\n.\ngather\n(\nrunnable\n.\nainvoke\n(\n2\n),\nrunnable\n.\nainvoke\n(\n3\n))\nasyncio\n.\nrun\n(\nconcurrent_runs\n())\n# Result:\n# on start callback starts at 2025-03-01T07:05:22.875378+00:00\n# on start callback starts at 2025-03-01T07:05:22.875495+00:00\n# on start callback ends at 2025-03-01T07:05:25.878862+00:00\n# on start callback ends at 2025-03-01T07:05:25.878947+00:00\n# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n# on end callback starts at 2025-03-01T07:05:27.882360+00:00\n# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n# on end callback starts at 2025-03-01T07:05:28.882428+00:00\n# on end callback ends at 2025-03-01T07:05:29.883893+00:00\n# on end callback ends at 2025-03-01T07:05:30.884831+00:00\nwith_types\n\u00b6\nwith_types\n(\n*\n,\ninput_type\n:\ntype\n[\nInput\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\ntype\n[\nOutput\n]\n|\nNone\n=\nNone\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninput_type\nThe input type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nInput\n] | None\nDEFAULT:\nNone\noutput_type\nThe output type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nOutput\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the types bound.\nwith_retry\n\u00b6\nwith_retry\n(\n*\n,\nretry_if_exception_type\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nwait_exponential_jitter\n:\nbool\n=\nTrue\n,\nexponential_jitter_params\n:\nExponentialJitterParams\n|\nNone\n=\nNone\n,\nstop_after_attempt\n:\nint\n=\n3\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nPARAMETER\nDESCRIPTION\nretry_if_exception_type\nA tuple of exception types to retry on.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nwait_exponential_jitter\nWhether to add jitter to the wait\ntime between retries.\nTYPE:\nbool\nDEFAULT:\nTrue\nstop_after_attempt\nThe maximum number of attempts to make before\ngiving up.\nTYPE:\nint\nDEFAULT:\n3\nexponential_jitter_params\nParameters for\ntenacity.wait_exponential_jitter\n. Namely:\ninitial\n,\nmax\n,\nexp_base\n, and\njitter\n(all\nfloat\nvalues).\nTYPE:\nExponentialJitterParams\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ncount\n=\n0\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nNone\n:\nglobal\ncount\ncount\n=\ncount\n+\n1\nif\nx\n==\n1\n:\nraise\nValueError\n(\n\"x is 1\"\n)\nelse\n:\npass\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\ntry\n:\nrunnable\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n,\nretry_if_exception_type\n=\n(\nValueError\n,),\n)\n.\ninvoke\n(\n1\n)\nexcept\nValueError\n:\npass\nassert\ncount\n==\n2\nmap\n\u00b6\nmap\n()\n->\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nCalls\ninvoke\nwith each input.\nRETURNS\nDESCRIPTION\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nA new\nRunnable\nthat maps a list of inputs to a list of outputs.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\nprint\n(\nrunnable\n.\nmap\n()\n.\ninvoke\n([\n1\n,\n2\n,\n3\n]))\n# [2, 3, 4]\nwith_fallbacks\n\u00b6\nwith_fallbacks\n(\nfallbacks\n:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]],\n*\n,\nexceptions_to_handle\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nexception_key\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nThe new\nRunnable\nwill try the original\nRunnable\n, and then each fallback\nin order, upon failures.\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nExample\nfrom\ntyping\nimport\nIterator\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\ndef\n_generate_immediate_error\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nraise\nValueError\n()\nyield\n\"\"\ndef\n_generate\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nyield from\n\"foo bar\"\nrunnable\n=\nRunnableGenerator\n(\n_generate_immediate_error\n)\n.\nwith_fallbacks\n(\n[\nRunnableGenerator\n(\n_generate\n)]\n)\nprint\n(\n\"\"\n.\njoin\n(\nrunnable\n.\nstream\n({})))\n# foo bar\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nas_tool\n\u00b6\nas_tool\n(\nargs_schema\n:\ntype\n[\nBaseModel\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\narg_types\n:\ndict\n[\nstr\n,\ntype\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nas_tool\nwill instantiate a\nBaseTool\nwith a name, description, and\nargs_schema\nfrom a\nRunnable\n. Where possible, schemas are inferred\nfrom\nrunnable.get_input_schema\n.\nAlternatively (e.g., if the\nRunnable\ntakes a dict as input and the specific\ndict\nkeys are not typed), the schema can be specified directly with\nargs_schema\n.\nYou can also pass\narg_types\nto just specify the required arguments and their\ntypes.\nPARAMETER\nDESCRIPTION\nargs_schema\nThe schema for the tool.\nTYPE:\ntype\n[\nBaseModel\n] | None\nDEFAULT:\nNone\nname\nThe name of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nThe description of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\narg_types\nA dictionary of argument names to types.\nTYPE:\ndict\n[\nstr\n,\ntype\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBaseTool\nA\nBaseTool\ninstance.\nTypedDict\ninput\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nclass\nArgs\n(\nTypedDict\n):\na\n:\nint\nb\n:\nlist\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\nargs_schema\nfrom\ntyping\nimport\nAny\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nclass\nFSchema\n(\nBaseModel\n):\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n...\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nlist\n[\nint\n]\n=\nField\n(\n...\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nFSchema\n)\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\narg_types\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nlist\n[\nint\n]})\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\nstr\ninput\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n(\n\"b\"\n)\nRunnableBinding\n\u00b6\nBases:\nRunnableBindingBase\n[\nInput\n,\nOutput\n]\nWrap a\nRunnable\nwith additional functionality.\nA\nRunnableBinding\ncan be thought of as a \"runnable decorator\" that\npreserves the essential features of\nRunnable\n; i.e., batching, streaming,\nand async support, while adding additional functionality.\nAny class that inherits from\nRunnable\ncan be bound to a\nRunnableBinding\n.\nRunnables expose a standard set of methods for creating\nRunnableBindings\nor sub-classes of\nRunnableBindings\n(e.g.,\nRunnableRetry\n,\nRunnableWithFallbacks\n) that add additional functionality.\nThese methods include:\nbind\n: Bind kwargs to pass to the underlying\nRunnable\nwhen running it.\nwith_config\n: Bind config to pass to the underlying\nRunnable\nwhen running\nit.\nwith_listeners\n:  Bind lifecycle listeners to the underlying\nRunnable\n.\nwith_types\n: Override the input and output types of the underlying\nRunnable\n.\nwith_retry\n: Bind a retry policy to the underlying\nRunnable\n.\nwith_fallbacks\n: Bind a fallback policy to the underlying\nRunnable\n.\nExample:\nbind\n: Bind kwargs to pass to the underlying\nRunnable\nwhen running it.\n```python\n# Create a Runnable binding that invokes the chat model with the\n# additional kwarg `stop=['-']` when running it.\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI()\nmodel.invoke('Say \"Parrot-MAGIC\"', stop=[\"-\"])  # Should return `Parrot`\n# Using it the easy way via `bind` method which returns a new\n# RunnableBinding\nrunnable_binding = model.bind(stop=[\"-\"])\nrunnable_binding.invoke('Say \"Parrot-MAGIC\"')  # Should return `Parrot`\n```\nCan also be done by instantiating a `RunnableBinding` directly (not\nrecommended):\n```python\nfrom langchain_core.runnables import RunnableBinding\nrunnable_binding = RunnableBinding(\nbound=model,\nkwargs={\"stop\": [\"-\"]},  # <-- Note the additional kwargs\n)\nrunnable_binding.invoke('Say \"Parrot-MAGIC\"')  # Should return `Parrot`\n```\nMETHOD\nDESCRIPTION\nbind\nBind additional kwargs to a\nRunnable\n, returning a new\nRunnable\n.\nwith_config\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nwith_listeners\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nwith_types\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nwith_retry\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nget_name\nGet the name of the\nRunnable\n.\nget_input_schema\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nget_input_jsonschema\nGet a JSON schema that represents the input to the\nRunnable\n.\nget_output_schema\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nget_output_jsonschema\nGet a JSON schema that represents the output of the\nRunnable\n.\nconfig_schema\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nget_config_jsonschema\nGet a JSON schema that represents the config of the\nRunnable\n.\nget_graph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\nReturn a list of prompts used by this\nRunnable\n.\n__or__\nRunnable \"or\" operator.\n__ror__\nRunnable \"reverse-or\" operator.\npipe\nPipe\nRunnable\nobjects.\npick\nPick keys from the output\ndict\nof this\nRunnable\n.\nassign\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\ninvoke\nTransform a single input into an output.\nainvoke\nTransform a single input into an output.\nbatch\nDefault implementation runs invoke in parallel using a thread pool executor.\nbatch_as_completed\nRun\ninvoke\nin parallel on a list of inputs.\nabatch\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nabatch_as_completed\nRun\nainvoke\nin parallel on a list of inputs.\nstream\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nastream\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nastream_log\nStream all output from a\nRunnable\n, as reported to the callback system.\nastream_events\nGenerate a stream of events.\ntransform\nTransform inputs to outputs.\natransform\nTransform inputs to outputs.\nwith_alisteners\nBind async lifecycle listeners to a\nRunnable\n.\nmap\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nwith_fallbacks\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nas_tool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\n__init__\nCreate a\nRunnableBinding\nfrom a\nRunnable\nand kwargs.\nis_lc_serializable\nReturn\nTrue\nas this class is serializable.\nget_lc_namespace\nGet the namespace of the LangChain object.\nlc_id\nReturn a unique identifier for this class for serialization purposes.\nto_json\nSerialize the\nRunnable\nto JSON.\nto_json_not_implemented\nSerialize a \"not implemented\" object.\nconfigurable_fields\nConfigure particular\nRunnable\nfields at runtime.\nconfigurable_alternatives\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nname\nclass-attribute\ninstance-attribute\n\u00b6\nname\n:\nstr\n|\nNone\n=\nNone\nThe name of the\nRunnable\n. Used for debugging and tracing.\nInputType\nproperty\n\u00b6\nInputType\n:\ntype\n[\nInput\n]\nInput type.\nThe type of input this\nRunnable\naccepts specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the input type cannot be inferred.\nOutputType\nproperty\n\u00b6\nOutputType\n:\ntype\n[\nOutput\n]\nOutput Type.\nThe type of output this\nRunnable\nproduces specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the output type cannot be inferred.\ninput_schema\nproperty\n\u00b6\ninput_schema\n:\ntype\n[\nBaseModel\n]\nThe type of input this\nRunnable\naccepts specified as a Pydantic model.\noutput_schema\nproperty\n\u00b6\noutput_schema\n:\ntype\n[\nBaseModel\n]\nOutput schema.\nThe type of output this\nRunnable\nproduces specified as a Pydantic model.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\n[\nConfigurableFieldSpec\n]\nList configurable fields for this\nRunnable\n.\nlc_secrets\nproperty\n\u00b6\nlc_secrets\n:\ndict\n[\nstr\n,\nstr\n]\nA map of constructor argument names to secret ids.\nFor example,\n{\"openai_api_key\": \"OPENAI_API_KEY\"}\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nList of attribute names that should be included in the serialized kwargs.\nThese attributes must be accepted by the constructor.\nDefault is an empty dictionary.\nbound\ninstance-attribute\n\u00b6\nbound\n:\nRunnable\n[\nInput\n,\nOutput\n]\nThe underlying\nRunnable\nthat this\nRunnable\ndelegates to.\nkwargs\nclass-attribute\ninstance-attribute\n\u00b6\nkwargs\n:\nMapping\n[\nstr\n,\nAny\n]\n=\nField\n(\ndefault_factory\n=\ndict\n)\nkwargs to pass to the underlying\nRunnable\nwhen running.\nFor example, when the\nRunnable\nbinding is invoked the underlying\nRunnable\nwill be invoked with the same input but with these additional\nkwargs.\nconfig\nclass-attribute\ninstance-attribute\n\u00b6\nconfig\n:\nRunnableConfig\n=\nconfig\nor\n{}\nThe config to bind to the underlying\nRunnable\n.\nconfig_factories\nclass-attribute\ninstance-attribute\n\u00b6\nconfig_factories\n:\nlist\n[\nCallable\n[[\nRunnableConfig\n],\nRunnableConfig\n]]\n=\nField\n(\ndefault_factory\n=\nlist\n)\nThe config factories to bind to the underlying\nRunnable\n.\ncustom_input_type\nclass-attribute\ninstance-attribute\n\u00b6\ncustom_input_type\n:\nAny\n|\nNone\n=\nNone\nOverride the input type of the underlying\nRunnable\nwith a custom type.\nThe type can be a Pydantic model, or a type annotation (e.g.,\nlist[str]\n).\ncustom_output_type\nclass-attribute\ninstance-attribute\n\u00b6\ncustom_output_type\n:\nAny\n|\nNone\n=\nNone\nOverride the output type of the underlying\nRunnable\nwith a custom type.\nThe type can be a Pydantic model, or a type annotation (e.g.,\nlist[str]\n).\nbind\n\u00b6\nbind\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind additional kwargs to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\n**kwargs\nThe kwargs to bind to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the same type and config as the original,\nRunnable\n[\nInput\n,\nOutput\n]\nbut with the additional kwargs bound.\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to bind to the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the config bound.\nwith_listeners\n\u00b6\nwith_listeners\n(\n*\n,\non_start\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_end\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_error\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nThe\nRun\nobject contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled before the\nRunnable\nstarts running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_end\nCalled after the\nRunnable\nfinishes running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_error\nCalled if the\nRunnable\nthrows an error, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nwith_types\n\u00b6\nwith_types\n(\ninput_type\n:\ntype\n[\nInput\n]\n|\nBaseModel\n|\nNone\n=\nNone\n,\noutput_type\n:\ntype\n[\nOutput\n]\n|\nBaseModel\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninput_type\nThe input type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nInput\n] | None\nDEFAULT:\nNone\noutput_type\nThe output type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nOutput\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the types bound.\nwith_retry\n\u00b6\nwith_retry\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nPARAMETER\nDESCRIPTION\nretry_if_exception_type\nA tuple of exception types to retry on.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nwait_exponential_jitter\nWhether to add jitter to the wait\ntime between retries.\nTYPE:\nbool\nDEFAULT:\nTrue\nstop_after_attempt\nThe maximum number of attempts to make before\ngiving up.\nTYPE:\nint\nDEFAULT:\n3\nexponential_jitter_params\nParameters for\ntenacity.wait_exponential_jitter\n. Namely:\ninitial\n,\nmax\n,\nexp_base\n, and\njitter\n(all\nfloat\nvalues).\nTYPE:\nExponentialJitterParams\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ncount\n=\n0\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nNone\n:\nglobal\ncount\ncount\n=\ncount\n+\n1\nif\nx\n==\n1\n:\nraise\nValueError\n(\n\"x is 1\"\n)\nelse\n:\npass\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\ntry\n:\nrunnable\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n,\nretry_if_exception_type\n=\n(\nValueError\n,),\n)\n.\ninvoke\n(\n1\n)\nexcept\nValueError\n:\npass\nassert\ncount\n==\n2\nget_name\n\u00b6\nget_name\n(\nsuffix\n:\nstr\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nstr\nGet the name of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nsuffix\nAn optional suffix to append to the name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nname\nAn optional name to use instead of the\nRunnable\n's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nstr\nThe name of the\nRunnable\n.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic input schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an input schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate input.\nget_input_jsonschema\n\u00b6\nget_input_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the input to the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the input to the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_input_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic output schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an output schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate output.\nget_output_jsonschema\n\u00b6\nget_output_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the output of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the output of the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_output_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nconfig_schema\n\u00b6\nconfig_schema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nTo mark a field as configurable, see the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate config.\nget_config_jsonschema\n\u00b6\nget_config_jsonschema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the config of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the config of the\nRunnable\n.\nAdded in\nlangchain-core\n0.3.0\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nGraph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\n\u00b6\nget_prompts\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nlist\n[\nBasePromptTemplate\n]\nReturn a list of prompts used by this\nRunnable\n.\n__or__\n\u00b6\n__or__\n(\nother\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nRunnable \"or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]] |\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]] |\nCallable\n[[\nAny\n],\nOther\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\n__ror__\n\u00b6\n__ror__\n(\nother\n:\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nRunnable \"reverse-or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]] |\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]] |\nCallable\n[[\nOther\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nOther\n],\nAny\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nA new\nRunnable\n.\npipe\n\u00b6\npipe\n(\n*\nothers\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n],\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nPipe\nRunnable\nobjects.\nCompose this\nRunnable\nwith\nRunnable\n-like objects to make a\nRunnableSequence\n.\nEquivalent to\nRunnableSequence(self, *others)\nor\nself | others[0] | ...\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nmul_two\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n*\n2\nrunnable_1\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable_2\n=\nRunnableLambda\n(\nmul_two\n)\nsequence\n=\nrunnable_1\n.\npipe\n(\nrunnable_2\n)\n# Or equivalently:\n# sequence = runnable_1 | runnable_2\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence\n.\ninvoke\n(\n1\n)\nawait\nsequence\n.\nainvoke\n(\n1\n)\n# -> 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\nawait\nsequence\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# -> [4, 6, 8]\nPARAMETER\nDESCRIPTION\n*others\nOther\nRunnable\nor\nRunnable\n-like objects to compose\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n]\nDEFAULT:\n()\nname\nAn optional name for the resulting\nRunnableSequence\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\npick\n\u00b6\npick\n(\nkeys\n:\nstr\n|\nlist\n[\nstr\n])\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nPick keys from the output\ndict\nof this\nRunnable\n.\nPick a single key\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\njson_only_chain\n=\nchain\n.\npick\n(\n\"json\"\n)\njson_only_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> [1, 2, 3]\nPick a list of keys\nfrom\ntyping\nimport\nAny\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\ndef\nas_bytes\n(\nx\n:\nAny\n)\n->\nbytes\n:\nreturn\nbytes\n(\nx\n,\n\"utf-8\"\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n,\nbytes\n=\nRunnableLambda\n(\nas_bytes\n)\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\njson_and_bytes_chain\n=\nchain\n.\npick\n([\n\"json\"\n,\n\"bytes\"\n])\njson_and_bytes_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\nPARAMETER\nDESCRIPTION\nkeys\nA key or list of keys to pick from the output dict.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\na new\nRunnable\n.\nassign\n\u00b6\nassign\n(\n**\nkwargs\n:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]],\n)\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nfrom\nlangchain_core.language_models.fake\nimport\nFakeStreamingListLLM\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core.prompts\nimport\nSystemMessagePromptTemplate\nfrom\nlangchain_core.runnables\nimport\nRunnable\nfrom\noperator\nimport\nitemgetter\nprompt\n=\n(\nSystemMessagePromptTemplate\n.\nfrom_template\n(\n\"You are a nice assistant.\"\n)\n+\n\"\n{question}\n\"\n)\nmodel\n=\nFakeStreamingListLLM\n(\nresponses\n=\n[\n\"foo-lish\"\n])\nchain\n:\nRunnable\n=\nprompt\n|\nmodel\n|\n{\n\"str\"\n:\nStrOutputParser\n()}\nchain_with_assign\n=\nchain\n.\nassign\n(\nhello\n=\nitemgetter\n(\n\"str\"\n)\n|\nmodel\n)\nprint\n(\nchain_with_assign\n.\ninput_schema\n.\nmodel_json_schema\n())\n# {'title': 'PromptInput', 'type': 'object', 'properties':\n{\n'question'\n:\n{\n'title'\n:\n'Question'\n,\n'type'\n:\n'string'\n}}}\nprint\n(\nchain_with_assign\n.\noutput_schema\n.\nmodel_json_schema\n())\n# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n{\n'str'\n:\n{\n'title'\n:\n'Str'\n,\n'type'\n:\n'string'\n},\n'hello'\n:\n{\n'title'\n:\n'Hello'\n,\n'type'\n:\n'string'\n}}}\nPARAMETER\nDESCRIPTION\n**kwargs\nA mapping of keys to\nRunnable\nor\nRunnable\n-like objects\nthat will be invoked with the entire output dict of this\nRunnable\n.\nTYPE:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\nA new\nRunnable\n.\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nbatch\n\u00b6\nbatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n. The config supports\nstandard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work\nto do in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nbatch_as_completed\n\u00b6\nbatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\ninvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\ntuple\n[\nint\n,\nOutput\n|\nException\n]\nTuples of the index of the input and the output from the\nRunnable\n.\nabatch\nasync\n\u00b6\nabatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nThe default implementation of\nbatch\nworks well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nabatch_as_completed\nasync\n\u00b6\nabatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\nainvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nA tuple of the index of the input and the output from the\nRunnable\n.\nstream\n\u00b6\nstream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nAsyncIterator\n[\nOutput\n]\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nastream_log\nasync\n\u00b6\nastream_log\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ndiff\n:\nbool\n=\nTrue\n,\nwith_streamed_output_list\n:\nbool\n=\nTrue\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nRunLogPatch\n]\n|\nAsyncIterator\n[\nRunLog\n]\nStream all output from a\nRunnable\n, as reported to the callback system.\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\nOutput is streamed as Log objects, which include a list of\nJsonpatch ops that describe how the state of the run has changed in each\nstep, and the final state of the run.\nThe Jsonpatch ops can be applied in order to construct state.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ndiff\nWhether to yield diffs between each step or the current state.\nTYPE:\nbool\nDEFAULT:\nTrue\nwith_streamed_output_list\nWhether to yield the\nstreamed_output\nlist.\nTYPE:\nbool\nDEFAULT:\nTrue\ninclude_names\nOnly include logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nRunLogPatch\n] |\nAsyncIterator\n[\nRunLog\n]\nA\nRunLogPatch\nor\nRunLog\nobject.\nastream_events\nasync\n\u00b6\nastream_events\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nAsyncIterator\n[\nStreamEvent\n]\nGenerate a stream of events.\nUse to create an iterator over\nStreamEvent\nthat provide real-time information\nabout the progress of the\nRunnable\n, including\nStreamEvent\nfrom intermediate\nresults.\nA\nStreamEvent\nis a dictionary with the following schema:\nevent\n: Event names are of the format:\non_[runnable_type]_(start|stream|end)\n.\nname\n: The name of the\nRunnable\nthat generated the event.\nrun_id\n: Randomly generated ID associated with the given execution of the\nRunnable\nthat emitted the event. A child\nRunnable\nthat gets invoked as\npart of the execution of a parent\nRunnable\nis assigned its own unique ID.\nparent_ids\n: The IDs of the parent runnables that generated the event. The\nroot\nRunnable\nwill have an empty list. The order of the parent IDs is from\nthe root to the immediate parent. Only available for v2 version of the API.\nThe v1 version of the API will return an empty list.\ntags\n: The tags of the\nRunnable\nthat generated the event.\nmetadata\n: The metadata of the\nRunnable\nthat generated the event.\ndata\n: The data associated with the event. The contents of this field\ndepend on the type of event. See the table below for more details.\nBelow is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table.\nNote\nThis reference table is for the v2 version of the schema.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n'[model name]'\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n'[model name]'\n{'input': 'hello'}\non_llm_stream\n'[model name]'\n'Hello'\non_llm_end\n'[model name]'\n'Hello human!'\non_chain_start\n'format_docs'\non_chain_stream\n'format_docs'\n'hello world!, goodbye world!'\non_chain_end\n'format_docs'\n[Document(...)]\n'hello world!, goodbye world!'\non_tool_start\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n'[retriever name]'\n{\"query\": \"hello\"}\non_retriever_end\n'[retriever name]'\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n'[template_name]'\n{\"question\": \"hello\"}\non_prompt_end\n'[template_name]'\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nIn addition to the standard events, users can also dispatch custom events (see example below).\nCustom events will be only be surfaced with in the v2 version of the API!\nA custom event has following format:\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nHere are declarations associated with the standard events shown above:\nformat_docs\n:\ndef\nformat_docs\n(\ndocs\n:\nlist\n[\nDocument\n])\n->\nstr\n:\n'''Format the docs.'''\nreturn\n\", \"\n.\njoin\n([\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n])\nformat_docs\n=\nRunnableLambda\n(\nformat_docs\n)\nsome_tool\n:\n@tool\ndef\nsome_tool\n(\nx\n:\nint\n,\ny\n:\nstr\n)\n->\ndict\n:\n'''Some_tool.'''\nreturn\n{\n\"x\"\n:\nx\n,\n\"y\"\n:\ny\n}\nprompt\n:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are Cat Agent 007\"\n),\n(\n\"human\"\n,\n\"\n{question}\n\"\n),\n]\n)\n.\nwith_config\n({\n\"run_name\"\n:\n\"my_template\"\n,\n\"tags\"\n:\n[\n\"my_template\"\n]})\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nasync\ndef\nreverse\n(\ns\n:\nstr\n)\n->\nstr\n:\nreturn\ns\n[::\n-\n1\n]\nchain\n=\nRunnableLambda\n(\nfunc\n=\nreverse\n)\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"hello\"\n,\nversion\n=\n\"v2\"\n)\n]\n# Will produce the following events\n# (run_id, and parent_ids has been omitted for brevity):\n[\n{\n\"data\"\n:\n{\n\"input\"\n:\n\"hello\"\n},\n\"event\"\n:\n\"on_chain_start\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"chunk\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_stream\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"output\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_end\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n]\nDispatch custom event\nfrom\nlangchain_core.callbacks.manager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableConfig\nimport\nasyncio\nasync\ndef\nslow_thing\n(\nsome_input\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n->\nstr\n:\n\"\"\"Do something that takes a long time.\"\"\"\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 1 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 2 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nreturn\n\"Done\"\nslow_thing\n=\nRunnableLambda\n(\nslow_thing\n)\nasync\nfor\nevent\nin\nslow_thing\n.\nastream_events\n(\n\"some_input\"\n,\nversion\n=\n\"v2\"\n):\nprint\n(\nevent\n)\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nversion\nThe version of the schema to use, either\n'v2'\nor\n'v1'\n.\nUsers should use\n'v2'\n.\n'v1'\nis for backwards compatibility and will be deprecated\nin\n0.4.0\n.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in\n'v2'\n.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\ninclude_names\nOnly include events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nThese will be passed to\nastream_log\nas this implementation\nof\nastream_events\nis built on top of\nastream_log\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nStreamEvent\n]\nAn async stream of\nStreamEvent\n.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the version is not\n'v1'\nor\n'v2'\n.\ntransform\n\u00b6\ntransform\n(\ninput\n:\nIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of transform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn iterator of inputs to the\nRunnable\n.\nTYPE:\nIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\natransform\nasync\n\u00b6\natransform\n(\ninput\n:\nAsyncIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAsyncIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of atransform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn async iterator of inputs to the\nRunnable\n.\nTYPE:\nAsyncIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nwith_alisteners\n\u00b6\nwith_alisteners\n(\n*\n,\non_start\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_end\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_error\n:\nAsyncListener\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind async lifecycle listeners to a\nRunnable\n.\nReturns a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled asynchronously before the\nRunnable\nstarts running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_end\nCalled asynchronously after the\nRunnable\nfinishes running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_error\nCalled asynchronously if the\nRunnable\nthrows an error,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnable\nfrom\ndatetime\nimport\ndatetime\n,\ntimezone\nimport\ntime\nimport\nasyncio\ndef\nformat_t\n(\ntimestamp\n:\nfloat\n)\n->\nstr\n:\nreturn\ndatetime\n.\nfromtimestamp\n(\ntimestamp\n,\ntz\n=\ntimezone\n.\nutc\n)\n.\nisoformat\n()\nasync\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\ntime_to_sleep\n)\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_start\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on start callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n3\n)\nprint\n(\nf\n\"on start callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_end\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on end callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n2\n)\nprint\n(\nf\n\"on end callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nrunnable\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_alisteners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nasync\ndef\nconcurrent_runs\n():\nawait\nasyncio\n.\ngather\n(\nrunnable\n.\nainvoke\n(\n2\n),\nrunnable\n.\nainvoke\n(\n3\n))\nasyncio\n.\nrun\n(\nconcurrent_runs\n())\n# Result:\n# on start callback starts at 2025-03-01T07:05:22.875378+00:00\n# on start callback starts at 2025-03-01T07:05:22.875495+00:00\n# on start callback ends at 2025-03-01T07:05:25.878862+00:00\n# on start callback ends at 2025-03-01T07:05:25.878947+00:00\n# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n# on end callback starts at 2025-03-01T07:05:27.882360+00:00\n# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n# on end callback starts at 2025-03-01T07:05:28.882428+00:00\n# on end callback ends at 2025-03-01T07:05:29.883893+00:00\n# on end callback ends at 2025-03-01T07:05:30.884831+00:00\nmap\n\u00b6\nmap\n()\n->\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nCalls\ninvoke\nwith each input.\nRETURNS\nDESCRIPTION\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nA new\nRunnable\nthat maps a list of inputs to a list of outputs.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\nprint\n(\nrunnable\n.\nmap\n()\n.\ninvoke\n([\n1\n,\n2\n,\n3\n]))\n# [2, 3, 4]\nwith_fallbacks\n\u00b6\nwith_fallbacks\n(\nfallbacks\n:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]],\n*\n,\nexceptions_to_handle\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nexception_key\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nThe new\nRunnable\nwill try the original\nRunnable\n, and then each fallback\nin order, upon failures.\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nExample\nfrom\ntyping\nimport\nIterator\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\ndef\n_generate_immediate_error\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nraise\nValueError\n()\nyield\n\"\"\ndef\n_generate\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nyield from\n\"foo bar\"\nrunnable\n=\nRunnableGenerator\n(\n_generate_immediate_error\n)\n.\nwith_fallbacks\n(\n[\nRunnableGenerator\n(\n_generate\n)]\n)\nprint\n(\n\"\"\n.\njoin\n(\nrunnable\n.\nstream\n({})))\n# foo bar\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nas_tool\n\u00b6\nas_tool\n(\nargs_schema\n:\ntype\n[\nBaseModel\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\narg_types\n:\ndict\n[\nstr\n,\ntype\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nas_tool\nwill instantiate a\nBaseTool\nwith a name, description, and\nargs_schema\nfrom a\nRunnable\n. Where possible, schemas are inferred\nfrom\nrunnable.get_input_schema\n.\nAlternatively (e.g., if the\nRunnable\ntakes a dict as input and the specific\ndict\nkeys are not typed), the schema can be specified directly with\nargs_schema\n.\nYou can also pass\narg_types\nto just specify the required arguments and their\ntypes.\nPARAMETER\nDESCRIPTION\nargs_schema\nThe schema for the tool.\nTYPE:\ntype\n[\nBaseModel\n] | None\nDEFAULT:\nNone\nname\nThe name of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nThe description of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\narg_types\nA dictionary of argument names to types.\nTYPE:\ndict\n[\nstr\n,\ntype\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBaseTool\nA\nBaseTool\ninstance.\nTypedDict\ninput\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nclass\nArgs\n(\nTypedDict\n):\na\n:\nint\nb\n:\nlist\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\nargs_schema\nfrom\ntyping\nimport\nAny\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nclass\nFSchema\n(\nBaseModel\n):\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n...\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nlist\n[\nint\n]\n=\nField\n(\n...\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nFSchema\n)\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\narg_types\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nlist\n[\nint\n]})\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\nstr\ninput\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n(\n\"b\"\n)\n__init__\n\u00b6\n__init__\n(\n*\n,\nbound\n:\nRunnable\n[\nInput\n,\nOutput\n],\nkwargs\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\nconfig_factories\n:\nlist\n[\nCallable\n[[\nRunnableConfig\n],\nRunnableConfig\n]]\n|\nNone\n=\nNone\n,\ncustom_input_type\n:\ntype\n[\nInput\n]\n|\nBaseModel\n|\nNone\n=\nNone\n,\ncustom_output_type\n:\ntype\n[\nOutput\n]\n|\nBaseModel\n|\nNone\n=\nNone\n,\n**\nother_kwargs\n:\nAny\n,\n)\n->\nNone\nCreate a\nRunnableBinding\nfrom a\nRunnable\nand kwargs.\nPARAMETER\nDESCRIPTION\nbound\nThe underlying\nRunnable\nthat this\nRunnable\ndelegates calls\nto.\nTYPE:\nRunnable\n[\nInput\n,\nOutput\n]\nkwargs\noptional kwargs to pass to the underlying\nRunnable\n, when running\nthe underlying\nRunnable\n(e.g., via\ninvoke\n,\nbatch\n,\ntransform\n, or\nstream\nor async variants)\nTYPE:\nMapping\n[\nstr\n,\nAny\n] | None\nDEFAULT:\nNone\nconfig\noptional config to bind to the underlying\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nconfig_factories\noptional list of config factories to apply to the\nconfig before binding to the underlying\nRunnable\n.\nTYPE:\nlist\n[\nCallable\n[[\nRunnableConfig\n],\nRunnableConfig\n]] | None\nDEFAULT:\nNone\ncustom_input_type\nSpecify to override the input type of the underlying\nRunnable\nwith a custom type.\nTYPE:\ntype\n[\nInput\n] |\nBaseModel\n| None\nDEFAULT:\nNone\ncustom_output_type\nSpecify to override the output type of the underlying\nRunnable\nwith a custom type.\nTYPE:\ntype\n[\nOutput\n] |\nBaseModel\n| None\nDEFAULT:\nNone\n**other_kwargs\nUnpacked into the base class.\nTYPE:\nAny\nDEFAULT:\n{}\nis_lc_serializable\nclassmethod\n\u00b6\nis_lc_serializable\n()\n->\nbool\nReturn\nTrue\nas this class is serializable.\nget_lc_namespace\nclassmethod\n\u00b6\nget_lc_namespace\n()\n->\nlist\n[\nstr\n]\nGet the namespace of the LangChain object.\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\n[\"langchain\", \"schema\", \"runnable\"]\nlc_id\nclassmethod\n\u00b6\nlc_id\n()\n->\nlist\n[\nstr\n]\nReturn a unique identifier for this class for serialization purposes.\nThe unique identifier is a list of strings that describes the path\nto the object.\nFor example, for the class\nlangchain.llms.openai.OpenAI\n, the id is\n[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]\n.\nto_json\n\u00b6\nto_json\n()\n->\nSerializedConstructor\n|\nSerializedNotImplemented\nSerialize the\nRunnable\nto JSON.\nRETURNS\nDESCRIPTION\nSerializedConstructor\n|\nSerializedNotImplemented\nA JSON-serializable representation of the\nRunnable\n.\nto_json_not_implemented\n\u00b6\nto_json_not_implemented\n()\n->\nSerializedNotImplemented\nSerialize a \"not implemented\" object.\nRETURNS\nDESCRIPTION\nSerializedNotImplemented\nSerializedNotImplemented\n.\nconfigurable_fields\n\u00b6\nconfigurable_fields\n(\n**\nkwargs\n:\nAnyConfigurableField\n,\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure particular\nRunnable\nfields at runtime.\nPARAMETER\nDESCRIPTION\n**kwargs\nA dictionary of\nConfigurableField\ninstances to configure.\nTYPE:\nAnyConfigurableField\nDEFAULT:\n{}\nRAISES\nDESCRIPTION\nValueError\nIf a configuration key is not found in the\nRunnable\n.\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the fields configured.\nExample\nfrom\nlangchain_core.runnables\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmax_tokens\n=\n20\n)\n.\nconfigurable_fields\n(\nmax_tokens\n=\nConfigurableField\n(\nid\n=\n\"output_token_number\"\n,\nname\n=\n\"Max tokens in the output\"\n,\ndescription\n=\n\"The maximum number of tokens in the output\"\n,\n)\n)\n# max_tokens = 20\nprint\n(\n\"max_tokens_20: \"\n,\nmodel\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n)\n# max_tokens = 200\nprint\n(\n\"max_tokens_200: \"\n,\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"output_token_number\"\n:\n200\n})\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n,\n)\nconfigurable_alternatives\n\u00b6\nconfigurable_alternatives\n(\nwhich\n:\nConfigurableField\n,\n*\n,\ndefault_key\n:\nstr\n=\n\"default\"\n,\nprefix_keys\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nRunnable\n[\nInput\n,\nOutput\n]\n|\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nPARAMETER\nDESCRIPTION\nwhich\nThe\nConfigurableField\ninstance that will be used to select the\nalternative.\nTYPE:\nConfigurableField\ndefault_key\nThe default key to use if no alternative is selected.\nTYPE:\nstr\nDEFAULT:\n'default'\nprefix_keys\nWhether to prefix the keys with the\nConfigurableField\nid.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nA dictionary of keys to\nRunnable\ninstances or callables that\nreturn\nRunnable\ninstances.\nTYPE:\nRunnable\n[\nInput\n,\nOutput\n] |\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the alternatives configured.\nExample\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core.runnables.utils\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatAnthropic\n(\nmodel_name\n=\n\"claude-sonnet-4-5-20250929\"\n)\n.\nconfigurable_alternatives\n(\nConfigurableField\n(\nid\n=\n\"llm\"\n),\ndefault_key\n=\n\"anthropic\"\n,\nopenai\n=\nChatOpenAI\n(),\n)\n# uses the default model ChatAnthropic\nprint\n(\nmodel\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\n# uses ChatOpenAI\nprint\n(\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"openai\"\n})\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\nRunnableGenerator\n\u00b6\nBases:\nRunnable\n[\nInput\n,\nOutput\n]\nRunnable\nthat runs a generator function.\nRunnableGenerator\ns can be instantiated directly or by using a generator within\na sequence.\nRunnableGenerator\ns can be used to implement custom behavior, such as custom\noutput parsers, while preserving streaming capabilities. Given a generator function\nwith a signature\nIterator[A] -> Iterator[B]\n, wrapping it in a\nRunnableGenerator\nallows it to emit output chunks as soon as they are streamed\nin from the previous step.\nNote\nIf a generator function has a\nsignature A -> Iterator[B]\n, such that it\nrequires its input from the previous step to be completed before emitting chunks\n(e.g., most LLMs need the entire prompt available to start generating), it can\ninstead be wrapped in a\nRunnableLambda\n.\nHere is an example to show the basic mechanics of a\nRunnableGenerator\n:\n```python\nfrom typing import Any, AsyncIterator, Iterator\nfrom langchain_core.runnables import RunnableGenerator\ndef gen(input: Iterator[Any]) -> Iterator[str]:\nfor token in [\"Have\", \" a\", \" nice\", \" day\"]:\nyield token\nrunnable = RunnableGenerator(gen)\nrunnable.invoke(None)  # \"Have a nice day\"\nlist(runnable.stream(None))  # [\"Have\", \" a\", \" nice\", \" day\"]\nrunnable.batch([None, None])  # [\"Have a nice day\", \"Have a nice day\"]\n# Async version:\nasync def agen(input: AsyncIterator[Any]) -> AsyncIterator[str]:\nfor token in [\"Have\", \" a\", \" nice\", \" day\"]:\nyield token\nrunnable = RunnableGenerator(agen)\nawait runnable.ainvoke(None)  # \"Have a nice day\"\n[p async for p in runnable.astream(None)]  # [\"Have\", \" a\", \" nice\", \" day\"]\n```\nRunnableGenerator\nmakes it easy to implement custom behavior within a streaming\ncontext. Below we show an example:\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableGenerator, RunnableLambda\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nmodel = ChatOpenAI()\nchant_chain = (\nChatPromptTemplate.from_template(\"Give me a 3 word chant about {topic}\")\n| model\n| StrOutputParser()\n)\ndef character_generator(input: Iterator[str]) -> Iterator[str]:\nfor token in input:\nif \",\" in token or \".\" in token:\nyield \"\ud83d\udc4f\" + token\nelse:\nyield token\nrunnable = chant_chain | character_generator\nassert type(runnable.last) is RunnableGenerator\n\"\".join(runnable.stream({\"topic\": \"waste\"}))  # Reduce\ud83d\udc4f, Reuse\ud83d\udc4f, Recycle\ud83d\udc4f.\n# Note that RunnableLambda can be used to delay streaming of one step in a\n# sequence until the previous step is finished:\ndef reverse_generator(input: str) -> Iterator[str]:\n# Yield characters of input in reverse order.\nfor character in input[::-1]:\nyield character\nrunnable = chant_chain | RunnableLambda(reverse_generator)\n\"\".join(runnable.stream({\"topic\": \"waste\"}))  # \".elcycer ,esuer ,ecudeR\"\n```\nMETHOD\nDESCRIPTION\n__init__\nInitialize a\nRunnableGenerator\n.\nget_input_schema\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nget_output_schema\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\ntransform\nTransform inputs to outputs.\nstream\nDefault implementation of\nstream\n, which calls\ninvoke\n.\ninvoke\nTransform a single input into an output.\natransform\nTransform inputs to outputs.\nastream\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nainvoke\nTransform a single input into an output.\nget_name\nGet the name of the\nRunnable\n.\nget_input_jsonschema\nGet a JSON schema that represents the input to the\nRunnable\n.\nget_output_jsonschema\nGet a JSON schema that represents the output of the\nRunnable\n.\nconfig_schema\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nget_config_jsonschema\nGet a JSON schema that represents the config of the\nRunnable\n.\nget_graph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\nReturn a list of prompts used by this\nRunnable\n.\n__or__\nRunnable \"or\" operator.\n__ror__\nRunnable \"reverse-or\" operator.\npipe\nPipe\nRunnable\nobjects.\npick\nPick keys from the output\ndict\nof this\nRunnable\n.\nassign\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nbatch\nDefault implementation runs invoke in parallel using a thread pool executor.\nbatch_as_completed\nRun\ninvoke\nin parallel on a list of inputs.\nabatch\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nabatch_as_completed\nRun\nainvoke\nin parallel on a list of inputs.\nastream_log\nStream all output from a\nRunnable\n, as reported to the callback system.\nastream_events\nGenerate a stream of events.\nbind\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nwith_config\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nwith_listeners\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nwith_alisteners\nBind async lifecycle listeners to a\nRunnable\n.\nwith_types\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nwith_retry\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nmap\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nwith_fallbacks\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nas_tool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nname\ninstance-attribute\n\u00b6\nname\n=\nname\nor\n__name__\nThe name of the\nRunnable\n. Used for debugging and tracing.\nInputType\nproperty\n\u00b6\nInputType\n:\nAny\nInput type.\nThe type of input this\nRunnable\naccepts specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the input type cannot be inferred.\nOutputType\nproperty\n\u00b6\nOutputType\n:\nAny\nOutput Type.\nThe type of output this\nRunnable\nproduces specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the output type cannot be inferred.\ninput_schema\nproperty\n\u00b6\ninput_schema\n:\ntype\n[\nBaseModel\n]\nThe type of input this\nRunnable\naccepts specified as a Pydantic model.\noutput_schema\nproperty\n\u00b6\noutput_schema\n:\ntype\n[\nBaseModel\n]\nOutput schema.\nThe type of output this\nRunnable\nproduces specified as a Pydantic model.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\n[\nConfigurableFieldSpec\n]\nList configurable fields for this\nRunnable\n.\n__init__\n\u00b6\n__init__\n(\ntransform\n:\nCallable\n[[\nIterator\n[\nInput\n]],\nIterator\n[\nOutput\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nInput\n]],\nAsyncIterator\n[\nOutput\n]],\natransform\n:\nCallable\n[[\nAsyncIterator\n[\nInput\n]],\nAsyncIterator\n[\nOutput\n]]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nNone\nInitialize a\nRunnableGenerator\n.\nPARAMETER\nDESCRIPTION\ntransform\nThe transform function.\nTYPE:\nCallable\n[[\nIterator\n[\nInput\n]],\nIterator\n[\nOutput\n]] |\nCallable\n[[\nAsyncIterator\n[\nInput\n]],\nAsyncIterator\n[\nOutput\n]]\natransform\nThe async transform function.\nTYPE:\nCallable\n[[\nAsyncIterator\n[\nInput\n]],\nAsyncIterator\n[\nOutput\n]] | None\nDEFAULT:\nNone\nname\nThe name of the\nRunnable\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRAISES\nDESCRIPTION\nTypeError\nIf the transform is not a generator function.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic input schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an input schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate input.\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic output schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an output schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate output.\ntransform\n\u00b6\ntransform\n(\ninput\n:\nIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of transform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn iterator of inputs to the\nRunnable\n.\nTYPE:\nIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nstream\n\u00b6\nstream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nIterator\n[\nOutput\n]\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\natransform\n\u00b6\natransform\n(\ninput\n:\nAsyncIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAsyncIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of atransform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn async iterator of inputs to the\nRunnable\n.\nTYPE:\nAsyncIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nastream\n\u00b6\nastream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAsyncIterator\n[\nOutput\n]\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nget_name\n\u00b6\nget_name\n(\nsuffix\n:\nstr\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nstr\nGet the name of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nsuffix\nAn optional suffix to append to the name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nname\nAn optional name to use instead of the\nRunnable\n's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nstr\nThe name of the\nRunnable\n.\nget_input_jsonschema\n\u00b6\nget_input_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the input to the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the input to the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_input_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nget_output_jsonschema\n\u00b6\nget_output_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the output of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the output of the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_output_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nconfig_schema\n\u00b6\nconfig_schema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nTo mark a field as configurable, see the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate config.\nget_config_jsonschema\n\u00b6\nget_config_jsonschema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the config of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the config of the\nRunnable\n.\nAdded in\nlangchain-core\n0.3.0\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nGraph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\n\u00b6\nget_prompts\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nlist\n[\nBasePromptTemplate\n]\nReturn a list of prompts used by this\nRunnable\n.\n__or__\n\u00b6\n__or__\n(\nother\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nRunnable \"or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]] |\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]] |\nCallable\n[[\nAny\n],\nOther\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\n__ror__\n\u00b6\n__ror__\n(\nother\n:\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nRunnable \"reverse-or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]] |\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]] |\nCallable\n[[\nOther\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nOther\n],\nAny\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nA new\nRunnable\n.\npipe\n\u00b6\npipe\n(\n*\nothers\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n],\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nPipe\nRunnable\nobjects.\nCompose this\nRunnable\nwith\nRunnable\n-like objects to make a\nRunnableSequence\n.\nEquivalent to\nRunnableSequence(self, *others)\nor\nself | others[0] | ...\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nmul_two\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n*\n2\nrunnable_1\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable_2\n=\nRunnableLambda\n(\nmul_two\n)\nsequence\n=\nrunnable_1\n.\npipe\n(\nrunnable_2\n)\n# Or equivalently:\n# sequence = runnable_1 | runnable_2\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence\n.\ninvoke\n(\n1\n)\nawait\nsequence\n.\nainvoke\n(\n1\n)\n# -> 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\nawait\nsequence\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# -> [4, 6, 8]\nPARAMETER\nDESCRIPTION\n*others\nOther\nRunnable\nor\nRunnable\n-like objects to compose\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n]\nDEFAULT:\n()\nname\nAn optional name for the resulting\nRunnableSequence\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\npick\n\u00b6\npick\n(\nkeys\n:\nstr\n|\nlist\n[\nstr\n])\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nPick keys from the output\ndict\nof this\nRunnable\n.\nPick a single key\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\njson_only_chain\n=\nchain\n.\npick\n(\n\"json\"\n)\njson_only_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> [1, 2, 3]\nPick a list of keys\nfrom\ntyping\nimport\nAny\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\ndef\nas_bytes\n(\nx\n:\nAny\n)\n->\nbytes\n:\nreturn\nbytes\n(\nx\n,\n\"utf-8\"\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n,\nbytes\n=\nRunnableLambda\n(\nas_bytes\n)\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\njson_and_bytes_chain\n=\nchain\n.\npick\n([\n\"json\"\n,\n\"bytes\"\n])\njson_and_bytes_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\nPARAMETER\nDESCRIPTION\nkeys\nA key or list of keys to pick from the output dict.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\na new\nRunnable\n.\nassign\n\u00b6\nassign\n(\n**\nkwargs\n:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]],\n)\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nfrom\nlangchain_core.language_models.fake\nimport\nFakeStreamingListLLM\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core.prompts\nimport\nSystemMessagePromptTemplate\nfrom\nlangchain_core.runnables\nimport\nRunnable\nfrom\noperator\nimport\nitemgetter\nprompt\n=\n(\nSystemMessagePromptTemplate\n.\nfrom_template\n(\n\"You are a nice assistant.\"\n)\n+\n\"\n{question}\n\"\n)\nmodel\n=\nFakeStreamingListLLM\n(\nresponses\n=\n[\n\"foo-lish\"\n])\nchain\n:\nRunnable\n=\nprompt\n|\nmodel\n|\n{\n\"str\"\n:\nStrOutputParser\n()}\nchain_with_assign\n=\nchain\n.\nassign\n(\nhello\n=\nitemgetter\n(\n\"str\"\n)\n|\nmodel\n)\nprint\n(\nchain_with_assign\n.\ninput_schema\n.\nmodel_json_schema\n())\n# {'title': 'PromptInput', 'type': 'object', 'properties':\n{\n'question'\n:\n{\n'title'\n:\n'Question'\n,\n'type'\n:\n'string'\n}}}\nprint\n(\nchain_with_assign\n.\noutput_schema\n.\nmodel_json_schema\n())\n# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n{\n'str'\n:\n{\n'title'\n:\n'Str'\n,\n'type'\n:\n'string'\n},\n'hello'\n:\n{\n'title'\n:\n'Hello'\n,\n'type'\n:\n'string'\n}}}\nPARAMETER\nDESCRIPTION\n**kwargs\nA mapping of keys to\nRunnable\nor\nRunnable\n-like objects\nthat will be invoked with the entire output dict of this\nRunnable\n.\nTYPE:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\nA new\nRunnable\n.\nbatch\n\u00b6\nbatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n. The config supports\nstandard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work\nto do in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nbatch_as_completed\n\u00b6\nbatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\ninvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\ntuple\n[\nint\n,\nOutput\n|\nException\n]\nTuples of the index of the input and the output from the\nRunnable\n.\nabatch\nasync\n\u00b6\nabatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nThe default implementation of\nbatch\nworks well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nabatch_as_completed\nasync\n\u00b6\nabatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\nainvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nA tuple of the index of the input and the output from the\nRunnable\n.\nastream_log\nasync\n\u00b6\nastream_log\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ndiff\n:\nbool\n=\nTrue\n,\nwith_streamed_output_list\n:\nbool\n=\nTrue\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nRunLogPatch\n]\n|\nAsyncIterator\n[\nRunLog\n]\nStream all output from a\nRunnable\n, as reported to the callback system.\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\nOutput is streamed as Log objects, which include a list of\nJsonpatch ops that describe how the state of the run has changed in each\nstep, and the final state of the run.\nThe Jsonpatch ops can be applied in order to construct state.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ndiff\nWhether to yield diffs between each step or the current state.\nTYPE:\nbool\nDEFAULT:\nTrue\nwith_streamed_output_list\nWhether to yield the\nstreamed_output\nlist.\nTYPE:\nbool\nDEFAULT:\nTrue\ninclude_names\nOnly include logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nRunLogPatch\n] |\nAsyncIterator\n[\nRunLog\n]\nA\nRunLogPatch\nor\nRunLog\nobject.\nastream_events\nasync\n\u00b6\nastream_events\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nversion\n:\nLiteral\n[\n\"v1\"\n,\n\"v2\"\n]\n=\n\"v2\"\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nStreamEvent\n]\nGenerate a stream of events.\nUse to create an iterator over\nStreamEvent\nthat provide real-time information\nabout the progress of the\nRunnable\n, including\nStreamEvent\nfrom intermediate\nresults.\nA\nStreamEvent\nis a dictionary with the following schema:\nevent\n: Event names are of the format:\non_[runnable_type]_(start|stream|end)\n.\nname\n: The name of the\nRunnable\nthat generated the event.\nrun_id\n: Randomly generated ID associated with the given execution of the\nRunnable\nthat emitted the event. A child\nRunnable\nthat gets invoked as\npart of the execution of a parent\nRunnable\nis assigned its own unique ID.\nparent_ids\n: The IDs of the parent runnables that generated the event. The\nroot\nRunnable\nwill have an empty list. The order of the parent IDs is from\nthe root to the immediate parent. Only available for v2 version of the API.\nThe v1 version of the API will return an empty list.\ntags\n: The tags of the\nRunnable\nthat generated the event.\nmetadata\n: The metadata of the\nRunnable\nthat generated the event.\ndata\n: The data associated with the event. The contents of this field\ndepend on the type of event. See the table below for more details.\nBelow is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table.\nNote\nThis reference table is for the v2 version of the schema.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n'[model name]'\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n'[model name]'\n{'input': 'hello'}\non_llm_stream\n'[model name]'\n'Hello'\non_llm_end\n'[model name]'\n'Hello human!'\non_chain_start\n'format_docs'\non_chain_stream\n'format_docs'\n'hello world!, goodbye world!'\non_chain_end\n'format_docs'\n[Document(...)]\n'hello world!, goodbye world!'\non_tool_start\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n'[retriever name]'\n{\"query\": \"hello\"}\non_retriever_end\n'[retriever name]'\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n'[template_name]'\n{\"question\": \"hello\"}\non_prompt_end\n'[template_name]'\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nIn addition to the standard events, users can also dispatch custom events (see example below).\nCustom events will be only be surfaced with in the v2 version of the API!\nA custom event has following format:\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nHere are declarations associated with the standard events shown above:\nformat_docs\n:\ndef\nformat_docs\n(\ndocs\n:\nlist\n[\nDocument\n])\n->\nstr\n:\n'''Format the docs.'''\nreturn\n\", \"\n.\njoin\n([\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n])\nformat_docs\n=\nRunnableLambda\n(\nformat_docs\n)\nsome_tool\n:\n@tool\ndef\nsome_tool\n(\nx\n:\nint\n,\ny\n:\nstr\n)\n->\ndict\n:\n'''Some_tool.'''\nreturn\n{\n\"x\"\n:\nx\n,\n\"y\"\n:\ny\n}\nprompt\n:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are Cat Agent 007\"\n),\n(\n\"human\"\n,\n\"\n{question}\n\"\n),\n]\n)\n.\nwith_config\n({\n\"run_name\"\n:\n\"my_template\"\n,\n\"tags\"\n:\n[\n\"my_template\"\n]})\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nasync\ndef\nreverse\n(\ns\n:\nstr\n)\n->\nstr\n:\nreturn\ns\n[::\n-\n1\n]\nchain\n=\nRunnableLambda\n(\nfunc\n=\nreverse\n)\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"hello\"\n,\nversion\n=\n\"v2\"\n)\n]\n# Will produce the following events\n# (run_id, and parent_ids has been omitted for brevity):\n[\n{\n\"data\"\n:\n{\n\"input\"\n:\n\"hello\"\n},\n\"event\"\n:\n\"on_chain_start\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"chunk\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_stream\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"output\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_end\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n]\nDispatch custom event\nfrom\nlangchain_core.callbacks.manager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableConfig\nimport\nasyncio\nasync\ndef\nslow_thing\n(\nsome_input\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n->\nstr\n:\n\"\"\"Do something that takes a long time.\"\"\"\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 1 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 2 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nreturn\n\"Done\"\nslow_thing\n=\nRunnableLambda\n(\nslow_thing\n)\nasync\nfor\nevent\nin\nslow_thing\n.\nastream_events\n(\n\"some_input\"\n,\nversion\n=\n\"v2\"\n):\nprint\n(\nevent\n)\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nversion\nThe version of the schema to use, either\n'v2'\nor\n'v1'\n.\nUsers should use\n'v2'\n.\n'v1'\nis for backwards compatibility and will be deprecated\nin\n0.4.0\n.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in\n'v2'\n.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\ninclude_names\nOnly include events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nThese will be passed to\nastream_log\nas this implementation\nof\nastream_events\nis built on top of\nastream_log\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nStreamEvent\n]\nAn async stream of\nStreamEvent\n.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the version is not\n'v1'\nor\n'v2'\n.\nbind\n\u00b6\nbind\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nUseful when a\nRunnable\nin a chain requires an argument that is not\nin the output of the previous\nRunnable\nor included in the user input.\nPARAMETER\nDESCRIPTION\n**kwargs\nThe arguments to bind to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the arguments bound.\nExample\nfrom\nlangchain_ollama\nimport\nChatOllama\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nmodel\n=\nChatOllama\n(\nmodel\n=\n\"llama3.1\"\n)\n# Without bind\nchain\n=\nmodel\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two three four five.'\n# With bind\nchain\n=\nmodel\n.\nbind\n(\nstop\n=\n[\n\"three\"\n])\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two'\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to bind to the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the config bound.\nwith_listeners\n\u00b6\nwith_listeners\n(\n*\n,\non_start\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_end\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_error\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled before the\nRunnable\nstarts running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_end\nCalled after the\nRunnable\nfinishes running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_error\nCalled if the\nRunnable\nthrows an error, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nfrom\nlangchain_core.tracers.schemas\nimport\nRun\nimport\ntime\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\ntime\n.\nsleep\n(\ntime_to_sleep\n)\ndef\nfn_start\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"start_time:\"\n,\nrun_obj\n.\nstart_time\n)\ndef\nfn_end\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"end_time:\"\n,\nrun_obj\n.\nend_time\n)\nchain\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_listeners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nchain\n.\ninvoke\n(\n2\n)\nwith_alisteners\n\u00b6\nwith_alisteners\n(\n*\n,\non_start\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_end\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_error\n:\nAsyncListener\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind async lifecycle listeners to a\nRunnable\n.\nReturns a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled asynchronously before the\nRunnable\nstarts running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_end\nCalled asynchronously after the\nRunnable\nfinishes running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_error\nCalled asynchronously if the\nRunnable\nthrows an error,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnable\nfrom\ndatetime\nimport\ndatetime\n,\ntimezone\nimport\ntime\nimport\nasyncio\ndef\nformat_t\n(\ntimestamp\n:\nfloat\n)\n->\nstr\n:\nreturn\ndatetime\n.\nfromtimestamp\n(\ntimestamp\n,\ntz\n=\ntimezone\n.\nutc\n)\n.\nisoformat\n()\nasync\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\ntime_to_sleep\n)\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_start\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on start callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n3\n)\nprint\n(\nf\n\"on start callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_end\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on end callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n2\n)\nprint\n(\nf\n\"on end callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nrunnable\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_alisteners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nasync\ndef\nconcurrent_runs\n():\nawait\nasyncio\n.\ngather\n(\nrunnable\n.\nainvoke\n(\n2\n),\nrunnable\n.\nainvoke\n(\n3\n))\nasyncio\n.\nrun\n(\nconcurrent_runs\n())\n# Result:\n# on start callback starts at 2025-03-01T07:05:22.875378+00:00\n# on start callback starts at 2025-03-01T07:05:22.875495+00:00\n# on start callback ends at 2025-03-01T07:05:25.878862+00:00\n# on start callback ends at 2025-03-01T07:05:25.878947+00:00\n# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n# on end callback starts at 2025-03-01T07:05:27.882360+00:00\n# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n# on end callback starts at 2025-03-01T07:05:28.882428+00:00\n# on end callback ends at 2025-03-01T07:05:29.883893+00:00\n# on end callback ends at 2025-03-01T07:05:30.884831+00:00\nwith_types\n\u00b6\nwith_types\n(\n*\n,\ninput_type\n:\ntype\n[\nInput\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\ntype\n[\nOutput\n]\n|\nNone\n=\nNone\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninput_type\nThe input type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nInput\n] | None\nDEFAULT:\nNone\noutput_type\nThe output type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nOutput\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the types bound.\nwith_retry\n\u00b6\nwith_retry\n(\n*\n,\nretry_if_exception_type\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nwait_exponential_jitter\n:\nbool\n=\nTrue\n,\nexponential_jitter_params\n:\nExponentialJitterParams\n|\nNone\n=\nNone\n,\nstop_after_attempt\n:\nint\n=\n3\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nPARAMETER\nDESCRIPTION\nretry_if_exception_type\nA tuple of exception types to retry on.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nwait_exponential_jitter\nWhether to add jitter to the wait\ntime between retries.\nTYPE:\nbool\nDEFAULT:\nTrue\nstop_after_attempt\nThe maximum number of attempts to make before\ngiving up.\nTYPE:\nint\nDEFAULT:\n3\nexponential_jitter_params\nParameters for\ntenacity.wait_exponential_jitter\n. Namely:\ninitial\n,\nmax\n,\nexp_base\n, and\njitter\n(all\nfloat\nvalues).\nTYPE:\nExponentialJitterParams\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ncount\n=\n0\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nNone\n:\nglobal\ncount\ncount\n=\ncount\n+\n1\nif\nx\n==\n1\n:\nraise\nValueError\n(\n\"x is 1\"\n)\nelse\n:\npass\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\ntry\n:\nrunnable\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n,\nretry_if_exception_type\n=\n(\nValueError\n,),\n)\n.\ninvoke\n(\n1\n)\nexcept\nValueError\n:\npass\nassert\ncount\n==\n2\nmap\n\u00b6\nmap\n()\n->\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nCalls\ninvoke\nwith each input.\nRETURNS\nDESCRIPTION\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nA new\nRunnable\nthat maps a list of inputs to a list of outputs.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\nprint\n(\nrunnable\n.\nmap\n()\n.\ninvoke\n([\n1\n,\n2\n,\n3\n]))\n# [2, 3, 4]\nwith_fallbacks\n\u00b6\nwith_fallbacks\n(\nfallbacks\n:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]],\n*\n,\nexceptions_to_handle\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nexception_key\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nThe new\nRunnable\nwill try the original\nRunnable\n, and then each fallback\nin order, upon failures.\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nExample\nfrom\ntyping\nimport\nIterator\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\ndef\n_generate_immediate_error\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nraise\nValueError\n()\nyield\n\"\"\ndef\n_generate\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nyield from\n\"foo bar\"\nrunnable\n=\nRunnableGenerator\n(\n_generate_immediate_error\n)\n.\nwith_fallbacks\n(\n[\nRunnableGenerator\n(\n_generate\n)]\n)\nprint\n(\n\"\"\n.\njoin\n(\nrunnable\n.\nstream\n({})))\n# foo bar\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nas_tool\n\u00b6\nas_tool\n(\nargs_schema\n:\ntype\n[\nBaseModel\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\narg_types\n:\ndict\n[\nstr\n,\ntype\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nas_tool\nwill instantiate a\nBaseTool\nwith a name, description, and\nargs_schema\nfrom a\nRunnable\n. Where possible, schemas are inferred\nfrom\nrunnable.get_input_schema\n.\nAlternatively (e.g., if the\nRunnable\ntakes a dict as input and the specific\ndict\nkeys are not typed), the schema can be specified directly with\nargs_schema\n.\nYou can also pass\narg_types\nto just specify the required arguments and their\ntypes.\nPARAMETER\nDESCRIPTION\nargs_schema\nThe schema for the tool.\nTYPE:\ntype\n[\nBaseModel\n] | None\nDEFAULT:\nNone\nname\nThe name of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nThe description of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\narg_types\nA dictionary of argument names to types.\nTYPE:\ndict\n[\nstr\n,\ntype\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBaseTool\nA\nBaseTool\ninstance.\nTypedDict\ninput\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nclass\nArgs\n(\nTypedDict\n):\na\n:\nint\nb\n:\nlist\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\nargs_schema\nfrom\ntyping\nimport\nAny\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nclass\nFSchema\n(\nBaseModel\n):\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n...\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nlist\n[\nint\n]\n=\nField\n(\n...\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nFSchema\n)\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\narg_types\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nlist\n[\nint\n]})\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\nstr\ninput\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n(\n\"b\"\n)\nRunnableLambda\n\u00b6\nBases:\nRunnable\n[\nInput\n,\nOutput\n]\nRunnableLambda\nconverts a python callable into a\nRunnable\n.\nWrapping a callable in a\nRunnableLambda\nmakes the callable usable\nwithin either a sync or async context.\nRunnableLambda\ncan be composed as any other\nRunnable\nand provides\nseamless integration with LangChain tracing.\nRunnableLambda\nis best suited for code that does not need to support\nstreaming. If you need to support streaming (i.e., be able to operate\non chunks of inputs and yield chunks of outputs), use\nRunnableGenerator\ninstead.\nNote that if a\nRunnableLambda\nreturns an instance of\nRunnable\n, that\ninstance is invoked (or streamed) during execution.\nExamples:\n# This is a RunnableLambda\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable\n.\ninvoke\n(\n1\n)\n# returns 2\nrunnable\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\n# returns [2, 3, 4]\n# Async is supported by default by delegating to the sync implementation\nawait\nrunnable\n.\nainvoke\n(\n1\n)\n# returns 2\nawait\nrunnable\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# returns [2, 3, 4]\n# Alternatively, can provide both synd and sync implementations\nasync\ndef\nadd_one_async\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n,\nafunc\n=\nadd_one_async\n)\nrunnable\n.\ninvoke\n(\n1\n)\n# Uses add_one\nawait\nrunnable\n.\nainvoke\n(\n1\n)\n# Uses add_one_async\nMETHOD\nDESCRIPTION\n__init__\nCreate a\nRunnableLambda\nfrom a callable, and async callable or both.\nget_input_schema\nThe Pydantic schema for the input to this\nRunnable\n.\nget_output_schema\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nget_graph\nReturn a graph representation of this\nRunnable\n.\n__repr__\nReturn a string representation of this\nRunnable\n.\ninvoke\nInvoke this\nRunnable\nsynchronously.\nainvoke\nInvoke this\nRunnable\nasynchronously.\ntransform\nTransform inputs to outputs.\nstream\nDefault implementation of\nstream\n, which calls\ninvoke\n.\natransform\nTransform inputs to outputs.\nastream\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nget_name\nGet the name of the\nRunnable\n.\nget_input_jsonschema\nGet a JSON schema that represents the input to the\nRunnable\n.\nget_output_jsonschema\nGet a JSON schema that represents the output of the\nRunnable\n.\nconfig_schema\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nget_config_jsonschema\nGet a JSON schema that represents the config of the\nRunnable\n.\nget_prompts\nReturn a list of prompts used by this\nRunnable\n.\n__or__\nRunnable \"or\" operator.\n__ror__\nRunnable \"reverse-or\" operator.\npipe\nPipe\nRunnable\nobjects.\npick\nPick keys from the output\ndict\nof this\nRunnable\n.\nassign\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nbatch\nDefault implementation runs invoke in parallel using a thread pool executor.\nbatch_as_completed\nRun\ninvoke\nin parallel on a list of inputs.\nabatch\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nabatch_as_completed\nRun\nainvoke\nin parallel on a list of inputs.\nastream_log\nStream all output from a\nRunnable\n, as reported to the callback system.\nastream_events\nGenerate a stream of events.\nbind\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nwith_config\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nwith_listeners\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nwith_alisteners\nBind async lifecycle listeners to a\nRunnable\n.\nwith_types\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nwith_retry\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nmap\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nwith_fallbacks\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nas_tool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nname\ninstance-attribute\n\u00b6\nname\n=\nname\nThe name of the\nRunnable\n. Used for debugging and tracing.\nInputType\nproperty\n\u00b6\nInputType\n:\nAny\nThe type of the input to this\nRunnable\n.\nOutputType\nproperty\n\u00b6\nOutputType\n:\nAny\nThe type of the output of this\nRunnable\nas a type annotation.\nRETURNS\nDESCRIPTION\nAny\nThe type of the output of this\nRunnable\n.\ndeps\ncached\nproperty\n\u00b6\ndeps\n:\nlist\n[\nRunnable\n]\nThe dependencies of this\nRunnable\n.\nRETURNS\nDESCRIPTION\nlist\n[\nRunnable\n]\nThe dependencies of this\nRunnable\n. If the function has nonlocal\nlist\n[\nRunnable\n]\nvariables that are\nRunnable\ns, they are considered dependencies.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\n[\nConfigurableFieldSpec\n]\nList configurable fields for this\nRunnable\n.\ninput_schema\nproperty\n\u00b6\ninput_schema\n:\ntype\n[\nBaseModel\n]\nThe type of input this\nRunnable\naccepts specified as a Pydantic model.\noutput_schema\nproperty\n\u00b6\noutput_schema\n:\ntype\n[\nBaseModel\n]\nOutput schema.\nThe type of output this\nRunnable\nproduces specified as a Pydantic model.\n__init__\n\u00b6\n__init__\n(\nfunc\n:\nCallable\n[[\nInput\n],\nIterator\n[\nOutput\n]]\n|\nCallable\n[[\nInput\n],\nRunnable\n[\nInput\n,\nOutput\n]]\n|\nCallable\n[[\nInput\n],\nOutput\n]\n|\nCallable\n[[\nInput\n,\nRunnableConfig\n],\nOutput\n]\n|\nCallable\n[[\nInput\n,\nCallbackManagerForChainRun\n],\nOutput\n]\n|\nCallable\n[[\nInput\n,\nCallbackManagerForChainRun\n,\nRunnableConfig\n],\nOutput\n]\n|\nCallable\n[[\nInput\n],\nAwaitable\n[\nOutput\n]]\n|\nCallable\n[[\nInput\n],\nAsyncIterator\n[\nOutput\n]]\n|\nCallable\n[[\nInput\n,\nRunnableConfig\n],\nAwaitable\n[\nOutput\n]]\n|\nCallable\n[[\nInput\n,\nAsyncCallbackManagerForChainRun\n],\nAwaitable\n[\nOutput\n]]\n|\nCallable\n[\n[\nInput\n,\nAsyncCallbackManagerForChainRun\n,\nRunnableConfig\n],\nAwaitable\n[\nOutput\n]\n],\nafunc\n:\nCallable\n[[\nInput\n],\nAwaitable\n[\nOutput\n]]\n|\nCallable\n[[\nInput\n],\nAsyncIterator\n[\nOutput\n]]\n|\nCallable\n[[\nInput\n,\nRunnableConfig\n],\nAwaitable\n[\nOutput\n]]\n|\nCallable\n[[\nInput\n,\nAsyncCallbackManagerForChainRun\n],\nAwaitable\n[\nOutput\n]]\n|\nCallable\n[\n[\nInput\n,\nAsyncCallbackManagerForChainRun\n,\nRunnableConfig\n],\nAwaitable\n[\nOutput\n]\n]\n|\nNone\n=\nNone\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nNone\nCreate a\nRunnableLambda\nfrom a callable, and async callable or both.\nAccepts both sync and async variants to allow providing efficient\nimplementations for sync and async execution.\nPARAMETER\nDESCRIPTION\nfunc\nEither sync or async callable\nTYPE:\nCallable\n[[\nInput\n],\nIterator\n[\nOutput\n]] |\nCallable\n[[\nInput\n],\nRunnable\n[\nInput\n,\nOutput\n]] |\nCallable\n[[\nInput\n],\nOutput\n] |\nCallable\n[[\nInput\n,\nRunnableConfig\n],\nOutput\n] |\nCallable\n[[\nInput\n,\nCallbackManagerForChainRun\n],\nOutput\n] |\nCallable\n[[\nInput\n,\nCallbackManagerForChainRun\n,\nRunnableConfig\n],\nOutput\n] |\nCallable\n[[\nInput\n],\nAwaitable\n[\nOutput\n]] |\nCallable\n[[\nInput\n],\nAsyncIterator\n[\nOutput\n]] |\nCallable\n[[\nInput\n,\nRunnableConfig\n],\nAwaitable\n[\nOutput\n]] |\nCallable\n[[\nInput\n,\nAsyncCallbackManagerForChainRun\n],\nAwaitable\n[\nOutput\n]] |\nCallable\n[[\nInput\n,\nAsyncCallbackManagerForChainRun\n,\nRunnableConfig\n],\nAwaitable\n[\nOutput\n]]\nafunc\nAn async callable that takes an input and returns an output.\nTYPE:\nCallable\n[[\nInput\n],\nAwaitable\n[\nOutput\n]] |\nCallable\n[[\nInput\n],\nAsyncIterator\n[\nOutput\n]] |\nCallable\n[[\nInput\n,\nRunnableConfig\n],\nAwaitable\n[\nOutput\n]] |\nCallable\n[[\nInput\n,\nAsyncCallbackManagerForChainRun\n],\nAwaitable\n[\nOutput\n]] |\nCallable\n[[\nInput\n,\nAsyncCallbackManagerForChainRun\n,\nRunnableConfig\n],\nAwaitable\n[\nOutput\n]] | None\nDEFAULT:\nNone\nname\nThe name of the\nRunnable\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRAISES\nDESCRIPTION\nTypeError\nIf the\nfunc\nis not a callable type.\nTypeError\nIf both\nfunc\nand\nafunc\nare provided.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe Pydantic schema for the input to this\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nThe input schema for this\nRunnable\n.\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic output schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an output schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate output.\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nGraph\nReturn a graph representation of this\nRunnable\n.\n__repr__\n\u00b6\n__repr__\n()\n->\nstr\nReturn a string representation of this\nRunnable\n.\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nOutput\nInvoke this\nRunnable\nsynchronously.\nPARAMETER\nDESCRIPTION\ninput\nThe input to this\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nOutput\nThe output of this\nRunnable\n.\nRAISES\nDESCRIPTION\nTypeError\nIf the\nRunnable\nis a coroutine function.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nOutput\nInvoke this\nRunnable\nasynchronously.\nPARAMETER\nDESCRIPTION\ninput\nThe input to this\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nOutput\nThe output of this\nRunnable\n.\ntransform\n\u00b6\ntransform\n(\ninput\n:\nIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of transform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn iterator of inputs to the\nRunnable\n.\nTYPE:\nIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nstream\n\u00b6\nstream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\natransform\nasync\n\u00b6\natransform\n(\ninput\n:\nAsyncIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of atransform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn async iterator of inputs to the\nRunnable\n.\nTYPE:\nAsyncIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nAsyncIterator\n[\nOutput\n]\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nget_name\n\u00b6\nget_name\n(\nsuffix\n:\nstr\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nstr\nGet the name of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nsuffix\nAn optional suffix to append to the name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nname\nAn optional name to use instead of the\nRunnable\n's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nstr\nThe name of the\nRunnable\n.\nget_input_jsonschema\n\u00b6\nget_input_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the input to the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the input to the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_input_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nget_output_jsonschema\n\u00b6\nget_output_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the output of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the output of the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_output_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nconfig_schema\n\u00b6\nconfig_schema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nTo mark a field as configurable, see the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate config.\nget_config_jsonschema\n\u00b6\nget_config_jsonschema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the config of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the config of the\nRunnable\n.\nAdded in\nlangchain-core\n0.3.0\nget_prompts\n\u00b6\nget_prompts\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nlist\n[\nBasePromptTemplate\n]\nReturn a list of prompts used by this\nRunnable\n.\n__or__\n\u00b6\n__or__\n(\nother\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nRunnable \"or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]] |\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]] |\nCallable\n[[\nAny\n],\nOther\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\n__ror__\n\u00b6\n__ror__\n(\nother\n:\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nRunnable \"reverse-or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]] |\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]] |\nCallable\n[[\nOther\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nOther\n],\nAny\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nA new\nRunnable\n.\npipe\n\u00b6\npipe\n(\n*\nothers\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n],\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nPipe\nRunnable\nobjects.\nCompose this\nRunnable\nwith\nRunnable\n-like objects to make a\nRunnableSequence\n.\nEquivalent to\nRunnableSequence(self, *others)\nor\nself | others[0] | ...\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nmul_two\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n*\n2\nrunnable_1\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable_2\n=\nRunnableLambda\n(\nmul_two\n)\nsequence\n=\nrunnable_1\n.\npipe\n(\nrunnable_2\n)\n# Or equivalently:\n# sequence = runnable_1 | runnable_2\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence\n.\ninvoke\n(\n1\n)\nawait\nsequence\n.\nainvoke\n(\n1\n)\n# -> 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\nawait\nsequence\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# -> [4, 6, 8]\nPARAMETER\nDESCRIPTION\n*others\nOther\nRunnable\nor\nRunnable\n-like objects to compose\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n]\nDEFAULT:\n()\nname\nAn optional name for the resulting\nRunnableSequence\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\npick\n\u00b6\npick\n(\nkeys\n:\nstr\n|\nlist\n[\nstr\n])\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nPick keys from the output\ndict\nof this\nRunnable\n.\nPick a single key\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\njson_only_chain\n=\nchain\n.\npick\n(\n\"json\"\n)\njson_only_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> [1, 2, 3]\nPick a list of keys\nfrom\ntyping\nimport\nAny\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\ndef\nas_bytes\n(\nx\n:\nAny\n)\n->\nbytes\n:\nreturn\nbytes\n(\nx\n,\n\"utf-8\"\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n,\nbytes\n=\nRunnableLambda\n(\nas_bytes\n)\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\njson_and_bytes_chain\n=\nchain\n.\npick\n([\n\"json\"\n,\n\"bytes\"\n])\njson_and_bytes_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\nPARAMETER\nDESCRIPTION\nkeys\nA key or list of keys to pick from the output dict.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\na new\nRunnable\n.\nassign\n\u00b6\nassign\n(\n**\nkwargs\n:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]],\n)\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nfrom\nlangchain_core.language_models.fake\nimport\nFakeStreamingListLLM\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core.prompts\nimport\nSystemMessagePromptTemplate\nfrom\nlangchain_core.runnables\nimport\nRunnable\nfrom\noperator\nimport\nitemgetter\nprompt\n=\n(\nSystemMessagePromptTemplate\n.\nfrom_template\n(\n\"You are a nice assistant.\"\n)\n+\n\"\n{question}\n\"\n)\nmodel\n=\nFakeStreamingListLLM\n(\nresponses\n=\n[\n\"foo-lish\"\n])\nchain\n:\nRunnable\n=\nprompt\n|\nmodel\n|\n{\n\"str\"\n:\nStrOutputParser\n()}\nchain_with_assign\n=\nchain\n.\nassign\n(\nhello\n=\nitemgetter\n(\n\"str\"\n)\n|\nmodel\n)\nprint\n(\nchain_with_assign\n.\ninput_schema\n.\nmodel_json_schema\n())\n# {'title': 'PromptInput', 'type': 'object', 'properties':\n{\n'question'\n:\n{\n'title'\n:\n'Question'\n,\n'type'\n:\n'string'\n}}}\nprint\n(\nchain_with_assign\n.\noutput_schema\n.\nmodel_json_schema\n())\n# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n{\n'str'\n:\n{\n'title'\n:\n'Str'\n,\n'type'\n:\n'string'\n},\n'hello'\n:\n{\n'title'\n:\n'Hello'\n,\n'type'\n:\n'string'\n}}}\nPARAMETER\nDESCRIPTION\n**kwargs\nA mapping of keys to\nRunnable\nor\nRunnable\n-like objects\nthat will be invoked with the entire output dict of this\nRunnable\n.\nTYPE:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\nA new\nRunnable\n.\nbatch\n\u00b6\nbatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n. The config supports\nstandard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work\nto do in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nbatch_as_completed\n\u00b6\nbatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\ninvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\ntuple\n[\nint\n,\nOutput\n|\nException\n]\nTuples of the index of the input and the output from the\nRunnable\n.\nabatch\nasync\n\u00b6\nabatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nThe default implementation of\nbatch\nworks well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nabatch_as_completed\nasync\n\u00b6\nabatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\nainvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nA tuple of the index of the input and the output from the\nRunnable\n.\nastream_log\nasync\n\u00b6\nastream_log\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ndiff\n:\nbool\n=\nTrue\n,\nwith_streamed_output_list\n:\nbool\n=\nTrue\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nRunLogPatch\n]\n|\nAsyncIterator\n[\nRunLog\n]\nStream all output from a\nRunnable\n, as reported to the callback system.\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\nOutput is streamed as Log objects, which include a list of\nJsonpatch ops that describe how the state of the run has changed in each\nstep, and the final state of the run.\nThe Jsonpatch ops can be applied in order to construct state.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ndiff\nWhether to yield diffs between each step or the current state.\nTYPE:\nbool\nDEFAULT:\nTrue\nwith_streamed_output_list\nWhether to yield the\nstreamed_output\nlist.\nTYPE:\nbool\nDEFAULT:\nTrue\ninclude_names\nOnly include logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nRunLogPatch\n] |\nAsyncIterator\n[\nRunLog\n]\nA\nRunLogPatch\nor\nRunLog\nobject.\nastream_events\nasync\n\u00b6\nastream_events\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nversion\n:\nLiteral\n[\n\"v1\"\n,\n\"v2\"\n]\n=\n\"v2\"\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nStreamEvent\n]\nGenerate a stream of events.\nUse to create an iterator over\nStreamEvent\nthat provide real-time information\nabout the progress of the\nRunnable\n, including\nStreamEvent\nfrom intermediate\nresults.\nA\nStreamEvent\nis a dictionary with the following schema:\nevent\n: Event names are of the format:\non_[runnable_type]_(start|stream|end)\n.\nname\n: The name of the\nRunnable\nthat generated the event.\nrun_id\n: Randomly generated ID associated with the given execution of the\nRunnable\nthat emitted the event. A child\nRunnable\nthat gets invoked as\npart of the execution of a parent\nRunnable\nis assigned its own unique ID.\nparent_ids\n: The IDs of the parent runnables that generated the event. The\nroot\nRunnable\nwill have an empty list. The order of the parent IDs is from\nthe root to the immediate parent. Only available for v2 version of the API.\nThe v1 version of the API will return an empty list.\ntags\n: The tags of the\nRunnable\nthat generated the event.\nmetadata\n: The metadata of the\nRunnable\nthat generated the event.\ndata\n: The data associated with the event. The contents of this field\ndepend on the type of event. See the table below for more details.\nBelow is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table.\nNote\nThis reference table is for the v2 version of the schema.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n'[model name]'\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n'[model name]'\n{'input': 'hello'}\non_llm_stream\n'[model name]'\n'Hello'\non_llm_end\n'[model name]'\n'Hello human!'\non_chain_start\n'format_docs'\non_chain_stream\n'format_docs'\n'hello world!, goodbye world!'\non_chain_end\n'format_docs'\n[Document(...)]\n'hello world!, goodbye world!'\non_tool_start\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n'[retriever name]'\n{\"query\": \"hello\"}\non_retriever_end\n'[retriever name]'\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n'[template_name]'\n{\"question\": \"hello\"}\non_prompt_end\n'[template_name]'\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nIn addition to the standard events, users can also dispatch custom events (see example below).\nCustom events will be only be surfaced with in the v2 version of the API!\nA custom event has following format:\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nHere are declarations associated with the standard events shown above:\nformat_docs\n:\ndef\nformat_docs\n(\ndocs\n:\nlist\n[\nDocument\n])\n->\nstr\n:\n'''Format the docs.'''\nreturn\n\", \"\n.\njoin\n([\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n])\nformat_docs\n=\nRunnableLambda\n(\nformat_docs\n)\nsome_tool\n:\n@tool\ndef\nsome_tool\n(\nx\n:\nint\n,\ny\n:\nstr\n)\n->\ndict\n:\n'''Some_tool.'''\nreturn\n{\n\"x\"\n:\nx\n,\n\"y\"\n:\ny\n}\nprompt\n:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are Cat Agent 007\"\n),\n(\n\"human\"\n,\n\"\n{question}\n\"\n),\n]\n)\n.\nwith_config\n({\n\"run_name\"\n:\n\"my_template\"\n,\n\"tags\"\n:\n[\n\"my_template\"\n]})\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nasync\ndef\nreverse\n(\ns\n:\nstr\n)\n->\nstr\n:\nreturn\ns\n[::\n-\n1\n]\nchain\n=\nRunnableLambda\n(\nfunc\n=\nreverse\n)\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"hello\"\n,\nversion\n=\n\"v2\"\n)\n]\n# Will produce the following events\n# (run_id, and parent_ids has been omitted for brevity):\n[\n{\n\"data\"\n:\n{\n\"input\"\n:\n\"hello\"\n},\n\"event\"\n:\n\"on_chain_start\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"chunk\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_stream\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"output\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_end\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n]\nDispatch custom event\nfrom\nlangchain_core.callbacks.manager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableConfig\nimport\nasyncio\nasync\ndef\nslow_thing\n(\nsome_input\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n->\nstr\n:\n\"\"\"Do something that takes a long time.\"\"\"\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 1 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 2 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nreturn\n\"Done\"\nslow_thing\n=\nRunnableLambda\n(\nslow_thing\n)\nasync\nfor\nevent\nin\nslow_thing\n.\nastream_events\n(\n\"some_input\"\n,\nversion\n=\n\"v2\"\n):\nprint\n(\nevent\n)\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nversion\nThe version of the schema to use, either\n'v2'\nor\n'v1'\n.\nUsers should use\n'v2'\n.\n'v1'\nis for backwards compatibility and will be deprecated\nin\n0.4.0\n.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in\n'v2'\n.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\ninclude_names\nOnly include events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nThese will be passed to\nastream_log\nas this implementation\nof\nastream_events\nis built on top of\nastream_log\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nStreamEvent\n]\nAn async stream of\nStreamEvent\n.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the version is not\n'v1'\nor\n'v2'\n.\nbind\n\u00b6\nbind\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nUseful when a\nRunnable\nin a chain requires an argument that is not\nin the output of the previous\nRunnable\nor included in the user input.\nPARAMETER\nDESCRIPTION\n**kwargs\nThe arguments to bind to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the arguments bound.\nExample\nfrom\nlangchain_ollama\nimport\nChatOllama\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nmodel\n=\nChatOllama\n(\nmodel\n=\n\"llama3.1\"\n)\n# Without bind\nchain\n=\nmodel\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two three four five.'\n# With bind\nchain\n=\nmodel\n.\nbind\n(\nstop\n=\n[\n\"three\"\n])\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two'\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to bind to the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the config bound.\nwith_listeners\n\u00b6\nwith_listeners\n(\n*\n,\non_start\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_end\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_error\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled before the\nRunnable\nstarts running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_end\nCalled after the\nRunnable\nfinishes running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_error\nCalled if the\nRunnable\nthrows an error, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nfrom\nlangchain_core.tracers.schemas\nimport\nRun\nimport\ntime\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\ntime\n.\nsleep\n(\ntime_to_sleep\n)\ndef\nfn_start\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"start_time:\"\n,\nrun_obj\n.\nstart_time\n)\ndef\nfn_end\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"end_time:\"\n,\nrun_obj\n.\nend_time\n)\nchain\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_listeners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nchain\n.\ninvoke\n(\n2\n)\nwith_alisteners\n\u00b6\nwith_alisteners\n(\n*\n,\non_start\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_end\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_error\n:\nAsyncListener\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind async lifecycle listeners to a\nRunnable\n.\nReturns a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled asynchronously before the\nRunnable\nstarts running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_end\nCalled asynchronously after the\nRunnable\nfinishes running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_error\nCalled asynchronously if the\nRunnable\nthrows an error,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnable\nfrom\ndatetime\nimport\ndatetime\n,\ntimezone\nimport\ntime\nimport\nasyncio\ndef\nformat_t\n(\ntimestamp\n:\nfloat\n)\n->\nstr\n:\nreturn\ndatetime\n.\nfromtimestamp\n(\ntimestamp\n,\ntz\n=\ntimezone\n.\nutc\n)\n.\nisoformat\n()\nasync\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\ntime_to_sleep\n)\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_start\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on start callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n3\n)\nprint\n(\nf\n\"on start callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_end\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on end callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n2\n)\nprint\n(\nf\n\"on end callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nrunnable\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_alisteners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nasync\ndef\nconcurrent_runs\n():\nawait\nasyncio\n.\ngather\n(\nrunnable\n.\nainvoke\n(\n2\n),\nrunnable\n.\nainvoke\n(\n3\n))\nasyncio\n.\nrun\n(\nconcurrent_runs\n())\n# Result:\n# on start callback starts at 2025-03-01T07:05:22.875378+00:00\n# on start callback starts at 2025-03-01T07:05:22.875495+00:00\n# on start callback ends at 2025-03-01T07:05:25.878862+00:00\n# on start callback ends at 2025-03-01T07:05:25.878947+00:00\n# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n# on end callback starts at 2025-03-01T07:05:27.882360+00:00\n# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n# on end callback starts at 2025-03-01T07:05:28.882428+00:00\n# on end callback ends at 2025-03-01T07:05:29.883893+00:00\n# on end callback ends at 2025-03-01T07:05:30.884831+00:00\nwith_types\n\u00b6\nwith_types\n(\n*\n,\ninput_type\n:\ntype\n[\nInput\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\ntype\n[\nOutput\n]\n|\nNone\n=\nNone\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninput_type\nThe input type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nInput\n] | None\nDEFAULT:\nNone\noutput_type\nThe output type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nOutput\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the types bound.\nwith_retry\n\u00b6\nwith_retry\n(\n*\n,\nretry_if_exception_type\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nwait_exponential_jitter\n:\nbool\n=\nTrue\n,\nexponential_jitter_params\n:\nExponentialJitterParams\n|\nNone\n=\nNone\n,\nstop_after_attempt\n:\nint\n=\n3\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nPARAMETER\nDESCRIPTION\nretry_if_exception_type\nA tuple of exception types to retry on.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nwait_exponential_jitter\nWhether to add jitter to the wait\ntime between retries.\nTYPE:\nbool\nDEFAULT:\nTrue\nstop_after_attempt\nThe maximum number of attempts to make before\ngiving up.\nTYPE:\nint\nDEFAULT:\n3\nexponential_jitter_params\nParameters for\ntenacity.wait_exponential_jitter\n. Namely:\ninitial\n,\nmax\n,\nexp_base\n, and\njitter\n(all\nfloat\nvalues).\nTYPE:\nExponentialJitterParams\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ncount\n=\n0\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nNone\n:\nglobal\ncount\ncount\n=\ncount\n+\n1\nif\nx\n==\n1\n:\nraise\nValueError\n(\n\"x is 1\"\n)\nelse\n:\npass\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\ntry\n:\nrunnable\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n,\nretry_if_exception_type\n=\n(\nValueError\n,),\n)\n.\ninvoke\n(\n1\n)\nexcept\nValueError\n:\npass\nassert\ncount\n==\n2\nmap\n\u00b6\nmap\n()\n->\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nCalls\ninvoke\nwith each input.\nRETURNS\nDESCRIPTION\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nA new\nRunnable\nthat maps a list of inputs to a list of outputs.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\nprint\n(\nrunnable\n.\nmap\n()\n.\ninvoke\n([\n1\n,\n2\n,\n3\n]))\n# [2, 3, 4]\nwith_fallbacks\n\u00b6\nwith_fallbacks\n(\nfallbacks\n:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]],\n*\n,\nexceptions_to_handle\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nexception_key\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nThe new\nRunnable\nwill try the original\nRunnable\n, and then each fallback\nin order, upon failures.\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nExample\nfrom\ntyping\nimport\nIterator\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\ndef\n_generate_immediate_error\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nraise\nValueError\n()\nyield\n\"\"\ndef\n_generate\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nyield from\n\"foo bar\"\nrunnable\n=\nRunnableGenerator\n(\n_generate_immediate_error\n)\n.\nwith_fallbacks\n(\n[\nRunnableGenerator\n(\n_generate\n)]\n)\nprint\n(\n\"\"\n.\njoin\n(\nrunnable\n.\nstream\n({})))\n# foo bar\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nas_tool\n\u00b6\nas_tool\n(\nargs_schema\n:\ntype\n[\nBaseModel\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\narg_types\n:\ndict\n[\nstr\n,\ntype\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nas_tool\nwill instantiate a\nBaseTool\nwith a name, description, and\nargs_schema\nfrom a\nRunnable\n. Where possible, schemas are inferred\nfrom\nrunnable.get_input_schema\n.\nAlternatively (e.g., if the\nRunnable\ntakes a dict as input and the specific\ndict\nkeys are not typed), the schema can be specified directly with\nargs_schema\n.\nYou can also pass\narg_types\nto just specify the required arguments and their\ntypes.\nPARAMETER\nDESCRIPTION\nargs_schema\nThe schema for the tool.\nTYPE:\ntype\n[\nBaseModel\n] | None\nDEFAULT:\nNone\nname\nThe name of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nThe description of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\narg_types\nA dictionary of argument names to types.\nTYPE:\ndict\n[\nstr\n,\ntype\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBaseTool\nA\nBaseTool\ninstance.\nTypedDict\ninput\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nclass\nArgs\n(\nTypedDict\n):\na\n:\nint\nb\n:\nlist\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\nargs_schema\nfrom\ntyping\nimport\nAny\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nclass\nFSchema\n(\nBaseModel\n):\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n...\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nlist\n[\nint\n]\n=\nField\n(\n...\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nFSchema\n)\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\narg_types\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nlist\n[\nint\n]})\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\nstr\ninput\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n(\n\"b\"\n)\nRunnableParallel\n\u00b6\nBases:\nRunnableSerializable\n[\nInput\n,\ndict\n[\nstr\n,\nAny\n]]\nRunnable that runs a mapping of\nRunnable\ns in parallel.\nReturns a mapping of their outputs.\nRunnableParallel\nis one of the two main composition primitives,\nalongside\nRunnableSequence\n. It invokes\nRunnable\ns concurrently, providing the\nsame input to each.\nA\nRunnableParallel\ncan be instantiated directly or by using a dict literal\nwithin a sequence.\nHere is a simple example that uses functions to illustrate the use of\nRunnableParallel\n:\n```python\nfrom langchain_core.runnables import RunnableLambda\ndef add_one(x: int) -> int:\nreturn x + 1\ndef mul_two(x: int) -> int:\nreturn x * 2\ndef mul_three(x: int) -> int:\nreturn x * 3\nrunnable_1 = RunnableLambda(add_one)\nrunnable_2 = RunnableLambda(mul_two)\nrunnable_3 = RunnableLambda(mul_three)\nsequence = runnable_1 | {  # this dict is coerced to a RunnableParallel\n\"mul_two\": runnable_2,\n\"mul_three\": runnable_3,\n}\n# Or equivalently:\n# sequence = runnable_1 | RunnableParallel(\n#     {\"mul_two\": runnable_2, \"mul_three\": runnable_3}\n# )\n# Also equivalently:\n# sequence = runnable_1 | RunnableParallel(\n#     mul_two=runnable_2,\n#     mul_three=runnable_3,\n# )\nsequence.invoke(1)\nawait sequence.ainvoke(1)\nsequence.batch([1, 2, 3])\nawait sequence.abatch([1, 2, 3])\n```\nRunnableParallel\nmakes it easy to run\nRunnable\ns in parallel. In the below\nexample, we simultaneously stream output from two different\nRunnable\nobjects:\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableParallel\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI()\njoke_chain = (\nChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n)\npoem_chain = (\nChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n| model\n)\nrunnable = RunnableParallel(joke=joke_chain, poem=poem_chain)\n# Display stream\noutput = {key: \"\" for key, _ in runnable.output_schema()}\nfor chunk in runnable.stream({\"topic\": \"bear\"}):\nfor key in chunk:\noutput[key] = output[key] + chunk[key].content\nprint(output)  # noqa: T201\n```\nMETHOD\nDESCRIPTION\n__init__\nCreate a\nRunnableParallel\n.\nis_lc_serializable\nReturn\nTrue\nas this class is serializable.\nget_lc_namespace\nGet the namespace of the LangChain object.\nget_name\nGet the name of the\nRunnable\n.\nget_input_schema\nGet the input schema of the\nRunnable\n.\nget_output_schema\nGet the output schema of the\nRunnable\n.\nget_graph\nGet the graph representation of the\nRunnable\n.\ninvoke\nTransform a single input into an output.\nainvoke\nTransform a single input into an output.\ntransform\nTransform inputs to outputs.\nstream\nDefault implementation of\nstream\n, which calls\ninvoke\n.\natransform\nTransform inputs to outputs.\nastream\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nget_input_jsonschema\nGet a JSON schema that represents the input to the\nRunnable\n.\nget_output_jsonschema\nGet a JSON schema that represents the output of the\nRunnable\n.\nconfig_schema\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nget_config_jsonschema\nGet a JSON schema that represents the config of the\nRunnable\n.\nget_prompts\nReturn a list of prompts used by this\nRunnable\n.\n__or__\nRunnable \"or\" operator.\n__ror__\nRunnable \"reverse-or\" operator.\npipe\nPipe\nRunnable\nobjects.\npick\nPick keys from the output\ndict\nof this\nRunnable\n.\nassign\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nbatch\nDefault implementation runs invoke in parallel using a thread pool executor.\nbatch_as_completed\nRun\ninvoke\nin parallel on a list of inputs.\nabatch\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nabatch_as_completed\nRun\nainvoke\nin parallel on a list of inputs.\nastream_log\nStream all output from a\nRunnable\n, as reported to the callback system.\nastream_events\nGenerate a stream of events.\nbind\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nwith_config\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nwith_listeners\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nwith_alisteners\nBind async lifecycle listeners to a\nRunnable\n.\nwith_types\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nwith_retry\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nmap\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nwith_fallbacks\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nas_tool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nlc_id\nReturn a unique identifier for this class for serialization purposes.\nto_json\nSerialize the\nRunnable\nto JSON.\nto_json_not_implemented\nSerialize a \"not implemented\" object.\nconfigurable_fields\nConfigure particular\nRunnable\nfields at runtime.\nconfigurable_alternatives\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nInputType\nproperty\n\u00b6\nInputType\n:\nAny\nThe type of the input to the\nRunnable\n.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\n[\nConfigurableFieldSpec\n]\nGet the config specs of the\nRunnable\n.\nRETURNS\nDESCRIPTION\nlist\n[\nConfigurableFieldSpec\n]\nThe config specs of the\nRunnable\n.\nname\nclass-attribute\ninstance-attribute\n\u00b6\nname\n:\nstr\n|\nNone\n=\nNone\nThe name of the\nRunnable\n. Used for debugging and tracing.\nOutputType\nproperty\n\u00b6\nOutputType\n:\ntype\n[\nOutput\n]\nOutput Type.\nThe type of output this\nRunnable\nproduces specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the output type cannot be inferred.\ninput_schema\nproperty\n\u00b6\ninput_schema\n:\ntype\n[\nBaseModel\n]\nThe type of input this\nRunnable\naccepts specified as a Pydantic model.\noutput_schema\nproperty\n\u00b6\noutput_schema\n:\ntype\n[\nBaseModel\n]\nOutput schema.\nThe type of output this\nRunnable\nproduces specified as a Pydantic model.\nlc_secrets\nproperty\n\u00b6\nlc_secrets\n:\ndict\n[\nstr\n,\nstr\n]\nA map of constructor argument names to secret ids.\nFor example,\n{\"openai_api_key\": \"OPENAI_API_KEY\"}\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nList of attribute names that should be included in the serialized kwargs.\nThese attributes must be accepted by the constructor.\nDefault is an empty dictionary.\n__init__\n\u00b6\n__init__\n(\nsteps__\n:\nMapping\n[\nstr\n,\nRunnable\n[\nInput\n,\nAny\n]\n|\nCallable\n[[\nInput\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nInput\n,\nAny\n]\n|\nCallable\n[[\nInput\n],\nAny\n]],\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nRunnable\n[\nInput\n,\nAny\n]\n|\nCallable\n[[\nInput\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nInput\n,\nAny\n]\n|\nCallable\n[[\nInput\n],\nAny\n]],\n)\n->\nNone\nCreate a\nRunnableParallel\n.\nPARAMETER\nDESCRIPTION\nsteps__\nThe steps to include.\nTYPE:\nMapping\n[\nstr\n,\nRunnable\n[\nInput\n,\nAny\n] |\nCallable\n[[\nInput\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nInput\n,\nAny\n] |\nCallable\n[[\nInput\n],\nAny\n]]] | None\nDEFAULT:\nNone\n**kwargs\nAdditional steps to include.\nTYPE:\nRunnable\n[\nInput\n,\nAny\n] |\nCallable\n[[\nInput\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nInput\n,\nAny\n] |\nCallable\n[[\nInput\n],\nAny\n]]\nDEFAULT:\n{}\nis_lc_serializable\nclassmethod\n\u00b6\nis_lc_serializable\n()\n->\nbool\nReturn\nTrue\nas this class is serializable.\nget_lc_namespace\nclassmethod\n\u00b6\nget_lc_namespace\n()\n->\nlist\n[\nstr\n]\nGet the namespace of the LangChain object.\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\n[\"langchain\", \"schema\", \"runnable\"]\nget_name\n\u00b6\nget_name\n(\nsuffix\n:\nstr\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nstr\nGet the name of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nsuffix\nThe suffix to use.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nname\nThe name to use.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nstr\nThe name of the\nRunnable\n.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet the input schema of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nThe input schema of the\nRunnable\n.\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet the output schema of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nThe output schema of the\nRunnable\n.\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nGraph\nGet the graph representation of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nGraph\nThe graph representation of the\nRunnable\n.\nRAISES\nDESCRIPTION\nValueError\nIf a\nRunnable\nhas no first or last node.\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\ndict\n[\nstr\n,\nAny\n]\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\ntransform\n\u00b6\ntransform\n(\ninput\n:\nIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nIterator\n[\ndict\n[\nstr\n,\nAny\n]]\nTransform inputs to outputs.\nDefault implementation of transform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn iterator of inputs to the\nRunnable\n.\nTYPE:\nIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nstream\n\u00b6\nstream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\ndict\n[\nstr\n,\nAny\n]]\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\natransform\nasync\n\u00b6\natransform\n(\ninput\n:\nAsyncIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nAsyncIterator\n[\ndict\n[\nstr\n,\nAny\n]]\nTransform inputs to outputs.\nDefault implementation of atransform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn async iterator of inputs to the\nRunnable\n.\nTYPE:\nAsyncIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nAsyncIterator\n[\ndict\n[\nstr\n,\nAny\n]]\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nget_input_jsonschema\n\u00b6\nget_input_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the input to the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the input to the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_input_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nget_output_jsonschema\n\u00b6\nget_output_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the output of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the output of the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_output_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nconfig_schema\n\u00b6\nconfig_schema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nTo mark a field as configurable, see the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate config.\nget_config_jsonschema\n\u00b6\nget_config_jsonschema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the config of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the config of the\nRunnable\n.\nAdded in\nlangchain-core\n0.3.0\nget_prompts\n\u00b6\nget_prompts\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nlist\n[\nBasePromptTemplate\n]\nReturn a list of prompts used by this\nRunnable\n.\n__or__\n\u00b6\n__or__\n(\nother\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nRunnable \"or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]] |\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]] |\nCallable\n[[\nAny\n],\nOther\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\n__ror__\n\u00b6\n__ror__\n(\nother\n:\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nRunnable \"reverse-or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]] |\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]] |\nCallable\n[[\nOther\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nOther\n],\nAny\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nA new\nRunnable\n.\npipe\n\u00b6\npipe\n(\n*\nothers\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n],\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nPipe\nRunnable\nobjects.\nCompose this\nRunnable\nwith\nRunnable\n-like objects to make a\nRunnableSequence\n.\nEquivalent to\nRunnableSequence(self, *others)\nor\nself | others[0] | ...\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nmul_two\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n*\n2\nrunnable_1\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable_2\n=\nRunnableLambda\n(\nmul_two\n)\nsequence\n=\nrunnable_1\n.\npipe\n(\nrunnable_2\n)\n# Or equivalently:\n# sequence = runnable_1 | runnable_2\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence\n.\ninvoke\n(\n1\n)\nawait\nsequence\n.\nainvoke\n(\n1\n)\n# -> 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\nawait\nsequence\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# -> [4, 6, 8]\nPARAMETER\nDESCRIPTION\n*others\nOther\nRunnable\nor\nRunnable\n-like objects to compose\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n]\nDEFAULT:\n()\nname\nAn optional name for the resulting\nRunnableSequence\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\npick\n\u00b6\npick\n(\nkeys\n:\nstr\n|\nlist\n[\nstr\n])\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nPick keys from the output\ndict\nof this\nRunnable\n.\nPick a single key\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\njson_only_chain\n=\nchain\n.\npick\n(\n\"json\"\n)\njson_only_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> [1, 2, 3]\nPick a list of keys\nfrom\ntyping\nimport\nAny\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\ndef\nas_bytes\n(\nx\n:\nAny\n)\n->\nbytes\n:\nreturn\nbytes\n(\nx\n,\n\"utf-8\"\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n,\nbytes\n=\nRunnableLambda\n(\nas_bytes\n)\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\njson_and_bytes_chain\n=\nchain\n.\npick\n([\n\"json\"\n,\n\"bytes\"\n])\njson_and_bytes_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\nPARAMETER\nDESCRIPTION\nkeys\nA key or list of keys to pick from the output dict.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\na new\nRunnable\n.\nassign\n\u00b6\nassign\n(\n**\nkwargs\n:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]],\n)\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nfrom\nlangchain_core.language_models.fake\nimport\nFakeStreamingListLLM\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core.prompts\nimport\nSystemMessagePromptTemplate\nfrom\nlangchain_core.runnables\nimport\nRunnable\nfrom\noperator\nimport\nitemgetter\nprompt\n=\n(\nSystemMessagePromptTemplate\n.\nfrom_template\n(\n\"You are a nice assistant.\"\n)\n+\n\"\n{question}\n\"\n)\nmodel\n=\nFakeStreamingListLLM\n(\nresponses\n=\n[\n\"foo-lish\"\n])\nchain\n:\nRunnable\n=\nprompt\n|\nmodel\n|\n{\n\"str\"\n:\nStrOutputParser\n()}\nchain_with_assign\n=\nchain\n.\nassign\n(\nhello\n=\nitemgetter\n(\n\"str\"\n)\n|\nmodel\n)\nprint\n(\nchain_with_assign\n.\ninput_schema\n.\nmodel_json_schema\n())\n# {'title': 'PromptInput', 'type': 'object', 'properties':\n{\n'question'\n:\n{\n'title'\n:\n'Question'\n,\n'type'\n:\n'string'\n}}}\nprint\n(\nchain_with_assign\n.\noutput_schema\n.\nmodel_json_schema\n())\n# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n{\n'str'\n:\n{\n'title'\n:\n'Str'\n,\n'type'\n:\n'string'\n},\n'hello'\n:\n{\n'title'\n:\n'Hello'\n,\n'type'\n:\n'string'\n}}}\nPARAMETER\nDESCRIPTION\n**kwargs\nA mapping of keys to\nRunnable\nor\nRunnable\n-like objects\nthat will be invoked with the entire output dict of this\nRunnable\n.\nTYPE:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\nA new\nRunnable\n.\nbatch\n\u00b6\nbatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n. The config supports\nstandard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work\nto do in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nbatch_as_completed\n\u00b6\nbatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\ninvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\ntuple\n[\nint\n,\nOutput\n|\nException\n]\nTuples of the index of the input and the output from the\nRunnable\n.\nabatch\nasync\n\u00b6\nabatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nThe default implementation of\nbatch\nworks well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nabatch_as_completed\nasync\n\u00b6\nabatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\nainvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nA tuple of the index of the input and the output from the\nRunnable\n.\nastream_log\nasync\n\u00b6\nastream_log\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ndiff\n:\nbool\n=\nTrue\n,\nwith_streamed_output_list\n:\nbool\n=\nTrue\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nRunLogPatch\n]\n|\nAsyncIterator\n[\nRunLog\n]\nStream all output from a\nRunnable\n, as reported to the callback system.\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\nOutput is streamed as Log objects, which include a list of\nJsonpatch ops that describe how the state of the run has changed in each\nstep, and the final state of the run.\nThe Jsonpatch ops can be applied in order to construct state.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ndiff\nWhether to yield diffs between each step or the current state.\nTYPE:\nbool\nDEFAULT:\nTrue\nwith_streamed_output_list\nWhether to yield the\nstreamed_output\nlist.\nTYPE:\nbool\nDEFAULT:\nTrue\ninclude_names\nOnly include logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nRunLogPatch\n] |\nAsyncIterator\n[\nRunLog\n]\nA\nRunLogPatch\nor\nRunLog\nobject.\nastream_events\nasync\n\u00b6\nastream_events\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nversion\n:\nLiteral\n[\n\"v1\"\n,\n\"v2\"\n]\n=\n\"v2\"\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nStreamEvent\n]\nGenerate a stream of events.\nUse to create an iterator over\nStreamEvent\nthat provide real-time information\nabout the progress of the\nRunnable\n, including\nStreamEvent\nfrom intermediate\nresults.\nA\nStreamEvent\nis a dictionary with the following schema:\nevent\n: Event names are of the format:\non_[runnable_type]_(start|stream|end)\n.\nname\n: The name of the\nRunnable\nthat generated the event.\nrun_id\n: Randomly generated ID associated with the given execution of the\nRunnable\nthat emitted the event. A child\nRunnable\nthat gets invoked as\npart of the execution of a parent\nRunnable\nis assigned its own unique ID.\nparent_ids\n: The IDs of the parent runnables that generated the event. The\nroot\nRunnable\nwill have an empty list. The order of the parent IDs is from\nthe root to the immediate parent. Only available for v2 version of the API.\nThe v1 version of the API will return an empty list.\ntags\n: The tags of the\nRunnable\nthat generated the event.\nmetadata\n: The metadata of the\nRunnable\nthat generated the event.\ndata\n: The data associated with the event. The contents of this field\ndepend on the type of event. See the table below for more details.\nBelow is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table.\nNote\nThis reference table is for the v2 version of the schema.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n'[model name]'\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n'[model name]'\n{'input': 'hello'}\non_llm_stream\n'[model name]'\n'Hello'\non_llm_end\n'[model name]'\n'Hello human!'\non_chain_start\n'format_docs'\non_chain_stream\n'format_docs'\n'hello world!, goodbye world!'\non_chain_end\n'format_docs'\n[Document(...)]\n'hello world!, goodbye world!'\non_tool_start\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n'[retriever name]'\n{\"query\": \"hello\"}\non_retriever_end\n'[retriever name]'\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n'[template_name]'\n{\"question\": \"hello\"}\non_prompt_end\n'[template_name]'\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nIn addition to the standard events, users can also dispatch custom events (see example below).\nCustom events will be only be surfaced with in the v2 version of the API!\nA custom event has following format:\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nHere are declarations associated with the standard events shown above:\nformat_docs\n:\ndef\nformat_docs\n(\ndocs\n:\nlist\n[\nDocument\n])\n->\nstr\n:\n'''Format the docs.'''\nreturn\n\", \"\n.\njoin\n([\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n])\nformat_docs\n=\nRunnableLambda\n(\nformat_docs\n)\nsome_tool\n:\n@tool\ndef\nsome_tool\n(\nx\n:\nint\n,\ny\n:\nstr\n)\n->\ndict\n:\n'''Some_tool.'''\nreturn\n{\n\"x\"\n:\nx\n,\n\"y\"\n:\ny\n}\nprompt\n:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are Cat Agent 007\"\n),\n(\n\"human\"\n,\n\"\n{question}\n\"\n),\n]\n)\n.\nwith_config\n({\n\"run_name\"\n:\n\"my_template\"\n,\n\"tags\"\n:\n[\n\"my_template\"\n]})\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nasync\ndef\nreverse\n(\ns\n:\nstr\n)\n->\nstr\n:\nreturn\ns\n[::\n-\n1\n]\nchain\n=\nRunnableLambda\n(\nfunc\n=\nreverse\n)\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"hello\"\n,\nversion\n=\n\"v2\"\n)\n]\n# Will produce the following events\n# (run_id, and parent_ids has been omitted for brevity):\n[\n{\n\"data\"\n:\n{\n\"input\"\n:\n\"hello\"\n},\n\"event\"\n:\n\"on_chain_start\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"chunk\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_stream\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"output\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_end\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n]\nDispatch custom event\nfrom\nlangchain_core.callbacks.manager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableConfig\nimport\nasyncio\nasync\ndef\nslow_thing\n(\nsome_input\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n->\nstr\n:\n\"\"\"Do something that takes a long time.\"\"\"\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 1 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 2 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nreturn\n\"Done\"\nslow_thing\n=\nRunnableLambda\n(\nslow_thing\n)\nasync\nfor\nevent\nin\nslow_thing\n.\nastream_events\n(\n\"some_input\"\n,\nversion\n=\n\"v2\"\n):\nprint\n(\nevent\n)\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nversion\nThe version of the schema to use, either\n'v2'\nor\n'v1'\n.\nUsers should use\n'v2'\n.\n'v1'\nis for backwards compatibility and will be deprecated\nin\n0.4.0\n.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in\n'v2'\n.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\ninclude_names\nOnly include events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nThese will be passed to\nastream_log\nas this implementation\nof\nastream_events\nis built on top of\nastream_log\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nStreamEvent\n]\nAn async stream of\nStreamEvent\n.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the version is not\n'v1'\nor\n'v2'\n.\nbind\n\u00b6\nbind\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nUseful when a\nRunnable\nin a chain requires an argument that is not\nin the output of the previous\nRunnable\nor included in the user input.\nPARAMETER\nDESCRIPTION\n**kwargs\nThe arguments to bind to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the arguments bound.\nExample\nfrom\nlangchain_ollama\nimport\nChatOllama\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nmodel\n=\nChatOllama\n(\nmodel\n=\n\"llama3.1\"\n)\n# Without bind\nchain\n=\nmodel\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two three four five.'\n# With bind\nchain\n=\nmodel\n.\nbind\n(\nstop\n=\n[\n\"three\"\n])\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two'\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to bind to the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the config bound.\nwith_listeners\n\u00b6\nwith_listeners\n(\n*\n,\non_start\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_end\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_error\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled before the\nRunnable\nstarts running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_end\nCalled after the\nRunnable\nfinishes running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_error\nCalled if the\nRunnable\nthrows an error, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nfrom\nlangchain_core.tracers.schemas\nimport\nRun\nimport\ntime\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\ntime\n.\nsleep\n(\ntime_to_sleep\n)\ndef\nfn_start\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"start_time:\"\n,\nrun_obj\n.\nstart_time\n)\ndef\nfn_end\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"end_time:\"\n,\nrun_obj\n.\nend_time\n)\nchain\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_listeners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nchain\n.\ninvoke\n(\n2\n)\nwith_alisteners\n\u00b6\nwith_alisteners\n(\n*\n,\non_start\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_end\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_error\n:\nAsyncListener\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind async lifecycle listeners to a\nRunnable\n.\nReturns a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled asynchronously before the\nRunnable\nstarts running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_end\nCalled asynchronously after the\nRunnable\nfinishes running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_error\nCalled asynchronously if the\nRunnable\nthrows an error,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnable\nfrom\ndatetime\nimport\ndatetime\n,\ntimezone\nimport\ntime\nimport\nasyncio\ndef\nformat_t\n(\ntimestamp\n:\nfloat\n)\n->\nstr\n:\nreturn\ndatetime\n.\nfromtimestamp\n(\ntimestamp\n,\ntz\n=\ntimezone\n.\nutc\n)\n.\nisoformat\n()\nasync\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\ntime_to_sleep\n)\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_start\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on start callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n3\n)\nprint\n(\nf\n\"on start callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_end\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on end callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n2\n)\nprint\n(\nf\n\"on end callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nrunnable\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_alisteners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nasync\ndef\nconcurrent_runs\n():\nawait\nasyncio\n.\ngather\n(\nrunnable\n.\nainvoke\n(\n2\n),\nrunnable\n.\nainvoke\n(\n3\n))\nasyncio\n.\nrun\n(\nconcurrent_runs\n())\n# Result:\n# on start callback starts at 2025-03-01T07:05:22.875378+00:00\n# on start callback starts at 2025-03-01T07:05:22.875495+00:00\n# on start callback ends at 2025-03-01T07:05:25.878862+00:00\n# on start callback ends at 2025-03-01T07:05:25.878947+00:00\n# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n# on end callback starts at 2025-03-01T07:05:27.882360+00:00\n# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n# on end callback starts at 2025-03-01T07:05:28.882428+00:00\n# on end callback ends at 2025-03-01T07:05:29.883893+00:00\n# on end callback ends at 2025-03-01T07:05:30.884831+00:00\nwith_types\n\u00b6\nwith_types\n(\n*\n,\ninput_type\n:\ntype\n[\nInput\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\ntype\n[\nOutput\n]\n|\nNone\n=\nNone\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninput_type\nThe input type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nInput\n] | None\nDEFAULT:\nNone\noutput_type\nThe output type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nOutput\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the types bound.\nwith_retry\n\u00b6\nwith_retry\n(\n*\n,\nretry_if_exception_type\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nwait_exponential_jitter\n:\nbool\n=\nTrue\n,\nexponential_jitter_params\n:\nExponentialJitterParams\n|\nNone\n=\nNone\n,\nstop_after_attempt\n:\nint\n=\n3\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nPARAMETER\nDESCRIPTION\nretry_if_exception_type\nA tuple of exception types to retry on.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nwait_exponential_jitter\nWhether to add jitter to the wait\ntime between retries.\nTYPE:\nbool\nDEFAULT:\nTrue\nstop_after_attempt\nThe maximum number of attempts to make before\ngiving up.\nTYPE:\nint\nDEFAULT:\n3\nexponential_jitter_params\nParameters for\ntenacity.wait_exponential_jitter\n. Namely:\ninitial\n,\nmax\n,\nexp_base\n, and\njitter\n(all\nfloat\nvalues).\nTYPE:\nExponentialJitterParams\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ncount\n=\n0\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nNone\n:\nglobal\ncount\ncount\n=\ncount\n+\n1\nif\nx\n==\n1\n:\nraise\nValueError\n(\n\"x is 1\"\n)\nelse\n:\npass\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\ntry\n:\nrunnable\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n,\nretry_if_exception_type\n=\n(\nValueError\n,),\n)\n.\ninvoke\n(\n1\n)\nexcept\nValueError\n:\npass\nassert\ncount\n==\n2\nmap\n\u00b6\nmap\n()\n->\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nCalls\ninvoke\nwith each input.\nRETURNS\nDESCRIPTION\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nA new\nRunnable\nthat maps a list of inputs to a list of outputs.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\nprint\n(\nrunnable\n.\nmap\n()\n.\ninvoke\n([\n1\n,\n2\n,\n3\n]))\n# [2, 3, 4]\nwith_fallbacks\n\u00b6\nwith_fallbacks\n(\nfallbacks\n:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]],\n*\n,\nexceptions_to_handle\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nexception_key\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nThe new\nRunnable\nwill try the original\nRunnable\n, and then each fallback\nin order, upon failures.\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nExample\nfrom\ntyping\nimport\nIterator\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\ndef\n_generate_immediate_error\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nraise\nValueError\n()\nyield\n\"\"\ndef\n_generate\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nyield from\n\"foo bar\"\nrunnable\n=\nRunnableGenerator\n(\n_generate_immediate_error\n)\n.\nwith_fallbacks\n(\n[\nRunnableGenerator\n(\n_generate\n)]\n)\nprint\n(\n\"\"\n.\njoin\n(\nrunnable\n.\nstream\n({})))\n# foo bar\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nas_tool\n\u00b6\nas_tool\n(\nargs_schema\n:\ntype\n[\nBaseModel\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\narg_types\n:\ndict\n[\nstr\n,\ntype\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nas_tool\nwill instantiate a\nBaseTool\nwith a name, description, and\nargs_schema\nfrom a\nRunnable\n. Where possible, schemas are inferred\nfrom\nrunnable.get_input_schema\n.\nAlternatively (e.g., if the\nRunnable\ntakes a dict as input and the specific\ndict\nkeys are not typed), the schema can be specified directly with\nargs_schema\n.\nYou can also pass\narg_types\nto just specify the required arguments and their\ntypes.\nPARAMETER\nDESCRIPTION\nargs_schema\nThe schema for the tool.\nTYPE:\ntype\n[\nBaseModel\n] | None\nDEFAULT:\nNone\nname\nThe name of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nThe description of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\narg_types\nA dictionary of argument names to types.\nTYPE:\ndict\n[\nstr\n,\ntype\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBaseTool\nA\nBaseTool\ninstance.\nTypedDict\ninput\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nclass\nArgs\n(\nTypedDict\n):\na\n:\nint\nb\n:\nlist\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\nargs_schema\nfrom\ntyping\nimport\nAny\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nclass\nFSchema\n(\nBaseModel\n):\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n...\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nlist\n[\nint\n]\n=\nField\n(\n...\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nFSchema\n)\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\narg_types\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nlist\n[\nint\n]})\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\nstr\ninput\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n(\n\"b\"\n)\nlc_id\nclassmethod\n\u00b6\nlc_id\n()\n->\nlist\n[\nstr\n]\nReturn a unique identifier for this class for serialization purposes.\nThe unique identifier is a list of strings that describes the path\nto the object.\nFor example, for the class\nlangchain.llms.openai.OpenAI\n, the id is\n[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]\n.\nto_json\n\u00b6\nto_json\n()\n->\nSerializedConstructor\n|\nSerializedNotImplemented\nSerialize the\nRunnable\nto JSON.\nRETURNS\nDESCRIPTION\nSerializedConstructor\n|\nSerializedNotImplemented\nA JSON-serializable representation of the\nRunnable\n.\nto_json_not_implemented\n\u00b6\nto_json_not_implemented\n()\n->\nSerializedNotImplemented\nSerialize a \"not implemented\" object.\nRETURNS\nDESCRIPTION\nSerializedNotImplemented\nSerializedNotImplemented\n.\nconfigurable_fields\n\u00b6\nconfigurable_fields\n(\n**\nkwargs\n:\nAnyConfigurableField\n,\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure particular\nRunnable\nfields at runtime.\nPARAMETER\nDESCRIPTION\n**kwargs\nA dictionary of\nConfigurableField\ninstances to configure.\nTYPE:\nAnyConfigurableField\nDEFAULT:\n{}\nRAISES\nDESCRIPTION\nValueError\nIf a configuration key is not found in the\nRunnable\n.\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the fields configured.\nExample\nfrom\nlangchain_core.runnables\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmax_tokens\n=\n20\n)\n.\nconfigurable_fields\n(\nmax_tokens\n=\nConfigurableField\n(\nid\n=\n\"output_token_number\"\n,\nname\n=\n\"Max tokens in the output\"\n,\ndescription\n=\n\"The maximum number of tokens in the output\"\n,\n)\n)\n# max_tokens = 20\nprint\n(\n\"max_tokens_20: \"\n,\nmodel\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n)\n# max_tokens = 200\nprint\n(\n\"max_tokens_200: \"\n,\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"output_token_number\"\n:\n200\n})\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n,\n)\nconfigurable_alternatives\n\u00b6\nconfigurable_alternatives\n(\nwhich\n:\nConfigurableField\n,\n*\n,\ndefault_key\n:\nstr\n=\n\"default\"\n,\nprefix_keys\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nRunnable\n[\nInput\n,\nOutput\n]\n|\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nPARAMETER\nDESCRIPTION\nwhich\nThe\nConfigurableField\ninstance that will be used to select the\nalternative.\nTYPE:\nConfigurableField\ndefault_key\nThe default key to use if no alternative is selected.\nTYPE:\nstr\nDEFAULT:\n'default'\nprefix_keys\nWhether to prefix the keys with the\nConfigurableField\nid.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nA dictionary of keys to\nRunnable\ninstances or callables that\nreturn\nRunnable\ninstances.\nTYPE:\nRunnable\n[\nInput\n,\nOutput\n] |\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the alternatives configured.\nExample\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core.runnables.utils\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatAnthropic\n(\nmodel_name\n=\n\"claude-sonnet-4-5-20250929\"\n)\n.\nconfigurable_alternatives\n(\nConfigurableField\n(\nid\n=\n\"llm\"\n),\ndefault_key\n=\n\"anthropic\"\n,\nopenai\n=\nChatOpenAI\n(),\n)\n# uses the default model ChatAnthropic\nprint\n(\nmodel\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\n# uses ChatOpenAI\nprint\n(\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"openai\"\n})\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\nRunnableSequence\n\u00b6\nBases:\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nSequence of\nRunnable\nobjects, where the output of one is the input of the next.\nRunnableSequence\nis the most important composition operator in LangChain\nas it is used in virtually every chain.\nA\nRunnableSequence\ncan be instantiated directly or more commonly by using the\n|\noperator where either the left or right operands (or both) must be a\nRunnable\n.\nAny\nRunnableSequence\nautomatically supports sync, async, batch.\nThe default implementations of\nbatch\nand\nabatch\nutilize threadpools and\nasyncio gather and will be faster than naive invocation of\ninvoke\nor\nainvoke\nfor IO bound\nRunnable\ns.\nBatching is implemented by invoking the batch method on each component of the\nRunnableSequence\nin order.\nA\nRunnableSequence\npreserves the streaming properties of its components, so if\nall components of the sequence implement a\ntransform\nmethod -- which\nis the method that implements the logic to map a streaming input to a streaming\noutput -- then the sequence will be able to stream input to output!\nIf any component of the sequence does not implement transform then the\nstreaming will only begin after this component is run. If there are\nmultiple blocking components, streaming begins after the last one.\nNote\nRunnableLambdas\ndo not support\ntransform\nby default! So if you need to\nuse a\nRunnableLambdas\nbe careful about where you place them in a\nRunnableSequence\n(if you need to use the\nstream\n/\nastream\nmethods).\nIf you need arbitrary logic and need streaming, you can subclass\nRunnable, and implement\ntransform\nfor whatever logic you need.\nHere is a simple example that uses simple functions to illustrate the use of\nRunnableSequence\n:\n```python\nfrom langchain_core.runnables import RunnableLambda\ndef add_one(x: int) -> int:\nreturn x + 1\ndef mul_two(x: int) -> int:\nreturn x * 2\nrunnable_1 = RunnableLambda(add_one)\nrunnable_2 = RunnableLambda(mul_two)\nsequence = runnable_1 | runnable_2\n# Or equivalently:\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence.invoke(1)\nawait sequence.ainvoke(1)\nsequence.batch([1, 2, 3])\nawait sequence.abatch([1, 2, 3])\n```\nHere's an example that uses streams JSON output generated by an LLM:\n```python\nfrom langchain_core.output_parsers.json import SimpleJsonOutputParser\nfrom langchain_openai import ChatOpenAI\nprompt = PromptTemplate.from_template(\n\"In JSON format, give me a list of {topic} and their \"\n\"corresponding names in French, Spanish and in a \"\n\"Cat Language.\"\n)\nmodel = ChatOpenAI()\nchain = prompt | model | SimpleJsonOutputParser()\nasync for chunk in chain.astream({\"topic\": \"colors\"}):\nprint(\"-\")  # noqa: T201\nprint(chunk, sep=\"\", flush=True)  # noqa: T201\n```\nMETHOD\nDESCRIPTION\n__init__\nCreate a new\nRunnableSequence\n.\nget_lc_namespace\nGet the namespace of the LangChain object.\nis_lc_serializable\nReturn\nTrue\nas this class is serializable.\nget_input_schema\nGet the input schema of the\nRunnable\n.\nget_output_schema\nGet the output schema of the\nRunnable\n.\nget_graph\nGet the graph representation of the\nRunnable\n.\n__or__\nRunnable \"or\" operator.\n__ror__\nRunnable \"reverse-or\" operator.\ninvoke\nTransform a single input into an output.\nainvoke\nTransform a single input into an output.\nbatch\nDefault implementation runs invoke in parallel using a thread pool executor.\nabatch\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\ntransform\nTransform inputs to outputs.\nstream\nDefault implementation of\nstream\n, which calls\ninvoke\n.\natransform\nTransform inputs to outputs.\nastream\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nget_name\nGet the name of the\nRunnable\n.\nget_input_jsonschema\nGet a JSON schema that represents the input to the\nRunnable\n.\nget_output_jsonschema\nGet a JSON schema that represents the output of the\nRunnable\n.\nconfig_schema\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nget_config_jsonschema\nGet a JSON schema that represents the config of the\nRunnable\n.\nget_prompts\nReturn a list of prompts used by this\nRunnable\n.\npipe\nPipe\nRunnable\nobjects.\npick\nPick keys from the output\ndict\nof this\nRunnable\n.\nassign\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nbatch_as_completed\nRun\ninvoke\nin parallel on a list of inputs.\nabatch_as_completed\nRun\nainvoke\nin parallel on a list of inputs.\nastream_log\nStream all output from a\nRunnable\n, as reported to the callback system.\nastream_events\nGenerate a stream of events.\nbind\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nwith_config\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nwith_listeners\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nwith_alisteners\nBind async lifecycle listeners to a\nRunnable\n.\nwith_types\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nwith_retry\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nmap\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nwith_fallbacks\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nas_tool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nlc_id\nReturn a unique identifier for this class for serialization purposes.\nto_json\nSerialize the\nRunnable\nto JSON.\nto_json_not_implemented\nSerialize a \"not implemented\" object.\nconfigurable_fields\nConfigure particular\nRunnable\nfields at runtime.\nconfigurable_alternatives\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nfirst\ninstance-attribute\n\u00b6\nfirst\n:\nRunnable\n[\nInput\n,\nAny\n]\nThe first\nRunnable\nin the sequence.\nmiddle\nclass-attribute\ninstance-attribute\n\u00b6\nmiddle\n:\nlist\n[\nRunnable\n[\nAny\n,\nAny\n]]\n=\nField\n(\ndefault_factory\n=\nlist\n)\nThe middle\nRunnable\nin the sequence.\nlast\ninstance-attribute\n\u00b6\nlast\n:\nRunnable\n[\nAny\n,\nOutput\n]\nThe last\nRunnable\nin the sequence.\nsteps\nproperty\n\u00b6\nsteps\n:\nlist\n[\nRunnable\n[\nAny\n,\nAny\n]]\nAll the\nRunnable\ns that make up the sequence in order.\nRETURNS\nDESCRIPTION\nlist\n[\nRunnable\n[\nAny\n,\nAny\n]]\nA list of\nRunnable\ns.\nInputType\nproperty\n\u00b6\nInputType\n:\ntype\n[\nInput\n]\nThe type of the input to the\nRunnable\n.\nOutputType\nproperty\n\u00b6\nOutputType\n:\ntype\n[\nOutput\n]\nThe type of the output of the\nRunnable\n.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\n[\nConfigurableFieldSpec\n]\nGet the config specs of the\nRunnable\n.\nRETURNS\nDESCRIPTION\nlist\n[\nConfigurableFieldSpec\n]\nThe config specs of the\nRunnable\n.\nname\nclass-attribute\ninstance-attribute\n\u00b6\nname\n:\nstr\n|\nNone\n=\nNone\nThe name of the\nRunnable\n. Used for debugging and tracing.\ninput_schema\nproperty\n\u00b6\ninput_schema\n:\ntype\n[\nBaseModel\n]\nThe type of input this\nRunnable\naccepts specified as a Pydantic model.\noutput_schema\nproperty\n\u00b6\noutput_schema\n:\ntype\n[\nBaseModel\n]\nOutput schema.\nThe type of output this\nRunnable\nproduces specified as a Pydantic model.\nlc_secrets\nproperty\n\u00b6\nlc_secrets\n:\ndict\n[\nstr\n,\nstr\n]\nA map of constructor argument names to secret ids.\nFor example,\n{\"openai_api_key\": \"OPENAI_API_KEY\"}\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nList of attribute names that should be included in the serialized kwargs.\nThese attributes must be accepted by the constructor.\nDefault is an empty dictionary.\n__init__\n\u00b6\n__init__\n(\n*\nsteps\n:\nRunnableLike\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\nfirst\n:\nRunnable\n[\nAny\n,\nAny\n]\n|\nNone\n=\nNone\n,\nmiddle\n:\nlist\n[\nRunnable\n[\nAny\n,\nAny\n]]\n|\nNone\n=\nNone\n,\nlast\n:\nRunnable\n[\nAny\n,\nAny\n]\n|\nNone\n=\nNone\n,\n)\n->\nNone\nCreate a new\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nsteps\nThe steps to include in the sequence.\nTYPE:\nRunnableLike\nDEFAULT:\n()\nname\nThe name of the\nRunnable\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nfirst\nThe first\nRunnable\nin the sequence.\nTYPE:\nRunnable\n[\nAny\n,\nAny\n] | None\nDEFAULT:\nNone\nmiddle\nThe middle\nRunnable\nobjects in the sequence.\nTYPE:\nlist\n[\nRunnable\n[\nAny\n,\nAny\n]] | None\nDEFAULT:\nNone\nlast\nThe last\nRunnable\nin the sequence.\nTYPE:\nRunnable\n[\nAny\n,\nAny\n] | None\nDEFAULT:\nNone\nRAISES\nDESCRIPTION\nValueError\nIf the sequence has less than 2 steps.\nget_lc_namespace\nclassmethod\n\u00b6\nget_lc_namespace\n()\n->\nlist\n[\nstr\n]\nGet the namespace of the LangChain object.\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\n[\"langchain\", \"schema\", \"runnable\"]\nis_lc_serializable\nclassmethod\n\u00b6\nis_lc_serializable\n()\n->\nbool\nReturn\nTrue\nas this class is serializable.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet the input schema of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nThe input schema of the\nRunnable\n.\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet the output schema of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nThe output schema of the\nRunnable\n.\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nGraph\nGet the graph representation of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to use.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nGraph\nThe graph representation of the\nRunnable\n.\nRAISES\nDESCRIPTION\nValueError\nIf a\nRunnable\nhas no first or last node.\n__or__\n\u00b6\n__or__\n(\nother\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nRunnable \"or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]] |\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]] |\nCallable\n[[\nAny\n],\nOther\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\n__ror__\n\u00b6\n__ror__\n(\nother\n:\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nRunnable \"reverse-or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]] |\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]] |\nCallable\n[[\nOther\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nOther\n],\nAny\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nA new\nRunnable\n.\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nbatch\n\u00b6\nbatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n. The config supports\nstandard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work\nto do in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nabatch\nasync\n\u00b6\nabatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nThe default implementation of\nbatch\nworks well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\ntransform\n\u00b6\ntransform\n(\ninput\n:\nIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of transform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn iterator of inputs to the\nRunnable\n.\nTYPE:\nIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nstream\n\u00b6\nstream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\natransform\nasync\n\u00b6\natransform\n(\ninput\n:\nAsyncIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of atransform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn async iterator of inputs to the\nRunnable\n.\nTYPE:\nAsyncIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nAsyncIterator\n[\nOutput\n]\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nget_name\n\u00b6\nget_name\n(\nsuffix\n:\nstr\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nstr\nGet the name of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nsuffix\nAn optional suffix to append to the name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nname\nAn optional name to use instead of the\nRunnable\n's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nstr\nThe name of the\nRunnable\n.\nget_input_jsonschema\n\u00b6\nget_input_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the input to the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the input to the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_input_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nget_output_jsonschema\n\u00b6\nget_output_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the output of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the output of the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_output_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nconfig_schema\n\u00b6\nconfig_schema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nTo mark a field as configurable, see the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate config.\nget_config_jsonschema\n\u00b6\nget_config_jsonschema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the config of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the config of the\nRunnable\n.\nAdded in\nlangchain-core\n0.3.0\nget_prompts\n\u00b6\nget_prompts\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nlist\n[\nBasePromptTemplate\n]\nReturn a list of prompts used by this\nRunnable\n.\npipe\n\u00b6\npipe\n(\n*\nothers\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n],\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nPipe\nRunnable\nobjects.\nCompose this\nRunnable\nwith\nRunnable\n-like objects to make a\nRunnableSequence\n.\nEquivalent to\nRunnableSequence(self, *others)\nor\nself | others[0] | ...\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nmul_two\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n*\n2\nrunnable_1\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable_2\n=\nRunnableLambda\n(\nmul_two\n)\nsequence\n=\nrunnable_1\n.\npipe\n(\nrunnable_2\n)\n# Or equivalently:\n# sequence = runnable_1 | runnable_2\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence\n.\ninvoke\n(\n1\n)\nawait\nsequence\n.\nainvoke\n(\n1\n)\n# -> 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\nawait\nsequence\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# -> [4, 6, 8]\nPARAMETER\nDESCRIPTION\n*others\nOther\nRunnable\nor\nRunnable\n-like objects to compose\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n]\nDEFAULT:\n()\nname\nAn optional name for the resulting\nRunnableSequence\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\npick\n\u00b6\npick\n(\nkeys\n:\nstr\n|\nlist\n[\nstr\n])\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nPick keys from the output\ndict\nof this\nRunnable\n.\nPick a single key\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\njson_only_chain\n=\nchain\n.\npick\n(\n\"json\"\n)\njson_only_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> [1, 2, 3]\nPick a list of keys\nfrom\ntyping\nimport\nAny\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\ndef\nas_bytes\n(\nx\n:\nAny\n)\n->\nbytes\n:\nreturn\nbytes\n(\nx\n,\n\"utf-8\"\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n,\nbytes\n=\nRunnableLambda\n(\nas_bytes\n)\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\njson_and_bytes_chain\n=\nchain\n.\npick\n([\n\"json\"\n,\n\"bytes\"\n])\njson_and_bytes_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\nPARAMETER\nDESCRIPTION\nkeys\nA key or list of keys to pick from the output dict.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\na new\nRunnable\n.\nassign\n\u00b6\nassign\n(\n**\nkwargs\n:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]],\n)\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nfrom\nlangchain_core.language_models.fake\nimport\nFakeStreamingListLLM\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core.prompts\nimport\nSystemMessagePromptTemplate\nfrom\nlangchain_core.runnables\nimport\nRunnable\nfrom\noperator\nimport\nitemgetter\nprompt\n=\n(\nSystemMessagePromptTemplate\n.\nfrom_template\n(\n\"You are a nice assistant.\"\n)\n+\n\"\n{question}\n\"\n)\nmodel\n=\nFakeStreamingListLLM\n(\nresponses\n=\n[\n\"foo-lish\"\n])\nchain\n:\nRunnable\n=\nprompt\n|\nmodel\n|\n{\n\"str\"\n:\nStrOutputParser\n()}\nchain_with_assign\n=\nchain\n.\nassign\n(\nhello\n=\nitemgetter\n(\n\"str\"\n)\n|\nmodel\n)\nprint\n(\nchain_with_assign\n.\ninput_schema\n.\nmodel_json_schema\n())\n# {'title': 'PromptInput', 'type': 'object', 'properties':\n{\n'question'\n:\n{\n'title'\n:\n'Question'\n,\n'type'\n:\n'string'\n}}}\nprint\n(\nchain_with_assign\n.\noutput_schema\n.\nmodel_json_schema\n())\n# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n{\n'str'\n:\n{\n'title'\n:\n'Str'\n,\n'type'\n:\n'string'\n},\n'hello'\n:\n{\n'title'\n:\n'Hello'\n,\n'type'\n:\n'string'\n}}}\nPARAMETER\nDESCRIPTION\n**kwargs\nA mapping of keys to\nRunnable\nor\nRunnable\n-like objects\nthat will be invoked with the entire output dict of this\nRunnable\n.\nTYPE:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\nA new\nRunnable\n.\nbatch_as_completed\n\u00b6\nbatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\ninvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\ntuple\n[\nint\n,\nOutput\n|\nException\n]\nTuples of the index of the input and the output from the\nRunnable\n.\nabatch_as_completed\nasync\n\u00b6\nabatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\nainvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nA tuple of the index of the input and the output from the\nRunnable\n.\nastream_log\nasync\n\u00b6\nastream_log\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ndiff\n:\nbool\n=\nTrue\n,\nwith_streamed_output_list\n:\nbool\n=\nTrue\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nRunLogPatch\n]\n|\nAsyncIterator\n[\nRunLog\n]\nStream all output from a\nRunnable\n, as reported to the callback system.\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\nOutput is streamed as Log objects, which include a list of\nJsonpatch ops that describe how the state of the run has changed in each\nstep, and the final state of the run.\nThe Jsonpatch ops can be applied in order to construct state.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ndiff\nWhether to yield diffs between each step or the current state.\nTYPE:\nbool\nDEFAULT:\nTrue\nwith_streamed_output_list\nWhether to yield the\nstreamed_output\nlist.\nTYPE:\nbool\nDEFAULT:\nTrue\ninclude_names\nOnly include logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nRunLogPatch\n] |\nAsyncIterator\n[\nRunLog\n]\nA\nRunLogPatch\nor\nRunLog\nobject.\nastream_events\nasync\n\u00b6\nastream_events\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nversion\n:\nLiteral\n[\n\"v1\"\n,\n\"v2\"\n]\n=\n\"v2\"\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nStreamEvent\n]\nGenerate a stream of events.\nUse to create an iterator over\nStreamEvent\nthat provide real-time information\nabout the progress of the\nRunnable\n, including\nStreamEvent\nfrom intermediate\nresults.\nA\nStreamEvent\nis a dictionary with the following schema:\nevent\n: Event names are of the format:\non_[runnable_type]_(start|stream|end)\n.\nname\n: The name of the\nRunnable\nthat generated the event.\nrun_id\n: Randomly generated ID associated with the given execution of the\nRunnable\nthat emitted the event. A child\nRunnable\nthat gets invoked as\npart of the execution of a parent\nRunnable\nis assigned its own unique ID.\nparent_ids\n: The IDs of the parent runnables that generated the event. The\nroot\nRunnable\nwill have an empty list. The order of the parent IDs is from\nthe root to the immediate parent. Only available for v2 version of the API.\nThe v1 version of the API will return an empty list.\ntags\n: The tags of the\nRunnable\nthat generated the event.\nmetadata\n: The metadata of the\nRunnable\nthat generated the event.\ndata\n: The data associated with the event. The contents of this field\ndepend on the type of event. See the table below for more details.\nBelow is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table.\nNote\nThis reference table is for the v2 version of the schema.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n'[model name]'\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n'[model name]'\n{'input': 'hello'}\non_llm_stream\n'[model name]'\n'Hello'\non_llm_end\n'[model name]'\n'Hello human!'\non_chain_start\n'format_docs'\non_chain_stream\n'format_docs'\n'hello world!, goodbye world!'\non_chain_end\n'format_docs'\n[Document(...)]\n'hello world!, goodbye world!'\non_tool_start\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n'[retriever name]'\n{\"query\": \"hello\"}\non_retriever_end\n'[retriever name]'\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n'[template_name]'\n{\"question\": \"hello\"}\non_prompt_end\n'[template_name]'\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nIn addition to the standard events, users can also dispatch custom events (see example below).\nCustom events will be only be surfaced with in the v2 version of the API!\nA custom event has following format:\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nHere are declarations associated with the standard events shown above:\nformat_docs\n:\ndef\nformat_docs\n(\ndocs\n:\nlist\n[\nDocument\n])\n->\nstr\n:\n'''Format the docs.'''\nreturn\n\", \"\n.\njoin\n([\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n])\nformat_docs\n=\nRunnableLambda\n(\nformat_docs\n)\nsome_tool\n:\n@tool\ndef\nsome_tool\n(\nx\n:\nint\n,\ny\n:\nstr\n)\n->\ndict\n:\n'''Some_tool.'''\nreturn\n{\n\"x\"\n:\nx\n,\n\"y\"\n:\ny\n}\nprompt\n:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are Cat Agent 007\"\n),\n(\n\"human\"\n,\n\"\n{question}\n\"\n),\n]\n)\n.\nwith_config\n({\n\"run_name\"\n:\n\"my_template\"\n,\n\"tags\"\n:\n[\n\"my_template\"\n]})\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nasync\ndef\nreverse\n(\ns\n:\nstr\n)\n->\nstr\n:\nreturn\ns\n[::\n-\n1\n]\nchain\n=\nRunnableLambda\n(\nfunc\n=\nreverse\n)\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"hello\"\n,\nversion\n=\n\"v2\"\n)\n]\n# Will produce the following events\n# (run_id, and parent_ids has been omitted for brevity):\n[\n{\n\"data\"\n:\n{\n\"input\"\n:\n\"hello\"\n},\n\"event\"\n:\n\"on_chain_start\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"chunk\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_stream\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"output\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_end\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n]\nDispatch custom event\nfrom\nlangchain_core.callbacks.manager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableConfig\nimport\nasyncio\nasync\ndef\nslow_thing\n(\nsome_input\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n->\nstr\n:\n\"\"\"Do something that takes a long time.\"\"\"\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 1 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 2 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nreturn\n\"Done\"\nslow_thing\n=\nRunnableLambda\n(\nslow_thing\n)\nasync\nfor\nevent\nin\nslow_thing\n.\nastream_events\n(\n\"some_input\"\n,\nversion\n=\n\"v2\"\n):\nprint\n(\nevent\n)\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nversion\nThe version of the schema to use, either\n'v2'\nor\n'v1'\n.\nUsers should use\n'v2'\n.\n'v1'\nis for backwards compatibility and will be deprecated\nin\n0.4.0\n.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in\n'v2'\n.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\ninclude_names\nOnly include events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nThese will be passed to\nastream_log\nas this implementation\nof\nastream_events\nis built on top of\nastream_log\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nStreamEvent\n]\nAn async stream of\nStreamEvent\n.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the version is not\n'v1'\nor\n'v2'\n.\nbind\n\u00b6\nbind\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nUseful when a\nRunnable\nin a chain requires an argument that is not\nin the output of the previous\nRunnable\nor included in the user input.\nPARAMETER\nDESCRIPTION\n**kwargs\nThe arguments to bind to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the arguments bound.\nExample\nfrom\nlangchain_ollama\nimport\nChatOllama\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nmodel\n=\nChatOllama\n(\nmodel\n=\n\"llama3.1\"\n)\n# Without bind\nchain\n=\nmodel\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two three four five.'\n# With bind\nchain\n=\nmodel\n.\nbind\n(\nstop\n=\n[\n\"three\"\n])\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two'\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to bind to the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the config bound.\nwith_listeners\n\u00b6\nwith_listeners\n(\n*\n,\non_start\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_end\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_error\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled before the\nRunnable\nstarts running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_end\nCalled after the\nRunnable\nfinishes running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_error\nCalled if the\nRunnable\nthrows an error, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nfrom\nlangchain_core.tracers.schemas\nimport\nRun\nimport\ntime\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\ntime\n.\nsleep\n(\ntime_to_sleep\n)\ndef\nfn_start\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"start_time:\"\n,\nrun_obj\n.\nstart_time\n)\ndef\nfn_end\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"end_time:\"\n,\nrun_obj\n.\nend_time\n)\nchain\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_listeners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nchain\n.\ninvoke\n(\n2\n)\nwith_alisteners\n\u00b6\nwith_alisteners\n(\n*\n,\non_start\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_end\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_error\n:\nAsyncListener\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind async lifecycle listeners to a\nRunnable\n.\nReturns a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled asynchronously before the\nRunnable\nstarts running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_end\nCalled asynchronously after the\nRunnable\nfinishes running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_error\nCalled asynchronously if the\nRunnable\nthrows an error,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnable\nfrom\ndatetime\nimport\ndatetime\n,\ntimezone\nimport\ntime\nimport\nasyncio\ndef\nformat_t\n(\ntimestamp\n:\nfloat\n)\n->\nstr\n:\nreturn\ndatetime\n.\nfromtimestamp\n(\ntimestamp\n,\ntz\n=\ntimezone\n.\nutc\n)\n.\nisoformat\n()\nasync\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\ntime_to_sleep\n)\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_start\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on start callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n3\n)\nprint\n(\nf\n\"on start callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_end\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on end callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n2\n)\nprint\n(\nf\n\"on end callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nrunnable\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_alisteners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nasync\ndef\nconcurrent_runs\n():\nawait\nasyncio\n.\ngather\n(\nrunnable\n.\nainvoke\n(\n2\n),\nrunnable\n.\nainvoke\n(\n3\n))\nasyncio\n.\nrun\n(\nconcurrent_runs\n())\n# Result:\n# on start callback starts at 2025-03-01T07:05:22.875378+00:00\n# on start callback starts at 2025-03-01T07:05:22.875495+00:00\n# on start callback ends at 2025-03-01T07:05:25.878862+00:00\n# on start callback ends at 2025-03-01T07:05:25.878947+00:00\n# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n# on end callback starts at 2025-03-01T07:05:27.882360+00:00\n# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n# on end callback starts at 2025-03-01T07:05:28.882428+00:00\n# on end callback ends at 2025-03-01T07:05:29.883893+00:00\n# on end callback ends at 2025-03-01T07:05:30.884831+00:00\nwith_types\n\u00b6\nwith_types\n(\n*\n,\ninput_type\n:\ntype\n[\nInput\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\ntype\n[\nOutput\n]\n|\nNone\n=\nNone\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninput_type\nThe input type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nInput\n] | None\nDEFAULT:\nNone\noutput_type\nThe output type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nOutput\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the types bound.\nwith_retry\n\u00b6\nwith_retry\n(\n*\n,\nretry_if_exception_type\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nwait_exponential_jitter\n:\nbool\n=\nTrue\n,\nexponential_jitter_params\n:\nExponentialJitterParams\n|\nNone\n=\nNone\n,\nstop_after_attempt\n:\nint\n=\n3\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nPARAMETER\nDESCRIPTION\nretry_if_exception_type\nA tuple of exception types to retry on.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nwait_exponential_jitter\nWhether to add jitter to the wait\ntime between retries.\nTYPE:\nbool\nDEFAULT:\nTrue\nstop_after_attempt\nThe maximum number of attempts to make before\ngiving up.\nTYPE:\nint\nDEFAULT:\n3\nexponential_jitter_params\nParameters for\ntenacity.wait_exponential_jitter\n. Namely:\ninitial\n,\nmax\n,\nexp_base\n, and\njitter\n(all\nfloat\nvalues).\nTYPE:\nExponentialJitterParams\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ncount\n=\n0\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nNone\n:\nglobal\ncount\ncount\n=\ncount\n+\n1\nif\nx\n==\n1\n:\nraise\nValueError\n(\n\"x is 1\"\n)\nelse\n:\npass\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\ntry\n:\nrunnable\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n,\nretry_if_exception_type\n=\n(\nValueError\n,),\n)\n.\ninvoke\n(\n1\n)\nexcept\nValueError\n:\npass\nassert\ncount\n==\n2\nmap\n\u00b6\nmap\n()\n->\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nCalls\ninvoke\nwith each input.\nRETURNS\nDESCRIPTION\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nA new\nRunnable\nthat maps a list of inputs to a list of outputs.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\nprint\n(\nrunnable\n.\nmap\n()\n.\ninvoke\n([\n1\n,\n2\n,\n3\n]))\n# [2, 3, 4]\nwith_fallbacks\n\u00b6\nwith_fallbacks\n(\nfallbacks\n:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]],\n*\n,\nexceptions_to_handle\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nexception_key\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nThe new\nRunnable\nwill try the original\nRunnable\n, and then each fallback\nin order, upon failures.\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nExample\nfrom\ntyping\nimport\nIterator\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\ndef\n_generate_immediate_error\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nraise\nValueError\n()\nyield\n\"\"\ndef\n_generate\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nyield from\n\"foo bar\"\nrunnable\n=\nRunnableGenerator\n(\n_generate_immediate_error\n)\n.\nwith_fallbacks\n(\n[\nRunnableGenerator\n(\n_generate\n)]\n)\nprint\n(\n\"\"\n.\njoin\n(\nrunnable\n.\nstream\n({})))\n# foo bar\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nas_tool\n\u00b6\nas_tool\n(\nargs_schema\n:\ntype\n[\nBaseModel\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\narg_types\n:\ndict\n[\nstr\n,\ntype\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nas_tool\nwill instantiate a\nBaseTool\nwith a name, description, and\nargs_schema\nfrom a\nRunnable\n. Where possible, schemas are inferred\nfrom\nrunnable.get_input_schema\n.\nAlternatively (e.g., if the\nRunnable\ntakes a dict as input and the specific\ndict\nkeys are not typed), the schema can be specified directly with\nargs_schema\n.\nYou can also pass\narg_types\nto just specify the required arguments and their\ntypes.\nPARAMETER\nDESCRIPTION\nargs_schema\nThe schema for the tool.\nTYPE:\ntype\n[\nBaseModel\n] | None\nDEFAULT:\nNone\nname\nThe name of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nThe description of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\narg_types\nA dictionary of argument names to types.\nTYPE:\ndict\n[\nstr\n,\ntype\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBaseTool\nA\nBaseTool\ninstance.\nTypedDict\ninput\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nclass\nArgs\n(\nTypedDict\n):\na\n:\nint\nb\n:\nlist\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\nargs_schema\nfrom\ntyping\nimport\nAny\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nclass\nFSchema\n(\nBaseModel\n):\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n...\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nlist\n[\nint\n]\n=\nField\n(\n...\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nFSchema\n)\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\narg_types\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nlist\n[\nint\n]})\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\nstr\ninput\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n(\n\"b\"\n)\nlc_id\nclassmethod\n\u00b6\nlc_id\n()\n->\nlist\n[\nstr\n]\nReturn a unique identifier for this class for serialization purposes.\nThe unique identifier is a list of strings that describes the path\nto the object.\nFor example, for the class\nlangchain.llms.openai.OpenAI\n, the id is\n[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]\n.\nto_json\n\u00b6\nto_json\n()\n->\nSerializedConstructor\n|\nSerializedNotImplemented\nSerialize the\nRunnable\nto JSON.\nRETURNS\nDESCRIPTION\nSerializedConstructor\n|\nSerializedNotImplemented\nA JSON-serializable representation of the\nRunnable\n.\nto_json_not_implemented\n\u00b6\nto_json_not_implemented\n()\n->\nSerializedNotImplemented\nSerialize a \"not implemented\" object.\nRETURNS\nDESCRIPTION\nSerializedNotImplemented\nSerializedNotImplemented\n.\nconfigurable_fields\n\u00b6\nconfigurable_fields\n(\n**\nkwargs\n:\nAnyConfigurableField\n,\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure particular\nRunnable\nfields at runtime.\nPARAMETER\nDESCRIPTION\n**kwargs\nA dictionary of\nConfigurableField\ninstances to configure.\nTYPE:\nAnyConfigurableField\nDEFAULT:\n{}\nRAISES\nDESCRIPTION\nValueError\nIf a configuration key is not found in the\nRunnable\n.\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the fields configured.\nExample\nfrom\nlangchain_core.runnables\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmax_tokens\n=\n20\n)\n.\nconfigurable_fields\n(\nmax_tokens\n=\nConfigurableField\n(\nid\n=\n\"output_token_number\"\n,\nname\n=\n\"Max tokens in the output\"\n,\ndescription\n=\n\"The maximum number of tokens in the output\"\n,\n)\n)\n# max_tokens = 20\nprint\n(\n\"max_tokens_20: \"\n,\nmodel\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n)\n# max_tokens = 200\nprint\n(\n\"max_tokens_200: \"\n,\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"output_token_number\"\n:\n200\n})\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n,\n)\nconfigurable_alternatives\n\u00b6\nconfigurable_alternatives\n(\nwhich\n:\nConfigurableField\n,\n*\n,\ndefault_key\n:\nstr\n=\n\"default\"\n,\nprefix_keys\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nRunnable\n[\nInput\n,\nOutput\n]\n|\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nPARAMETER\nDESCRIPTION\nwhich\nThe\nConfigurableField\ninstance that will be used to select the\nalternative.\nTYPE:\nConfigurableField\ndefault_key\nThe default key to use if no alternative is selected.\nTYPE:\nstr\nDEFAULT:\n'default'\nprefix_keys\nWhether to prefix the keys with the\nConfigurableField\nid.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nA dictionary of keys to\nRunnable\ninstances or callables that\nreturn\nRunnable\ninstances.\nTYPE:\nRunnable\n[\nInput\n,\nOutput\n] |\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the alternatives configured.\nExample\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core.runnables.utils\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatAnthropic\n(\nmodel_name\n=\n\"claude-sonnet-4-5-20250929\"\n)\n.\nconfigurable_alternatives\n(\nConfigurableField\n(\nid\n=\n\"llm\"\n),\ndefault_key\n=\n\"anthropic\"\n,\nopenai\n=\nChatOpenAI\n(),\n)\n# uses the default model ChatAnthropic\nprint\n(\nmodel\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\n# uses ChatOpenAI\nprint\n(\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"openai\"\n})\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\nRunnableSerializable\n\u00b6\nBases:\nSerializable\n,\nRunnable\n[\nInput\n,\nOutput\n]\nRunnable that can be serialized to JSON.\nMETHOD\nDESCRIPTION\nto_json\nSerialize the\nRunnable\nto JSON.\nconfigurable_fields\nConfigure particular\nRunnable\nfields at runtime.\nconfigurable_alternatives\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nget_name\nGet the name of the\nRunnable\n.\nget_input_schema\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nget_input_jsonschema\nGet a JSON schema that represents the input to the\nRunnable\n.\nget_output_schema\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nget_output_jsonschema\nGet a JSON schema that represents the output of the\nRunnable\n.\nconfig_schema\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nget_config_jsonschema\nGet a JSON schema that represents the config of the\nRunnable\n.\nget_graph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\nReturn a list of prompts used by this\nRunnable\n.\n__or__\nRunnable \"or\" operator.\n__ror__\nRunnable \"reverse-or\" operator.\npipe\nPipe\nRunnable\nobjects.\npick\nPick keys from the output\ndict\nof this\nRunnable\n.\nassign\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\ninvoke\nTransform a single input into an output.\nainvoke\nTransform a single input into an output.\nbatch\nDefault implementation runs invoke in parallel using a thread pool executor.\nbatch_as_completed\nRun\ninvoke\nin parallel on a list of inputs.\nabatch\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nabatch_as_completed\nRun\nainvoke\nin parallel on a list of inputs.\nstream\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nastream\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nastream_log\nStream all output from a\nRunnable\n, as reported to the callback system.\nastream_events\nGenerate a stream of events.\ntransform\nTransform inputs to outputs.\natransform\nTransform inputs to outputs.\nbind\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nwith_config\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nwith_listeners\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nwith_alisteners\nBind async lifecycle listeners to a\nRunnable\n.\nwith_types\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nwith_retry\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nmap\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nwith_fallbacks\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nas_tool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\n__init__\nis_lc_serializable\nIs this class serializable?\nget_lc_namespace\nGet the namespace of the LangChain object.\nlc_id\nReturn a unique identifier for this class for serialization purposes.\nto_json_not_implemented\nSerialize a \"not implemented\" object.\nname\nclass-attribute\ninstance-attribute\n\u00b6\nname\n:\nstr\n|\nNone\n=\nNone\nThe name of the\nRunnable\n. Used for debugging and tracing.\nInputType\nproperty\n\u00b6\nInputType\n:\ntype\n[\nInput\n]\nInput type.\nThe type of input this\nRunnable\naccepts specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the input type cannot be inferred.\nOutputType\nproperty\n\u00b6\nOutputType\n:\ntype\n[\nOutput\n]\nOutput Type.\nThe type of output this\nRunnable\nproduces specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the output type cannot be inferred.\ninput_schema\nproperty\n\u00b6\ninput_schema\n:\ntype\n[\nBaseModel\n]\nThe type of input this\nRunnable\naccepts specified as a Pydantic model.\noutput_schema\nproperty\n\u00b6\noutput_schema\n:\ntype\n[\nBaseModel\n]\nOutput schema.\nThe type of output this\nRunnable\nproduces specified as a Pydantic model.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\n[\nConfigurableFieldSpec\n]\nList configurable fields for this\nRunnable\n.\nlc_secrets\nproperty\n\u00b6\nlc_secrets\n:\ndict\n[\nstr\n,\nstr\n]\nA map of constructor argument names to secret ids.\nFor example,\n{\"openai_api_key\": \"OPENAI_API_KEY\"}\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nList of attribute names that should be included in the serialized kwargs.\nThese attributes must be accepted by the constructor.\nDefault is an empty dictionary.\nto_json\n\u00b6\nto_json\n()\n->\nSerializedConstructor\n|\nSerializedNotImplemented\nSerialize the\nRunnable\nto JSON.\nRETURNS\nDESCRIPTION\nSerializedConstructor\n|\nSerializedNotImplemented\nA JSON-serializable representation of the\nRunnable\n.\nconfigurable_fields\n\u00b6\nconfigurable_fields\n(\n**\nkwargs\n:\nAnyConfigurableField\n,\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure particular\nRunnable\nfields at runtime.\nPARAMETER\nDESCRIPTION\n**kwargs\nA dictionary of\nConfigurableField\ninstances to configure.\nTYPE:\nAnyConfigurableField\nDEFAULT:\n{}\nRAISES\nDESCRIPTION\nValueError\nIf a configuration key is not found in the\nRunnable\n.\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the fields configured.\nExample\nfrom\nlangchain_core.runnables\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmax_tokens\n=\n20\n)\n.\nconfigurable_fields\n(\nmax_tokens\n=\nConfigurableField\n(\nid\n=\n\"output_token_number\"\n,\nname\n=\n\"Max tokens in the output\"\n,\ndescription\n=\n\"The maximum number of tokens in the output\"\n,\n)\n)\n# max_tokens = 20\nprint\n(\n\"max_tokens_20: \"\n,\nmodel\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n)\n# max_tokens = 200\nprint\n(\n\"max_tokens_200: \"\n,\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"output_token_number\"\n:\n200\n})\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n,\n)\nconfigurable_alternatives\n\u00b6\nconfigurable_alternatives\n(\nwhich\n:\nConfigurableField\n,\n*\n,\ndefault_key\n:\nstr\n=\n\"default\"\n,\nprefix_keys\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nRunnable\n[\nInput\n,\nOutput\n]\n|\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nPARAMETER\nDESCRIPTION\nwhich\nThe\nConfigurableField\ninstance that will be used to select the\nalternative.\nTYPE:\nConfigurableField\ndefault_key\nThe default key to use if no alternative is selected.\nTYPE:\nstr\nDEFAULT:\n'default'\nprefix_keys\nWhether to prefix the keys with the\nConfigurableField\nid.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nA dictionary of keys to\nRunnable\ninstances or callables that\nreturn\nRunnable\ninstances.\nTYPE:\nRunnable\n[\nInput\n,\nOutput\n] |\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the alternatives configured.\nExample\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core.runnables.utils\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatAnthropic\n(\nmodel_name\n=\n\"claude-sonnet-4-5-20250929\"\n)\n.\nconfigurable_alternatives\n(\nConfigurableField\n(\nid\n=\n\"llm\"\n),\ndefault_key\n=\n\"anthropic\"\n,\nopenai\n=\nChatOpenAI\n(),\n)\n# uses the default model ChatAnthropic\nprint\n(\nmodel\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\n# uses ChatOpenAI\nprint\n(\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"openai\"\n})\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\nget_name\n\u00b6\nget_name\n(\nsuffix\n:\nstr\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nstr\nGet the name of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nsuffix\nAn optional suffix to append to the name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nname\nAn optional name to use instead of the\nRunnable\n's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nstr\nThe name of the\nRunnable\n.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic input schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an input schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate input.\nget_input_jsonschema\n\u00b6\nget_input_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the input to the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the input to the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_input_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic output schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an output schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate output.\nget_output_jsonschema\n\u00b6\nget_output_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the output of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the output of the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_output_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nconfig_schema\n\u00b6\nconfig_schema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nTo mark a field as configurable, see the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate config.\nget_config_jsonschema\n\u00b6\nget_config_jsonschema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the config of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the config of the\nRunnable\n.\nAdded in\nlangchain-core\n0.3.0\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nGraph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\n\u00b6\nget_prompts\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nlist\n[\nBasePromptTemplate\n]\nReturn a list of prompts used by this\nRunnable\n.\n__or__\n\u00b6\n__or__\n(\nother\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nRunnable \"or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]] |\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]] |\nCallable\n[[\nAny\n],\nOther\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\n__ror__\n\u00b6\n__ror__\n(\nother\n:\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nRunnable \"reverse-or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]] |\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]] |\nCallable\n[[\nOther\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nOther\n],\nAny\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nA new\nRunnable\n.\npipe\n\u00b6\npipe\n(\n*\nothers\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n],\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nPipe\nRunnable\nobjects.\nCompose this\nRunnable\nwith\nRunnable\n-like objects to make a\nRunnableSequence\n.\nEquivalent to\nRunnableSequence(self, *others)\nor\nself | others[0] | ...\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nmul_two\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n*\n2\nrunnable_1\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable_2\n=\nRunnableLambda\n(\nmul_two\n)\nsequence\n=\nrunnable_1\n.\npipe\n(\nrunnable_2\n)\n# Or equivalently:\n# sequence = runnable_1 | runnable_2\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence\n.\ninvoke\n(\n1\n)\nawait\nsequence\n.\nainvoke\n(\n1\n)\n# -> 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\nawait\nsequence\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# -> [4, 6, 8]\nPARAMETER\nDESCRIPTION\n*others\nOther\nRunnable\nor\nRunnable\n-like objects to compose\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n]\nDEFAULT:\n()\nname\nAn optional name for the resulting\nRunnableSequence\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\npick\n\u00b6\npick\n(\nkeys\n:\nstr\n|\nlist\n[\nstr\n])\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nPick keys from the output\ndict\nof this\nRunnable\n.\nPick a single key\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\njson_only_chain\n=\nchain\n.\npick\n(\n\"json\"\n)\njson_only_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> [1, 2, 3]\nPick a list of keys\nfrom\ntyping\nimport\nAny\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\ndef\nas_bytes\n(\nx\n:\nAny\n)\n->\nbytes\n:\nreturn\nbytes\n(\nx\n,\n\"utf-8\"\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n,\nbytes\n=\nRunnableLambda\n(\nas_bytes\n)\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\njson_and_bytes_chain\n=\nchain\n.\npick\n([\n\"json\"\n,\n\"bytes\"\n])\njson_and_bytes_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\nPARAMETER\nDESCRIPTION\nkeys\nA key or list of keys to pick from the output dict.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\na new\nRunnable\n.\nassign\n\u00b6\nassign\n(\n**\nkwargs\n:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]],\n)\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nfrom\nlangchain_core.language_models.fake\nimport\nFakeStreamingListLLM\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core.prompts\nimport\nSystemMessagePromptTemplate\nfrom\nlangchain_core.runnables\nimport\nRunnable\nfrom\noperator\nimport\nitemgetter\nprompt\n=\n(\nSystemMessagePromptTemplate\n.\nfrom_template\n(\n\"You are a nice assistant.\"\n)\n+\n\"\n{question}\n\"\n)\nmodel\n=\nFakeStreamingListLLM\n(\nresponses\n=\n[\n\"foo-lish\"\n])\nchain\n:\nRunnable\n=\nprompt\n|\nmodel\n|\n{\n\"str\"\n:\nStrOutputParser\n()}\nchain_with_assign\n=\nchain\n.\nassign\n(\nhello\n=\nitemgetter\n(\n\"str\"\n)\n|\nmodel\n)\nprint\n(\nchain_with_assign\n.\ninput_schema\n.\nmodel_json_schema\n())\n# {'title': 'PromptInput', 'type': 'object', 'properties':\n{\n'question'\n:\n{\n'title'\n:\n'Question'\n,\n'type'\n:\n'string'\n}}}\nprint\n(\nchain_with_assign\n.\noutput_schema\n.\nmodel_json_schema\n())\n# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n{\n'str'\n:\n{\n'title'\n:\n'Str'\n,\n'type'\n:\n'string'\n},\n'hello'\n:\n{\n'title'\n:\n'Hello'\n,\n'type'\n:\n'string'\n}}}\nPARAMETER\nDESCRIPTION\n**kwargs\nA mapping of keys to\nRunnable\nor\nRunnable\n-like objects\nthat will be invoked with the entire output dict of this\nRunnable\n.\nTYPE:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\nA new\nRunnable\n.\ninvoke\nabstractmethod\n\u00b6\ninvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nOutput\nTransform a single input into an output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nbatch\n\u00b6\nbatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n. The config supports\nstandard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work\nto do in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nbatch_as_completed\n\u00b6\nbatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\ninvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\ntuple\n[\nint\n,\nOutput\n|\nException\n]\nTuples of the index of the input and the output from the\nRunnable\n.\nabatch\nasync\n\u00b6\nabatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nThe default implementation of\nbatch\nworks well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nabatch_as_completed\nasync\n\u00b6\nabatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\nainvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nA tuple of the index of the input and the output from the\nRunnable\n.\nstream\n\u00b6\nstream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nAsyncIterator\n[\nOutput\n]\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nastream_log\nasync\n\u00b6\nastream_log\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ndiff\n:\nbool\n=\nTrue\n,\nwith_streamed_output_list\n:\nbool\n=\nTrue\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nRunLogPatch\n]\n|\nAsyncIterator\n[\nRunLog\n]\nStream all output from a\nRunnable\n, as reported to the callback system.\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\nOutput is streamed as Log objects, which include a list of\nJsonpatch ops that describe how the state of the run has changed in each\nstep, and the final state of the run.\nThe Jsonpatch ops can be applied in order to construct state.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ndiff\nWhether to yield diffs between each step or the current state.\nTYPE:\nbool\nDEFAULT:\nTrue\nwith_streamed_output_list\nWhether to yield the\nstreamed_output\nlist.\nTYPE:\nbool\nDEFAULT:\nTrue\ninclude_names\nOnly include logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nRunLogPatch\n] |\nAsyncIterator\n[\nRunLog\n]\nA\nRunLogPatch\nor\nRunLog\nobject.\nastream_events\nasync\n\u00b6\nastream_events\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nversion\n:\nLiteral\n[\n\"v1\"\n,\n\"v2\"\n]\n=\n\"v2\"\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nStreamEvent\n]\nGenerate a stream of events.\nUse to create an iterator over\nStreamEvent\nthat provide real-time information\nabout the progress of the\nRunnable\n, including\nStreamEvent\nfrom intermediate\nresults.\nA\nStreamEvent\nis a dictionary with the following schema:\nevent\n: Event names are of the format:\non_[runnable_type]_(start|stream|end)\n.\nname\n: The name of the\nRunnable\nthat generated the event.\nrun_id\n: Randomly generated ID associated with the given execution of the\nRunnable\nthat emitted the event. A child\nRunnable\nthat gets invoked as\npart of the execution of a parent\nRunnable\nis assigned its own unique ID.\nparent_ids\n: The IDs of the parent runnables that generated the event. The\nroot\nRunnable\nwill have an empty list. The order of the parent IDs is from\nthe root to the immediate parent. Only available for v2 version of the API.\nThe v1 version of the API will return an empty list.\ntags\n: The tags of the\nRunnable\nthat generated the event.\nmetadata\n: The metadata of the\nRunnable\nthat generated the event.\ndata\n: The data associated with the event. The contents of this field\ndepend on the type of event. See the table below for more details.\nBelow is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table.\nNote\nThis reference table is for the v2 version of the schema.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n'[model name]'\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n'[model name]'\n{'input': 'hello'}\non_llm_stream\n'[model name]'\n'Hello'\non_llm_end\n'[model name]'\n'Hello human!'\non_chain_start\n'format_docs'\non_chain_stream\n'format_docs'\n'hello world!, goodbye world!'\non_chain_end\n'format_docs'\n[Document(...)]\n'hello world!, goodbye world!'\non_tool_start\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n'[retriever name]'\n{\"query\": \"hello\"}\non_retriever_end\n'[retriever name]'\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n'[template_name]'\n{\"question\": \"hello\"}\non_prompt_end\n'[template_name]'\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nIn addition to the standard events, users can also dispatch custom events (see example below).\nCustom events will be only be surfaced with in the v2 version of the API!\nA custom event has following format:\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nHere are declarations associated with the standard events shown above:\nformat_docs\n:\ndef\nformat_docs\n(\ndocs\n:\nlist\n[\nDocument\n])\n->\nstr\n:\n'''Format the docs.'''\nreturn\n\", \"\n.\njoin\n([\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n])\nformat_docs\n=\nRunnableLambda\n(\nformat_docs\n)\nsome_tool\n:\n@tool\ndef\nsome_tool\n(\nx\n:\nint\n,\ny\n:\nstr\n)\n->\ndict\n:\n'''Some_tool.'''\nreturn\n{\n\"x\"\n:\nx\n,\n\"y\"\n:\ny\n}\nprompt\n:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are Cat Agent 007\"\n),\n(\n\"human\"\n,\n\"\n{question}\n\"\n),\n]\n)\n.\nwith_config\n({\n\"run_name\"\n:\n\"my_template\"\n,\n\"tags\"\n:\n[\n\"my_template\"\n]})\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nasync\ndef\nreverse\n(\ns\n:\nstr\n)\n->\nstr\n:\nreturn\ns\n[::\n-\n1\n]\nchain\n=\nRunnableLambda\n(\nfunc\n=\nreverse\n)\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"hello\"\n,\nversion\n=\n\"v2\"\n)\n]\n# Will produce the following events\n# (run_id, and parent_ids has been omitted for brevity):\n[\n{\n\"data\"\n:\n{\n\"input\"\n:\n\"hello\"\n},\n\"event\"\n:\n\"on_chain_start\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"chunk\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_stream\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"output\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_end\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n]\nDispatch custom event\nfrom\nlangchain_core.callbacks.manager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableConfig\nimport\nasyncio\nasync\ndef\nslow_thing\n(\nsome_input\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n->\nstr\n:\n\"\"\"Do something that takes a long time.\"\"\"\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 1 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 2 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nreturn\n\"Done\"\nslow_thing\n=\nRunnableLambda\n(\nslow_thing\n)\nasync\nfor\nevent\nin\nslow_thing\n.\nastream_events\n(\n\"some_input\"\n,\nversion\n=\n\"v2\"\n):\nprint\n(\nevent\n)\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nversion\nThe version of the schema to use, either\n'v2'\nor\n'v1'\n.\nUsers should use\n'v2'\n.\n'v1'\nis for backwards compatibility and will be deprecated\nin\n0.4.0\n.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in\n'v2'\n.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\ninclude_names\nOnly include events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nThese will be passed to\nastream_log\nas this implementation\nof\nastream_events\nis built on top of\nastream_log\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nStreamEvent\n]\nAn async stream of\nStreamEvent\n.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the version is not\n'v1'\nor\n'v2'\n.\ntransform\n\u00b6\ntransform\n(\ninput\n:\nIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of transform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn iterator of inputs to the\nRunnable\n.\nTYPE:\nIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\natransform\nasync\n\u00b6\natransform\n(\ninput\n:\nAsyncIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of atransform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn async iterator of inputs to the\nRunnable\n.\nTYPE:\nAsyncIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nbind\n\u00b6\nbind\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nUseful when a\nRunnable\nin a chain requires an argument that is not\nin the output of the previous\nRunnable\nor included in the user input.\nPARAMETER\nDESCRIPTION\n**kwargs\nThe arguments to bind to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the arguments bound.\nExample\nfrom\nlangchain_ollama\nimport\nChatOllama\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nmodel\n=\nChatOllama\n(\nmodel\n=\n\"llama3.1\"\n)\n# Without bind\nchain\n=\nmodel\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two three four five.'\n# With bind\nchain\n=\nmodel\n.\nbind\n(\nstop\n=\n[\n\"three\"\n])\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two'\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to bind to the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the config bound.\nwith_listeners\n\u00b6\nwith_listeners\n(\n*\n,\non_start\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_end\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_error\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled before the\nRunnable\nstarts running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_end\nCalled after the\nRunnable\nfinishes running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_error\nCalled if the\nRunnable\nthrows an error, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nfrom\nlangchain_core.tracers.schemas\nimport\nRun\nimport\ntime\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\ntime\n.\nsleep\n(\ntime_to_sleep\n)\ndef\nfn_start\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"start_time:\"\n,\nrun_obj\n.\nstart_time\n)\ndef\nfn_end\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"end_time:\"\n,\nrun_obj\n.\nend_time\n)\nchain\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_listeners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nchain\n.\ninvoke\n(\n2\n)\nwith_alisteners\n\u00b6\nwith_alisteners\n(\n*\n,\non_start\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_end\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_error\n:\nAsyncListener\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind async lifecycle listeners to a\nRunnable\n.\nReturns a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled asynchronously before the\nRunnable\nstarts running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_end\nCalled asynchronously after the\nRunnable\nfinishes running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_error\nCalled asynchronously if the\nRunnable\nthrows an error,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnable\nfrom\ndatetime\nimport\ndatetime\n,\ntimezone\nimport\ntime\nimport\nasyncio\ndef\nformat_t\n(\ntimestamp\n:\nfloat\n)\n->\nstr\n:\nreturn\ndatetime\n.\nfromtimestamp\n(\ntimestamp\n,\ntz\n=\ntimezone\n.\nutc\n)\n.\nisoformat\n()\nasync\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\ntime_to_sleep\n)\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_start\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on start callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n3\n)\nprint\n(\nf\n\"on start callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_end\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on end callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n2\n)\nprint\n(\nf\n\"on end callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nrunnable\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_alisteners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nasync\ndef\nconcurrent_runs\n():\nawait\nasyncio\n.\ngather\n(\nrunnable\n.\nainvoke\n(\n2\n),\nrunnable\n.\nainvoke\n(\n3\n))\nasyncio\n.\nrun\n(\nconcurrent_runs\n())\n# Result:\n# on start callback starts at 2025-03-01T07:05:22.875378+00:00\n# on start callback starts at 2025-03-01T07:05:22.875495+00:00\n# on start callback ends at 2025-03-01T07:05:25.878862+00:00\n# on start callback ends at 2025-03-01T07:05:25.878947+00:00\n# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n# on end callback starts at 2025-03-01T07:05:27.882360+00:00\n# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n# on end callback starts at 2025-03-01T07:05:28.882428+00:00\n# on end callback ends at 2025-03-01T07:05:29.883893+00:00\n# on end callback ends at 2025-03-01T07:05:30.884831+00:00\nwith_types\n\u00b6\nwith_types\n(\n*\n,\ninput_type\n:\ntype\n[\nInput\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\ntype\n[\nOutput\n]\n|\nNone\n=\nNone\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninput_type\nThe input type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nInput\n] | None\nDEFAULT:\nNone\noutput_type\nThe output type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nOutput\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the types bound.\nwith_retry\n\u00b6\nwith_retry\n(\n*\n,\nretry_if_exception_type\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nwait_exponential_jitter\n:\nbool\n=\nTrue\n,\nexponential_jitter_params\n:\nExponentialJitterParams\n|\nNone\n=\nNone\n,\nstop_after_attempt\n:\nint\n=\n3\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nPARAMETER\nDESCRIPTION\nretry_if_exception_type\nA tuple of exception types to retry on.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nwait_exponential_jitter\nWhether to add jitter to the wait\ntime between retries.\nTYPE:\nbool\nDEFAULT:\nTrue\nstop_after_attempt\nThe maximum number of attempts to make before\ngiving up.\nTYPE:\nint\nDEFAULT:\n3\nexponential_jitter_params\nParameters for\ntenacity.wait_exponential_jitter\n. Namely:\ninitial\n,\nmax\n,\nexp_base\n, and\njitter\n(all\nfloat\nvalues).\nTYPE:\nExponentialJitterParams\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ncount\n=\n0\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nNone\n:\nglobal\ncount\ncount\n=\ncount\n+\n1\nif\nx\n==\n1\n:\nraise\nValueError\n(\n\"x is 1\"\n)\nelse\n:\npass\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\ntry\n:\nrunnable\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n,\nretry_if_exception_type\n=\n(\nValueError\n,),\n)\n.\ninvoke\n(\n1\n)\nexcept\nValueError\n:\npass\nassert\ncount\n==\n2\nmap\n\u00b6\nmap\n()\n->\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nCalls\ninvoke\nwith each input.\nRETURNS\nDESCRIPTION\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nA new\nRunnable\nthat maps a list of inputs to a list of outputs.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\nprint\n(\nrunnable\n.\nmap\n()\n.\ninvoke\n([\n1\n,\n2\n,\n3\n]))\n# [2, 3, 4]\nwith_fallbacks\n\u00b6\nwith_fallbacks\n(\nfallbacks\n:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]],\n*\n,\nexceptions_to_handle\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nexception_key\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nThe new\nRunnable\nwill try the original\nRunnable\n, and then each fallback\nin order, upon failures.\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nExample\nfrom\ntyping\nimport\nIterator\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\ndef\n_generate_immediate_error\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nraise\nValueError\n()\nyield\n\"\"\ndef\n_generate\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nyield from\n\"foo bar\"\nrunnable\n=\nRunnableGenerator\n(\n_generate_immediate_error\n)\n.\nwith_fallbacks\n(\n[\nRunnableGenerator\n(\n_generate\n)]\n)\nprint\n(\n\"\"\n.\njoin\n(\nrunnable\n.\nstream\n({})))\n# foo bar\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nas_tool\n\u00b6\nas_tool\n(\nargs_schema\n:\ntype\n[\nBaseModel\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\narg_types\n:\ndict\n[\nstr\n,\ntype\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nas_tool\nwill instantiate a\nBaseTool\nwith a name, description, and\nargs_schema\nfrom a\nRunnable\n. Where possible, schemas are inferred\nfrom\nrunnable.get_input_schema\n.\nAlternatively (e.g., if the\nRunnable\ntakes a dict as input and the specific\ndict\nkeys are not typed), the schema can be specified directly with\nargs_schema\n.\nYou can also pass\narg_types\nto just specify the required arguments and their\ntypes.\nPARAMETER\nDESCRIPTION\nargs_schema\nThe schema for the tool.\nTYPE:\ntype\n[\nBaseModel\n] | None\nDEFAULT:\nNone\nname\nThe name of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nThe description of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\narg_types\nA dictionary of argument names to types.\nTYPE:\ndict\n[\nstr\n,\ntype\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBaseTool\nA\nBaseTool\ninstance.\nTypedDict\ninput\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nclass\nArgs\n(\nTypedDict\n):\na\n:\nint\nb\n:\nlist\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\nargs_schema\nfrom\ntyping\nimport\nAny\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nclass\nFSchema\n(\nBaseModel\n):\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n...\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nlist\n[\nint\n]\n=\nField\n(\n...\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nFSchema\n)\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\narg_types\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nlist\n[\nint\n]})\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\nstr\ninput\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n(\n\"b\"\n)\n__init__\n\u00b6\n__init__\n(\n*\nargs\n:\nAny\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nis_lc_serializable\nclassmethod\n\u00b6\nis_lc_serializable\n()\n->\nbool\nIs this class serializable?\nBy design, even if a class inherits from\nSerializable\n, it is not serializable\nby default. This is to prevent accidental serialization of objects that should\nnot be serialized.\nRETURNS\nDESCRIPTION\nbool\nWhether the class is serializable. Default is\nFalse\n.\nget_lc_namespace\nclassmethod\n\u00b6\nget_lc_namespace\n()\n->\nlist\n[\nstr\n]\nGet the namespace of the LangChain object.\nFor example, if the class is\nlangchain.llms.openai.OpenAI\n,\nthen the namespace is\n[\"langchain\", \"llms\", \"openai\"]\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nThe namespace.\nlc_id\nclassmethod\n\u00b6\nlc_id\n()\n->\nlist\n[\nstr\n]\nReturn a unique identifier for this class for serialization purposes.\nThe unique identifier is a list of strings that describes the path\nto the object.\nFor example, for the class\nlangchain.llms.openai.OpenAI\n, the id is\n[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]\n.\nto_json_not_implemented\n\u00b6\nto_json_not_implemented\n()\n->\nSerializedNotImplemented\nSerialize a \"not implemented\" object.\nRETURNS\nDESCRIPTION\nSerializedNotImplemented\nSerializedNotImplemented\n.\nRunnableConfig\n\u00b6\nBases:\nTypedDict\nConfiguration for a\nRunnable\n.\nSee the\nreference docs\nfor more details.\ntags\ninstance-attribute\n\u00b6\ntags\n:\nlist\n[\nstr\n]\nTags for this call and any sub-calls (e.g. a Chain calling an LLM).\nYou can use these to filter calls.\nmetadata\ninstance-attribute\n\u00b6\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\nMetadata for this call and any sub-calls (e.g. a Chain calling an LLM).\nKeys should be strings, values should be JSON-serializable.\ncallbacks\ninstance-attribute\n\u00b6\ncallbacks\n:\nCallbacks\nCallbacks for this call and any sub-calls (e.g. a Chain calling an LLM).\nTags are passed to all callbacks, metadata is passed to handle*Start callbacks.\nrun_name\ninstance-attribute\n\u00b6\nrun_name\n:\nstr\nName for the tracer run for this call.\nDefaults to the name of the class.\nmax_concurrency\ninstance-attribute\n\u00b6\nmax_concurrency\n:\nint\n|\nNone\nMaximum number of parallel calls to make.\nIf not provided, defaults to\nThreadPoolExecutor\n's default.\nrecursion_limit\ninstance-attribute\n\u00b6\nrecursion_limit\n:\nint\nMaximum number of times a call can recurse.\nIf not provided, defaults to\n25\n.\nconfigurable\ninstance-attribute\n\u00b6\nconfigurable\n:\ndict\n[\nstr\n,\nAny\n]\nRuntime values for attributes previously made configurable on this\nRunnable\n,\nor sub-Runnables, through\nconfigurable_fields\nor\nconfigurable_alternatives\n.\nCheck\noutput_schema\nfor a description of the attributes that have been made\nconfigurable.\nrun_id\ninstance-attribute\n\u00b6\nrun_id\n:\nUUID\n|\nNone\nUnique identifier for the tracer run for this call.\nIf not provided, a new UUID will be generated.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain_core/runnables/",
      "title": "Runnables | LangChain Reference",
      "heading": "Runnables"
    }
  },
  {
    "page_content": "Vector stores | LangChain Reference\nSkip to content\nLangChain Reference\nVector stores\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangChain\nlangchain\nlangchain\nAgents\nMiddleware\nModels\nMessages\nTools\nEmbeddings\nlangchain-core\nlangchain-core\nCaches\nCallbacks\nDocuments\nDocument loaders\nEmbeddings\nExceptions\nLanguage models\nSerialization\nOutput parsers\nPrompts\nRate limiters\nRetrievers\nRunnables\nUtilities\nVector stores\nVector stores\nTable of contents\nVectorStore\nembeddings\nadd_texts\ndelete\nget_by_ids\naget_by_ids\nadelete\naadd_texts\nadd_documents\naadd_documents\nsearch\nasearch\nsimilarity_search\nsimilarity_search_with_score\nasimilarity_search_with_score\nsimilarity_search_with_relevance_scores\nasimilarity_search_with_relevance_scores\nasimilarity_search\nsimilarity_search_by_vector\nasimilarity_search_by_vector\nmax_marginal_relevance_search\namax_marginal_relevance_search\nmax_marginal_relevance_search_by_vector\namax_marginal_relevance_search_by_vector\nfrom_documents\nafrom_documents\nfrom_texts\nafrom_texts\nas_retriever\nVectorStoreRetriever\nvectorstore\nsearch_type\nsearch_kwargs\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nlc_secrets\nlc_attributes\ntags\nmetadata\nvalidate_search_type\nadd_documents\naadd_documents\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nInMemoryVectorStore\nembeddings\n__init__\ndelete\nadelete\nadd_documents\naadd_documents\nget_by_ids\naget_by_ids\nsimilarity_search_with_score_by_vector\nsimilarity_search_with_score\nasimilarity_search_with_score\nsimilarity_search_by_vector\nasimilarity_search_by_vector\nsimilarity_search\nasimilarity_search\nmax_marginal_relevance_search_by_vector\nmax_marginal_relevance_search\namax_marginal_relevance_search\nfrom_texts\nafrom_texts\nload\ndump\nadd_texts\naadd_texts\nsearch\nasearch\nsimilarity_search_with_relevance_scores\nasimilarity_search_with_relevance_scores\namax_marginal_relevance_search_by_vector\nfrom_documents\nafrom_documents\nas_retriever\nlangchain-text-splitters\nlangchain-text-splitters\nlangchain-mcp-adapters\nlangchain-mcp-adapters\nlangchain-tests\nlangchain-tests\nUnit tests\nIntegration tests\nlangchain-classic\nlangchain-classic\nAgents\nCallbacks\nChains\nChat models\nEmbeddings\nEvaluation\nGlobals\nHub\nMemory\nOutput parsers\nRetrievers\nRunnables\nLangSmith\nStorage\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\nVectorStore\nembeddings\nadd_texts\ndelete\nget_by_ids\naget_by_ids\nadelete\naadd_texts\nadd_documents\naadd_documents\nsearch\nasearch\nsimilarity_search\nsimilarity_search_with_score\nasimilarity_search_with_score\nsimilarity_search_with_relevance_scores\nasimilarity_search_with_relevance_scores\nasimilarity_search\nsimilarity_search_by_vector\nasimilarity_search_by_vector\nmax_marginal_relevance_search\namax_marginal_relevance_search\nmax_marginal_relevance_search_by_vector\namax_marginal_relevance_search_by_vector\nfrom_documents\nafrom_documents\nfrom_texts\nafrom_texts\nas_retriever\nVectorStoreRetriever\nvectorstore\nsearch_type\nsearch_kwargs\nname\nInputType\nOutputType\ninput_schema\noutput_schema\nconfig_specs\nlc_secrets\nlc_attributes\ntags\nmetadata\nvalidate_search_type\nadd_documents\naadd_documents\nget_name\nget_input_schema\nget_input_jsonschema\nget_output_schema\nget_output_jsonschema\nconfig_schema\nget_config_jsonschema\nget_graph\nget_prompts\n__or__\n__ror__\npipe\npick\nassign\ninvoke\nainvoke\nbatch\nbatch_as_completed\nabatch\nabatch_as_completed\nstream\nastream\nastream_log\nastream_events\ntransform\natransform\nbind\nwith_config\nwith_listeners\nwith_alisteners\nwith_types\nwith_retry\nmap\nwith_fallbacks\nas_tool\n__init__\nis_lc_serializable\nget_lc_namespace\nlc_id\nto_json\nto_json_not_implemented\nconfigurable_fields\nconfigurable_alternatives\nInMemoryVectorStore\nembeddings\n__init__\ndelete\nadelete\nadd_documents\naadd_documents\nget_by_ids\naget_by_ids\nsimilarity_search_with_score_by_vector\nsimilarity_search_with_score\nasimilarity_search_with_score\nsimilarity_search_by_vector\nasimilarity_search_by_vector\nsimilarity_search\nasimilarity_search\nmax_marginal_relevance_search_by_vector\nmax_marginal_relevance_search\namax_marginal_relevance_search\nfrom_texts\nafrom_texts\nload\ndump\nadd_texts\naadd_texts\nsearch\nasearch\nsimilarity_search_with_relevance_scores\nasimilarity_search_with_relevance_scores\namax_marginal_relevance_search_by_vector\nfrom_documents\nafrom_documents\nas_retriever\nVector stores\nVectorStore\n\u00b6\nBases:\nABC\nInterface for vector store.\nMETHOD\nDESCRIPTION\nadd_texts\nRun more texts through the embeddings and add to the\nVectorStore\n.\ndelete\nDelete by vector ID or other criteria.\nget_by_ids\nGet documents by their IDs.\naget_by_ids\nAsync get documents by their IDs.\nadelete\nAsync delete by vector ID or other criteria.\naadd_texts\nAsync run more texts through the embeddings and add to the\nVectorStore\n.\nadd_documents\nAdd or update documents in the\nVectorStore\n.\naadd_documents\nAsync run more documents through the embeddings and add to the\nVectorStore\n.\nsearch\nReturn docs most similar to query using a specified search type.\nasearch\nAsync return docs most similar to query using a specified search type.\nsimilarity_search\nReturn docs most similar to query.\nsimilarity_search_with_score\nRun similarity search with distance.\nasimilarity_search_with_score\nAsync run similarity search with distance.\nsimilarity_search_with_relevance_scores\nReturn docs and relevance scores in the range\n[0, 1]\n.\nasimilarity_search_with_relevance_scores\nAsync return docs and relevance scores in the range\n[0, 1]\n.\nasimilarity_search\nAsync return docs most similar to query.\nsimilarity_search_by_vector\nReturn docs most similar to embedding vector.\nasimilarity_search_by_vector\nAsync return docs most similar to embedding vector.\nmax_marginal_relevance_search\nReturn docs selected using the maximal marginal relevance.\namax_marginal_relevance_search\nAsync return docs selected using the maximal marginal relevance.\nmax_marginal_relevance_search_by_vector\nReturn docs selected using the maximal marginal relevance.\namax_marginal_relevance_search_by_vector\nAsync return docs selected using the maximal marginal relevance.\nfrom_documents\nReturn\nVectorStore\ninitialized from documents and embeddings.\nafrom_documents\nAsync return\nVectorStore\ninitialized from documents and embeddings.\nfrom_texts\nReturn\nVectorStore\ninitialized from texts and embeddings.\nafrom_texts\nAsync return\nVectorStore\ninitialized from texts and embeddings.\nas_retriever\nReturn\nVectorStoreRetriever\ninitialized from this\nVectorStore\n.\nembeddings\nproperty\n\u00b6\nembeddings\n:\nEmbeddings\n|\nNone\nAccess the query embedding object if available.\nadd_texts\n\u00b6\nadd_texts\n(\ntexts\n:\nIterable\n[\nstr\n],\nmetadatas\n:\nlist\n[\ndict\n]\n|\nNone\n=\nNone\n,\n*\n,\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nstr\n]\nRun more texts through the embeddings and add to the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ntexts\nIterable of strings to add to the\nVectorStore\n.\nTYPE:\nIterable\n[\nstr\n]\nmetadatas\nOptional list of metadatas associated with the texts.\nTYPE:\nlist\n[\ndict\n] | None\nDEFAULT:\nNone\nids\nOptional list of IDs associated with the texts.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nVectorStore\nspecific parameters.\nOne of the kwargs should be\nids\nwhich is a list of ids\nassociated with the texts.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs from adding the texts into the\nVectorStore\n.\nRAISES\nDESCRIPTION\nValueError\nIf the number of metadatas does not match the number of texts.\nValueError\nIf the number of IDs does not match the number of texts.\ndelete\n\u00b6\ndelete\n(\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nbool\n|\nNone\nDelete by vector ID or other criteria.\nPARAMETER\nDESCRIPTION\nids\nList of IDs to delete. If\nNone\n, delete all.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nOther keyword arguments that subclasses might use.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nbool\n| None\nTrue\nif deletion is successful,\nFalse\notherwise,\nNone\nif not\nimplemented.\nget_by_ids\n\u00b6\nget_by_ids\n(\nids\n:\nSequence\n[\nstr\n])\n->\nlist\n[\nDocument\n]\nGet documents by their IDs.\nThe returned documents are expected to have the ID field set to the ID of the\ndocument in the vector store.\nFewer documents may be returned than requested if some IDs are not found or\nif there are duplicated IDs.\nUsers should not assume that the order of the returned documents matches\nthe order of the input IDs. Instead, users should rely on the ID field of the\nreturned documents.\nThis method should\nNOT\nraise exceptions if no documents are found for\nsome IDs.\nPARAMETER\nDESCRIPTION\nids\nList of IDs to retrieve.\nTYPE:\nSequence\n[\nstr\n]\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects.\naget_by_ids\nasync\n\u00b6\naget_by_ids\n(\nids\n:\nSequence\n[\nstr\n])\n->\nlist\n[\nDocument\n]\nAsync get documents by their IDs.\nThe returned documents are expected to have the ID field set to the ID of the\ndocument in the vector store.\nFewer documents may be returned than requested if some IDs are not found or\nif there are duplicated IDs.\nUsers should not assume that the order of the returned documents matches\nthe order of the input IDs. Instead, users should rely on the ID field of the\nreturned documents.\nThis method should\nNOT\nraise exceptions if no documents are found for\nsome IDs.\nPARAMETER\nDESCRIPTION\nids\nList of IDs to retrieve.\nTYPE:\nSequence\n[\nstr\n]\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects.\nadelete\nasync\n\u00b6\nadelete\n(\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nbool\n|\nNone\nAsync delete by vector ID or other criteria.\nPARAMETER\nDESCRIPTION\nids\nList of IDs to delete. If\nNone\n, delete all.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nOther keyword arguments that subclasses might use.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nbool\n| None\nTrue\nif deletion is successful,\nFalse\notherwise,\nNone\nif not\nimplemented.\naadd_texts\nasync\n\u00b6\naadd_texts\n(\ntexts\n:\nIterable\n[\nstr\n],\nmetadatas\n:\nlist\n[\ndict\n]\n|\nNone\n=\nNone\n,\n*\n,\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nstr\n]\nAsync run more texts through the embeddings and add to the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ntexts\nIterable of strings to add to the\nVectorStore\n.\nTYPE:\nIterable\n[\nstr\n]\nmetadatas\nOptional list of metadatas associated with the texts.\nTYPE:\nlist\n[\ndict\n] | None\nDEFAULT:\nNone\nids\nOptional list\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nVectorStore\nspecific parameters.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs from adding the texts into the\nVectorStore\n.\nRAISES\nDESCRIPTION\nValueError\nIf the number of metadatas does not match the number of texts.\nValueError\nIf the number of IDs does not match the number of texts.\nadd_documents\n\u00b6\nadd_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nstr\n]\nAdd or update documents in the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ndocuments\nDocuments to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\n**kwargs\nAdditional keyword arguments.\nIf kwargs contains IDs and documents contain ids, the IDs in the kwargs\nwill receive precedence.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs of the added texts.\naadd_documents\nasync\n\u00b6\naadd_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nstr\n]\nAsync run more documents through the embeddings and add to the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ndocuments\nDocuments to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs of the added texts.\nsearch\n\u00b6\nsearch\n(\nquery\n:\nstr\n,\nsearch_type\n:\nstr\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nReturn docs most similar to query using a specified search type.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nsearch_type\nType of search to perform.\nCan be\n'similarity'\n,\n'mmr'\n, or\n'similarity_score_threshold'\n.\nTYPE:\nstr\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query.\nRAISES\nDESCRIPTION\nValueError\nIf\nsearch_type\nis not one of\n'similarity'\n,\n'mmr'\n, or\n'similarity_score_threshold'\n.\nasearch\nasync\n\u00b6\nasearch\n(\nquery\n:\nstr\n,\nsearch_type\n:\nstr\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nAsync return docs most similar to query using a specified search type.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nsearch_type\nType of search to perform.\nCan be\n'similarity'\n,\n'mmr'\n, or\n'similarity_score_threshold'\n.\nTYPE:\nstr\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query.\nRAISES\nDESCRIPTION\nValueError\nIf\nsearch_type\nis not one of\n'similarity'\n,\n'mmr'\n, or\n'similarity_score_threshold'\n.\nsimilarity_search\nabstractmethod\n\u00b6\nsimilarity_search\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nReturn docs most similar to query.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query.\nsimilarity_search_with_score\n\u00b6\nsimilarity_search_with_score\n(\n*\nargs\n:\nAny\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nRun similarity search with distance.\nPARAMETER\nDESCRIPTION\n*args\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n()\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nList of tuples of\n(doc, similarity_score)\n.\nasimilarity_search_with_score\nasync\n\u00b6\nasimilarity_search_with_score\n(\n*\nargs\n:\nAny\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nAsync run similarity search with distance.\nPARAMETER\nDESCRIPTION\n*args\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n()\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nList of tuples of\n(doc, similarity_score)\n.\nsimilarity_search_with_relevance_scores\n\u00b6\nsimilarity_search_with_relevance_scores\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nReturn docs and relevance scores in the range\n[0, 1]\n.\n0\nis dissimilar,\n1\nis most similar.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nKwargs to be passed to similarity search.\nShould include\nscore_threshold\n, an optional floating point value\nbetween\n0\nto\n1\nto filter the resulting set of retrieved docs.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nList of tuples of\n(doc, similarity_score)\n.\nasimilarity_search_with_relevance_scores\nasync\n\u00b6\nasimilarity_search_with_relevance_scores\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nAsync return docs and relevance scores in the range\n[0, 1]\n.\n0\nis dissimilar,\n1\nis most similar.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nKwargs to be passed to similarity search.\nShould include\nscore_threshold\n, an optional floating point value\nbetween\n0\nto\n1\nto filter the resulting set of retrieved docs.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nList of tuples of\n(doc, similarity_score)\nasimilarity_search\nasync\n\u00b6\nasimilarity_search\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nAsync return docs most similar to query.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query.\nsimilarity_search_by_vector\n\u00b6\nsimilarity_search_by_vector\n(\nembedding\n:\nlist\n[\nfloat\n],\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nReturn docs most similar to embedding vector.\nPARAMETER\nDESCRIPTION\nembedding\nEmbedding to look up documents similar to.\nTYPE:\nlist\n[\nfloat\n]\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query vector.\nasimilarity_search_by_vector\nasync\n\u00b6\nasimilarity_search_by_vector\n(\nembedding\n:\nlist\n[\nfloat\n],\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nAsync return docs most similar to embedding vector.\nPARAMETER\nDESCRIPTION\nembedding\nEmbedding to look up documents similar to.\nTYPE:\nlist\n[\nfloat\n]\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query vector.\nmax_marginal_relevance_search\n\u00b6\nmax_marginal_relevance_search\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\nfetch_k\n:\nint\n=\n20\n,\nlambda_mult\n:\nfloat\n=\n0.5\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nReturn docs selected using the maximal marginal relevance.\nMaximal marginal relevance optimizes for similarity to query AND diversity\namong selected documents.\nPARAMETER\nDESCRIPTION\nquery\nText to look up documents similar to.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\nfetch_k\nNumber of\nDocument\nobjects to fetch to pass to MMR algorithm.\nTYPE:\nint\nDEFAULT:\n20\nlambda_mult\nNumber between\n0\nand\n1\nthat determines the degree\nof diversity among the results with\n0\ncorresponding\nto maximum diversity and\n1\nto minimum diversity.\nTYPE:\nfloat\nDEFAULT:\n0.5\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects selected by maximal marginal relevance.\namax_marginal_relevance_search\nasync\n\u00b6\namax_marginal_relevance_search\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\nfetch_k\n:\nint\n=\n20\n,\nlambda_mult\n:\nfloat\n=\n0.5\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nAsync return docs selected using the maximal marginal relevance.\nMaximal marginal relevance optimizes for similarity to query AND diversity\namong selected documents.\nPARAMETER\nDESCRIPTION\nquery\nText to look up documents similar to.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\nfetch_k\nNumber of\nDocument\nobjects to fetch to pass to MMR algorithm.\nTYPE:\nint\nDEFAULT:\n20\nlambda_mult\nNumber between\n0\nand\n1\nthat determines the degree\nof diversity among the results with\n0\ncorresponding\nto maximum diversity and\n1\nto minimum diversity.\nTYPE:\nfloat\nDEFAULT:\n0.5\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects selected by maximal marginal relevance.\nmax_marginal_relevance_search_by_vector\n\u00b6\nmax_marginal_relevance_search_by_vector\n(\nembedding\n:\nlist\n[\nfloat\n],\nk\n:\nint\n=\n4\n,\nfetch_k\n:\nint\n=\n20\n,\nlambda_mult\n:\nfloat\n=\n0.5\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nDocument\n]\nReturn docs selected using the maximal marginal relevance.\nMaximal marginal relevance optimizes for similarity to query AND diversity\namong selected documents.\nPARAMETER\nDESCRIPTION\nembedding\nEmbedding to look up documents similar to.\nTYPE:\nlist\n[\nfloat\n]\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\nfetch_k\nNumber of\nDocument\nobjects to fetch to pass to MMR algorithm.\nTYPE:\nint\nDEFAULT:\n20\nlambda_mult\nNumber between\n0\nand\n1\nthat determines the degree\nof diversity among the results with\n0\ncorresponding\nto maximum diversity and\n1\nto minimum diversity.\nTYPE:\nfloat\nDEFAULT:\n0.5\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects selected by maximal marginal relevance.\namax_marginal_relevance_search_by_vector\nasync\n\u00b6\namax_marginal_relevance_search_by_vector\n(\nembedding\n:\nlist\n[\nfloat\n],\nk\n:\nint\n=\n4\n,\nfetch_k\n:\nint\n=\n20\n,\nlambda_mult\n:\nfloat\n=\n0.5\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nDocument\n]\nAsync return docs selected using the maximal marginal relevance.\nMaximal marginal relevance optimizes for similarity to query AND diversity\namong selected documents.\nPARAMETER\nDESCRIPTION\nembedding\nEmbedding to look up documents similar to.\nTYPE:\nlist\n[\nfloat\n]\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\nfetch_k\nNumber of\nDocument\nobjects to fetch to pass to MMR algorithm.\nTYPE:\nint\nDEFAULT:\n20\nlambda_mult\nNumber between\n0\nand\n1\nthat determines the degree\nof diversity among the results with\n0\ncorresponding\nto maximum diversity and\n1\nto minimum diversity.\nTYPE:\nfloat\nDEFAULT:\n0.5\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects selected by maximal marginal relevance.\nfrom_documents\nclassmethod\n\u00b6\nfrom_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\nembedding\n:\nEmbeddings\n,\n**\nkwargs\n:\nAny\n)\n->\nSelf\nReturn\nVectorStore\ninitialized from documents and embeddings.\nPARAMETER\nDESCRIPTION\ndocuments\nList of\nDocument\nobjects to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\nembedding\nEmbedding function to use.\nTYPE:\nEmbeddings\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nSelf\nVectorStore\ninitialized from documents and embeddings.\nafrom_documents\nasync\nclassmethod\n\u00b6\nafrom_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\nembedding\n:\nEmbeddings\n,\n**\nkwargs\n:\nAny\n)\n->\nSelf\nAsync return\nVectorStore\ninitialized from documents and embeddings.\nPARAMETER\nDESCRIPTION\ndocuments\nList of\nDocument\nobjects to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\nembedding\nEmbedding function to use.\nTYPE:\nEmbeddings\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nSelf\nVectorStore\ninitialized from documents and embeddings.\nfrom_texts\nabstractmethod\nclassmethod\n\u00b6\nfrom_texts\n(\ntexts\n:\nlist\n[\nstr\n],\nembedding\n:\nEmbeddings\n,\nmetadatas\n:\nlist\n[\ndict\n]\n|\nNone\n=\nNone\n,\n*\n,\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nVST\nReturn\nVectorStore\ninitialized from texts and embeddings.\nPARAMETER\nDESCRIPTION\ntexts\nTexts to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nstr\n]\nembedding\nEmbedding function to use.\nTYPE:\nEmbeddings\nmetadatas\nOptional list of metadatas associated with the texts.\nTYPE:\nlist\n[\ndict\n] | None\nDEFAULT:\nNone\nids\nOptional list of IDs associated with the texts.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nVST\nVectorStore\ninitialized from texts and embeddings.\nafrom_texts\nasync\nclassmethod\n\u00b6\nafrom_texts\n(\ntexts\n:\nlist\n[\nstr\n],\nembedding\n:\nEmbeddings\n,\nmetadatas\n:\nlist\n[\ndict\n]\n|\nNone\n=\nNone\n,\n*\n,\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nSelf\nAsync return\nVectorStore\ninitialized from texts and embeddings.\nPARAMETER\nDESCRIPTION\ntexts\nTexts to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nstr\n]\nembedding\nEmbedding function to use.\nTYPE:\nEmbeddings\nmetadatas\nOptional list of metadatas associated with the texts.\nTYPE:\nlist\n[\ndict\n] | None\nDEFAULT:\nNone\nids\nOptional list of IDs associated with the texts.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nSelf\nVectorStore\ninitialized from texts and embeddings.\nas_retriever\n\u00b6\nas_retriever\n(\n**\nkwargs\n:\nAny\n)\n->\nVectorStoreRetriever\nReturn\nVectorStoreRetriever\ninitialized from this\nVectorStore\n.\nPARAMETER\nDESCRIPTION\n**kwargs\nKeyword arguments to pass to the search function.\nCan include:\nsearch_type\n: Defines the type of search that the Retriever should\nperform. Can be\n'similarity'\n(default),\n'mmr'\n, or\n'similarity_score_threshold'\n.\nsearch_kwargs\n: Keyword arguments to pass to the search function.\nCan include things like:\nk\n: Amount of documents to return (Default:\n4\n)\nscore_threshold\n: Minimum relevance threshold\nfor\nsimilarity_score_threshold\nfetch_k\n: Amount of documents to pass to MMR algorithm\n(Default:\n20\n)\nlambda_mult\n: Diversity of results returned by MMR;\n1\nfor minimum diversity and 0 for maximum. (Default:\n0.5\n)\nfilter\n: Filter by document metadata\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nVectorStoreRetriever\nRetriever class for\nVectorStore\n.\nExamples:\n# Retrieve more documents with higher diversity\n# Useful if your dataset has many similar documents\ndocsearch\n.\nas_retriever\n(\nsearch_type\n=\n\"mmr\"\n,\nsearch_kwargs\n=\n{\n\"k\"\n:\n6\n,\n\"lambda_mult\"\n:\n0.25\n}\n)\n# Fetch more documents for the MMR algorithm to consider\n# But only return the top 5\ndocsearch\n.\nas_retriever\n(\nsearch_type\n=\n\"mmr\"\n,\nsearch_kwargs\n=\n{\n\"k\"\n:\n5\n,\n\"fetch_k\"\n:\n50\n})\n# Only retrieve documents that have a relevance score\n# Above a certain threshold\ndocsearch\n.\nas_retriever\n(\nsearch_type\n=\n\"similarity_score_threshold\"\n,\nsearch_kwargs\n=\n{\n\"score_threshold\"\n:\n0.8\n},\n)\n# Only get the single most similar document from the dataset\ndocsearch\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n})\n# Use a filter to only retrieve documents from a specific paper\ndocsearch\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"filter\"\n:\n{\n\"paper_title\"\n:\n\"GPT-4 Technical Report\"\n}}\n)\nVectorStoreRetriever\n\u00b6\nBases:\nBaseRetriever\nBase Retriever class for VectorStore.\nMETHOD\nDESCRIPTION\nvalidate_search_type\nValidate search type.\nadd_documents\nAdd documents to the\nVectorStore\n.\naadd_documents\nAsync add documents to the\nVectorStore\n.\nget_name\nGet the name of the\nRunnable\n.\nget_input_schema\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nget_input_jsonschema\nGet a JSON schema that represents the input to the\nRunnable\n.\nget_output_schema\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nget_output_jsonschema\nGet a JSON schema that represents the output of the\nRunnable\n.\nconfig_schema\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nget_config_jsonschema\nGet a JSON schema that represents the config of the\nRunnable\n.\nget_graph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\nReturn a list of prompts used by this\nRunnable\n.\n__or__\nRunnable \"or\" operator.\n__ror__\nRunnable \"reverse-or\" operator.\npipe\nPipe\nRunnable\nobjects.\npick\nPick keys from the output\ndict\nof this\nRunnable\n.\nassign\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\ninvoke\nInvoke the retriever to get relevant documents.\nainvoke\nAsynchronously invoke the retriever to get relevant documents.\nbatch\nDefault implementation runs invoke in parallel using a thread pool executor.\nbatch_as_completed\nRun\ninvoke\nin parallel on a list of inputs.\nabatch\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nabatch_as_completed\nRun\nainvoke\nin parallel on a list of inputs.\nstream\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nastream\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nastream_log\nStream all output from a\nRunnable\n, as reported to the callback system.\nastream_events\nGenerate a stream of events.\ntransform\nTransform inputs to outputs.\natransform\nTransform inputs to outputs.\nbind\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nwith_config\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nwith_listeners\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nwith_alisteners\nBind async lifecycle listeners to a\nRunnable\n.\nwith_types\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nwith_retry\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nmap\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nwith_fallbacks\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nas_tool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\n__init__\nis_lc_serializable\nIs this class serializable?\nget_lc_namespace\nGet the namespace of the LangChain object.\nlc_id\nReturn a unique identifier for this class for serialization purposes.\nto_json\nSerialize the\nRunnable\nto JSON.\nto_json_not_implemented\nSerialize a \"not implemented\" object.\nconfigurable_fields\nConfigure particular\nRunnable\nfields at runtime.\nconfigurable_alternatives\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nvectorstore\ninstance-attribute\n\u00b6\nvectorstore\n:\nVectorStore\nVectorStore to use for retrieval.\nsearch_type\nclass-attribute\ninstance-attribute\n\u00b6\nsearch_type\n:\nstr\n=\n'similarity'\nType of search to perform.\nsearch_kwargs\nclass-attribute\ninstance-attribute\n\u00b6\nsearch_kwargs\n:\ndict\n=\nField\n(\ndefault_factory\n=\ndict\n)\nKeyword arguments to pass to the search function.\nname\nclass-attribute\ninstance-attribute\n\u00b6\nname\n:\nstr\n|\nNone\n=\nNone\nThe name of the\nRunnable\n. Used for debugging and tracing.\nInputType\nproperty\n\u00b6\nInputType\n:\ntype\n[\nInput\n]\nInput type.\nThe type of input this\nRunnable\naccepts specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the input type cannot be inferred.\nOutputType\nproperty\n\u00b6\nOutputType\n:\ntype\n[\nOutput\n]\nOutput Type.\nThe type of output this\nRunnable\nproduces specified as a type annotation.\nRAISES\nDESCRIPTION\nTypeError\nIf the output type cannot be inferred.\ninput_schema\nproperty\n\u00b6\ninput_schema\n:\ntype\n[\nBaseModel\n]\nThe type of input this\nRunnable\naccepts specified as a Pydantic model.\noutput_schema\nproperty\n\u00b6\noutput_schema\n:\ntype\n[\nBaseModel\n]\nOutput schema.\nThe type of output this\nRunnable\nproduces specified as a Pydantic model.\nconfig_specs\nproperty\n\u00b6\nconfig_specs\n:\nlist\n[\nConfigurableFieldSpec\n]\nList configurable fields for this\nRunnable\n.\nlc_secrets\nproperty\n\u00b6\nlc_secrets\n:\ndict\n[\nstr\n,\nstr\n]\nA map of constructor argument names to secret ids.\nFor example,\n{\"openai_api_key\": \"OPENAI_API_KEY\"}\nlc_attributes\nproperty\n\u00b6\nlc_attributes\n:\ndict\nList of attribute names that should be included in the serialized kwargs.\nThese attributes must be accepted by the constructor.\nDefault is an empty dictionary.\ntags\nclass-attribute\ninstance-attribute\n\u00b6\ntags\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\nOptional list of tags associated with the retriever.\nThese tags will be associated with each call to this retriever,\nand passed as arguments to the handlers defined in\ncallbacks\n.\nYou can use these to eg identify a specific instance of a retriever with its\nuse case.\nmetadata\nclass-attribute\ninstance-attribute\n\u00b6\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\nOptional metadata associated with the retriever.\nThis metadata will be associated with each call to this retriever,\nand passed as arguments to the handlers defined in\ncallbacks\n.\nYou can use these to eg identify a specific instance of a retriever with its\nuse case.\nvalidate_search_type\nclassmethod\n\u00b6\nvalidate_search_type\n(\nvalues\n:\ndict\n)\n->\nAny\nValidate search type.\nPARAMETER\nDESCRIPTION\nvalues\nValues to validate.\nTYPE:\ndict\nRETURNS\nDESCRIPTION\nAny\nValidated values.\nRAISES\nDESCRIPTION\nValueError\nIf\nsearch_type\nis not one of the allowed search types.\nValueError\nIf\nscore_threshold\nis not specified with a float value(\n0~1\n)\nadd_documents\n\u00b6\nadd_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nstr\n]\nAdd documents to the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ndocuments\nDocuments to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\n**kwargs\nOther keyword arguments that subclasses might use.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs of the added texts.\naadd_documents\nasync\n\u00b6\naadd_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nstr\n]\nAsync add documents to the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ndocuments\nDocuments to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\n**kwargs\nOther keyword arguments that subclasses might use.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs of the added texts.\nget_name\n\u00b6\nget_name\n(\nsuffix\n:\nstr\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nstr\nGet the name of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nsuffix\nAn optional suffix to append to the name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nname\nAn optional name to use instead of the\nRunnable\n's name.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nstr\nThe name of the\nRunnable\n.\nget_input_schema\n\u00b6\nget_input_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate input to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic input schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an input schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate input.\nget_input_jsonschema\n\u00b6\nget_input_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the input to the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the input to the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_input_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nget_output_schema\n\u00b6\nget_output_schema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nGet a Pydantic model that can be used to validate output to the\nRunnable\n.\nRunnable\nobjects that leverage the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods will have a dynamic output schema that\ndepends on which configuration the\nRunnable\nis invoked with.\nThis method allows to get an output schema for a specific configuration.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate output.\nget_output_jsonschema\n\u00b6\nget_output_jsonschema\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the output of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nA config to use when generating the schema.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the output of the\nRunnable\n.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\nadd_one\n)\nprint\n(\nrunnable\n.\nget_output_jsonschema\n())\nAdded in\nlangchain-core\n0.3.0\nconfig_schema\n\u00b6\nconfig_schema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ntype\n[\nBaseModel\n]\nThe type of config this\nRunnable\naccepts specified as a Pydantic model.\nTo mark a field as configurable, see the\nconfigurable_fields\nand\nconfigurable_alternatives\nmethods.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ntype\n[\nBaseModel\n]\nA Pydantic model that can be used to validate config.\nget_config_jsonschema\n\u00b6\nget_config_jsonschema\n(\n*\n,\ninclude\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n)\n->\ndict\n[\nstr\n,\nAny\n]\nGet a JSON schema that represents the config of the\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninclude\nA list of fields to include in the config schema.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\ndict\n[\nstr\n,\nAny\n]\nA JSON schema that represents the config of the\nRunnable\n.\nAdded in\nlangchain-core\n0.3.0\nget_graph\n\u00b6\nget_graph\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nGraph\nReturn a graph representation of this\nRunnable\n.\nget_prompts\n\u00b6\nget_prompts\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n)\n->\nlist\n[\nBasePromptTemplate\n]\nReturn a list of prompts used by this\nRunnable\n.\n__or__\n\u00b6\n__or__\n(\nother\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nRunnable \"or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nIterator\n[\nAny\n]],\nIterator\n[\nOther\n]] |\nCallable\n[[\nAsyncIterator\n[\nAny\n]],\nAsyncIterator\n[\nOther\n]] |\nCallable\n[[\nAny\n],\nOther\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\n__ror__\n\u00b6\n__ror__\n(\nother\n:\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]]\n|\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n]\n|\nCallable\n[[\nOther\n],\nAny\n]\n|\nAny\n],\n)\n->\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nRunnable \"reverse-or\" operator.\nCompose this\nRunnable\nwith another object to create a\nRunnableSequence\n.\nPARAMETER\nDESCRIPTION\nother\nAnother\nRunnable\nor a\nRunnable\n-like object.\nTYPE:\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nIterator\n[\nOther\n]],\nIterator\n[\nAny\n]] |\nCallable\n[[\nAsyncIterator\n[\nOther\n]],\nAsyncIterator\n[\nAny\n]] |\nCallable\n[[\nOther\n],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\nOther\n,\nAny\n] |\nCallable\n[[\nOther\n],\nAny\n] |\nAny\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nOther\n,\nOutput\n]\nA new\nRunnable\n.\npipe\n\u00b6\npipe\n(\n*\nothers\n:\nRunnable\n[\nAny\n,\nOther\n]\n|\nCallable\n[[\nAny\n],\nOther\n],\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nRunnableSerializable\n[\nInput\n,\nOther\n]\nPipe\nRunnable\nobjects.\nCompose this\nRunnable\nwith\nRunnable\n-like objects to make a\nRunnableSequence\n.\nEquivalent to\nRunnableSequence(self, *others)\nor\nself | others[0] | ...\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nadd_one\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\ndef\nmul_two\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n*\n2\nrunnable_1\n=\nRunnableLambda\n(\nadd_one\n)\nrunnable_2\n=\nRunnableLambda\n(\nmul_two\n)\nsequence\n=\nrunnable_1\n.\npipe\n(\nrunnable_2\n)\n# Or equivalently:\n# sequence = runnable_1 | runnable_2\n# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\nsequence\n.\ninvoke\n(\n1\n)\nawait\nsequence\n.\nainvoke\n(\n1\n)\n# -> 4\nsequence\n.\nbatch\n([\n1\n,\n2\n,\n3\n])\nawait\nsequence\n.\nabatch\n([\n1\n,\n2\n,\n3\n])\n# -> [4, 6, 8]\nPARAMETER\nDESCRIPTION\n*others\nOther\nRunnable\nor\nRunnable\n-like objects to compose\nTYPE:\nRunnable\n[\nAny\n,\nOther\n] |\nCallable\n[[\nAny\n],\nOther\n]\nDEFAULT:\n()\nname\nAn optional name for the resulting\nRunnableSequence\n.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOther\n]\nA new\nRunnable\n.\npick\n\u00b6\npick\n(\nkeys\n:\nstr\n|\nlist\n[\nstr\n])\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nPick keys from the output\ndict\nof this\nRunnable\n.\nPick a single key\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\njson_only_chain\n=\nchain\n.\npick\n(\n\"json\"\n)\njson_only_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> [1, 2, 3]\nPick a list of keys\nfrom\ntyping\nimport\nAny\nimport\njson\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableMap\nas_str\n=\nRunnableLambda\n(\nstr\n)\nas_json\n=\nRunnableLambda\n(\njson\n.\nloads\n)\ndef\nas_bytes\n(\nx\n:\nAny\n)\n->\nbytes\n:\nreturn\nbytes\n(\nx\n,\n\"utf-8\"\n)\nchain\n=\nRunnableMap\n(\nstr\n=\nas_str\n,\njson\n=\nas_json\n,\nbytes\n=\nRunnableLambda\n(\nas_bytes\n)\n)\nchain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\njson_and_bytes_chain\n=\nchain\n.\npick\n([\n\"json\"\n,\n\"bytes\"\n])\njson_and_bytes_chain\n.\ninvoke\n(\n\"[1, 2, 3]\"\n)\n# -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\nPARAMETER\nDESCRIPTION\nkeys\nA key or list of keys to pick from the output dict.\nTYPE:\nstr\n|\nlist\n[\nstr\n]\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\na new\nRunnable\n.\nassign\n\u00b6\nassign\n(\n**\nkwargs\n:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]\n|\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n]\n|\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]],\n)\n->\nRunnableSerializable\n[\nAny\n,\nAny\n]\nAssigns new fields to the\ndict\noutput of this\nRunnable\n.\nfrom\nlangchain_core.language_models.fake\nimport\nFakeStreamingListLLM\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nfrom\nlangchain_core.prompts\nimport\nSystemMessagePromptTemplate\nfrom\nlangchain_core.runnables\nimport\nRunnable\nfrom\noperator\nimport\nitemgetter\nprompt\n=\n(\nSystemMessagePromptTemplate\n.\nfrom_template\n(\n\"You are a nice assistant.\"\n)\n+\n\"\n{question}\n\"\n)\nmodel\n=\nFakeStreamingListLLM\n(\nresponses\n=\n[\n\"foo-lish\"\n])\nchain\n:\nRunnable\n=\nprompt\n|\nmodel\n|\n{\n\"str\"\n:\nStrOutputParser\n()}\nchain_with_assign\n=\nchain\n.\nassign\n(\nhello\n=\nitemgetter\n(\n\"str\"\n)\n|\nmodel\n)\nprint\n(\nchain_with_assign\n.\ninput_schema\n.\nmodel_json_schema\n())\n# {'title': 'PromptInput', 'type': 'object', 'properties':\n{\n'question'\n:\n{\n'title'\n:\n'Question'\n,\n'type'\n:\n'string'\n}}}\nprint\n(\nchain_with_assign\n.\noutput_schema\n.\nmodel_json_schema\n())\n# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n{\n'str'\n:\n{\n'title'\n:\n'Str'\n,\n'type'\n:\n'string'\n},\n'hello'\n:\n{\n'title'\n:\n'Hello'\n,\n'type'\n:\n'string'\n}}}\nPARAMETER\nDESCRIPTION\n**kwargs\nA mapping of keys to\nRunnable\nor\nRunnable\n-like objects\nthat will be invoked with the entire output dict of this\nRunnable\n.\nTYPE:\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n] |\nMapping\n[\nstr\n,\nRunnable\n[\ndict\n[\nstr\n,\nAny\n],\nAny\n] |\nCallable\n[[\ndict\n[\nstr\n,\nAny\n]],\nAny\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nAny\n,\nAny\n]\nA new\nRunnable\n.\ninvoke\n\u00b6\ninvoke\n(\ninput\n:\nstr\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nInvoke the retriever to get relevant documents.\nMain entry point for synchronous retriever invocations.\nPARAMETER\nDESCRIPTION\ninput\nThe query string.\nTYPE:\nstr\nconfig\nConfiguration for the retriever.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional arguments to pass to the retriever.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of relevant documents.\nExamples:\nretriever\n.\ninvoke\n(\n\"query\"\n)\nainvoke\nasync\n\u00b6\nainvoke\n(\ninput\n:\nstr\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nAsynchronously invoke the retriever to get relevant documents.\nMain entry point for asynchronous retriever invocations.\nPARAMETER\nDESCRIPTION\ninput\nThe query string.\nTYPE:\nstr\nconfig\nConfiguration for the retriever.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional arguments to pass to the retriever.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of relevant documents.\nExamples:\nawait\nretriever\n.\nainvoke\n(\n\"query\"\n)\nbatch\n\u00b6\nbatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n. The config supports\nstandard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work\nto do in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nbatch_as_completed\n\u00b6\nbatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\ninvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\ntuple\n[\nint\n,\nOutput\n|\nException\n]\nTuples of the index of the input and the output from the\nRunnable\n.\nabatch\nasync\n\u00b6\nabatch\n(\ninputs\n:\nlist\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nlist\n[\nOutput\n]\nDefault implementation runs\nainvoke\nin parallel using\nasyncio.gather\n.\nThe default implementation of\nbatch\nworks well for IO bound runnables.\nSubclasses must override this method if they can batch more efficiently;\ne.g., if the underlying\nRunnable\nuses an API which supports a batch mode.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nlist\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nlist\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nOutput\n]\nA list of outputs from the\nRunnable\n.\nabatch_as_completed\nasync\n\u00b6\nabatch_as_completed\n(\ninputs\n:\nSequence\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n]\n|\nNone\n=\nNone\n,\n*\n,\nreturn_exceptions\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nRun\nainvoke\nin parallel on a list of inputs.\nYields results as they complete.\nPARAMETER\nDESCRIPTION\ninputs\nA list of inputs to the\nRunnable\n.\nTYPE:\nSequence\n[\nInput\n]\nconfig\nA config to use when invoking the\nRunnable\n.\nThe config supports standard keys like\n'tags'\n,\n'metadata'\nfor\ntracing purposes,\n'max_concurrency'\nfor controlling how much work to\ndo in parallel, and other keys.\nPlease refer to\nRunnableConfig\nfor more details.\nTYPE:\nRunnableConfig\n|\nSequence\n[\nRunnableConfig\n] | None\nDEFAULT:\nNone\nreturn_exceptions\nWhether to return exceptions instead of raising them.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\ntuple\n[\nint\n,\nOutput\n|\nException\n]]\nA tuple of the index of the input and the output from the\nRunnable\n.\nstream\n\u00b6\nstream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nDefault implementation of\nstream\n, which calls\ninvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\nastream\nasync\n\u00b6\nastream\n(\ninput\n:\nInput\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nAsyncIterator\n[\nOutput\n]\nDefault implementation of\nastream\n, which calls\nainvoke\n.\nSubclasses must override this method if they support streaming output.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nInput\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nastream_log\nasync\n\u00b6\nastream_log\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\ndiff\n:\nbool\n=\nTrue\n,\nwith_streamed_output_list\n:\nbool\n=\nTrue\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nRunLogPatch\n]\n|\nAsyncIterator\n[\nRunLog\n]\nStream all output from a\nRunnable\n, as reported to the callback system.\nThis includes all inner runs of LLMs, Retrievers, Tools, etc.\nOutput is streamed as Log objects, which include a list of\nJsonpatch ops that describe how the state of the run has changed in each\nstep, and the final state of the run.\nThe Jsonpatch ops can be applied in order to construct state.\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\ndiff\nWhether to yield diffs between each step or the current state.\nTYPE:\nbool\nDEFAULT:\nTrue\nwith_streamed_output_list\nWhether to yield the\nstreamed_output\nlist.\nTYPE:\nbool\nDEFAULT:\nTrue\ninclude_names\nOnly include logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude logs with these names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude logs with these types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude logs with these tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nRunLogPatch\n] |\nAsyncIterator\n[\nRunLog\n]\nA\nRunLogPatch\nor\nRunLog\nobject.\nastream_events\nasync\n\u00b6\nastream_events\n(\ninput\n:\nAny\n,\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n*\n,\nversion\n:\nLiteral\n[\n\"v1\"\n,\n\"v2\"\n]\n=\n\"v2\"\n,\ninclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\ninclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_names\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_types\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\nexclude_tags\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nAsyncIterator\n[\nStreamEvent\n]\nGenerate a stream of events.\nUse to create an iterator over\nStreamEvent\nthat provide real-time information\nabout the progress of the\nRunnable\n, including\nStreamEvent\nfrom intermediate\nresults.\nA\nStreamEvent\nis a dictionary with the following schema:\nevent\n: Event names are of the format:\non_[runnable_type]_(start|stream|end)\n.\nname\n: The name of the\nRunnable\nthat generated the event.\nrun_id\n: Randomly generated ID associated with the given execution of the\nRunnable\nthat emitted the event. A child\nRunnable\nthat gets invoked as\npart of the execution of a parent\nRunnable\nis assigned its own unique ID.\nparent_ids\n: The IDs of the parent runnables that generated the event. The\nroot\nRunnable\nwill have an empty list. The order of the parent IDs is from\nthe root to the immediate parent. Only available for v2 version of the API.\nThe v1 version of the API will return an empty list.\ntags\n: The tags of the\nRunnable\nthat generated the event.\nmetadata\n: The metadata of the\nRunnable\nthat generated the event.\ndata\n: The data associated with the event. The contents of this field\ndepend on the type of event. See the table below for more details.\nBelow is a table that illustrates some events that might be emitted by various\nchains. Metadata fields have been omitted from the table for brevity.\nChain definitions have been included after the table.\nNote\nThis reference table is for the v2 version of the schema.\nevent\nname\nchunk\ninput\noutput\non_chat_model_start\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\non_chat_model_stream\n'[model name]'\nAIMessageChunk(content=\"hello\")\non_chat_model_end\n'[model name]'\n{\"messages\": [[SystemMessage, HumanMessage]]}\nAIMessageChunk(content=\"hello world\")\non_llm_start\n'[model name]'\n{'input': 'hello'}\non_llm_stream\n'[model name]'\n'Hello'\non_llm_end\n'[model name]'\n'Hello human!'\non_chain_start\n'format_docs'\non_chain_stream\n'format_docs'\n'hello world!, goodbye world!'\non_chain_end\n'format_docs'\n[Document(...)]\n'hello world!, goodbye world!'\non_tool_start\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_tool_end\n'some_tool'\n{\"x\": 1, \"y\": \"2\"}\non_retriever_start\n'[retriever name]'\n{\"query\": \"hello\"}\non_retriever_end\n'[retriever name]'\n{\"query\": \"hello\"}\n[Document(...), ..]\non_prompt_start\n'[template_name]'\n{\"question\": \"hello\"}\non_prompt_end\n'[template_name]'\n{\"question\": \"hello\"}\nChatPromptValue(messages: [SystemMessage, ...])\nIn addition to the standard events, users can also dispatch custom events (see example below).\nCustom events will be only be surfaced with in the v2 version of the API!\nA custom event has following format:\nAttribute\nType\nDescription\nname\nstr\nA user defined name for the event.\ndata\nAny\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\nHere are declarations associated with the standard events shown above:\nformat_docs\n:\ndef\nformat_docs\n(\ndocs\n:\nlist\n[\nDocument\n])\n->\nstr\n:\n'''Format the docs.'''\nreturn\n\", \"\n.\njoin\n([\ndoc\n.\npage_content\nfor\ndoc\nin\ndocs\n])\nformat_docs\n=\nRunnableLambda\n(\nformat_docs\n)\nsome_tool\n:\n@tool\ndef\nsome_tool\n(\nx\n:\nint\n,\ny\n:\nstr\n)\n->\ndict\n:\n'''Some_tool.'''\nreturn\n{\n\"x\"\n:\nx\n,\n\"y\"\n:\ny\n}\nprompt\n:\ntemplate\n=\nChatPromptTemplate\n.\nfrom_messages\n(\n[\n(\n\"system\"\n,\n\"You are Cat Agent 007\"\n),\n(\n\"human\"\n,\n\"\n{question}\n\"\n),\n]\n)\n.\nwith_config\n({\n\"run_name\"\n:\n\"my_template\"\n,\n\"tags\"\n:\n[\n\"my_template\"\n]})\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nasync\ndef\nreverse\n(\ns\n:\nstr\n)\n->\nstr\n:\nreturn\ns\n[::\n-\n1\n]\nchain\n=\nRunnableLambda\n(\nfunc\n=\nreverse\n)\nevents\n=\n[\nevent\nasync\nfor\nevent\nin\nchain\n.\nastream_events\n(\n\"hello\"\n,\nversion\n=\n\"v2\"\n)\n]\n# Will produce the following events\n# (run_id, and parent_ids has been omitted for brevity):\n[\n{\n\"data\"\n:\n{\n\"input\"\n:\n\"hello\"\n},\n\"event\"\n:\n\"on_chain_start\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"chunk\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_stream\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n{\n\"data\"\n:\n{\n\"output\"\n:\n\"olleh\"\n},\n\"event\"\n:\n\"on_chain_end\"\n,\n\"metadata\"\n:\n{},\n\"name\"\n:\n\"reverse\"\n,\n\"tags\"\n:\n[],\n},\n]\nDispatch custom event\nfrom\nlangchain_core.callbacks.manager\nimport\n(\nadispatch_custom_event\n,\n)\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnableConfig\nimport\nasyncio\nasync\ndef\nslow_thing\n(\nsome_input\n:\nstr\n,\nconfig\n:\nRunnableConfig\n)\n->\nstr\n:\n\"\"\"Do something that takes a long time.\"\"\"\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 1 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nawait\nadispatch_custom_event\n(\n\"progress_event\"\n,\n{\n\"message\"\n:\n\"Finished step 2 of 3\"\n},\nconfig\n=\nconfig\n# Must be included for python < 3.10\n)\nawait\nasyncio\n.\nsleep\n(\n1\n)\n# Placeholder for some slow operation\nreturn\n\"Done\"\nslow_thing\n=\nRunnableLambda\n(\nslow_thing\n)\nasync\nfor\nevent\nin\nslow_thing\n.\nastream_events\n(\n\"some_input\"\n,\nversion\n=\n\"v2\"\n):\nprint\n(\nevent\n)\nPARAMETER\nDESCRIPTION\ninput\nThe input to the\nRunnable\n.\nTYPE:\nAny\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\nversion\nThe version of the schema to use, either\n'v2'\nor\n'v1'\n.\nUsers should use\n'v2'\n.\n'v1'\nis for backwards compatibility and will be deprecated\nin\n0.4.0\n.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in\n'v2'\n.\nTYPE:\nLiteral\n['v1', 'v2']\nDEFAULT:\n'v2'\ninclude_names\nOnly include events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_types\nOnly include events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\ninclude_tags\nOnly include events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_names\nExclude events from\nRunnable\nobjects with matching names.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_types\nExclude events from\nRunnable\nobjects with matching types.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\nexclude_tags\nExclude events from\nRunnable\nobjects with matching tags.\nTYPE:\nSequence\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nThese will be passed to\nastream_log\nas this implementation\nof\nastream_events\nis built on top of\nastream_log\n.\nTYPE:\nAny\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nStreamEvent\n]\nAn async stream of\nStreamEvent\n.\nRAISES\nDESCRIPTION\nNotImplementedError\nIf the version is not\n'v1'\nor\n'v2'\n.\ntransform\n\u00b6\ntransform\n(\ninput\n:\nIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n)\n->\nIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of transform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn iterator of inputs to the\nRunnable\n.\nTYPE:\nIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nOutput\nThe output of the\nRunnable\n.\natransform\nasync\n\u00b6\natransform\n(\ninput\n:\nAsyncIterator\n[\nInput\n],\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nOutput\n]\nTransform inputs to outputs.\nDefault implementation of atransform, which buffers input and calls\nastream\n.\nSubclasses must override this method if they can start producing output while\ninput is still being generated.\nPARAMETER\nDESCRIPTION\ninput\nAn async iterator of inputs to the\nRunnable\n.\nTYPE:\nAsyncIterator\n[\nInput\n]\nconfig\nThe config to use for the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\n| None\nDEFAULT:\n{}\nYIELDS\nDESCRIPTION\nAsyncIterator\n[\nOutput\n]\nThe output of the\nRunnable\n.\nbind\n\u00b6\nbind\n(\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind arguments to a\nRunnable\n, returning a new\nRunnable\n.\nUseful when a\nRunnable\nin a chain requires an argument that is not\nin the output of the previous\nRunnable\nor included in the user input.\nPARAMETER\nDESCRIPTION\n**kwargs\nThe arguments to bind to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the arguments bound.\nExample\nfrom\nlangchain_ollama\nimport\nChatOllama\nfrom\nlangchain_core.output_parsers\nimport\nStrOutputParser\nmodel\n=\nChatOllama\n(\nmodel\n=\n\"llama3.1\"\n)\n# Without bind\nchain\n=\nmodel\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two three four five.'\n# With bind\nchain\n=\nmodel\n.\nbind\n(\nstop\n=\n[\n\"three\"\n])\n|\nStrOutputParser\n()\nchain\n.\ninvoke\n(\n\"Repeat quoted words exactly: 'One two three four five.'\"\n)\n# Output is 'One two'\nwith_config\n\u00b6\nwith_config\n(\nconfig\n:\nRunnableConfig\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind config to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\nconfig\nThe config to bind to the\nRunnable\n.\nTYPE:\nRunnableConfig\n| None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments to pass to the\nRunnable\n.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the config bound.\nwith_listeners\n\u00b6\nwith_listeners\n(\n*\n,\non_start\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_end\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\non_error\n:\nCallable\n[[\nRun\n],\nNone\n]\n|\nCallable\n[[\nRun\n,\nRunnableConfig\n],\nNone\n]\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind lifecycle listeners to a\nRunnable\n, returning a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled before the\nRunnable\nstarts running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_end\nCalled after the\nRunnable\nfinishes running, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\non_error\nCalled if the\nRunnable\nthrows an error, with the\nRun\nobject.\nTYPE:\nCallable\n[[\nRun\n], None] |\nCallable\n[[\nRun\n,\nRunnableConfig\n], None] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nfrom\nlangchain_core.tracers.schemas\nimport\nRun\nimport\ntime\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\ntime\n.\nsleep\n(\ntime_to_sleep\n)\ndef\nfn_start\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"start_time:\"\n,\nrun_obj\n.\nstart_time\n)\ndef\nfn_end\n(\nrun_obj\n:\nRun\n):\nprint\n(\n\"end_time:\"\n,\nrun_obj\n.\nend_time\n)\nchain\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_listeners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nchain\n.\ninvoke\n(\n2\n)\nwith_alisteners\n\u00b6\nwith_alisteners\n(\n*\n,\non_start\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_end\n:\nAsyncListener\n|\nNone\n=\nNone\n,\non_error\n:\nAsyncListener\n|\nNone\n=\nNone\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind async lifecycle listeners to a\nRunnable\n.\nReturns a new\nRunnable\n.\nThe Run object contains information about the run, including its\nid\n,\ntype\n,\ninput\n,\noutput\n,\nerror\n,\nstart_time\n,\nend_time\n, and\nany tags or metadata added to the run.\nPARAMETER\nDESCRIPTION\non_start\nCalled asynchronously before the\nRunnable\nstarts running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_end\nCalled asynchronously after the\nRunnable\nfinishes running,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\non_error\nCalled asynchronously if the\nRunnable\nthrows an error,\nwith the\nRun\nobject.\nTYPE:\nAsyncListener\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the listeners bound.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\n,\nRunnable\nfrom\ndatetime\nimport\ndatetime\n,\ntimezone\nimport\ntime\nimport\nasyncio\ndef\nformat_t\n(\ntimestamp\n:\nfloat\n)\n->\nstr\n:\nreturn\ndatetime\n.\nfromtimestamp\n(\ntimestamp\n,\ntz\n=\ntimezone\n.\nutc\n)\n.\nisoformat\n()\nasync\ndef\ntest_runnable\n(\ntime_to_sleep\n:\nint\n):\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\ntime_to_sleep\n)\nprint\n(\nf\n\"Runnable[\n{\ntime_to_sleep\n}\ns]: ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_start\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on start callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n3\n)\nprint\n(\nf\n\"on start callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nasync\ndef\nfn_end\n(\nrun_obj\n:\nRunnable\n):\nprint\n(\nf\n\"on end callback starts at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nawait\nasyncio\n.\nsleep\n(\n2\n)\nprint\n(\nf\n\"on end callback ends at\n{\nformat_t\n(\ntime\n.\ntime\n())\n}\n\"\n)\nrunnable\n=\nRunnableLambda\n(\ntest_runnable\n)\n.\nwith_alisteners\n(\non_start\n=\nfn_start\n,\non_end\n=\nfn_end\n)\nasync\ndef\nconcurrent_runs\n():\nawait\nasyncio\n.\ngather\n(\nrunnable\n.\nainvoke\n(\n2\n),\nrunnable\n.\nainvoke\n(\n3\n))\nasyncio\n.\nrun\n(\nconcurrent_runs\n())\n# Result:\n# on start callback starts at 2025-03-01T07:05:22.875378+00:00\n# on start callback starts at 2025-03-01T07:05:22.875495+00:00\n# on start callback ends at 2025-03-01T07:05:25.878862+00:00\n# on start callback ends at 2025-03-01T07:05:25.878947+00:00\n# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n# on end callback starts at 2025-03-01T07:05:27.882360+00:00\n# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n# on end callback starts at 2025-03-01T07:05:28.882428+00:00\n# on end callback ends at 2025-03-01T07:05:29.883893+00:00\n# on end callback ends at 2025-03-01T07:05:30.884831+00:00\nwith_types\n\u00b6\nwith_types\n(\n*\n,\ninput_type\n:\ntype\n[\nInput\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\ntype\n[\nOutput\n]\n|\nNone\n=\nNone\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nBind input and output types to a\nRunnable\n, returning a new\nRunnable\n.\nPARAMETER\nDESCRIPTION\ninput_type\nThe input type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nInput\n] | None\nDEFAULT:\nNone\noutput_type\nThe output type to bind to the\nRunnable\n.\nTYPE:\ntype\n[\nOutput\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the types bound.\nwith_retry\n\u00b6\nwith_retry\n(\n*\n,\nretry_if_exception_type\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nwait_exponential_jitter\n:\nbool\n=\nTrue\n,\nexponential_jitter_params\n:\nExponentialJitterParams\n|\nNone\n=\nNone\n,\nstop_after_attempt\n:\nint\n=\n3\n,\n)\n->\nRunnable\n[\nInput\n,\nOutput\n]\nCreate a new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nPARAMETER\nDESCRIPTION\nretry_if_exception_type\nA tuple of exception types to retry on.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nwait_exponential_jitter\nWhether to add jitter to the wait\ntime between retries.\nTYPE:\nbool\nDEFAULT:\nTrue\nstop_after_attempt\nThe maximum number of attempts to make before\ngiving up.\nTYPE:\nint\nDEFAULT:\n3\nexponential_jitter_params\nParameters for\ntenacity.wait_exponential_jitter\n. Namely:\ninitial\n,\nmax\n,\nexp_base\n, and\njitter\n(all\nfloat\nvalues).\nTYPE:\nExponentialJitterParams\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat retries the original\nRunnable\non exceptions.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ncount\n=\n0\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nNone\n:\nglobal\ncount\ncount\n=\ncount\n+\n1\nif\nx\n==\n1\n:\nraise\nValueError\n(\n\"x is 1\"\n)\nelse\n:\npass\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\ntry\n:\nrunnable\n.\nwith_retry\n(\nstop_after_attempt\n=\n2\n,\nretry_if_exception_type\n=\n(\nValueError\n,),\n)\n.\ninvoke\n(\n1\n)\nexcept\nValueError\n:\npass\nassert\ncount\n==\n2\nmap\n\u00b6\nmap\n()\n->\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nReturn a new\nRunnable\nthat maps a list of inputs to a list of outputs.\nCalls\ninvoke\nwith each input.\nRETURNS\nDESCRIPTION\nRunnable\n[\nlist\n[\nInput\n],\nlist\n[\nOutput\n]]\nA new\nRunnable\nthat maps a list of inputs to a list of outputs.\nExample\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\n_lambda\n(\nx\n:\nint\n)\n->\nint\n:\nreturn\nx\n+\n1\nrunnable\n=\nRunnableLambda\n(\n_lambda\n)\nprint\n(\nrunnable\n.\nmap\n()\n.\ninvoke\n([\n1\n,\n2\n,\n3\n]))\n# [2, 3, 4]\nwith_fallbacks\n\u00b6\nwith_fallbacks\n(\nfallbacks\n:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]],\n*\n,\nexceptions_to_handle\n:\ntuple\n[\ntype\n[\nBaseException\n],\n...\n]\n=\n(\nException\n,),\nexception_key\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nAdd fallbacks to a\nRunnable\n, returning a new\nRunnable\n.\nThe new\nRunnable\nwill try the original\nRunnable\n, and then each fallback\nin order, upon failures.\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nExample\nfrom\ntyping\nimport\nIterator\nfrom\nlangchain_core.runnables\nimport\nRunnableGenerator\ndef\n_generate_immediate_error\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nraise\nValueError\n()\nyield\n\"\"\ndef\n_generate\n(\ninput\n:\nIterator\n)\n->\nIterator\n[\nstr\n]:\nyield from\n\"foo bar\"\nrunnable\n=\nRunnableGenerator\n(\n_generate_immediate_error\n)\n.\nwith_fallbacks\n(\n[\nRunnableGenerator\n(\n_generate\n)]\n)\nprint\n(\n\"\"\n.\njoin\n(\nrunnable\n.\nstream\n({})))\n# foo bar\nPARAMETER\nDESCRIPTION\nfallbacks\nA sequence of runnables to try if the original\nRunnable\nfails.\nTYPE:\nSequence\n[\nRunnable\n[\nInput\n,\nOutput\n]]\nexceptions_to_handle\nA tuple of exception types to handle.\nTYPE:\ntuple\n[\ntype\n[\nBaseException\n], ...]\nDEFAULT:\n(\nException\n,)\nexception_key\nIf\nstring\nis specified then handled exceptions will be\npassed to fallbacks as part of the input under the specified key.\nIf\nNone\n, exceptions will not be passed to fallbacks.\nIf used, the base\nRunnable\nand its fallbacks must accept a\ndictionary as input.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nRunnableWithFallbacks\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nthat will try the original\nRunnable\n, and then each\nFallback in order, upon failures.\nas_tool\n\u00b6\nas_tool\n(\nargs_schema\n:\ntype\n[\nBaseModel\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ndescription\n:\nstr\n|\nNone\n=\nNone\n,\narg_types\n:\ndict\n[\nstr\n,\ntype\n]\n|\nNone\n=\nNone\n,\n)\n->\nBaseTool\nCreate a\nBaseTool\nfrom a\nRunnable\n.\nas_tool\nwill instantiate a\nBaseTool\nwith a name, description, and\nargs_schema\nfrom a\nRunnable\n. Where possible, schemas are inferred\nfrom\nrunnable.get_input_schema\n.\nAlternatively (e.g., if the\nRunnable\ntakes a dict as input and the specific\ndict\nkeys are not typed), the schema can be specified directly with\nargs_schema\n.\nYou can also pass\narg_types\nto just specify the required arguments and their\ntypes.\nPARAMETER\nDESCRIPTION\nargs_schema\nThe schema for the tool.\nTYPE:\ntype\n[\nBaseModel\n] | None\nDEFAULT:\nNone\nname\nThe name of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ndescription\nThe description of the tool.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\narg_types\nA dictionary of argument names to types.\nTYPE:\ndict\n[\nstr\n,\ntype\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nBaseTool\nA\nBaseTool\ninstance.\nTypedDict\ninput\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\nclass\nArgs\n(\nTypedDict\n):\na\n:\nint\nb\n:\nlist\n[\nint\n]\ndef\nf\n(\nx\n:\nArgs\n)\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\nargs_schema\nfrom\ntyping\nimport\nAny\nfrom\npydantic\nimport\nBaseModel\n,\nField\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nclass\nFSchema\n(\nBaseModel\n):\n\"\"\"Apply a function to an integer and list of integers.\"\"\"\na\n:\nint\n=\nField\n(\n...\n,\ndescription\n=\n\"Integer\"\n)\nb\n:\nlist\n[\nint\n]\n=\nField\n(\n...\n,\ndescription\n=\n\"List of ints\"\n)\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\nFSchema\n)\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\ndict\ninput, specifying schema via\narg_types\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\ndict\n[\nstr\n,\nAny\n])\n->\nstr\n:\nreturn\nstr\n(\nx\n[\n\"a\"\n]\n*\nmax\n(\nx\n[\n\"b\"\n]))\nrunnable\n=\nRunnableLambda\n(\nf\n)\nas_tool\n=\nrunnable\n.\nas_tool\n(\narg_types\n=\n{\n\"a\"\n:\nint\n,\n\"b\"\n:\nlist\n[\nint\n]})\nas_tool\n.\ninvoke\n({\n\"a\"\n:\n3\n,\n\"b\"\n:\n[\n1\n,\n2\n]})\nstr\ninput\nfrom\nlangchain_core.runnables\nimport\nRunnableLambda\ndef\nf\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"a\"\ndef\ng\n(\nx\n:\nstr\n)\n->\nstr\n:\nreturn\nx\n+\n\"z\"\nrunnable\n=\nRunnableLambda\n(\nf\n)\n|\ng\nas_tool\n=\nrunnable\n.\nas_tool\n()\nas_tool\n.\ninvoke\n(\n\"b\"\n)\n__init__\n\u00b6\n__init__\n(\n*\nargs\n:\nAny\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nis_lc_serializable\nclassmethod\n\u00b6\nis_lc_serializable\n()\n->\nbool\nIs this class serializable?\nBy design, even if a class inherits from\nSerializable\n, it is not serializable\nby default. This is to prevent accidental serialization of objects that should\nnot be serialized.\nRETURNS\nDESCRIPTION\nbool\nWhether the class is serializable. Default is\nFalse\n.\nget_lc_namespace\nclassmethod\n\u00b6\nget_lc_namespace\n()\n->\nlist\n[\nstr\n]\nGet the namespace of the LangChain object.\nFor example, if the class is\nlangchain.llms.openai.OpenAI\n,\nthen the namespace is\n[\"langchain\", \"llms\", \"openai\"]\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nThe namespace.\nlc_id\nclassmethod\n\u00b6\nlc_id\n()\n->\nlist\n[\nstr\n]\nReturn a unique identifier for this class for serialization purposes.\nThe unique identifier is a list of strings that describes the path\nto the object.\nFor example, for the class\nlangchain.llms.openai.OpenAI\n, the id is\n[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]\n.\nto_json\n\u00b6\nto_json\n()\n->\nSerializedConstructor\n|\nSerializedNotImplemented\nSerialize the\nRunnable\nto JSON.\nRETURNS\nDESCRIPTION\nSerializedConstructor\n|\nSerializedNotImplemented\nA JSON-serializable representation of the\nRunnable\n.\nto_json_not_implemented\n\u00b6\nto_json_not_implemented\n()\n->\nSerializedNotImplemented\nSerialize a \"not implemented\" object.\nRETURNS\nDESCRIPTION\nSerializedNotImplemented\nSerializedNotImplemented\n.\nconfigurable_fields\n\u00b6\nconfigurable_fields\n(\n**\nkwargs\n:\nAnyConfigurableField\n,\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure particular\nRunnable\nfields at runtime.\nPARAMETER\nDESCRIPTION\n**kwargs\nA dictionary of\nConfigurableField\ninstances to configure.\nTYPE:\nAnyConfigurableField\nDEFAULT:\n{}\nRAISES\nDESCRIPTION\nValueError\nIf a configuration key is not found in the\nRunnable\n.\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the fields configured.\nExample\nfrom\nlangchain_core.runnables\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI\n(\nmax_tokens\n=\n20\n)\n.\nconfigurable_fields\n(\nmax_tokens\n=\nConfigurableField\n(\nid\n=\n\"output_token_number\"\n,\nname\n=\n\"Max tokens in the output\"\n,\ndescription\n=\n\"The maximum number of tokens in the output\"\n,\n)\n)\n# max_tokens = 20\nprint\n(\n\"max_tokens_20: \"\n,\nmodel\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n)\n# max_tokens = 200\nprint\n(\n\"max_tokens_200: \"\n,\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"output_token_number\"\n:\n200\n})\n.\ninvoke\n(\n\"tell me something about chess\"\n)\n.\ncontent\n,\n)\nconfigurable_alternatives\n\u00b6\nconfigurable_alternatives\n(\nwhich\n:\nConfigurableField\n,\n*\n,\ndefault_key\n:\nstr\n=\n\"default\"\n,\nprefix_keys\n:\nbool\n=\nFalse\n,\n**\nkwargs\n:\nRunnable\n[\nInput\n,\nOutput\n]\n|\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]],\n)\n->\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nConfigure alternatives for\nRunnable\nobjects that can be set at runtime.\nPARAMETER\nDESCRIPTION\nwhich\nThe\nConfigurableField\ninstance that will be used to select the\nalternative.\nTYPE:\nConfigurableField\ndefault_key\nThe default key to use if no alternative is selected.\nTYPE:\nstr\nDEFAULT:\n'default'\nprefix_keys\nWhether to prefix the keys with the\nConfigurableField\nid.\nTYPE:\nbool\nDEFAULT:\nFalse\n**kwargs\nA dictionary of keys to\nRunnable\ninstances or callables that\nreturn\nRunnable\ninstances.\nTYPE:\nRunnable\n[\nInput\n,\nOutput\n] |\nCallable\n[[],\nRunnable\n[\nInput\n,\nOutput\n]]\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nRunnableSerializable\n[\nInput\n,\nOutput\n]\nA new\nRunnable\nwith the alternatives configured.\nExample\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlangchain_core.runnables.utils\nimport\nConfigurableField\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatAnthropic\n(\nmodel_name\n=\n\"claude-sonnet-4-5-20250929\"\n)\n.\nconfigurable_alternatives\n(\nConfigurableField\n(\nid\n=\n\"llm\"\n),\ndefault_key\n=\n\"anthropic\"\n,\nopenai\n=\nChatOpenAI\n(),\n)\n# uses the default model ChatAnthropic\nprint\n(\nmodel\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\n# uses ChatOpenAI\nprint\n(\nmodel\n.\nwith_config\n(\nconfigurable\n=\n{\n\"llm\"\n:\n\"openai\"\n})\n.\ninvoke\n(\n\"which organization created you?\"\n)\n.\ncontent\n)\nInMemoryVectorStore\n\u00b6\nBases:\nVectorStore\nIn-memory vector store implementation.\nUses a dictionary, and computes cosine similarity for search using numpy.\nSetup\nInstall\nlangchain-core\n.\npip\ninstall\n-U\nlangchain-core\nKey init args \u2014 indexing params:\nembedding_function: Embeddings\nEmbedding function to use.\nInstantiate\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nvector_store\n=\nInMemoryVectorStore\n(\nOpenAIEmbeddings\n())\nAdd Documents\nfrom\nlangchain_core.documents\nimport\nDocument\ndocument_1\n=\nDocument\n(\nid\n=\n\"1\"\n,\npage_content\n=\n\"foo\"\n,\nmetadata\n=\n{\n\"baz\"\n:\n\"bar\"\n})\ndocument_2\n=\nDocument\n(\nid\n=\n\"2\"\n,\npage_content\n=\n\"thud\"\n,\nmetadata\n=\n{\n\"bar\"\n:\n\"baz\"\n})\ndocument_3\n=\nDocument\n(\nid\n=\n\"3\"\n,\npage_content\n=\n\"i will be deleted :(\"\n)\ndocuments\n=\n[\ndocument_1\n,\ndocument_2\n,\ndocument_3\n]\nvector_store\n.\nadd_documents\n(\ndocuments\n=\ndocuments\n)\nInspect documents\ntop_n\n=\n10\nfor\nindex\n,\n(\nid\n,\ndoc\n)\nin\nenumerate\n(\nvector_store\n.\nstore\n.\nitems\n()):\nif\nindex\n<\ntop_n\n:\n# docs have keys 'id', 'vector', 'text', 'metadata'\nprint\n(\nf\n\"\n{\nid\n}\n:\n{\ndoc\n[\n'text'\n]\n}\n\"\n)\nelse\n:\nbreak\nDelete Documents\nvector_store\n.\ndelete\n(\nids\n=\n[\n\"3\"\n])\nSearch\nresults\n=\nvector_store\n.\nsimilarity_search\n(\nquery\n=\n\"thud\"\n,\nk\n=\n1\n)\nfor\ndoc\nin\nresults\n:\nprint\n(\nf\n\"*\n{\ndoc\n.\npage_content\n}\n[\n{\ndoc\n.\nmetadata\n}\n]\"\n)\n* thud [{'bar': 'baz'}]\nSearch with filter\ndef\n_filter_function\n(\ndoc\n:\nDocument\n)\n->\nbool\n:\nreturn\ndoc\n.\nmetadata\n.\nget\n(\n\"bar\"\n)\n==\n\"baz\"\nresults\n=\nvector_store\n.\nsimilarity_search\n(\nquery\n=\n\"thud\"\n,\nk\n=\n1\n,\nfilter\n=\n_filter_function\n)\nfor\ndoc\nin\nresults\n:\nprint\n(\nf\n\"*\n{\ndoc\n.\npage_content\n}\n[\n{\ndoc\n.\nmetadata\n}\n]\"\n)\n* thud [{'bar': 'baz'}]\nSearch with score\nresults\n=\nvector_store\n.\nsimilarity_search_with_score\n(\nquery\n=\n\"qux\"\n,\nk\n=\n1\n)\nfor\ndoc\n,\nscore\nin\nresults\n:\nprint\n(\nf\n\"* [SIM=\n{\nscore\n:\n3f\n}\n]\n{\ndoc\n.\npage_content\n}\n[\n{\ndoc\n.\nmetadata\n}\n]\"\n)\n* [SIM=0.832268] foo [{'baz': 'bar'}]\nAsync\n# add documents\n# await vector_store.aadd_documents(documents=documents)\n# delete documents\n# await vector_store.adelete(ids=[\"3\"])\n# search\n# results = vector_store.asimilarity_search(query=\"thud\", k=1)\n# search with score\nresults\n=\nawait\nvector_store\n.\nasimilarity_search_with_score\n(\nquery\n=\n\"qux\"\n,\nk\n=\n1\n)\nfor\ndoc\n,\nscore\nin\nresults\n:\nprint\n(\nf\n\"* [SIM=\n{\nscore\n:\n3f\n}\n]\n{\ndoc\n.\npage_content\n}\n[\n{\ndoc\n.\nmetadata\n}\n]\"\n)\n* [SIM=0.832268] foo [{'baz': 'bar'}]\nUse as Retriever\nretriever\n=\nvector_store\n.\nas_retriever\n(\nsearch_type\n=\n\"mmr\"\n,\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n,\n\"fetch_k\"\n:\n2\n,\n\"lambda_mult\"\n:\n0.5\n},\n)\nretriever\n.\ninvoke\n(\n\"thud\"\n)\n[Document(id='2', metadata={'bar': 'baz'}, page_content='thud')]\nMETHOD\nDESCRIPTION\n__init__\nInitialize with the given embedding function.\ndelete\nDelete by vector ID or other criteria.\nadelete\nAsync delete by vector ID or other criteria.\nadd_documents\nAdd or update documents in the\nVectorStore\n.\naadd_documents\nAsync run more documents through the embeddings and add to the\nVectorStore\n.\nget_by_ids\nGet documents by their ids.\naget_by_ids\nAsync get documents by their ids.\nsimilarity_search_with_score_by_vector\nSearch for the most similar documents to the given embedding.\nsimilarity_search_with_score\nRun similarity search with distance.\nasimilarity_search_with_score\nAsync run similarity search with distance.\nsimilarity_search_by_vector\nReturn docs most similar to embedding vector.\nasimilarity_search_by_vector\nAsync return docs most similar to embedding vector.\nsimilarity_search\nReturn docs most similar to query.\nasimilarity_search\nAsync return docs most similar to query.\nmax_marginal_relevance_search_by_vector\nReturn docs selected using the maximal marginal relevance.\nmax_marginal_relevance_search\nReturn docs selected using the maximal marginal relevance.\namax_marginal_relevance_search\nAsync return docs selected using the maximal marginal relevance.\nfrom_texts\nReturn\nVectorStore\ninitialized from texts and embeddings.\nafrom_texts\nAsync return\nVectorStore\ninitialized from texts and embeddings.\nload\nLoad a vector store from a file.\ndump\nDump the vector store to a file.\nadd_texts\nRun more texts through the embeddings and add to the\nVectorStore\n.\naadd_texts\nAsync run more texts through the embeddings and add to the\nVectorStore\n.\nsearch\nReturn docs most similar to query using a specified search type.\nasearch\nAsync return docs most similar to query using a specified search type.\nsimilarity_search_with_relevance_scores\nReturn docs and relevance scores in the range\n[0, 1]\n.\nasimilarity_search_with_relevance_scores\nAsync return docs and relevance scores in the range\n[0, 1]\n.\namax_marginal_relevance_search_by_vector\nAsync return docs selected using the maximal marginal relevance.\nfrom_documents\nReturn\nVectorStore\ninitialized from documents and embeddings.\nafrom_documents\nAsync return\nVectorStore\ninitialized from documents and embeddings.\nas_retriever\nReturn\nVectorStoreRetriever\ninitialized from this\nVectorStore\n.\nembeddings\nproperty\n\u00b6\nembeddings\n:\nEmbeddings\nAccess the query embedding object if available.\n__init__\n\u00b6\n__init__\n(\nembedding\n:\nEmbeddings\n)\n->\nNone\nInitialize with the given embedding function.\nPARAMETER\nDESCRIPTION\nembedding\nembedding function to use.\nTYPE:\nEmbeddings\ndelete\n\u00b6\ndelete\n(\nids\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nDelete by vector ID or other criteria.\nPARAMETER\nDESCRIPTION\nids\nList of IDs to delete. If\nNone\n, delete all.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nOther keyword arguments that subclasses might use.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nbool\n| None\nTrue\nif deletion is successful,\nFalse\notherwise,\nNone\nif not\nimplemented.\nadelete\nasync\n\u00b6\nadelete\n(\nids\n:\nSequence\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nNone\nAsync delete by vector ID or other criteria.\nPARAMETER\nDESCRIPTION\nids\nList of IDs to delete. If\nNone\n, delete all.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nOther keyword arguments that subclasses might use.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nbool\n| None\nTrue\nif deletion is successful,\nFalse\notherwise,\nNone\nif not\nimplemented.\nadd_documents\n\u00b6\nadd_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nstr\n]\nAdd or update documents in the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ndocuments\nDocuments to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\n**kwargs\nAdditional keyword arguments.\nIf kwargs contains IDs and documents contain ids, the IDs in the kwargs\nwill receive precedence.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs of the added texts.\naadd_documents\nasync\n\u00b6\naadd_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nstr\n]\nAsync run more documents through the embeddings and add to the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ndocuments\nDocuments to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs of the added texts.\nget_by_ids\n\u00b6\nget_by_ids\n(\nids\n:\nSequence\n[\nstr\n])\n->\nlist\n[\nDocument\n]\nGet documents by their ids.\nPARAMETER\nDESCRIPTION\nids\nThe IDs of the documents to get.\nTYPE:\nSequence\n[\nstr\n]\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nA list of\nDocument\nobjects.\naget_by_ids\nasync\n\u00b6\naget_by_ids\n(\nids\n:\nSequence\n[\nstr\n])\n->\nlist\n[\nDocument\n]\nAsync get documents by their ids.\nPARAMETER\nDESCRIPTION\nids\nThe IDs of the documents to get.\nTYPE:\nSequence\n[\nstr\n]\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nA list of\nDocument\nobjects.\nsimilarity_search_with_score_by_vector\n\u00b6\nsimilarity_search_with_score_by_vector\n(\nembedding\n:\nlist\n[\nfloat\n],\nk\n:\nint\n=\n4\n,\nfilter\n:\nCallable\n[[\nDocument\n],\nbool\n]\n|\nNone\n=\nNone\n,\n**\n_kwargs\n:\nAny\n,\n)\n->\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nSearch for the most similar documents to the given embedding.\nPARAMETER\nDESCRIPTION\nembedding\nThe embedding to search for.\nTYPE:\nlist\n[\nfloat\n]\nk\nThe number of documents to return.\nTYPE:\nint\nDEFAULT:\n4\nfilter\nA function to filter the documents.\nTYPE:\nCallable\n[[\nDocument\n],\nbool\n] | None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nA list of tuples of Document objects and their similarity scores.\nsimilarity_search_with_score\n\u00b6\nsimilarity_search_with_score\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nRun similarity search with distance.\nPARAMETER\nDESCRIPTION\n*args\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n()\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nList of tuples of\n(doc, similarity_score)\n.\nasimilarity_search_with_score\nasync\n\u00b6\nasimilarity_search_with_score\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nAsync run similarity search with distance.\nPARAMETER\nDESCRIPTION\n*args\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n()\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nList of tuples of\n(doc, similarity_score)\n.\nsimilarity_search_by_vector\n\u00b6\nsimilarity_search_by_vector\n(\nembedding\n:\nlist\n[\nfloat\n],\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nReturn docs most similar to embedding vector.\nPARAMETER\nDESCRIPTION\nembedding\nEmbedding to look up documents similar to.\nTYPE:\nlist\n[\nfloat\n]\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query vector.\nasimilarity_search_by_vector\nasync\n\u00b6\nasimilarity_search_by_vector\n(\nembedding\n:\nlist\n[\nfloat\n],\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nAsync return docs most similar to embedding vector.\nPARAMETER\nDESCRIPTION\nembedding\nEmbedding to look up documents similar to.\nTYPE:\nlist\n[\nfloat\n]\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query vector.\nsimilarity_search\n\u00b6\nsimilarity_search\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nReturn docs most similar to query.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query.\nasimilarity_search\nasync\n\u00b6\nasimilarity_search\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nAsync return docs most similar to query.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query.\nmax_marginal_relevance_search_by_vector\n\u00b6\nmax_marginal_relevance_search_by_vector\n(\nembedding\n:\nlist\n[\nfloat\n],\nk\n:\nint\n=\n4\n,\nfetch_k\n:\nint\n=\n20\n,\nlambda_mult\n:\nfloat\n=\n0.5\n,\n*\n,\nfilter\n:\nCallable\n[[\nDocument\n],\nbool\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nDocument\n]\nReturn docs selected using the maximal marginal relevance.\nMaximal marginal relevance optimizes for similarity to query AND diversity\namong selected documents.\nPARAMETER\nDESCRIPTION\nembedding\nEmbedding to look up documents similar to.\nTYPE:\nlist\n[\nfloat\n]\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\nfetch_k\nNumber of\nDocument\nobjects to fetch to pass to MMR algorithm.\nTYPE:\nint\nDEFAULT:\n20\nlambda_mult\nNumber between\n0\nand\n1\nthat determines the degree\nof diversity among the results with\n0\ncorresponding\nto maximum diversity and\n1\nto minimum diversity.\nTYPE:\nfloat\nDEFAULT:\n0.5\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects selected by maximal marginal relevance.\nmax_marginal_relevance_search\n\u00b6\nmax_marginal_relevance_search\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\nfetch_k\n:\nint\n=\n20\n,\nlambda_mult\n:\nfloat\n=\n0.5\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nReturn docs selected using the maximal marginal relevance.\nMaximal marginal relevance optimizes for similarity to query AND diversity\namong selected documents.\nPARAMETER\nDESCRIPTION\nquery\nText to look up documents similar to.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\nfetch_k\nNumber of\nDocument\nobjects to fetch to pass to MMR algorithm.\nTYPE:\nint\nDEFAULT:\n20\nlambda_mult\nNumber between\n0\nand\n1\nthat determines the degree\nof diversity among the results with\n0\ncorresponding\nto maximum diversity and\n1\nto minimum diversity.\nTYPE:\nfloat\nDEFAULT:\n0.5\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects selected by maximal marginal relevance.\namax_marginal_relevance_search\nasync\n\u00b6\namax_marginal_relevance_search\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\nfetch_k\n:\nint\n=\n20\n,\nlambda_mult\n:\nfloat\n=\n0.5\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nAsync return docs selected using the maximal marginal relevance.\nMaximal marginal relevance optimizes for similarity to query AND diversity\namong selected documents.\nPARAMETER\nDESCRIPTION\nquery\nText to look up documents similar to.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\nfetch_k\nNumber of\nDocument\nobjects to fetch to pass to MMR algorithm.\nTYPE:\nint\nDEFAULT:\n20\nlambda_mult\nNumber between\n0\nand\n1\nthat determines the degree\nof diversity among the results with\n0\ncorresponding\nto maximum diversity and\n1\nto minimum diversity.\nTYPE:\nfloat\nDEFAULT:\n0.5\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects selected by maximal marginal relevance.\nfrom_texts\nclassmethod\n\u00b6\nfrom_texts\n(\ntexts\n:\nlist\n[\nstr\n],\nembedding\n:\nEmbeddings\n,\nmetadatas\n:\nlist\n[\ndict\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nInMemoryVectorStore\nReturn\nVectorStore\ninitialized from texts and embeddings.\nPARAMETER\nDESCRIPTION\ntexts\nTexts to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nstr\n]\nembedding\nEmbedding function to use.\nTYPE:\nEmbeddings\nmetadatas\nOptional list of metadatas associated with the texts.\nTYPE:\nlist\n[\ndict\n] | None\nDEFAULT:\nNone\nids\nOptional list of IDs associated with the texts.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nVST\nVectorStore\ninitialized from texts and embeddings.\nafrom_texts\nasync\nclassmethod\n\u00b6\nafrom_texts\n(\ntexts\n:\nlist\n[\nstr\n],\nembedding\n:\nEmbeddings\n,\nmetadatas\n:\nlist\n[\ndict\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nInMemoryVectorStore\nAsync return\nVectorStore\ninitialized from texts and embeddings.\nPARAMETER\nDESCRIPTION\ntexts\nTexts to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nstr\n]\nembedding\nEmbedding function to use.\nTYPE:\nEmbeddings\nmetadatas\nOptional list of metadatas associated with the texts.\nTYPE:\nlist\n[\ndict\n] | None\nDEFAULT:\nNone\nids\nOptional list of IDs associated with the texts.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nSelf\nVectorStore\ninitialized from texts and embeddings.\nload\nclassmethod\n\u00b6\nload\n(\npath\n:\nstr\n,\nembedding\n:\nEmbeddings\n,\n**\nkwargs\n:\nAny\n)\n->\nInMemoryVectorStore\nLoad a vector store from a file.\nPARAMETER\nDESCRIPTION\npath\nThe path to load the vector store from.\nTYPE:\nstr\nembedding\nThe embedding to use.\nTYPE:\nEmbeddings\n**kwargs\nAdditional arguments to pass to the constructor.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nInMemoryVectorStore\nA VectorStore object.\ndump\n\u00b6\ndump\n(\npath\n:\nstr\n)\n->\nNone\nDump the vector store to a file.\nPARAMETER\nDESCRIPTION\npath\nThe path to dump the vector store to.\nTYPE:\nstr\nadd_texts\n\u00b6\nadd_texts\n(\ntexts\n:\nIterable\n[\nstr\n],\nmetadatas\n:\nlist\n[\ndict\n]\n|\nNone\n=\nNone\n,\n*\n,\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nstr\n]\nRun more texts through the embeddings and add to the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ntexts\nIterable of strings to add to the\nVectorStore\n.\nTYPE:\nIterable\n[\nstr\n]\nmetadatas\nOptional list of metadatas associated with the texts.\nTYPE:\nlist\n[\ndict\n] | None\nDEFAULT:\nNone\nids\nOptional list of IDs associated with the texts.\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nVectorStore\nspecific parameters.\nOne of the kwargs should be\nids\nwhich is a list of ids\nassociated with the texts.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs from adding the texts into the\nVectorStore\n.\nRAISES\nDESCRIPTION\nValueError\nIf the number of metadatas does not match the number of texts.\nValueError\nIf the number of IDs does not match the number of texts.\naadd_texts\nasync\n\u00b6\naadd_texts\n(\ntexts\n:\nIterable\n[\nstr\n],\nmetadatas\n:\nlist\n[\ndict\n]\n|\nNone\n=\nNone\n,\n*\n,\nids\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nstr\n]\nAsync run more texts through the embeddings and add to the\nVectorStore\n.\nPARAMETER\nDESCRIPTION\ntexts\nIterable of strings to add to the\nVectorStore\n.\nTYPE:\nIterable\n[\nstr\n]\nmetadatas\nOptional list of metadatas associated with the texts.\nTYPE:\nlist\n[\ndict\n] | None\nDEFAULT:\nNone\nids\nOptional list\nTYPE:\nlist\n[\nstr\n] | None\nDEFAULT:\nNone\n**kwargs\nVectorStore\nspecific parameters.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nstr\n]\nList of IDs from adding the texts into the\nVectorStore\n.\nRAISES\nDESCRIPTION\nValueError\nIf the number of metadatas does not match the number of texts.\nValueError\nIf the number of IDs does not match the number of texts.\nsearch\n\u00b6\nsearch\n(\nquery\n:\nstr\n,\nsearch_type\n:\nstr\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nReturn docs most similar to query using a specified search type.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nsearch_type\nType of search to perform.\nCan be\n'similarity'\n,\n'mmr'\n, or\n'similarity_score_threshold'\n.\nTYPE:\nstr\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query.\nRAISES\nDESCRIPTION\nValueError\nIf\nsearch_type\nis not one of\n'similarity'\n,\n'mmr'\n, or\n'similarity_score_threshold'\n.\nasearch\nasync\n\u00b6\nasearch\n(\nquery\n:\nstr\n,\nsearch_type\n:\nstr\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\nDocument\n]\nAsync return docs most similar to query using a specified search type.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nsearch_type\nType of search to perform.\nCan be\n'similarity'\n,\n'mmr'\n, or\n'similarity_score_threshold'\n.\nTYPE:\nstr\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects most similar to the query.\nRAISES\nDESCRIPTION\nValueError\nIf\nsearch_type\nis not one of\n'similarity'\n,\n'mmr'\n, or\n'similarity_score_threshold'\n.\nsimilarity_search_with_relevance_scores\n\u00b6\nsimilarity_search_with_relevance_scores\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nReturn docs and relevance scores in the range\n[0, 1]\n.\n0\nis dissimilar,\n1\nis most similar.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nKwargs to be passed to similarity search.\nShould include\nscore_threshold\n, an optional floating point value\nbetween\n0\nto\n1\nto filter the resulting set of retrieved docs.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nList of tuples of\n(doc, similarity_score)\n.\nasimilarity_search_with_relevance_scores\nasync\n\u00b6\nasimilarity_search_with_relevance_scores\n(\nquery\n:\nstr\n,\nk\n:\nint\n=\n4\n,\n**\nkwargs\n:\nAny\n)\n->\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nAsync return docs and relevance scores in the range\n[0, 1]\n.\n0\nis dissimilar,\n1\nis most similar.\nPARAMETER\nDESCRIPTION\nquery\nInput text.\nTYPE:\nstr\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\n**kwargs\nKwargs to be passed to similarity search.\nShould include\nscore_threshold\n, an optional floating point value\nbetween\n0\nto\n1\nto filter the resulting set of retrieved docs.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\ntuple\n[\nDocument\n,\nfloat\n]]\nList of tuples of\n(doc, similarity_score)\namax_marginal_relevance_search_by_vector\nasync\n\u00b6\namax_marginal_relevance_search_by_vector\n(\nembedding\n:\nlist\n[\nfloat\n],\nk\n:\nint\n=\n4\n,\nfetch_k\n:\nint\n=\n20\n,\nlambda_mult\n:\nfloat\n=\n0.5\n,\n**\nkwargs\n:\nAny\n,\n)\n->\nlist\n[\nDocument\n]\nAsync return docs selected using the maximal marginal relevance.\nMaximal marginal relevance optimizes for similarity to query AND diversity\namong selected documents.\nPARAMETER\nDESCRIPTION\nembedding\nEmbedding to look up documents similar to.\nTYPE:\nlist\n[\nfloat\n]\nk\nNumber of\nDocument\nobjects to return.\nTYPE:\nint\nDEFAULT:\n4\nfetch_k\nNumber of\nDocument\nobjects to fetch to pass to MMR algorithm.\nTYPE:\nint\nDEFAULT:\n20\nlambda_mult\nNumber between\n0\nand\n1\nthat determines the degree\nof diversity among the results with\n0\ncorresponding\nto maximum diversity and\n1\nto minimum diversity.\nTYPE:\nfloat\nDEFAULT:\n0.5\n**kwargs\nArguments to pass to the search method.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nlist\n[\nDocument\n]\nList of\nDocument\nobjects selected by maximal marginal relevance.\nfrom_documents\nclassmethod\n\u00b6\nfrom_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\nembedding\n:\nEmbeddings\n,\n**\nkwargs\n:\nAny\n)\n->\nSelf\nReturn\nVectorStore\ninitialized from documents and embeddings.\nPARAMETER\nDESCRIPTION\ndocuments\nList of\nDocument\nobjects to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\nembedding\nEmbedding function to use.\nTYPE:\nEmbeddings\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nSelf\nVectorStore\ninitialized from documents and embeddings.\nafrom_documents\nasync\nclassmethod\n\u00b6\nafrom_documents\n(\ndocuments\n:\nlist\n[\nDocument\n],\nembedding\n:\nEmbeddings\n,\n**\nkwargs\n:\nAny\n)\n->\nSelf\nAsync return\nVectorStore\ninitialized from documents and embeddings.\nPARAMETER\nDESCRIPTION\ndocuments\nList of\nDocument\nobjects to add to the\nVectorStore\n.\nTYPE:\nlist\n[\nDocument\n]\nembedding\nEmbedding function to use.\nTYPE:\nEmbeddings\n**kwargs\nAdditional keyword arguments.\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nSelf\nVectorStore\ninitialized from documents and embeddings.\nas_retriever\n\u00b6\nas_retriever\n(\n**\nkwargs\n:\nAny\n)\n->\nVectorStoreRetriever\nReturn\nVectorStoreRetriever\ninitialized from this\nVectorStore\n.\nPARAMETER\nDESCRIPTION\n**kwargs\nKeyword arguments to pass to the search function.\nCan include:\nsearch_type\n: Defines the type of search that the Retriever should\nperform. Can be\n'similarity'\n(default),\n'mmr'\n, or\n'similarity_score_threshold'\n.\nsearch_kwargs\n: Keyword arguments to pass to the search function.\nCan include things like:\nk\n: Amount of documents to return (Default:\n4\n)\nscore_threshold\n: Minimum relevance threshold\nfor\nsimilarity_score_threshold\nfetch_k\n: Amount of documents to pass to MMR algorithm\n(Default:\n20\n)\nlambda_mult\n: Diversity of results returned by MMR;\n1\nfor minimum diversity and 0 for maximum. (Default:\n0.5\n)\nfilter\n: Filter by document metadata\nTYPE:\nAny\nDEFAULT:\n{}\nRETURNS\nDESCRIPTION\nVectorStoreRetriever\nRetriever class for\nVectorStore\n.\nExamples:\n# Retrieve more documents with higher diversity\n# Useful if your dataset has many similar documents\ndocsearch\n.\nas_retriever\n(\nsearch_type\n=\n\"mmr\"\n,\nsearch_kwargs\n=\n{\n\"k\"\n:\n6\n,\n\"lambda_mult\"\n:\n0.25\n}\n)\n# Fetch more documents for the MMR algorithm to consider\n# But only return the top 5\ndocsearch\n.\nas_retriever\n(\nsearch_type\n=\n\"mmr\"\n,\nsearch_kwargs\n=\n{\n\"k\"\n:\n5\n,\n\"fetch_k\"\n:\n50\n})\n# Only retrieve documents that have a relevance score\n# Above a certain threshold\ndocsearch\n.\nas_retriever\n(\nsearch_type\n=\n\"similarity_score_threshold\"\n,\nsearch_kwargs\n=\n{\n\"score_threshold\"\n:\n0.8\n},\n)\n# Only get the single most similar document from the dataset\ndocsearch\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n})\n# Use a filter to only retrieve documents from a specific paper\ndocsearch\n.\nas_retriever\n(\nsearch_kwargs\n=\n{\n\"filter\"\n:\n{\n\"paper_title\"\n:\n\"GPT-4 Technical Report\"\n}}\n)\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/langchain_core/vectorstores/#langchain_core.vectorstores.base.VectorStore",
      "title": "Vector stores | LangChain Reference",
      "heading": "Vector stores"
    }
  },
  {
    "page_content": "Deep Agents overview | LangChain Reference\nSkip to content\nLangChain Reference\nDeep Agents overview\nInitializing search\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLangSmith\nLangChain Reference\nlangchain-ai/docs\nGet started\nLangChain\nLangGraph\nDeep Agents\nDeep Agents\nIntegrations\nLangSmith\nTable of contents\ndeepagents\nFilesystemMiddleware\nname\nstate_schema\ntools\nbefore_agent\nabefore_agent\nbefore_model\nabefore_model\nafter_model\naafter_model\nafter_agent\naafter_agent\n__init__\nwrap_model_call\nawrap_model_call\nwrap_tool_call\nawrap_tool_call\nCompiledSubAgent\nname\ndescription\nrunnable\nSubAgent\nname\ndescription\nsystem_prompt\ntools\nmodel\nmiddleware\ninterrupt_on\nSubAgentMiddleware\nstate_schema\nname\ntools\nbefore_agent\nabefore_agent\nbefore_model\nabefore_model\nafter_model\naafter_model\nafter_agent\naafter_agent\nwrap_tool_call\nawrap_tool_call\n__init__\nwrap_model_call\nawrap_model_call\ncreate_deep_agent\nDeep Agents reference\nWelcome to the\nDeep Agents\nreference documentation!\nWork in progress\nThis page is a work in progress, and we appreciate your patience as we continue to expand and improve the content.\ndeepagents\n\u00b6\nDeepAgents package.\nFUNCTION\nDESCRIPTION\ncreate_deep_agent\nCreate a deep agent.\nFilesystemMiddleware\n\u00b6\nBases:\nAgentMiddleware\nMiddleware for providing filesystem and optional execution tools to an agent.\nThis middleware adds filesystem tools to the agent: ls, read_file, write_file,\nedit_file, glob, and grep. Files can be stored using any backend that implements\nthe BackendProtocol.\nIf the backend implements SandboxBackendProtocol, an execute tool is also added\nfor running shell commands.\nPARAMETER\nDESCRIPTION\nbackend\nBackend for file storage and optional execution. If not provided, defaults to StateBackend\n(ephemeral storage in agent state). For persistent storage or hybrid setups,\nuse CompositeBackend with custom routes. For execution support, use a backend\nthat implements SandboxBackendProtocol.\nTYPE:\nBACKEND_TYPES\n| None\nDEFAULT:\nNone\nsystem_prompt\nOptional custom system prompt override.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ncustom_tool_descriptions\nOptional custom tool descriptions override.\nTYPE:\ndict\n[\nstr\n,\nstr\n] | None\nDEFAULT:\nNone\ntool_token_limit_before_evict\nOptional token limit before evicting a tool result to the filesystem.\nTYPE:\nint\n| None\nDEFAULT:\n20000\nExample\nfrom\ndeepagents.middleware.filesystem\nimport\nFilesystemMiddleware\nfrom\ndeepagents.backends\nimport\nStateBackend\n,\nStoreBackend\n,\nCompositeBackend\nfrom\nlangchain.agents\nimport\ncreate_agent\n# Ephemeral storage only (default, no execution)\nagent\n=\ncreate_agent\n(\nmiddleware\n=\n[\nFilesystemMiddleware\n()])\n# With hybrid storage (ephemeral + persistent /memories/)\nbackend\n=\nCompositeBackend\n(\ndefault\n=\nStateBackend\n(),\nroutes\n=\n{\n\"/memories/\"\n:\nStoreBackend\n()})\nagent\n=\ncreate_agent\n(\nmiddleware\n=\n[\nFilesystemMiddleware\n(\nbackend\n=\nbackend\n)])\n# With sandbox backend (supports execution)\nfrom\nmy_sandbox\nimport\nDockerSandboxBackend\nsandbox\n=\nDockerSandboxBackend\n(\ncontainer_id\n=\n\"my-container\"\n)\nagent\n=\ncreate_agent\n(\nmiddleware\n=\n[\nFilesystemMiddleware\n(\nbackend\n=\nsandbox\n)])\nMETHOD\nDESCRIPTION\nbefore_agent\nLogic to run before the agent execution starts.\nabefore_agent\nAsync logic to run before the agent execution starts.\nbefore_model\nLogic to run before the model is called.\nabefore_model\nAsync logic to run before the model is called.\nafter_model\nLogic to run after the model is called.\naafter_model\nAsync logic to run after the model is called.\nafter_agent\nLogic to run after the agent execution completes.\naafter_agent\nAsync logic to run after the agent execution completes.\n__init__\nInitialize the filesystem middleware.\nwrap_model_call\nUpdate the system prompt and filter tools based on backend capabilities.\nawrap_model_call\n(async) Update the system prompt and filter tools based on backend capabilities.\nwrap_tool_call\nCheck the size of the tool call result and evict to filesystem if too large.\nawrap_tool_call\n(async)Check the size of the tool call result and evict to filesystem if too large.\nname\nproperty\n\u00b6\nname\n:\nstr\nThe name of the middleware instance.\nDefaults to the class name, but can be overridden for custom naming.\nstate_schema\nclass-attribute\ninstance-attribute\n\u00b6\nstate_schema\n=\nFilesystemState\nThe schema for state passed to the middleware nodes.\ntools\ninstance-attribute\n\u00b6\ntools\n=\n_get_filesystem_tools\n(\nbackend\n,\ncustom_tool_descriptions\n)\nAdditional tools registered by the middleware.\nbefore_agent\n\u00b6\nbefore_agent\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nLogic to run before the agent execution starts.\nAsync version is\nabefore_agent\nabefore_agent\nasync\n\u00b6\nabefore_agent\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nAsync logic to run before the agent execution starts.\nbefore_model\n\u00b6\nbefore_model\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nLogic to run before the model is called.\nAsync version is\nabefore_model\nabefore_model\nasync\n\u00b6\nabefore_model\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nAsync logic to run before the model is called.\nafter_model\n\u00b6\nafter_model\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nLogic to run after the model is called.\nAsync version is\naafter_model\naafter_model\nasync\n\u00b6\naafter_model\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nAsync logic to run after the model is called.\nafter_agent\n\u00b6\nafter_agent\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nLogic to run after the agent execution completes.\naafter_agent\nasync\n\u00b6\naafter_agent\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nAsync logic to run after the agent execution completes.\n__init__\n\u00b6\n__init__\n(\n*\n,\nbackend\n:\nBACKEND_TYPES\n|\nNone\n=\nNone\n,\nsystem_prompt\n:\nstr\n|\nNone\n=\nNone\n,\ncustom_tool_descriptions\n:\ndict\n[\nstr\n,\nstr\n]\n|\nNone\n=\nNone\n,\ntool_token_limit_before_evict\n:\nint\n|\nNone\n=\n20000\n,\n)\n->\nNone\nInitialize the filesystem middleware.\nPARAMETER\nDESCRIPTION\nbackend\nBackend for file storage and optional execution, or a factory callable.\nDefaults to StateBackend if not provided.\nTYPE:\nBACKEND_TYPES\n| None\nDEFAULT:\nNone\nsystem_prompt\nOptional custom system prompt override.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ncustom_tool_descriptions\nOptional custom tool descriptions override.\nTYPE:\ndict\n[\nstr\n,\nstr\n] | None\nDEFAULT:\nNone\ntool_token_limit_before_evict\nOptional token limit before evicting a tool result to the filesystem.\nTYPE:\nint\n| None\nDEFAULT:\n20000\nwrap_model_call\n\u00b6\nwrap_model_call\n(\nrequest\n:\nModelRequest\n,\nhandler\n:\nCallable\n[[\nModelRequest\n],\nModelResponse\n]\n)\n->\nModelResponse\nUpdate the system prompt and filter tools based on backend capabilities.\nPARAMETER\nDESCRIPTION\nrequest\nThe model request being processed.\nTYPE:\nModelRequest\nhandler\nThe handler function to call with the modified request.\nTYPE:\nCallable\n[[\nModelRequest\n],\nModelResponse\n]\nRETURNS\nDESCRIPTION\nModelResponse\nThe model response from the handler.\nawrap_model_call\nasync\n\u00b6\nawrap_model_call\n(\nrequest\n:\nModelRequest\n,\nhandler\n:\nCallable\n[[\nModelRequest\n],\nAwaitable\n[\nModelResponse\n]]\n)\n->\nModelResponse\n(async) Update the system prompt and filter tools based on backend capabilities.\nPARAMETER\nDESCRIPTION\nrequest\nThe model request being processed.\nTYPE:\nModelRequest\nhandler\nThe handler function to call with the modified request.\nTYPE:\nCallable\n[[\nModelRequest\n],\nAwaitable\n[\nModelResponse\n]]\nRETURNS\nDESCRIPTION\nModelResponse\nThe model response from the handler.\nwrap_tool_call\n\u00b6\nwrap_tool_call\n(\nrequest\n:\nToolCallRequest\n,\nhandler\n:\nCallable\n[[\nToolCallRequest\n],\nToolMessage\n|\nCommand\n],\n)\n->\nToolMessage\n|\nCommand\nCheck the size of the tool call result and evict to filesystem if too large.\nPARAMETER\nDESCRIPTION\nrequest\nThe tool call request being processed.\nTYPE:\nToolCallRequest\nhandler\nThe handler function to call with the modified request.\nTYPE:\nCallable\n[[\nToolCallRequest\n],\nToolMessage\n|\nCommand\n]\nRETURNS\nDESCRIPTION\nToolMessage\n|\nCommand\nThe raw ToolMessage, or a pseudo tool message with the ToolResult in state.\nawrap_tool_call\nasync\n\u00b6\nawrap_tool_call\n(\nrequest\n:\nToolCallRequest\n,\nhandler\n:\nCallable\n[[\nToolCallRequest\n],\nAwaitable\n[\nToolMessage\n|\nCommand\n]],\n)\n->\nToolMessage\n|\nCommand\n(async)Check the size of the tool call result and evict to filesystem if too large.\nPARAMETER\nDESCRIPTION\nrequest\nThe tool call request being processed.\nTYPE:\nToolCallRequest\nhandler\nThe handler function to call with the modified request.\nTYPE:\nCallable\n[[\nToolCallRequest\n],\nAwaitable\n[\nToolMessage\n|\nCommand\n]]\nRETURNS\nDESCRIPTION\nToolMessage\n|\nCommand\nThe raw ToolMessage, or a pseudo tool message with the ToolResult in state.\nCompiledSubAgent\n\u00b6\nBases:\nTypedDict\nA pre-compiled agent spec.\nname\ninstance-attribute\n\u00b6\nname\n:\nstr\nThe name of the agent.\ndescription\ninstance-attribute\n\u00b6\ndescription\n:\nstr\nThe description of the agent.\nrunnable\ninstance-attribute\n\u00b6\nrunnable\n:\nRunnable\nThe Runnable to use for the agent.\nSubAgent\n\u00b6\nBases:\nTypedDict\nSpecification for an agent.\nWhen specifying custom agents, the\ndefault_middleware\nfrom\nSubAgentMiddleware\nwill be applied first, followed by any\nmiddleware\nspecified in this spec.\nTo use only custom middleware without the defaults, pass\ndefault_middleware=[]\nto\nSubAgentMiddleware\n.\nname\ninstance-attribute\n\u00b6\nname\n:\nstr\nThe name of the agent.\ndescription\ninstance-attribute\n\u00b6\ndescription\n:\nstr\nThe description of the agent.\nsystem_prompt\ninstance-attribute\n\u00b6\nsystem_prompt\n:\nstr\nThe system prompt to use for the agent.\ntools\ninstance-attribute\n\u00b6\ntools\n:\nSequence\n[\nBaseTool\n|\nCallable\n|\ndict\n[\nstr\n,\nAny\n]]\nThe tools to use for the agent.\nmodel\ninstance-attribute\n\u00b6\nmodel\n:\nNotRequired\n[\nstr\n|\nBaseChatModel\n]\nThe model for the agent. Defaults to\ndefault_model\n.\nmiddleware\ninstance-attribute\n\u00b6\nmiddleware\n:\nNotRequired\n[\nlist\n[\nAgentMiddleware\n]]\nAdditional middleware to append after\ndefault_middleware\n.\ninterrupt_on\ninstance-attribute\n\u00b6\ninterrupt_on\n:\nNotRequired\n[\ndict\n[\nstr\n,\nbool\n|\nInterruptOnConfig\n]]\nThe tool configs to use for the agent.\nSubAgentMiddleware\n\u00b6\nBases:\nAgentMiddleware\nMiddleware for providing subagents to an agent via a\ntask\ntool.\nThis  middleware adds a\ntask\ntool to the agent that can be used to invoke subagents.\nSubagents are useful for handling complex tasks that require multiple steps, or tasks\nthat require a lot of context to resolve.\nA chief benefit of subagents is that they can handle multi-step tasks, and then return\na clean, concise response to the main agent.\nSubagents are also great for different domains of expertise that require a narrower\nsubset of tools and focus.\nThis middleware comes with a default general-purpose subagent that can be used to\nhandle the same tasks as the main agent, but with isolated context.\nPARAMETER\nDESCRIPTION\ndefault_model\nThe model to use for subagents.\nCan be a LanguageModelLike or a dict for init_chat_model.\nTYPE:\nstr\n|\nBaseChatModel\ndefault_tools\nThe tools to use for the default general-purpose subagent.\nTYPE:\nSequence\n[\nBaseTool\n|\nCallable\n|\ndict\n[\nstr\n,\nAny\n]] | None\nDEFAULT:\nNone\ndefault_middleware\nDefault middleware to apply to all subagents. If\nNone\n(default),\nno default middleware is applied. Pass a list to specify custom middleware.\nTYPE:\nlist\n[\nAgentMiddleware\n] | None\nDEFAULT:\nNone\ndefault_interrupt_on\nThe tool configs to use for the default general-purpose subagent. These\nare also the fallback for any subagents that don't specify their own tool configs.\nTYPE:\ndict\n[\nstr\n,\nbool\n|\nInterruptOnConfig\n] | None\nDEFAULT:\nNone\nsubagents\nA list of additional subagents to provide to the agent.\nTYPE:\nlist\n[\nSubAgent\n|\nCompiledSubAgent\n] | None\nDEFAULT:\nNone\nsystem_prompt\nFull system prompt override. When provided, completely replaces\nthe agent's system prompt.\nTYPE:\nstr\n| None\nDEFAULT:\nTASK_SYSTEM_PROMPT\ngeneral_purpose_agent\nWhether to include the general-purpose agent. Defaults to\nTrue\n.\nTYPE:\nbool\nDEFAULT:\nTrue\ntask_description\nCustom description for the task tool. If\nNone\n, uses the\ndefault description template.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nExample\nfrom\nlangchain.agents.middleware.subagents\nimport\nSubAgentMiddleware\nfrom\nlangchain.agents\nimport\ncreate_agent\n# Basic usage with defaults (no default middleware)\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\nmiddleware\n=\n[\nSubAgentMiddleware\n(\ndefault_model\n=\n\"openai:gpt-5-mini\"\n,\nsubagents\n=\n[],\n)\n],\n)\n# Add custom middleware to subagents\nagent\n=\ncreate_agent\n(\n\"openai:gpt-5-mini\"\n,\nmiddleware\n=\n[\nSubAgentMiddleware\n(\ndefault_model\n=\n\"openai:gpt-5-mini\"\n,\ndefault_middleware\n=\n[\nTodoListMiddleware\n()],\nsubagents\n=\n[],\n)\n],\n)\nMETHOD\nDESCRIPTION\nbefore_agent\nLogic to run before the agent execution starts.\nabefore_agent\nAsync logic to run before the agent execution starts.\nbefore_model\nLogic to run before the model is called.\nabefore_model\nAsync logic to run before the model is called.\nafter_model\nLogic to run after the model is called.\naafter_model\nAsync logic to run after the model is called.\nafter_agent\nLogic to run after the agent execution completes.\naafter_agent\nAsync logic to run after the agent execution completes.\nwrap_tool_call\nIntercept tool execution for retries, monitoring, or modification.\nawrap_tool_call\nIntercept and control async tool execution via handler callback.\n__init__\nInitialize the SubAgentMiddleware.\nwrap_model_call\nUpdate the system prompt to include instructions on using subagents.\nawrap_model_call\n(async) Update the system prompt to include instructions on using subagents.\nstate_schema\nclass-attribute\ninstance-attribute\n\u00b6\nstate_schema\n:\ntype\n[\nStateT\n]\n=\ncast\n(\n'type[StateT]'\n,\nAgentState\n)\nThe schema for state passed to the middleware nodes.\nname\nproperty\n\u00b6\nname\n:\nstr\nThe name of the middleware instance.\nDefaults to the class name, but can be overridden for custom naming.\ntools\ninstance-attribute\n\u00b6\ntools\n=\n[\ntask_tool\n]\nAdditional tools registered by the middleware.\nbefore_agent\n\u00b6\nbefore_agent\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nLogic to run before the agent execution starts.\nAsync version is\nabefore_agent\nabefore_agent\nasync\n\u00b6\nabefore_agent\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nAsync logic to run before the agent execution starts.\nbefore_model\n\u00b6\nbefore_model\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nLogic to run before the model is called.\nAsync version is\nabefore_model\nabefore_model\nasync\n\u00b6\nabefore_model\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nAsync logic to run before the model is called.\nafter_model\n\u00b6\nafter_model\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nLogic to run after the model is called.\nAsync version is\naafter_model\naafter_model\nasync\n\u00b6\naafter_model\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nAsync logic to run after the model is called.\nafter_agent\n\u00b6\nafter_agent\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nLogic to run after the agent execution completes.\naafter_agent\nasync\n\u00b6\naafter_agent\n(\nstate\n:\nStateT\n,\nruntime\n:\nRuntime\n[\nContextT\n])\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nAsync logic to run after the agent execution completes.\nwrap_tool_call\n\u00b6\nwrap_tool_call\n(\nrequest\n:\nToolCallRequest\n,\nhandler\n:\nCallable\n[[\nToolCallRequest\n],\nToolMessage\n|\nCommand\n],\n)\n->\nToolMessage\n|\nCommand\nIntercept tool execution for retries, monitoring, or modification.\nAsync version is\nawrap_tool_call\nMultiple middleware compose automatically (first defined = outermost).\nExceptions propagate unless\nhandle_tool_errors\nis configured on\nToolNode\n.\nPARAMETER\nDESCRIPTION\nrequest\nTool call request with call\ndict\n,\nBaseTool\n, state, and runtime.\nAccess state via\nrequest.state\nand runtime via\nrequest.runtime\n.\nTYPE:\nToolCallRequest\nhandler\nCallable\nto execute the tool (can be called multiple times).\nTYPE:\nCallable\n[[\nToolCallRequest\n],\nToolMessage\n|\nCommand\n]\nRETURNS\nDESCRIPTION\nToolMessage\n|\nCommand\nToolMessage\nor\nCommand\n(the final result).\nThe handler\nCallable\ncan be invoked multiple times for retry logic.\nEach call to handler is independent and stateless.\nExamples:\nModify request before execution\ndef\nwrap_tool_call\n(\nself\n,\nrequest\n,\nhandler\n):\nmodified_call\n=\n{\n**\nrequest\n.\ntool_call\n,\n\"args\"\n:\n{\n**\nrequest\n.\ntool_call\n[\n\"args\"\n],\n\"value\"\n:\nrequest\n.\ntool_call\n[\n\"args\"\n][\n\"value\"\n]\n*\n2\n,\n},\n}\nrequest\n=\nrequest\n.\noverride\n(\ntool_call\n=\nmodified_call\n)\nreturn\nhandler\n(\nrequest\n)\nRetry on error (call handler multiple times)\ndef\nwrap_tool_call\n(\nself\n,\nrequest\n,\nhandler\n):\nfor\nattempt\nin\nrange\n(\n3\n):\ntry\n:\nresult\n=\nhandler\n(\nrequest\n)\nif\nis_valid\n(\nresult\n):\nreturn\nresult\nexcept\nException\n:\nif\nattempt\n==\n2\n:\nraise\nreturn\nresult\nConditional retry based on response\ndef\nwrap_tool_call\n(\nself\n,\nrequest\n,\nhandler\n):\nfor\nattempt\nin\nrange\n(\n3\n):\nresult\n=\nhandler\n(\nrequest\n)\nif\nisinstance\n(\nresult\n,\nToolMessage\n)\nand\nresult\n.\nstatus\n!=\n\"error\"\n:\nreturn\nresult\nif\nattempt\n<\n2\n:\ncontinue\nreturn\nresult\nawrap_tool_call\nasync\n\u00b6\nawrap_tool_call\n(\nrequest\n:\nToolCallRequest\n,\nhandler\n:\nCallable\n[[\nToolCallRequest\n],\nAwaitable\n[\nToolMessage\n|\nCommand\n]],\n)\n->\nToolMessage\n|\nCommand\nIntercept and control async tool execution via handler callback.\nThe handler callback executes the tool call and returns a\nToolMessage\nor\nCommand\n. Middleware can call the handler multiple times for retry logic, skip\ncalling it to short-circuit, or modify the request/response. Multiple middleware\ncompose with first in list as outermost layer.\nPARAMETER\nDESCRIPTION\nrequest\nTool call request with call\ndict\n,\nBaseTool\n, state, and runtime.\nAccess state via\nrequest.state\nand runtime via\nrequest.runtime\n.\nTYPE:\nToolCallRequest\nhandler\nAsync callable to execute the tool and returns\nToolMessage\nor\nCommand\n.\nCall this to execute the tool.\nCan be called multiple times for retry logic.\nCan skip calling it to short-circuit.\nTYPE:\nCallable\n[[\nToolCallRequest\n],\nAwaitable\n[\nToolMessage\n|\nCommand\n]]\nRETURNS\nDESCRIPTION\nToolMessage\n|\nCommand\nToolMessage\nor\nCommand\n(the final result).\nThe handler\nCallable\ncan be invoked multiple times for retry logic.\nEach call to handler is independent and stateless.\nExamples:\nAsync retry on error\nasync\ndef\nawrap_tool_call\n(\nself\n,\nrequest\n,\nhandler\n):\nfor\nattempt\nin\nrange\n(\n3\n):\ntry\n:\nresult\n=\nawait\nhandler\n(\nrequest\n)\nif\nis_valid\n(\nresult\n):\nreturn\nresult\nexcept\nException\n:\nif\nattempt\n==\n2\n:\nraise\nreturn\nresult\nasync\ndef\nawrap_tool_call\n(\nself\n,\nrequest\n,\nhandler\n):\nif\ncached\n:=\nawait\nget_cache_async\n(\nrequest\n):\nreturn\nToolMessage\n(\ncontent\n=\ncached\n,\ntool_call_id\n=\nrequest\n.\ntool_call\n[\n\"id\"\n])\nresult\n=\nawait\nhandler\n(\nrequest\n)\nawait\nsave_cache_async\n(\nrequest\n,\nresult\n)\nreturn\nresult\n__init__\n\u00b6\n__init__\n(\n*\n,\ndefault_model\n:\nstr\n|\nBaseChatModel\n,\ndefault_tools\n:\nSequence\n[\nBaseTool\n|\nCallable\n|\ndict\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\ndefault_middleware\n:\nlist\n[\nAgentMiddleware\n]\n|\nNone\n=\nNone\n,\ndefault_interrupt_on\n:\ndict\n[\nstr\n,\nbool\n|\nInterruptOnConfig\n]\n|\nNone\n=\nNone\n,\nsubagents\n:\nlist\n[\nSubAgent\n|\nCompiledSubAgent\n]\n|\nNone\n=\nNone\n,\nsystem_prompt\n:\nstr\n|\nNone\n=\nTASK_SYSTEM_PROMPT\n,\ngeneral_purpose_agent\n:\nbool\n=\nTrue\n,\ntask_description\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nNone\nInitialize the SubAgentMiddleware.\nwrap_model_call\n\u00b6\nwrap_model_call\n(\nrequest\n:\nModelRequest\n,\nhandler\n:\nCallable\n[[\nModelRequest\n],\nModelResponse\n]\n)\n->\nModelResponse\nUpdate the system prompt to include instructions on using subagents.\nawrap_model_call\nasync\n\u00b6\nawrap_model_call\n(\nrequest\n:\nModelRequest\n,\nhandler\n:\nCallable\n[[\nModelRequest\n],\nAwaitable\n[\nModelResponse\n]]\n)\n->\nModelResponse\n(async) Update the system prompt to include instructions on using subagents.\ncreate_deep_agent\n\u00b6\ncreate_deep_agent\n(\nmodel\n:\nstr\n|\nBaseChatModel\n|\nNone\n=\nNone\n,\ntools\n:\nSequence\n[\nBaseTool\n|\nCallable\n|\ndict\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\n*\n,\nsystem_prompt\n:\nstr\n|\nNone\n=\nNone\n,\nmiddleware\n:\nSequence\n[\nAgentMiddleware\n]\n=\n(),\nsubagents\n:\nlist\n[\nSubAgent\n|\nCompiledSubAgent\n]\n|\nNone\n=\nNone\n,\nresponse_format\n:\nResponseFormat\n|\nNone\n=\nNone\n,\ncontext_schema\n:\ntype\n[\nAny\n]\n|\nNone\n=\nNone\n,\ncheckpointer\n:\nCheckpointer\n|\nNone\n=\nNone\n,\nstore\n:\nBaseStore\n|\nNone\n=\nNone\n,\nbackend\n:\nBackendProtocol\n|\nBackendFactory\n|\nNone\n=\nNone\n,\ninterrupt_on\n:\ndict\n[\nstr\n,\nbool\n|\nInterruptOnConfig\n]\n|\nNone\n=\nNone\n,\ndebug\n:\nbool\n=\nFalse\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\ncache\n:\nBaseCache\n|\nNone\n=\nNone\n,\n)\n->\nCompiledStateGraph\nCreate a deep agent.\nThis agent will by default have access to a tool to write todos (write_todos),\nseven file and execution tools: ls, read_file, write_file, edit_file, glob, grep, execute,\nand a tool to call subagents.\nThe execute tool allows running shell commands if the backend implements SandboxBackendProtocol.\nFor non-sandbox backends, the execute tool will return an error message.\nPARAMETER\nDESCRIPTION\nmodel\nThe model to use. Defaults to Claude Sonnet 4.\nTYPE:\nstr\n|\nBaseChatModel\n| None\nDEFAULT:\nNone\ntools\nThe tools the agent should have access to.\nTYPE:\nSequence\n[\nBaseTool\n|\nCallable\n|\ndict\n[\nstr\n,\nAny\n]] | None\nDEFAULT:\nNone\nsystem_prompt\nThe additional instructions the agent should have. Will go in\nthe system prompt.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\nmiddleware\nAdditional middleware to apply after standard middleware.\nTYPE:\nSequence\n[\nAgentMiddleware\n]\nDEFAULT:\n()\nsubagents\nThe subagents to use. Each subagent should be a dictionary with the\nfollowing keys:\n-\nname\n-\ndescription\n(used by the main agent to decide whether to call the\nsub agent)\n-\nprompt\n(used as the system prompt in the subagent)\n- (optional)\ntools\n- (optional)\nmodel\n(either a LanguageModelLike instance or dict\nsettings)\n- (optional)\nmiddleware\n(list of AgentMiddleware)\nTYPE:\nlist\n[\nSubAgent\n|\nCompiledSubAgent\n] | None\nDEFAULT:\nNone\nresponse_format\nA structured output response format to use for the agent.\nTYPE:\nResponseFormat\n| None\nDEFAULT:\nNone\ncontext_schema\nThe schema of the deep agent.\nTYPE:\ntype\n[\nAny\n] | None\nDEFAULT:\nNone\ncheckpointer\nOptional checkpointer for persisting agent state between runs.\nTYPE:\nCheckpointer\n| None\nDEFAULT:\nNone\nstore\nOptional store for persistent storage (required if backend uses StoreBackend).\nTYPE:\nBaseStore\n| None\nDEFAULT:\nNone\nbackend\nOptional backend for file storage and execution. Pass either a Backend instance\nor a callable factory like\nlambda rt: StateBackend(rt)\n. For execution support,\nuse a backend that implements SandboxBackendProtocol.\nTYPE:\nBackendProtocol\n|\nBackendFactory\n| None\nDEFAULT:\nNone\ninterrupt_on\nOptional Dict[str, bool | InterruptOnConfig] mapping tool names to\ninterrupt configs.\nTYPE:\ndict\n[\nstr\n,\nbool\n|\nInterruptOnConfig\n] | None\nDEFAULT:\nNone\ndebug\nWhether to enable debug mode. Passed through to create_agent.\nTYPE:\nbool\nDEFAULT:\nFalse\nname\nThe name of the agent. Passed through to create_agent.\nTYPE:\nstr\n| None\nDEFAULT:\nNone\ncache\nThe cache to use for the agent. Passed through to create_agent.\nTYPE:\nBaseCache\n| None\nDEFAULT:\nNone\nRETURNS\nDESCRIPTION\nCompiledStateGraph\nA configured deep agent.\nBack to top",
    "metadata": {
      "source": "https://reference.langchain.com/python/deepagents/",
      "title": "Deep Agents overview | LangChain Reference",
      "heading": "Deep Agents reference"
    }
  },
  {
    "page_content": "Hierarchical Agent Teams\nSkip to content\nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025.\nVisit the v1.0 alpha docs\nLangGraph\nHierarchical Agent Teams\nInitializing search\nGitHub\nReference\nLangGraph\nGitHub\nReference\nTable of contents\nSetup\nCreate Tools\nHelper Utilities\nDefine Agent Teams\nResearch Team\nDocument Writing Team\nAdd Layers\nHierarchical Agent Teams\n\u00b6\nIn our previous example (\nAgent Supervisor\n), we introduced the concept of a single\nsupervisor node\nto route work between different worker nodes.\nBut what if the job for a single worker becomes too complex? What if the number of workers becomes too large?\nFor some applications, the system may be more effective if work is distributed\nhierarchically\n.\nYou can do this by composing different subgraphs and creating a top-level supervisor, along with mid-level supervisors.\nTo do this, let's build a simple research assistant! The graph will look something like the following:\nThis notebook is inspired by the paper\nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n, by Wu, et. al. In the rest of this notebook, you will:\nDefine the agents' tools to access the web and write files\nDefine some utilities to help create the graph and agents\nCreate and define each team (web research + doc writing)\nCompose everything together.\nSetup\n\u00b6\nFirst, let's install our required packages and set our API keys\npip\ninstall\n-U\nlanggraph\nlangchain_community\nlangchain_anthropic\nlangchain-tavily\nlangchain_experimental\nimport\ngetpass\nimport\nos\ndef\n_set_if_undefined\n(\nvar\n:\nstr\n):\nif\nnot\nos\n.\nenviron\n.\nget\n(\nvar\n):\nos\n.\nenviron\n[\nvar\n]\n=\ngetpass\n.\ngetpass\n(\nf\n\"Please provide your\n{\nvar\n}\n\"\n)\n_set_if_undefined\n(\n\"OPENAI_API_KEY\"\n)\n_set_if_undefined\n(\n\"TAVILY_API_KEY\"\n)\nSet up\nLangSmith\nfor LangGraph development\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started\nhere\n.\nCreate Tools\n\u00b6\nEach team will be composed of one or more agents each with one or more tools. Below, define all the tools to be used by your different teams.\nWe'll start with the research team.\nResearchTeam tools\nThe research team can use a search engine and url scraper to find information on the web. Feel free to add additional functionality below to boost the team performance!\nAPI Reference:\nWebBaseLoader\n|\nTavilySearch\n|\ntool\nfrom\ntyping\nimport\nAnnotated\n,\nList\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain_tavily\nimport\nTavilySearch\nfrom\nlangchain_core.tools\nimport\ntool\ntavily_tool\n=\nTavilySearch\n(\nmax_results\n=\n5\n)\n@tool\ndef\nscrape_webpages\n(\nurls\n:\nList\n[\nstr\n])\n->\nstr\n:\n\"\"\"Use requests and bs4 to scrape the provided web pages for detailed information.\"\"\"\nloader\n=\nWebBaseLoader\n(\nurls\n)\ndocs\n=\nloader\n.\nload\n()\nreturn\n\"\n\\n\\n\n\"\n.\njoin\n(\n[\nf\n'<Document name=\"\n{\ndoc\n.\nmetadata\n.\nget\n(\n\"title\"\n,\n\"\"\n)\n}\n\">\n\\n\n{\ndoc\n.\npage_content\n}\n\\n\n</Document>'\nfor\ndoc\nin\ndocs\n]\n)\nDocument writing team tools\nNext up, we will give some tools for the doc writing team to use.\nWe define some bare-bones file-access tools below.\nNote that this gives the agents access to your file-system, which can be unsafe. We also haven't optimized the tool descriptions for performance.\nAPI Reference:\nPythonREPL\nfrom\npathlib\nimport\nPath\nfrom\ntempfile\nimport\nTemporaryDirectory\nfrom\ntyping\nimport\nDict\n,\nOptional\nfrom\nlangchain_experimental.utilities\nimport\nPythonREPL\nfrom\ntyping_extensions\nimport\nTypedDict\n_TEMP_DIRECTORY\n=\nTemporaryDirectory\n()\nWORKING_DIRECTORY\n=\nPath\n(\n_TEMP_DIRECTORY\n.\nname\n)\n@tool\ndef\ncreate_outline\n(\npoints\n:\nAnnotated\n[\nList\n[\nstr\n],\n\"List of main points or sections.\"\n],\nfile_name\n:\nAnnotated\n[\nstr\n,\n\"File path to save the outline.\"\n],\n)\n->\nAnnotated\n[\nstr\n,\n\"Path of the saved outline file.\"\n]:\n\"\"\"Create and save an outline.\"\"\"\nwith\n(\nWORKING_DIRECTORY\n/\nfile_name\n)\n.\nopen\n(\n\"w\"\n)\nas\nfile\n:\nfor\ni\n,\npoint\nin\nenumerate\n(\npoints\n):\nfile\n.\nwrite\n(\nf\n\"\n{\ni\n+\n1\n}\n.\n{\npoint\n}\n\\n\n\"\n)\nreturn\nf\n\"Outline saved to\n{\nfile_name\n}\n\"\n@tool\ndef\nread_document\n(\nfile_name\n:\nAnnotated\n[\nstr\n,\n\"File path to read the document from.\"\n],\nstart\n:\nAnnotated\n[\nOptional\n[\nint\n],\n\"The start line. Default is 0\"\n]\n=\nNone\n,\nend\n:\nAnnotated\n[\nOptional\n[\nint\n],\n\"The end line. Default is None\"\n]\n=\nNone\n,\n)\n->\nstr\n:\n\"\"\"Read the specified document.\"\"\"\nwith\n(\nWORKING_DIRECTORY\n/\nfile_name\n)\n.\nopen\n(\n\"r\"\n)\nas\nfile\n:\nlines\n=\nfile\n.\nreadlines\n()\nif\nstart\nis\nNone\n:\nstart\n=\n0\nreturn\n\"\n\\n\n\"\n.\njoin\n(\nlines\n[\nstart\n:\nend\n])\n@tool\ndef\nwrite_document\n(\ncontent\n:\nAnnotated\n[\nstr\n,\n\"Text content to be written into the document.\"\n],\nfile_name\n:\nAnnotated\n[\nstr\n,\n\"File path to save the document.\"\n],\n)\n->\nAnnotated\n[\nstr\n,\n\"Path of the saved document file.\"\n]:\n\"\"\"Create and save a text document.\"\"\"\nwith\n(\nWORKING_DIRECTORY\n/\nfile_name\n)\n.\nopen\n(\n\"w\"\n)\nas\nfile\n:\nfile\n.\nwrite\n(\ncontent\n)\nreturn\nf\n\"Document saved to\n{\nfile_name\n}\n\"\n@tool\ndef\nedit_document\n(\nfile_name\n:\nAnnotated\n[\nstr\n,\n\"Path of the document to be edited.\"\n],\ninserts\n:\nAnnotated\n[\nDict\n[\nint\n,\nstr\n],\n\"Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.\"\n,\n],\n)\n->\nAnnotated\n[\nstr\n,\n\"Path of the edited document file.\"\n]:\n\"\"\"Edit a document by inserting text at specific line numbers.\"\"\"\nwith\n(\nWORKING_DIRECTORY\n/\nfile_name\n)\n.\nopen\n(\n\"r\"\n)\nas\nfile\n:\nlines\n=\nfile\n.\nreadlines\n()\nsorted_inserts\n=\nsorted\n(\ninserts\n.\nitems\n())\nfor\nline_number\n,\ntext\nin\nsorted_inserts\n:\nif\n1\n<=\nline_number\n<=\nlen\n(\nlines\n)\n+\n1\n:\nlines\n.\ninsert\n(\nline_number\n-\n1\n,\ntext\n+\n\"\n\\n\n\"\n)\nelse\n:\nreturn\nf\n\"Error: Line number\n{\nline_number\n}\nis out of range.\"\nwith\n(\nWORKING_DIRECTORY\n/\nfile_name\n)\n.\nopen\n(\n\"w\"\n)\nas\nfile\n:\nfile\n.\nwritelines\n(\nlines\n)\nreturn\nf\n\"Document edited and saved to\n{\nfile_name\n}\n\"\n# Warning: This executes code locally, which can be unsafe when not sandboxed\nrepl\n=\nPythonREPL\n()\n@tool\ndef\npython_repl_tool\n(\ncode\n:\nAnnotated\n[\nstr\n,\n\"The python code to execute to generate your chart.\"\n],\n):\n\"\"\"Use this to execute python code. If you want to see the output of a value,\nyou should print it out with `print(...)`. This is visible to the user.\"\"\"\ntry\n:\nresult\n=\nrepl\n.\nrun\n(\ncode\n)\nexcept\nBaseException\nas\ne\n:\nreturn\nf\n\"Failed to execute. Error:\n{\nrepr\n(\ne\n)\n}\n\"\nreturn\nf\n\"Successfully executed:\n\\n\n\\`\\`\\`python\n\\n\n{\ncode\n}\n\\n\n\\`\\`\\`\n\\n\nStdout:\n{\nresult\n}\n\"\nHelper Utilities\n\u00b6\nWe are going to create a few utility functions to make it more concise when we want to:\nCreate a worker agent.\nCreate a supervisor for the sub-graph.\nThese will simplify the graph compositional code at the end for us so it's easier to see what's going on.\nAPI Reference:\nBaseChatModel\n|\nStateGraph\n|\nSTART\n|\nEND\n|\nCommand\n|\nHumanMessage\n|\ntrim_messages\nfrom\ntyping\nimport\nList\n,\nOptional\n,\nLiteral\nfrom\nlangchain_core.language_models.chat_models\nimport\nBaseChatModel\nfrom\nlanggraph.graph\nimport\nStateGraph\n,\nMessagesState\n,\nSTART\n,\nEND\nfrom\nlanggraph.types\nimport\nCommand\nfrom\nlangchain_core.messages\nimport\nHumanMessage\n,\ntrim_messages\nclass\nState\n(\nMessagesState\n):\nnext\n:\nstr\ndef\nmake_supervisor_node\n(\nllm\n:\nBaseChatModel\n,\nmembers\n:\nlist\n[\nstr\n])\n->\nstr\n:\noptions\n=\n[\n\"FINISH\"\n]\n+\nmembers\nsystem_prompt\n=\n(\n\"You are a supervisor tasked with managing a conversation between the\"\nf\n\" following workers:\n{\nmembers\n}\n. Given the following user request,\"\n\" respond with the worker to act next. Each worker will perform a\"\n\" task and respond with their results and status. When finished,\"\n\" respond with FINISH.\"\n)\nclass\nRouter\n(\nTypedDict\n):\n\"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\nnext\n:\nLiteral\n[\n*\noptions\n]\ndef\nsupervisor_node\n(\nstate\n:\nState\n)\n->\nCommand\n[\nLiteral\n[\n*\nmembers\n,\n\"__end__\"\n]]:\n\"\"\"An LLM-based router.\"\"\"\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\nsystem_prompt\n},\n]\n+\nstate\n[\n\"messages\"\n]\nresponse\n=\nllm\n.\nwith_structured_output\n(\nRouter\n)\n.\ninvoke\n(\nmessages\n)\ngoto\n=\nresponse\n[\n\"next\"\n]\nif\ngoto\n==\n\"FINISH\"\n:\ngoto\n=\nEND\nreturn\nCommand\n(\ngoto\n=\ngoto\n,\nupdate\n=\n{\n\"next\"\n:\ngoto\n})\nreturn\nsupervisor_node\nDefine Agent Teams\n\u00b6\nNow we can get to define our hierarchical teams. \"Choose your player!\"\nResearch Team\n\u00b6\nThe research team will have a search agent and a web scraping \"research_agent\" as the two worker nodes. Let's create those, as well as the team supervisor.\nAPI Reference:\nHumanMessage\n|\nChatOpenAI\n|\ncreate_react_agent\nfrom\nlangchain_core.messages\nimport\nHumanMessage\nfrom\nlangchain_openai\nimport\nChatOpenAI\nfrom\nlanggraph.prebuilt\nimport\ncreate_react_agent\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-5-mini\"\n)\nsearch_agent\n=\ncreate_react_agent\n(\nllm\n,\ntools\n=\n[\ntavily_tool\n])\ndef\nsearch_node\n(\nstate\n:\nState\n)\n->\nCommand\n[\nLiteral\n[\n\"supervisor\"\n]]:\nresult\n=\nsearch_agent\n.\ninvoke\n(\nstate\n)\nreturn\nCommand\n(\nupdate\n=\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\nresult\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n,\nname\n=\n\"search\"\n)\n]\n},\n# We want our workers to ALWAYS \"report back\" to the supervisor when done\ngoto\n=\n\"supervisor\"\n,\n)\nweb_scraper_agent\n=\ncreate_react_agent\n(\nllm\n,\ntools\n=\n[\nscrape_webpages\n])\ndef\nweb_scraper_node\n(\nstate\n:\nState\n)\n->\nCommand\n[\nLiteral\n[\n\"supervisor\"\n]]:\nresult\n=\nweb_scraper_agent\n.\ninvoke\n(\nstate\n)\nreturn\nCommand\n(\nupdate\n=\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\nresult\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n,\nname\n=\n\"web_scraper\"\n)\n]\n},\n# We want our workers to ALWAYS \"report back\" to the supervisor when done\ngoto\n=\n\"supervisor\"\n,\n)\nresearch_supervisor_node\n=\nmake_supervisor_node\n(\nllm\n,\n[\n\"search\"\n,\n\"web_scraper\"\n])\nNow that we've created the necessary components, defining their interactions is easy. Add the nodes to the team graph, and define the edges, which determine the transition criteria.\nresearch_builder\n=\nStateGraph\n(\nState\n)\nresearch_builder\n.\nadd_node\n(\n\"supervisor\"\n,\nresearch_supervisor_node\n)\nresearch_builder\n.\nadd_node\n(\n\"search\"\n,\nsearch_node\n)\nresearch_builder\n.\nadd_node\n(\n\"web_scraper\"\n,\nweb_scraper_node\n)\nresearch_builder\n.\nadd_edge\n(\nSTART\n,\n\"supervisor\"\n)\nresearch_graph\n=\nresearch_builder\n.\ncompile\n()\nfrom\nIPython.display\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\nresearch_graph\n.\nget_graph\n()\n.\ndraw_mermaid_png\n()))\nWe can give this team work directly. Try it out below.\nfor\ns\nin\nresearch_graph\n.\nstream\n(\n{\n\"messages\"\n:\n[(\n\"user\"\n,\n\"when is Taylor Swift's next tour?\"\n)]},\n{\n\"recursion_limit\"\n:\n100\n},\n):\nprint\n(\ns\n)\nprint\n(\n\"---\"\n)\n{'supervisor': {'next': 'search'}}\n---\n{'search': {'messages': [HumanMessage(content=\"Taylor Swift's next tour is The Eras Tour, which includes both U.S. and international dates. She announced additional U.S. dates for 2024. You can find more details about the tour and ticket information on platforms like Ticketmaster and official announcements.\", additional_kwargs={}, response_metadata={}, name='search', id='4df8687b-50a8-4342-aad5-680732c4a10f')]}}\n---\n{'supervisor': {'next': 'web_scraper'}}\n---\n{'web_scraper': {'messages': [HumanMessage(content='Taylor Swift\\'s next tour is \"The Eras Tour.\" Here are some of the upcoming international dates for 2024 that were listed on Ticketmaster:\\n\\n1. **Toronto, ON, Canada** at Rogers Centre\\n   - November 21, 2024\\n   - November 22, 2024\\n   - November 23, 2024\\n\\n2. **Vancouver, BC, Canada** at BC Place\\n   - December 6, 2024\\n   - December 7, 2024\\n   - December 8, 2024\\n\\nFor the most current information and additional dates, you can check platforms like Ticketmaster or Taylor Swift\\'s [official website](https://www.taylorswift.com/events).', additional_kwargs={}, response_metadata={}, name='web_scraper', id='27524ebc-d179-4733-831d-ee10a58a2528')]}}\n---\n{'supervisor': {'next': '__end__'}}\n---\nDocument Writing Team\n\u00b6\nCreate the document writing team below using a similar approach. This time, we will give each agent access to different file-writing tools.\nNote that we are giving file-system access to our agent here, which is not safe in all cases.\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-5-mini\"\n)\ndoc_writer_agent\n=\ncreate_react_agent\n(\nllm\n,\ntools\n=\n[\nwrite_document\n,\nedit_document\n,\nread_document\n],\nprompt\n=\n(\n\"You can read, write and edit documents based on note-taker's outlines. \"\n\"Don't ask follow-up questions.\"\n),\n)\ndef\ndoc_writing_node\n(\nstate\n:\nState\n)\n->\nCommand\n[\nLiteral\n[\n\"supervisor\"\n]]:\nresult\n=\ndoc_writer_agent\n.\ninvoke\n(\nstate\n)\nreturn\nCommand\n(\nupdate\n=\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\nresult\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n,\nname\n=\n\"doc_writer\"\n)\n]\n},\n# We want our workers to ALWAYS \"report back\" to the supervisor when done\ngoto\n=\n\"supervisor\"\n,\n)\nnote_taking_agent\n=\ncreate_react_agent\n(\nllm\n,\ntools\n=\n[\ncreate_outline\n,\nread_document\n],\nprompt\n=\n(\n\"You can read documents and create outlines for the document writer. \"\n\"Don't ask follow-up questions.\"\n),\n)\ndef\nnote_taking_node\n(\nstate\n:\nState\n)\n->\nCommand\n[\nLiteral\n[\n\"supervisor\"\n]]:\nresult\n=\nnote_taking_agent\n.\ninvoke\n(\nstate\n)\nreturn\nCommand\n(\nupdate\n=\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\nresult\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n,\nname\n=\n\"note_taker\"\n)\n]\n},\n# We want our workers to ALWAYS \"report back\" to the supervisor when done\ngoto\n=\n\"supervisor\"\n,\n)\nchart_generating_agent\n=\ncreate_react_agent\n(\nllm\n,\ntools\n=\n[\nread_document\n,\npython_repl_tool\n]\n)\ndef\nchart_generating_node\n(\nstate\n:\nState\n)\n->\nCommand\n[\nLiteral\n[\n\"supervisor\"\n]]:\nresult\n=\nchart_generating_agent\n.\ninvoke\n(\nstate\n)\nreturn\nCommand\n(\nupdate\n=\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\nresult\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n,\nname\n=\n\"chart_generator\"\n)\n]\n},\n# We want our workers to ALWAYS \"report back\" to the supervisor when done\ngoto\n=\n\"supervisor\"\n,\n)\ndoc_writing_supervisor_node\n=\nmake_supervisor_node\n(\nllm\n,\n[\n\"doc_writer\"\n,\n\"note_taker\"\n,\n\"chart_generator\"\n]\n)\nWith the objects themselves created, we can form the graph.\n# Create the graph here\npaper_writing_builder\n=\nStateGraph\n(\nState\n)\npaper_writing_builder\n.\nadd_node\n(\n\"supervisor\"\n,\ndoc_writing_supervisor_node\n)\npaper_writing_builder\n.\nadd_node\n(\n\"doc_writer\"\n,\ndoc_writing_node\n)\npaper_writing_builder\n.\nadd_node\n(\n\"note_taker\"\n,\nnote_taking_node\n)\npaper_writing_builder\n.\nadd_node\n(\n\"chart_generator\"\n,\nchart_generating_node\n)\npaper_writing_builder\n.\nadd_edge\n(\nSTART\n,\n\"supervisor\"\n)\npaper_writing_graph\n=\npaper_writing_builder\n.\ncompile\n()\nfrom\nIPython.display\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\npaper_writing_graph\n.\nget_graph\n()\n.\ndraw_mermaid_png\n()))\nfor\ns\nin\npaper_writing_graph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"Write an outline for poem about cats and then write the poem to disk.\"\n,\n)\n]\n},\n{\n\"recursion_limit\"\n:\n100\n},\n):\nprint\n(\ns\n)\nprint\n(\n\"---\"\n)\n{'supervisor': {'next': 'note_taker'}}\n---\n{'note_taker': {'messages': [HumanMessage(content='The outline for the poem about cats has been created and saved as \"cats_poem_outline.txt\".', additional_kwargs={}, response_metadata={}, name='note_taker', id='14a5d8ca-9092-416f-96ee-ba16686e8658')]}}\n---\n{'supervisor': {'next': 'doc_writer'}}\n---\n{'doc_writer': {'messages': [HumanMessage(content='The poem about cats has been written and saved as \"cats_poem.txt\".', additional_kwargs={}, response_metadata={}, name='doc_writer', id='c4e31a94-63ae-4632-9e80-1166f3f138b2')]}}\n---\n{'supervisor': {'next': '__end__'}}\n---\nAdd Layers\n\u00b6\nIn this design, we are enforcing a top-down planning policy. We've created two graphs already, but we have to decide how to route work between the two.\nWe'll create a\nthird\ngraph to orchestrate the previous two, and add some connectors to define how this top-level state is shared between the different graphs.\nAPI Reference:\nBaseMessage\nfrom\nlangchain_core.messages\nimport\nBaseMessage\nllm\n=\nChatOpenAI\n(\nmodel\n=\n\"gpt-5-mini\"\n)\nteams_supervisor_node\n=\nmake_supervisor_node\n(\nllm\n,\n[\n\"research_team\"\n,\n\"writing_team\"\n])\ndef\ncall_research_team\n(\nstate\n:\nState\n)\n->\nCommand\n[\nLiteral\n[\n\"supervisor\"\n]]:\nresponse\n=\nresearch_graph\n.\ninvoke\n({\n\"messages\"\n:\nstate\n[\n\"messages\"\n][\n-\n1\n]})\nreturn\nCommand\n(\nupdate\n=\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\nresponse\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n,\nname\n=\n\"research_team\"\n)\n]\n},\ngoto\n=\n\"supervisor\"\n,\n)\ndef\ncall_paper_writing_team\n(\nstate\n:\nState\n)\n->\nCommand\n[\nLiteral\n[\n\"supervisor\"\n]]:\nresponse\n=\npaper_writing_graph\n.\ninvoke\n({\n\"messages\"\n:\nstate\n[\n\"messages\"\n][\n-\n1\n]})\nreturn\nCommand\n(\nupdate\n=\n{\n\"messages\"\n:\n[\nHumanMessage\n(\ncontent\n=\nresponse\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n,\nname\n=\n\"writing_team\"\n)\n]\n},\ngoto\n=\n\"supervisor\"\n,\n)\n# Define the graph.\nsuper_builder\n=\nStateGraph\n(\nState\n)\nsuper_builder\n.\nadd_node\n(\n\"supervisor\"\n,\nteams_supervisor_node\n)\nsuper_builder\n.\nadd_node\n(\n\"research_team\"\n,\ncall_research_team\n)\nsuper_builder\n.\nadd_node\n(\n\"writing_team\"\n,\ncall_paper_writing_team\n)\nsuper_builder\n.\nadd_edge\n(\nSTART\n,\n\"supervisor\"\n)\nsuper_graph\n=\nsuper_builder\n.\ncompile\n()\nfrom\nIPython.display\nimport\nImage\n,\ndisplay\ndisplay\n(\nImage\n(\nsuper_graph\n.\nget_graph\n()\n.\ndraw_mermaid_png\n()))\nfor\ns\nin\nsuper_graph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"Research AI agents and write a brief report about them.\"\n)\n],\n},\n{\n\"recursion_limit\"\n:\n150\n},\n):\nprint\n(\ns\n)\nprint\n(\n\"---\"\n)\n{'supervisor': {'next': 'research_team'}}\n---\n{'research_team': {'messages': [HumanMessage(content=\"**AI Agents Overview 2023**\\n\\nAI agents are sophisticated technologies that automate and enhance various processes across industries, becoming increasingly integral to business operations. In 2023, these agents are notable for their advanced capabilities in communication, data visualization, and language processing.\\n\\n**Popular AI Agents in 2023:**\\n1. **Auto GPT**: This agent is renowned for its seamless integration abilities, significantly impacting industries by improving communication and operational workflows.\\n2. **ChartGPT**: Specializing in data visualization, ChartGPT enables users to interact with data innovatively, providing deeper insights and comprehension.\\n3. **LLMops**: With advanced language capabilities, LLMops is a versatile tool seeing widespread use across multiple sectors.\\n\\n**Market Trends:**\\nThe AI agents market is experiencing rapid growth, with significant advancements anticipated by 2030. There's a growing demand for AI agents in personalized interactions, particularly within customer service, healthcare, and marketing sectors. This trend is fueled by the need for more efficient and tailored customer experiences.\\n\\n**Key Players:**\\nLeading companies such as Microsoft, IBM, Google, Oracle, and AWS are key players in the AI agents market, highlighting the widespread adoption and investment in these technologies.\\n\\n**Technological Innovations:**\\nAI agents are being developed alongside simulation technologies for robust testing and deployment environments. Innovations in generative AI are accelerating, supported by advancements in large language models and platforms like ChatGPT.\\n\\n**Applications in Healthcare:**\\nIn healthcare, AI agents are automating routine tasks, allowing medical professionals to focus more on patient care. They're poised to significantly enhance healthcare delivery and efficiency.\\n\\n**Future Prospects:**\\nThe future of AI agents is promising, with continued evolution and integration into various platforms and ecosystems, offering more seamless and intelligent interactions. As these technologies advance, they are expected to redefine business operations and customer interactions.\", additional_kwargs={}, response_metadata={}, name='research_team', id='5f6606e0-838c-406c-b50d-9f9f6a076322')]}}\n---\n{'supervisor': {'next': 'writing_team'}}\n---\n{'writing_team': {'messages': [HumanMessage(content=\"Here are the contents of the documents:\\n\\n### AI Agents Overview 2023\\n\\n**AI Agents Overview 2023**\\n\\nAI agents are sophisticated technologies that automate and enhance various processes across industries, becoming increasingly integral to business operations. In 2023, these agents are notable for their advanced capabilities in communication, data visualization, and language processing.\\n\\n**Popular AI Agents in 2023:**\\n1. **Auto GPT**: This agent is renowned for its seamless integration abilities, significantly impacting industries by improving communication and operational workflows.\\n2. **ChartGPT**: Specializing in data visualization, ChartGPT enables users to interact with data innovatively, providing deeper insights and comprehension.\\n3. **LLMops**: With advanced language capabilities, LLMops is a versatile tool seeing widespread use across multiple sectors.\\n\\n**Market Trends:**\\nThe AI agents market is experiencing rapid growth, with significant advancements anticipated by 2030. There's a growing demand for AI agents in personalized interactions, particularly within customer service, healthcare, and marketing sectors. This trend is fueled by the need for more efficient and tailored customer experiences.\\n\\n**Key Players:**\\nLeading companies such as Microsoft, IBM, Google, Oracle, and AWS are key players in the AI agents market, highlighting the widespread adoption and investment in these technologies.\\n\\n**Technological Innovations:**\\nAI agents are being developed alongside simulation technologies for robust testing and deployment environments. Innovations in generative AI are accelerating, supported by advancements in large language models and platforms like ChatGPT.\\n\\n**Applications in Healthcare:**\\nIn healthcare, AI agents are automating routine tasks, allowing medical professionals to focus more on patient care. They're poised to significantly enhance healthcare delivery and efficiency.\\n\\n**Future Prospects:**\\nThe future of AI agents is promising, with continued evolution and integration into various platforms and ecosystems, offering more seamless and intelligent interactions. As these technologies advance, they are expected to redefine business operations and customer interactions.\\n\\n### AI_Agents_Overview_2023_Outline\\n\\n1. Introduction to AI Agents in 2023\\n2. Popular AI Agents: Auto GPT, ChartGPT, LLMops\\n3. Market Trends and Growth\\n4. Key Players in the AI Agents Market\\n5. Technological Innovations: Simulation and Generative AI\\n6. Applications of AI Agents in Healthcare\\n7. Future Prospects of AI Agents\", additional_kwargs={}, response_metadata={}, name='writing_team', id='851bd8a6-740e-488c-8928-1f9e05e96ea0')]}}\n---\n{'supervisor': {'next': 'writing_team'}}\n---\n{'writing_team': {'messages': [HumanMessage(content='The documents have been successfully created and saved:\\n\\n1. **AI_Agents_Overview_2023.txt** - Contains the detailed overview of AI agents in 2023.\\n2. **AI_Agents_Overview_2023_Outline.txt** - Contains the outline of the document.', additional_kwargs={}, response_metadata={}, name='writing_team', id='c87c0778-a085-4a8e-8ee1-9b43b9b0b143')]}}\n---\n{'supervisor': {'next': '__end__'}}\n---\nBack to top\nCopyright \u00a9 2025 LangChain, Inc |\nConsent Preferences\nMade with\nMaterial for MkDocs",
    "metadata": {
      "source": "https://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/",
      "title": "Hierarchical Agent Teams",
      "heading": "Hierarchical Agent Teams\u00b6"
    }
  },
  {
    "page_content": "Multi-agent network\nSkip to content\nThese docs will be deprecated and removed with the release of LangGraph v1.0 in October 2025.\nVisit the v1.0 alpha docs\nLangGraph\nMulti-agent network\nInitializing search\nGitHub\nReference\nLangGraph\nGitHub\nReference\nTable of contents\nSetup\nDefine tools\nCreate graph\nDefine Agent Nodes\nDefine the Graph\nInvoke\nMulti-agent network\n\u00b6\nA single agent can usually operate effectively using a handful of tools within a single domain, but even using powerful models like\ngpt-4\n, it can be less effective at using many tools.\nOne way to approach complicated tasks is through a \"divide-and-conquer\" approach: create a specialized agent for each task or domain and route tasks to the correct \"expert\". This is an example of a\nmulti-agent network\narchitecture.\nThis notebook (inspired by the paper\nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n, by Wu, et. al.) shows one way to do this using LangGraph.\nThe resulting graph will look something like the following diagram:\nBefore we get started, a quick note: this and other multi-agent notebooks are designed to show\nhow\nyou can implement certain design patterns in LangGraph. If the pattern suits your needs, we recommend combining it with some of the other fundamental patterns described elsewhere in the docs for best performance.\nSetup\n\u00b6\nFirst, let's install our required packages and set our API keys:\npip\ninstall\n-U\nlangchain_community\nlangchain_anthropic\nlangchain-tavily\nlangchain_experimental\nmatplotlib\nlanggraph\nimport\ngetpass\nimport\nos\ndef\n_set_if_undefined\n(\nvar\n:\nstr\n):\nif\nnot\nos\n.\nenviron\n.\nget\n(\nvar\n):\nos\n.\nenviron\n[\nvar\n]\n=\ngetpass\n.\ngetpass\n(\nf\n\"Please provide your\n{\nvar\n}\n\"\n)\n_set_if_undefined\n(\n\"ANTHROPIC_API_KEY\"\n)\n_set_if_undefined\n(\n\"TAVILY_API_KEY\"\n)\nSet up\nLangSmith\nfor LangGraph development\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started\nhere\n.\nDefine tools\n\u00b6\nWe will also define some tools that our agents will use in the future\nAPI Reference:\nTavilySearch\n|\ntool\n|\nPythonREPL\nfrom\ntyping\nimport\nAnnotated\nfrom\nlangchain_tavily\nimport\nTavilySearch\nfrom\nlangchain_core.tools\nimport\ntool\nfrom\nlangchain_experimental.utilities\nimport\nPythonREPL\ntavily_tool\n=\nTavilySearch\n(\nmax_results\n=\n5\n)\n# Warning: This executes code locally, which can be unsafe when not sandboxed\nrepl\n=\nPythonREPL\n()\n@tool\ndef\npython_repl_tool\n(\ncode\n:\nAnnotated\n[\nstr\n,\n\"The python code to execute to generate your chart.\"\n],\n):\n\"\"\"Use this to execute python code. If you want to see the output of a value,\nyou should print it out with `print(...)`. This is visible to the user.\"\"\"\ntry\n:\nresult\n=\nrepl\n.\nrun\n(\ncode\n)\nexcept\nBaseException\nas\ne\n:\nreturn\nf\n\"Failed to execute. Error:\n{\nrepr\n(\ne\n)\n}\n\"\nresult_str\n=\nf\n\"Successfully executed:\n\\n\n\\`\\`\\`python\n\\n\n{\ncode\n}\n\\n\n\\`\\`\\`\n\\n\nStdout:\n{\nresult\n}\n\"\nreturn\n(\nresult_str\n+\n\"\n\\n\\n\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n)\nCreate graph\n\u00b6\nNow that we've defined our tools and made some helper functions, will create the individual agents below and tell them how to talk to each other using LangGraph.\nDefine Agent Nodes\n\u00b6\nWe now need to define the nodes.\nFirst, we'll create a utility to create a system prompt for each agent.\ndef\nmake_system_prompt\n(\nsuffix\n:\nstr\n)\n->\nstr\n:\nreturn\n(\n\"You are a helpful AI assistant, collaborating with other assistants.\"\n\" Use the provided tools to progress towards answering the question.\"\n\" If you are unable to fully answer, that's OK, another assistant with different tools \"\n\" will help where you left off. Execute what you can to make progress.\"\n\" If you or any of the other assistants have the final answer or deliverable,\"\n\" prefix your response with FINAL ANSWER so the team knows to stop.\"\nf\n\"\n\\n\n{\nsuffix\n}\n\"\n)\nAPI Reference:\nBaseMessage\n|\nHumanMessage\n|\nChatAnthropic\n|\ncreate_react_agent\n|\nEND\n|\nCommand\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain_core.messages\nimport\nBaseMessage\n,\nHumanMessage\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\nfrom\nlanggraph.prebuilt\nimport\ncreate_react_agent\nfrom\nlanggraph.graph\nimport\nMessagesState\n,\nEND\nfrom\nlanggraph.types\nimport\nCommand\nllm\n=\nChatAnthropic\n(\nmodel\n=\n\"claude-3-5-sonnet-latest\"\n)\ndef\nget_next_node\n(\nlast_message\n:\nBaseMessage\n,\ngoto\n:\nstr\n):\nif\n\"FINAL ANSWER\"\nin\nlast_message\n.\ncontent\n:\n# Any agent decided the work is done\nreturn\nEND\nreturn\ngoto\n# Research agent and node\nresearch_agent\n=\ncreate_react_agent\n(\nllm\n,\ntools\n=\n[\ntavily_tool\n],\nprompt\n=\nmake_system_prompt\n(\n\"You can only do research. You are working with a chart generator colleague.\"\n),\n)\ndef\nresearch_node\n(\nstate\n:\nMessagesState\n,\n)\n->\nCommand\n[\nLiteral\n[\n\"chart_generator\"\n,\nEND\n]]:\nresult\n=\nresearch_agent\n.\ninvoke\n(\nstate\n)\ngoto\n=\nget_next_node\n(\nresult\n[\n\"messages\"\n][\n-\n1\n],\n\"chart_generator\"\n)\n# wrap in a human message, as not all providers allow\n# AI message at the last position of the input messages list\nresult\n[\n\"messages\"\n][\n-\n1\n]\n=\nHumanMessage\n(\ncontent\n=\nresult\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n,\nname\n=\n\"researcher\"\n)\nreturn\nCommand\n(\nupdate\n=\n{\n# share internal message history of research agent with other agents\n\"messages\"\n:\nresult\n[\n\"messages\"\n],\n},\ngoto\n=\ngoto\n,\n)\n# Chart generator agent and node\n# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\nchart_agent\n=\ncreate_react_agent\n(\nllm\n,\n[\npython_repl_tool\n],\nprompt\n=\nmake_system_prompt\n(\n\"You can only generate charts. You are working with a researcher colleague.\"\n),\n)\ndef\nchart_node\n(\nstate\n:\nMessagesState\n)\n->\nCommand\n[\nLiteral\n[\n\"researcher\"\n,\nEND\n]]:\nresult\n=\nchart_agent\n.\ninvoke\n(\nstate\n)\ngoto\n=\nget_next_node\n(\nresult\n[\n\"messages\"\n][\n-\n1\n],\n\"researcher\"\n)\n# wrap in a human message, as not all providers allow\n# AI message at the last position of the input messages list\nresult\n[\n\"messages\"\n][\n-\n1\n]\n=\nHumanMessage\n(\ncontent\n=\nresult\n[\n\"messages\"\n][\n-\n1\n]\n.\ncontent\n,\nname\n=\n\"chart_generator\"\n)\nreturn\nCommand\n(\nupdate\n=\n{\n# share internal message history of chart agent with other agents\n\"messages\"\n:\nresult\n[\n\"messages\"\n],\n},\ngoto\n=\ngoto\n,\n)\nDefine the Graph\n\u00b6\nWe can now put it all together and define the graph!\nAPI Reference:\nStateGraph\n|\nSTART\nfrom\nlanggraph.graph\nimport\nStateGraph\n,\nSTART\nworkflow\n=\nStateGraph\n(\nMessagesState\n)\nworkflow\n.\nadd_node\n(\n\"researcher\"\n,\nresearch_node\n)\nworkflow\n.\nadd_node\n(\n\"chart_generator\"\n,\nchart_node\n)\nworkflow\n.\nadd_edge\n(\nSTART\n,\n\"researcher\"\n)\ngraph\n=\nworkflow\n.\ncompile\n()\nfrom\nIPython.display\nimport\nImage\n,\ndisplay\ntry\n:\ndisplay\n(\nImage\n(\ngraph\n.\nget_graph\n()\n.\ndraw_mermaid_png\n()))\nexcept\nException\n:\n# This requires some extra dependencies and is optional\npass\nInvoke\n\u00b6\nWith the graph created, you can invoke it! Let's have it chart some stats for us.\nevents\n=\ngraph\n.\nstream\n(\n{\n\"messages\"\n:\n[\n(\n\"user\"\n,\n\"First, get the UK's GDP over the past 5 years, then make a line chart of it. \"\n\"Once you make the chart, finish.\"\n,\n)\n],\n},\n# Maximum number of steps to take in the graph\n{\n\"recursion_limit\"\n:\n150\n},\n)\nfor\ns\nin\nevents\n:\nprint\n(\ns\n)\nprint\n(\n\"----\"\n)\n{'researcher': {'messages': [HumanMessage(content=\"First, get the UK's GDP over the past 5 years, then make a line chart of it. Once you make the chart, finish.\", additional_kwargs={}, response_metadata={}, id='fa1f5e95-9e1a-47d4-b4b6-e93f345e339d'), AIMessage(content=[{'text': \"I'll help search for the UK's GDP data over the past 5 years. Then my colleague can help create the line chart.\", 'type': 'text'}, {'id': 'toolu_01Jd9dxa4Ss2NhzBhCuwUX3E', 'input': {'query': 'UK GDP annual data past 5 years 2019-2023'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_014nCkfVHnG6LAsiS6pY7zcd', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 555, 'output_tokens': 101}}, id='run-e2297529-9972-4de6-835d-23d920b0e29b-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'UK GDP annual data past 5 years 2019-2023'}, 'id': 'toolu_01Jd9dxa4Ss2NhzBhCuwUX3E', 'type': 'tool_call'}], usage_metadata={'input_tokens': 555, 'output_tokens': 101, 'total_tokens': 656, 'input_token_details': {}}), ToolMessage(content='[{\"url\": \"https://www.macrotrends.net/global-metrics/countries/GBR/united-kingdom/gdp-gross-domestic-product\", \"content\": \"Dollar figures for GDP are converted from domestic currencies using single year official exchange rates. For a few countries where the official exchange rate does not reflect the rate effectively applied to actual foreign exchange transactions, an alternative conversion factor is used. U.K. gdp for 2023 was $3,340.03B, a 8.13% increase from 2022.\"}, {\"url\": \"https://www.statista.com/topics/3795/gdp-of-the-uk/\", \"content\": \"Monthly growth of gross domestic product in the United Kingdom from January 2019 to November 2023\\\\nContribution to GDP growth in the UK 2023, by sector\\\\nContribution to gross domestic product growth in the United Kingdom in January 2023, by sector\\\\nGDP growth rate in the UK 1999-2021, by country\\\\nAnnual growth rates of gross domestic product in the United Kingdom from 1999 to 2021, by country\\\\nGDP growth rate in the UK 2021, by region\\\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\\\nGDP growth of Scotland 2021, by local area\\\\nAnnual growth rates of gross domestic product in Scotland in 2021, by local (ITL 3) area\\\\nGDP growth of Wales 2021, by local area\\\\nAnnual growth rates of gross domestic product in Wales in 2021, by local (ITL 3) area\\\\nGDP growth of Northern Ireland 2021, by local area\\\\nAnnual growth rates of gross domestic product in Northern Ireland in 2021, by local (ITL 3) area\\\\nGDP per capita\\\\nGDP per capita\\\\nGDP per capita in the UK 1955-2022\\\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\\\nAnnual GDP per capita growth in the UK 1956-2022\\\\nAnnual GDP per capita growth in the United Kingdom from 1956 to 2022\\\\nQuarterly GDP per capita in the UK 2019-2023\\\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\\\nQuarterly GDP per capita growth in the UK 2019-2023\\\\nQuarterly GDP per capita growth in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\\\nGDP per capita of the UK 1999-2021, by country\\\\nGross domestic product per capita of the United Kingdom from 1999 to 2021, by country (in GBP)\\\\nGDP per capita of the UK 2021, by region\\\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\\\nGlobal Comparisons\\\\nGlobal Comparisons\\\\nCountries with the largest gross domestic product (GDP) 2022\\\\n Monthly GDP of the UK 2019-2023\\\\nMonthly index of gross domestic product in the United Kingdom from January 2019 to November 2023 (2019=100)\\\\nGVA of the UK 2022, by sector\\\\nGross value added of the United Kingdom in 2022, by industry sector (in million GBP)\\\\nGDP of the UK 2021, by country\\\\nGross domestic product of the United Kingdom in 2021, by country (in million GBP)\\\\nGDP of the UK 2021, by region\\\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\\\nGDP of Scotland 2021, by local area\\\\nGross domestic product of Scotland in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP of Wales 2021, by local area\\\\nGross domestic product of Wales in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP of Northern Ireland 2021, by local area\\\\nGross domestic product of Northern Ireland in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP growth\\\\nGDP growth\\\\nGDP growth forecast for the UK 2000-2028\\\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\\\nAnnual GDP growth in the UK 1949-2022\\\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\\\nQuarterly GDP growth of the UK 2019-2023\\\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023\\\\nMonthly GDP growth of the UK 2019-2023\\\\n Transforming data into design:\\\\nStatista Content & Design\\\\nStrategy and business building for the data-driven economy:\\\\nUK GDP - Statistics & Facts\\\\nUK economy expected to shrink in 2023\\\\nCharacteristics of UK GDP\\\\nKey insights\\\\nDetailed statistics\\\\nGDP of the UK 1948-2022\\\\nDetailed statistics\\\\nAnnual GDP growth in the UK 1949-2022\\\\nDetailed statistics\\\\nGDP per capita in the UK 1955-2022\\\\nEditor\u2019s Picks\\\\nCurrent statistics on this topic\\\\nCurrent statistics on this topic\\\\nKey Economic Indicators\\\\nMonthly GDP growth of the UK 2019-2023\\\\nKey Economic Indicators\\\\nMonthly GDP of the UK 2019-2023\\\\nKey Economic Indicators\\\\nContribution to GDP growth in the UK 2023, by sector\\\\nRelated topics\\\\nRecommended\\\\nRecommended statistics\\\\nGDP\\\\nGDP\\\\nGDP of the UK 1948-2022\\\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\\\nQuarterly GDP of the UK 2019-2023\\\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in million GBP)\\\\n The 20 countries with the largest gross domestic product (GDP) in 2022 (in billion U.S. dollars)\\\\nGDP of European countries in 2022\\\\nGross domestic product at current market prices of selected European countries in 2022 (in million euros)\\\\nReal GDP growth rates in Europe 2023\\\\nAnnual real gross domestic product (GDP) growth rate in European countries in 2023\\\\nGross domestic product (GDP) of Europe\\'s largest economies 1980-2028\\\\nGross domestic product (GDP) at current prices of Europe\\'s largest economies from 1980 to 2028 (in billion U.S dollars)\\\\nUnited Kingdom\\'s share of global gross domestic product (GDP) 2028\\\\nUnited Kingdom (UK): Share of global gross domestic product (GDP) adjusted for Purchasing Power Parity (PPP) from 2018 to 2028\\\\nRelated topics\\\\nRecommended\\\\nReport on the topic\\\\nKey figures\\\\nThe most important key figures provide you with a compact summary of the topic of \\\\\"UK GDP\\\\\" and take you straight to the corresponding statistics.\\\\n Industry Overview\\\\nDigital & Trend reports\\\\nOverview and forecasts on trending topics\\\\nIndustry & Market reports\\\\nIndustry and market insights and forecasts\\\\nCompanies & Products reports\\\\nKey figures and rankings about companies and products\\\\nConsumer & Brand reports\\\\nConsumer and brand insights and preferences in various industries\\\\nPolitics & Society reports\\\\nDetailed information about political and social topics\\\\nCountry & Region reports\\\\nAll key figures about countries and regions\\\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\\\nInsights on consumer attitudes and behavior worldwide\\\\nBusiness information on 100m+ public and private companies\\\\nExplore Company Insights\\\\nDetailed information for 39,000+ online stores and marketplaces\\\\nDirectly accessible data for 170 industries from 150+ countries\\\\nand over 1\\xa0Mio. facts.\\\\n\"}, {\"url\": \"https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB\", \"content\": \"GDP growth (annual %) - United Kingdom | Data - World Bank Data\"}, {\"url\": \"https://www.statista.com/topics/6500/the-british-economy/\", \"content\": \"Output per hour worked in the UK 1971 to 2023\\\\nEconomic output per hour worked in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023 (2019=100)\\\\nAnnual unemployment rate in the UK 2000-2028\\\\nAnnual unemployment rate in the United Kingdom from 2000 to 2028\\\\nInflation\\\\nInflation\\\\nInflation rate in the UK 1989-2023\\\\nInflation rate for the Consumer Price Index (CPI) in the United Kingdom from January 1989 to October 2023\\\\nRPI inflation rate in the UK 1948-2023\\\\nInflation rate for the Retail Price Index (RPI) in the United Kingdom from June 1948 to October 2023\\\\nCPIH inflation rate in the UK 1989-2023\\\\nInflation rate for the Consumer Price Index including owner occupiers\\' housing costs (CPIH) in the United Kingdom from January 1989 to October 2023\\\\nPPI in the UK 2010-2023\\\\nProducer Price Index (PPI) in the United Kingdom from October 2010 to October 2023\\\\nCPI inflation rate in the UK 2023, by sector\\\\nInflation rate for the Consumer Price Index (CPI) in the United Kingdom in October 2023, by sector\\\\nConsumer Price Index in the UK 1988-2023\\\\nConsumer Price Index (CPI) in the United Kingdom from 1st quarter 1988 to 3rd quarter 2023\\\\nRetail Price Index in the UK 1987-2023\\\\nRetail Price Index (RPI) in the United Kingdom from 1st quarter 1987 to 3rd quarter 2023\\\\nConsumer Price Index including housing in the UK 1988-2023\\\\nConsumer Price Index including owner occupiers\\' housing costs (CPIH) in the United Kingdom from 1st quarter 1988 to 3rd quarter 2023\\\\nRPI annual inflation rate UK 2000-2028\\\\nAnnual inflation rate of the Retail Price Index in the United Kingdom from 2000 to 2028\\\\nCPI annual inflation rate UK 2000-2028\\\\nAnnual inflation rate of the Consumer Price Index in the United Kingdom from 2000 to 2028\\\\nGovernment finances\\\\nGovernment finances\\\\nGovernment spending as a percentage of GDP in the UK 1900-2029\\\\nTotal managed expenditure expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\\\nGovernment revenue as a percentage of GDP in the UK 1900-2029\\\\nTotal public sector current receipts expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29 (in million GBP)\\\\nGovernment borrowing as a percentage of GDP in the UK 1900-2029\\\\nPublic sector borrowing expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\\\nNational debt as a percentage of GDP in the UK 1900-2029\\\\nPublic sector net debt expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\\\nPublic sector spending in the United Kingdom 2023/24\\\\nBudgeted public sector expenditure on services in the United Kingdom in 2023/24, by function (in billion GBP)\\\\nGovernment revenue sources in the United Kingdom 2023/24\\\\nExpected public sector current receipts in the United Kingdom in 2023/24, by function (in billion GBP)\\\\nBusiness Enterprise\\\\nBusiness Enterprise\\\\nLargest companies in the United Kingdom based on revenue 2022\\\\nLargest companies in the United Kingdom based on revenue in 2022 (in billion US dollars)\\\\nLargest UK companies based on number of global employees 2020\\\\nLargest companies based in the United Kingdom on number of employees worldwide in 2020 (in 1,000s)\\\\nNumber of private sector businesses in the UK 2000-2023\\\\nNumber of private sector businesses in the United Kingdom from 2000 to 2023 (in millions)\\\\nNumber of private sector businesses in the UK 2023, by sector\\\\nNumber of private sector businesses in the United Kingdom in 2023, by sector\\\\nNumber of businesses by enterprise size in the UK 2023\\\\nNumber of private sector businesses in the United Kingdom in 2023, by employment size\\\\nNumber of private sector businesses in the UK 2023, by region\\\\nNumber of private sector businesses in the United Kingdom in 2023, by region\\\\nNumber of local business units in the UK 2012-2023\\\\nNumber of local units in VAT and/or PAYE based enterprises in the United Kingdom from 2012 to 2023 (in millions)\\\\nBusiness investment index in the UK 1997-2023\\\\nBusiness investment index in the United Kingdom from 1st quarter 1997 to 2nd quarter 2023 (Q1 1997=100)\\\\nBusiness confidence Index in the UK 1977-2023\\\\nBusiness confidence Index of the United Kingdom from March 1977 to November 2023 (100 = long-term average)\\\\nRelated topics\\\\nRecommended\\\\nReport on the topic\\\\nKey figures\\\\nThe most important key figures provide you with a compact summary of the topic of \\\\\"The UK economy\\\\\" and take you straight to the corresponding statistics.\\\\n Monthly GDP growth of the UK 2020-2023\\\\nMonthly growth of gross domestic product in the United Kingdom from January 2020 to September 2023\\\\nGDP of the UK 2021, by region\\\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\\\nGDP growth rate in the UK 2021, by region\\\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\\\nGDP per capita of the UK 2021, by region\\\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\\\nGDP growth forecast for the UK 2000-2028\\\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\\\nLabor Market\\\\nLabor Market\\\\nUnemployment rate of the UK 1971-2023\\\\nUnemployment rate in the United Kingdom from March 1971 to September 2023\\\\nEmployment rate in the UK 1971-2022\\\\nEmployment rate in the United Kingdom from March 1971 to July 2023\\\\nNumber of people unemployed in the UK 1971-2023\\\\nNumber of people unemployed in the United Kingdom from March 1971 to July 2023 (in 1,000s)\\\\nNumber of people employed in the UK 1971-2021\\\\nNumber of people employed in the United Kingdom from March 1971 to July 2023 (in 1,000s)\\\\nUnemployment rate in the UK 1971-2023, by gender\\\\nUnemployment rate in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023, by gender\\\\nUnemployment rate in the UK 1992-2023, by age group\\\\nUnemployment rate in the United Kingdom from May 1992 to July 2023, by age group\\\\nYouth unemployment rate in the UK 1992-2023\\\\nYouth unemployment rate in the United Kingdom from May 1992 to July 2023\\\\nAverage annual earnings for full-time employees in the UK 1999-2023\\\\nMedian annual earnings for full-time employees in the United Kingdom from 1999 to 2023 (in nominal GBP)\\\\nAverage weekly earning growth in the UK 2001-2023\\\\nAverage year-on-year growth of weekly earnings (3 month average) in the United Kingdom from March 2001 to October 2023\\\\nNumber of redundancies in the UK 1995-2023\\\\nAverage number of people made redundant in the United Kingdom from May 1995 to July 2023 (in 1,000s)\\\\nOverall weekly hours worked in the UK 1971-2023\\\\nOverall weekly hours worked for all employees in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023 (in million hours worked)\\\\n Transforming data into design:\\\\nStatista Content & Design\\\\nStrategy and business building for the data-driven economy:\\\\nThe UK economy - Statistics & Facts\\\\nUK households under pressure in 2023\\\\nCoronavirus devastates UK economy in 2020\\\\nKey insights\\\\nDetailed statistics\\\\nGDP of the UK 1948-2022\\\\nDetailed statistics\\\\nUnemployment rate of the UK 1971-2023\\\\nDetailed statistics\\\\nInflation rate in the UK 1989-2023\\\\nEditor\u2019s Picks\\\\nCurrent statistics on this topic\\\\nCurrent statistics on this topic\\\\nWages & Salaries\\\\nAverage weekly earning growth in the UK 2001-2023\\\\nIncome & Expenditure\\\\nPublic sector spending in the United Kingdom 2023/24\\\\nEmployment\\\\nNumber of people employed in the UK 1971-2021\\\\nRelated topics\\\\nRecommended\\\\nRecommended statistics\\\\nGross domestic product\\\\nGross domestic product\\\\nGDP of the UK 1948-2022\\\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\\\nAnnual GDP growth in the UK 1949-2022\\\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\\\nGDP per capita in the UK 1955-2022\\\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\\\nQuarterly GDP of the UK 1955-2023\\\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 1955 to 3rd quarter 2023 (in million GBP)\\\\nQuarterly GDP growth of the UK 2015-2023\\\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2015 to 3rd quarter 2023\\\\nQuarterly GDP per capita in the UK 1955-2023\\\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 1955 to 3rd quarter 2023 (in GBP)\\\\nMonthly GDP of the UK 1997-2023\\\\nMonthly index of gross domestic product in the United Kingdom from January 1997 to September 2023 (2019=100)\\\\n GDP\\\\nAnnual GDP growth in the UK 1949-2022\\\\nQuarterly GDP per capita growth in the UK 2015-2023\\\\nMonthly GDP growth of the UK 2020-2023\\\\nGDP per capita in the UK 1955-2022\\\\nLabor market\\\\nNumber of people employed in the UK 1971-2021\\\\nNumber of people unemployed in the UK 1971-2023\\\\nDaily number of jobs furloughed in the UK 2020-2021\\\\nAverage annual earnings for full-time employees in the UK 1999-2023\\\\nForecasts for 2023\\\\nGDP growth forecast for the UK 2000-2028\\\\nAnnual unemployment rate in the UK 2000-2028\\\\nCPI annual inflation rate UK 2000-2028\\\\nRPI annual inflation rate UK 2000-2028\\\\n Industry Overview\\\\nDigital & Trend reports\\\\nOverview and forecasts on trending topics\\\\nIndustry & Market reports\\\\nIndustry and market insights and forecasts\\\\nCompanies & Products reports\\\\nKey figures and rankings about companies and products\\\\nConsumer & Brand reports\\\\nConsumer and brand insights and preferences in various industries\\\\nPolitics & Society reports\\\\nDetailed information about political and social topics\\\\nCountry & Region reports\\\\nAll key figures about countries and regions\\\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\\\nInsights on consumer attitudes and behavior worldwide\\\\nBusiness information on 100m+ public and private companies\\\\nExplore Company Insights\\\\nDetailed information for 39,000+ online stores and marketplaces\\\\nDirectly accessible data for 170 industries from 150+ countries\\\\nand over 1\\xa0Mio. facts.\\\\n\"}, {\"url\": \"https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB&most_recent_value_desc=false\", \"content\": \"GDP growth (annual %) - United Kingdom | Data Data GDP growth (annual %)United Kingdom Data Catalog Data Programs International Debt Statistics Other Books and Reports For Developers GDP growth (annual %) - United Kingdom ====================================== Similar values Highest values Lowest values GDP (constant 2015 US$)  GDP (current US$)  GDP (constant LCU)  GDP: linked series (current LCU)  GDP, PPP (constant 2021 international $)  GDP (current LCU)  GDP, PPP (current international $)  GDP per capita growth (annual %)  Country Most Recent Value All Countries and Economies Country Most Recent Value This site uses cookies to optimize functionality and give you the best possible experience. If you continue to navigate this website beyond this page, cookies will be placed on your browser.\"}]', name='tavily_search_results_json', id='4c88089f-0ac4-4eeb-9141-722f0463b78d', tool_call_id='toolu_01Jd9dxa4Ss2NhzBhCuwUX3E', artifact={'query': 'UK GDP annual data past 5 years 2019-2023', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'U.K. GDP 1960-2024 - Macrotrends', 'url': 'https://www.macrotrends.net/global-metrics/countries/GBR/united-kingdom/gdp-gross-domestic-product', 'content': 'Dollar figures for GDP are converted from domestic currencies using single year official exchange rates. For a few countries where the official exchange rate does not reflect the rate effectively applied to actual foreign exchange transactions, an alternative conversion factor is used. U.K. gdp for 2023 was $3,340.03B, a 8.13% increase from 2022.', 'score': 0.97675806, 'raw_content': None}, {'title': 'UK GDP - Statistics & Facts | Statista', 'url': 'https://www.statista.com/topics/3795/gdp-of-the-uk/', 'content': 'Monthly growth of gross domestic product in the United Kingdom from January 2019 to November 2023\\nContribution to GDP growth in the UK 2023, by sector\\nContribution to gross domestic product growth in the United Kingdom in January 2023, by sector\\nGDP growth rate in the UK 1999-2021, by country\\nAnnual growth rates of gross domestic product in the United Kingdom from 1999 to 2021, by country\\nGDP growth rate in the UK 2021, by region\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\nGDP growth of Scotland 2021, by local area\\nAnnual growth rates of gross domestic product in Scotland in 2021, by local (ITL 3) area\\nGDP growth of Wales 2021, by local area\\nAnnual growth rates of gross domestic product in Wales in 2021, by local (ITL 3) area\\nGDP growth of Northern Ireland 2021, by local area\\nAnnual growth rates of gross domestic product in Northern Ireland in 2021, by local (ITL 3) area\\nGDP per capita\\nGDP per capita\\nGDP per capita in the UK 1955-2022\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\nAnnual GDP per capita growth in the UK 1956-2022\\nAnnual GDP per capita growth in the United Kingdom from 1956 to 2022\\nQuarterly GDP per capita in the UK 2019-2023\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\nQuarterly GDP per capita growth in the UK 2019-2023\\nQuarterly GDP per capita growth in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\nGDP per capita of the UK 1999-2021, by country\\nGross domestic product per capita of the United Kingdom from 1999 to 2021, by country (in GBP)\\nGDP per capita of the UK 2021, by region\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\nGlobal Comparisons\\nGlobal Comparisons\\nCountries with the largest gross domestic product (GDP) 2022\\n Monthly GDP of the UK 2019-2023\\nMonthly index of gross domestic product in the United Kingdom from January 2019 to November 2023 (2019=100)\\nGVA of the UK 2022, by sector\\nGross value added of the United Kingdom in 2022, by industry sector (in million GBP)\\nGDP of the UK 2021, by country\\nGross domestic product of the United Kingdom in 2021, by country (in million GBP)\\nGDP of the UK 2021, by region\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\nGDP of Scotland 2021, by local area\\nGross domestic product of Scotland in 2021, by local (ITL 3) area (in million GBP)\\nGDP of Wales 2021, by local area\\nGross domestic product of Wales in 2021, by local (ITL 3) area (in million GBP)\\nGDP of Northern Ireland 2021, by local area\\nGross domestic product of Northern Ireland in 2021, by local (ITL 3) area (in million GBP)\\nGDP growth\\nGDP growth\\nGDP growth forecast for the UK 2000-2028\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\nAnnual GDP growth in the UK 1949-2022\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\nQuarterly GDP growth of the UK 2019-2023\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023\\nMonthly GDP growth of the UK 2019-2023\\n Transforming data into design:\\nStatista Content & Design\\nStrategy and business building for the data-driven economy:\\nUK GDP - Statistics & Facts\\nUK economy expected to shrink in 2023\\nCharacteristics of UK GDP\\nKey insights\\nDetailed statistics\\nGDP of the UK 1948-2022\\nDetailed statistics\\nAnnual GDP growth in the UK 1949-2022\\nDetailed statistics\\nGDP per capita in the UK 1955-2022\\nEditor\u2019s Picks\\nCurrent statistics on this topic\\nCurrent statistics on this topic\\nKey Economic Indicators\\nMonthly GDP growth of the UK 2019-2023\\nKey Economic Indicators\\nMonthly GDP of the UK 2019-2023\\nKey Economic Indicators\\nContribution to GDP growth in the UK 2023, by sector\\nRelated topics\\nRecommended\\nRecommended statistics\\nGDP\\nGDP\\nGDP of the UK 1948-2022\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\nQuarterly GDP of the UK 2019-2023\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in million GBP)\\n The 20 countries with the largest gross domestic product (GDP) in 2022 (in billion U.S. dollars)\\nGDP of European countries in 2022\\nGross domestic product at current market prices of selected European countries in 2022 (in million euros)\\nReal GDP growth rates in Europe 2023\\nAnnual real gross domestic product (GDP) growth rate in European countries in 2023\\nGross domestic product (GDP) of Europe\\'s largest economies 1980-2028\\nGross domestic product (GDP) at current prices of Europe\\'s largest economies from 1980 to 2028 (in billion U.S dollars)\\nUnited Kingdom\\'s share of global gross domestic product (GDP) 2028\\nUnited Kingdom (UK): Share of global gross domestic product (GDP) adjusted for Purchasing Power Parity (PPP) from 2018 to 2028\\nRelated topics\\nRecommended\\nReport on the topic\\nKey figures\\nThe most important key figures provide you with a compact summary of the topic of \"UK GDP\" and take you straight to the corresponding statistics.\\n Industry Overview\\nDigital & Trend reports\\nOverview and forecasts on trending topics\\nIndustry & Market reports\\nIndustry and market insights and forecasts\\nCompanies & Products reports\\nKey figures and rankings about companies and products\\nConsumer & Brand reports\\nConsumer and brand insights and preferences in various industries\\nPolitics & Society reports\\nDetailed information about political and social topics\\nCountry & Region reports\\nAll key figures about countries and regions\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\nInsights on consumer attitudes and behavior worldwide\\nBusiness information on 100m+ public and private companies\\nExplore Company Insights\\nDetailed information for 39,000+ online stores and marketplaces\\nDirectly accessible data for 170 industries from 150+ countries\\nand over 1\\xa0Mio. facts.\\n', 'score': 0.97057647, 'raw_content': None}, {'title': 'GDP growth (annual %) - United Kingdom | Data - World Bank Data', 'url': 'https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB', 'content': 'GDP growth (annual %) - United Kingdom | Data - World Bank Data', 'score': 0.97052056, 'raw_content': None}, {'title': 'The UK economy - Statistics & Facts | Statista', 'url': 'https://www.statista.com/topics/6500/the-british-economy/', 'content': 'Output per hour worked in the UK 1971 to 2023\\nEconomic output per hour worked in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023 (2019=100)\\nAnnual unemployment rate in the UK 2000-2028\\nAnnual unemployment rate in the United Kingdom from 2000 to 2028\\nInflation\\nInflation\\nInflation rate in the UK 1989-2023\\nInflation rate for the Consumer Price Index (CPI) in the United Kingdom from January 1989 to October 2023\\nRPI inflation rate in the UK 1948-2023\\nInflation rate for the Retail Price Index (RPI) in the United Kingdom from June 1948 to October 2023\\nCPIH inflation rate in the UK 1989-2023\\nInflation rate for the Consumer Price Index including owner occupiers\\' housing costs (CPIH) in the United Kingdom from January 1989 to October 2023\\nPPI in the UK 2010-2023\\nProducer Price Index (PPI) in the United Kingdom from October 2010 to October 2023\\nCPI inflation rate in the UK 2023, by sector\\nInflation rate for the Consumer Price Index (CPI) in the United Kingdom in October 2023, by sector\\nConsumer Price Index in the UK 1988-2023\\nConsumer Price Index (CPI) in the United Kingdom from 1st quarter 1988 to 3rd quarter 2023\\nRetail Price Index in the UK 1987-2023\\nRetail Price Index (RPI) in the United Kingdom from 1st quarter 1987 to 3rd quarter 2023\\nConsumer Price Index including housing in the UK 1988-2023\\nConsumer Price Index including owner occupiers\\' housing costs (CPIH) in the United Kingdom from 1st quarter 1988 to 3rd quarter 2023\\nRPI annual inflation rate UK 2000-2028\\nAnnual inflation rate of the Retail Price Index in the United Kingdom from 2000 to 2028\\nCPI annual inflation rate UK 2000-2028\\nAnnual inflation rate of the Consumer Price Index in the United Kingdom from 2000 to 2028\\nGovernment finances\\nGovernment finances\\nGovernment spending as a percentage of GDP in the UK 1900-2029\\nTotal managed expenditure expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\nGovernment revenue as a percentage of GDP in the UK 1900-2029\\nTotal public sector current receipts expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29 (in million GBP)\\nGovernment borrowing as a percentage of GDP in the UK 1900-2029\\nPublic sector borrowing expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\nNational debt as a percentage of GDP in the UK 1900-2029\\nPublic sector net debt expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\nPublic sector spending in the United Kingdom 2023/24\\nBudgeted public sector expenditure on services in the United Kingdom in 2023/24, by function (in billion GBP)\\nGovernment revenue sources in the United Kingdom 2023/24\\nExpected public sector current receipts in the United Kingdom in 2023/24, by function (in billion GBP)\\nBusiness Enterprise\\nBusiness Enterprise\\nLargest companies in the United Kingdom based on revenue 2022\\nLargest companies in the United Kingdom based on revenue in 2022 (in billion US dollars)\\nLargest UK companies based on number of global employees 2020\\nLargest companies based in the United Kingdom on number of employees worldwide in 2020 (in 1,000s)\\nNumber of private sector businesses in the UK 2000-2023\\nNumber of private sector businesses in the United Kingdom from 2000 to 2023 (in millions)\\nNumber of private sector businesses in the UK 2023, by sector\\nNumber of private sector businesses in the United Kingdom in 2023, by sector\\nNumber of businesses by enterprise size in the UK 2023\\nNumber of private sector businesses in the United Kingdom in 2023, by employment size\\nNumber of private sector businesses in the UK 2023, by region\\nNumber of private sector businesses in the United Kingdom in 2023, by region\\nNumber of local business units in the UK 2012-2023\\nNumber of local units in VAT and/or PAYE based enterprises in the United Kingdom from 2012 to 2023 (in millions)\\nBusiness investment index in the UK 1997-2023\\nBusiness investment index in the United Kingdom from 1st quarter 1997 to 2nd quarter 2023 (Q1 1997=100)\\nBusiness confidence Index in the UK 1977-2023\\nBusiness confidence Index of the United Kingdom from March 1977 to November 2023 (100 = long-term average)\\nRelated topics\\nRecommended\\nReport on the topic\\nKey figures\\nThe most important key figures provide you with a compact summary of the topic of \"The UK economy\" and take you straight to the corresponding statistics.\\n Monthly GDP growth of the UK 2020-2023\\nMonthly growth of gross domestic product in the United Kingdom from January 2020 to September 2023\\nGDP of the UK 2021, by region\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\nGDP growth rate in the UK 2021, by region\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\nGDP per capita of the UK 2021, by region\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\nGDP growth forecast for the UK 2000-2028\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\nLabor Market\\nLabor Market\\nUnemployment rate of the UK 1971-2023\\nUnemployment rate in the United Kingdom from March 1971 to September 2023\\nEmployment rate in the UK 1971-2022\\nEmployment rate in the United Kingdom from March 1971 to July 2023\\nNumber of people unemployed in the UK 1971-2023\\nNumber of people unemployed in the United Kingdom from March 1971 to July 2023 (in 1,000s)\\nNumber of people employed in the UK 1971-2021\\nNumber of people employed in the United Kingdom from March 1971 to July 2023 (in 1,000s)\\nUnemployment rate in the UK 1971-2023, by gender\\nUnemployment rate in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023, by gender\\nUnemployment rate in the UK 1992-2023, by age group\\nUnemployment rate in the United Kingdom from May 1992 to July 2023, by age group\\nYouth unemployment rate in the UK 1992-2023\\nYouth unemployment rate in the United Kingdom from May 1992 to July 2023\\nAverage annual earnings for full-time employees in the UK 1999-2023\\nMedian annual earnings for full-time employees in the United Kingdom from 1999 to 2023 (in nominal GBP)\\nAverage weekly earning growth in the UK 2001-2023\\nAverage year-on-year growth of weekly earnings (3 month average) in the United Kingdom from March 2001 to October 2023\\nNumber of redundancies in the UK 1995-2023\\nAverage number of people made redundant in the United Kingdom from May 1995 to July 2023 (in 1,000s)\\nOverall weekly hours worked in the UK 1971-2023\\nOverall weekly hours worked for all employees in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023 (in million hours worked)\\n Transforming data into design:\\nStatista Content & Design\\nStrategy and business building for the data-driven economy:\\nThe UK economy - Statistics & Facts\\nUK households under pressure in 2023\\nCoronavirus devastates UK economy in 2020\\nKey insights\\nDetailed statistics\\nGDP of the UK 1948-2022\\nDetailed statistics\\nUnemployment rate of the UK 1971-2023\\nDetailed statistics\\nInflation rate in the UK 1989-2023\\nEditor\u2019s Picks\\nCurrent statistics on this topic\\nCurrent statistics on this topic\\nWages & Salaries\\nAverage weekly earning growth in the UK 2001-2023\\nIncome & Expenditure\\nPublic sector spending in the United Kingdom 2023/24\\nEmployment\\nNumber of people employed in the UK 1971-2021\\nRelated topics\\nRecommended\\nRecommended statistics\\nGross domestic product\\nGross domestic product\\nGDP of the UK 1948-2022\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\nAnnual GDP growth in the UK 1949-2022\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\nGDP per capita in the UK 1955-2022\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\nQuarterly GDP of the UK 1955-2023\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 1955 to 3rd quarter 2023 (in million GBP)\\nQuarterly GDP growth of the UK 2015-2023\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2015 to 3rd quarter 2023\\nQuarterly GDP per capita in the UK 1955-2023\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 1955 to 3rd quarter 2023 (in GBP)\\nMonthly GDP of the UK 1997-2023\\nMonthly index of gross domestic product in the United Kingdom from January 1997 to September 2023 (2019=100)\\n GDP\\nAnnual GDP growth in the UK 1949-2022\\nQuarterly GDP per capita growth in the UK 2015-2023\\nMonthly GDP growth of the UK 2020-2023\\nGDP per capita in the UK 1955-2022\\nLabor market\\nNumber of people employed in the UK 1971-2021\\nNumber of people unemployed in the UK 1971-2023\\nDaily number of jobs furloughed in the UK 2020-2021\\nAverage annual earnings for full-time employees in the UK 1999-2023\\nForecasts for 2023\\nGDP growth forecast for the UK 2000-2028\\nAnnual unemployment rate in the UK 2000-2028\\nCPI annual inflation rate UK 2000-2028\\nRPI annual inflation rate UK 2000-2028\\n Industry Overview\\nDigital & Trend reports\\nOverview and forecasts on trending topics\\nIndustry & Market reports\\nIndustry and market insights and forecasts\\nCompanies & Products reports\\nKey figures and rankings about companies and products\\nConsumer & Brand reports\\nConsumer and brand insights and preferences in various industries\\nPolitics & Society reports\\nDetailed information about political and social topics\\nCountry & Region reports\\nAll key figures about countries and regions\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\nInsights on consumer attitudes and behavior worldwide\\nBusiness information on 100m+ public and private companies\\nExplore Company Insights\\nDetailed information for 39,000+ online stores and marketplaces\\nDirectly accessible data for 170 industries from 150+ countries\\nand over 1\\xa0Mio. facts.\\n', 'score': 0.95998776, 'raw_content': None}, {'title': 'GDP growth (annual %) - United Kingdom | Data - World Bank Data', 'url': 'https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB&most_recent_value_desc=false', 'content': 'GDP growth (annual %) - United Kingdom | Data Data GDP growth (annual %)United Kingdom Data Catalog Data Programs International Debt Statistics Other Books and Reports For Developers GDP growth (annual %) - United Kingdom ====================================== Similar values Highest values Lowest values GDP (constant 2015 US$)  GDP (current US$)  GDP (constant LCU)  GDP: linked series (current LCU)  GDP, PPP (constant 2021 international $)  GDP (current LCU)  GDP, PPP (current international $)  GDP per capita growth (annual %)  Country Most Recent Value All Countries and Economies Country Most Recent Value This site uses cookies to optimize functionality and give you the best possible experience. If you continue to navigate this website beyond this page, cookies will be placed on your browser.', 'score': 0.7892337, 'raw_content': None}], 'response_time': 2.3}), AIMessage(content=[{'text': 'Let me search for more specific data.', 'type': 'text'}, {'id': 'toolu_019dPRXojLJoVNYFLzzSWw4w', 'input': {'query': 'UK GDP values by year 2019 2020 2021 2022 2023'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01Ac9vcTFneb5dvcEYXJyf1P', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 5890, 'output_tokens': 87}}, id='run-3504417f-c0b5-4908-82e2-89a18abb1b8e-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'UK GDP values by year 2019 2020 2021 2022 2023'}, 'id': 'toolu_019dPRXojLJoVNYFLzzSWw4w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 5890, 'output_tokens': 87, 'total_tokens': 5977, 'input_token_details': {}}), ToolMessage(content='[{\"url\": \"https://www.macrotrends.net/global-metrics/countries/GBR/united-kingdom/gdp-gross-domestic-product\", \"content\": \"U.K. gdp for 2023 was $3,340.03B, a 8.13% increase from 2022. U.K. gdp for 2022 was $3,088.84B, a 1.68% decline from 2021. U.K. gdp for 2021 was $3,141.51B, a 16.45% increase from 2020. U.K. gdp for 2020 was $2,697.81B, a 5.39% decline from 2019.\"}, {\"url\": \"https://countryeconomy.com/gdp/uk?year=2023\", \"content\": \"Gross Domestic Product of United Kingdom grew 0.3% in 2023 compared to last year. This rate is 45 -tenths of one percent less than the figure of 4.8% published in 2022. The GDP figure in 2023 was $3,380,855 million, leaving United Kingdom placed 6th in the ranking of GDP of the 196 countries that we publish.\"}, {\"url\": \"https://www.statista.com/topics/3795/gdp-of-the-uk/\", \"content\": \"Monthly growth of gross domestic product in the United Kingdom from January 2019 to November 2023\\\\nContribution to GDP growth in the UK 2023, by sector\\\\nContribution to gross domestic product growth in the United Kingdom in January 2023, by sector\\\\nGDP growth rate in the UK 1999-2021, by country\\\\nAnnual growth rates of gross domestic product in the United Kingdom from 1999 to 2021, by country\\\\nGDP growth rate in the UK 2021, by region\\\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\\\nGDP growth of Scotland 2021, by local area\\\\nAnnual growth rates of gross domestic product in Scotland in 2021, by local (ITL 3) area\\\\nGDP growth of Wales 2021, by local area\\\\nAnnual growth rates of gross domestic product in Wales in 2021, by local (ITL 3) area\\\\nGDP growth of Northern Ireland 2021, by local area\\\\nAnnual growth rates of gross domestic product in Northern Ireland in 2021, by local (ITL 3) area\\\\nGDP per capita\\\\nGDP per capita\\\\nGDP per capita in the UK 1955-2022\\\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\\\nAnnual GDP per capita growth in the UK 1956-2022\\\\nAnnual GDP per capita growth in the United Kingdom from 1956 to 2022\\\\nQuarterly GDP per capita in the UK 2019-2023\\\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\\\nQuarterly GDP per capita growth in the UK 2019-2023\\\\nQuarterly GDP per capita growth in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\\\nGDP per capita of the UK 1999-2021, by country\\\\nGross domestic product per capita of the United Kingdom from 1999 to 2021, by country (in GBP)\\\\nGDP per capita of the UK 2021, by region\\\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\\\nGlobal Comparisons\\\\nGlobal Comparisons\\\\nCountries with the largest gross domestic product (GDP) 2022\\\\n Monthly GDP of the UK 2019-2023\\\\nMonthly index of gross domestic product in the United Kingdom from January 2019 to November 2023 (2019=100)\\\\nGVA of the UK 2022, by sector\\\\nGross value added of the United Kingdom in 2022, by industry sector (in million GBP)\\\\nGDP of the UK 2021, by country\\\\nGross domestic product of the United Kingdom in 2021, by country (in million GBP)\\\\nGDP of the UK 2021, by region\\\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\\\nGDP of Scotland 2021, by local area\\\\nGross domestic product of Scotland in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP of Wales 2021, by local area\\\\nGross domestic product of Wales in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP of Northern Ireland 2021, by local area\\\\nGross domestic product of Northern Ireland in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP growth\\\\nGDP growth\\\\nGDP growth forecast for the UK 2000-2028\\\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\\\nAnnual GDP growth in the UK 1949-2022\\\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\\\nQuarterly GDP growth of the UK 2019-2023\\\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023\\\\nMonthly GDP growth of the UK 2019-2023\\\\n Transforming data into design:\\\\nStatista Content & Design\\\\nStrategy and business building for the data-driven economy:\\\\nUK GDP - Statistics & Facts\\\\nUK economy expected to shrink in 2023\\\\nCharacteristics of UK GDP\\\\nKey insights\\\\nDetailed statistics\\\\nGDP of the UK 1948-2022\\\\nDetailed statistics\\\\nAnnual GDP growth in the UK 1949-2022\\\\nDetailed statistics\\\\nGDP per capita in the UK 1955-2022\\\\nEditor\u2019s Picks\\\\nCurrent statistics on this topic\\\\nCurrent statistics on this topic\\\\nKey Economic Indicators\\\\nMonthly GDP growth of the UK 2019-2023\\\\nKey Economic Indicators\\\\nMonthly GDP of the UK 2019-2023\\\\nKey Economic Indicators\\\\nContribution to GDP growth in the UK 2023, by sector\\\\nRelated topics\\\\nRecommended\\\\nRecommended statistics\\\\nGDP\\\\nGDP\\\\nGDP of the UK 1948-2022\\\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\\\nQuarterly GDP of the UK 2019-2023\\\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in million GBP)\\\\n The 20 countries with the largest gross domestic product (GDP) in 2022 (in billion U.S. dollars)\\\\nGDP of European countries in 2022\\\\nGross domestic product at current market prices of selected European countries in 2022 (in million euros)\\\\nReal GDP growth rates in Europe 2023\\\\nAnnual real gross domestic product (GDP) growth rate in European countries in 2023\\\\nGross domestic product (GDP) of Europe\\'s largest economies 1980-2028\\\\nGross domestic product (GDP) at current prices of Europe\\'s largest economies from 1980 to 2028 (in billion U.S dollars)\\\\nUnited Kingdom\\'s share of global gross domestic product (GDP) 2028\\\\nUnited Kingdom (UK): Share of global gross domestic product (GDP) adjusted for Purchasing Power Parity (PPP) from 2018 to 2028\\\\nRelated topics\\\\nRecommended\\\\nReport on the topic\\\\nKey figures\\\\nThe most important key figures provide you with a compact summary of the topic of \\\\\"UK GDP\\\\\" and take you straight to the corresponding statistics.\\\\n Industry Overview\\\\nDigital & Trend reports\\\\nOverview and forecasts on trending topics\\\\nIndustry & Market reports\\\\nIndustry and market insights and forecasts\\\\nCompanies & Products reports\\\\nKey figures and rankings about companies and products\\\\nConsumer & Brand reports\\\\nConsumer and brand insights and preferences in various industries\\\\nPolitics & Society reports\\\\nDetailed information about political and social topics\\\\nCountry & Region reports\\\\nAll key figures about countries and regions\\\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\\\nInsights on consumer attitudes and behavior worldwide\\\\nBusiness information on 100m+ public and private companies\\\\nExplore Company Insights\\\\nDetailed information for 39,000+ online stores and marketplaces\\\\nDirectly accessible data for 170 industries from 150+ countries\\\\nand over 1\\xa0Mio. facts.\\\\n\"}, {\"url\": \"https://www.ons.gov.uk/economy/grossdomesticproductgdp/compendium/unitedkingdomnationalaccountsthebluebook/2024/nationalaccountsataglance\", \"content\": \"Real gross domestic product (GDP) is estimated to have increased by 0.3% in 2023, following a recovery from the impacts of the coronavirus (COVID-19) pandemic over the two previous years (Figure 1). Data for the UK are the Office for National Statistics (ONS) measure of real gross domestic product (GDP). Figure 9: Real GDP per head fell in 2023 when compared with 2022 in six G10 economies, including the UK Data for the UK are the Office for National Statistics (ONS) measure of real gross domestic product (GDP) per head. Download this chart Figure 9: Real GDP per head fell in 2023 when compared with 2022 in six G10 economies, including the UK\"}, {\"url\": \"https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB&most_recent_value_desc=false\", \"content\": \"GDP growth (annual %) - United Kingdom | Data Data GDP growth (annual %)United Kingdom Data Catalog Data Programs International Debt Statistics Other Books and Reports For Developers GDP growth (annual %) - United Kingdom ====================================== Similar values Highest values Lowest values GDP (constant 2015 US$)  GDP (current US$)  GDP (constant LCU)  GDP: linked series (current LCU)  GDP, PPP (constant 2021 international $)  GDP (current LCU)  GDP, PPP (current international $)  GDP per capita growth (annual %)  Country Most Recent Value All Countries and Economies Country Most Recent Value This site uses cookies to optimize functionality and give you the best possible experience. If you continue to navigate this website beyond this page, cookies will be placed on your browser.\"}]', name='tavily_search_results_json', id='84c571ca-27c6-4023-93a2-f0c2e8b6abb0', tool_call_id='toolu_019dPRXojLJoVNYFLzzSWw4w', artifact={'query': 'UK GDP values by year 2019 2020 2021 2022 2023', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'U.K. GDP 1960-2024 - Macrotrends', 'url': 'https://www.macrotrends.net/global-metrics/countries/GBR/united-kingdom/gdp-gross-domestic-product', 'content': 'U.K. gdp for 2023 was $3,340.03B, a 8.13% increase from 2022. U.K. gdp for 2022 was $3,088.84B, a 1.68% decline from 2021. U.K. gdp for 2021 was $3,141.51B, a 16.45% increase from 2020. U.K. gdp for 2020 was $2,697.81B, a 5.39% decline from 2019.', 'score': 0.9974491, 'raw_content': None}, {'title': 'United Kingdom (UK) GDP - Gross Domestic Product 2023', 'url': 'https://countryeconomy.com/gdp/uk?year=2023', 'content': 'Gross Domestic Product of United Kingdom grew 0.3% in 2023 compared to last year. This rate is 45 -tenths of one percent less than the figure of 4.8% published in 2022. The GDP figure in 2023 was $3,380,855 million, leaving United Kingdom placed 6th in the ranking of GDP of the 196 countries that we publish.', 'score': 0.9964064, 'raw_content': None}, {'title': 'UK GDP - Statistics & Facts | Statista', 'url': 'https://www.statista.com/topics/3795/gdp-of-the-uk/', 'content': 'Monthly growth of gross domestic product in the United Kingdom from January 2019 to November 2023\\nContribution to GDP growth in the UK 2023, by sector\\nContribution to gross domestic product growth in the United Kingdom in January 2023, by sector\\nGDP growth rate in the UK 1999-2021, by country\\nAnnual growth rates of gross domestic product in the United Kingdom from 1999 to 2021, by country\\nGDP growth rate in the UK 2021, by region\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\nGDP growth of Scotland 2021, by local area\\nAnnual growth rates of gross domestic product in Scotland in 2021, by local (ITL 3) area\\nGDP growth of Wales 2021, by local area\\nAnnual growth rates of gross domestic product in Wales in 2021, by local (ITL 3) area\\nGDP growth of Northern Ireland 2021, by local area\\nAnnual growth rates of gross domestic product in Northern Ireland in 2021, by local (ITL 3) area\\nGDP per capita\\nGDP per capita\\nGDP per capita in the UK 1955-2022\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\nAnnual GDP per capita growth in the UK 1956-2022\\nAnnual GDP per capita growth in the United Kingdom from 1956 to 2022\\nQuarterly GDP per capita in the UK 2019-2023\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\nQuarterly GDP per capita growth in the UK 2019-2023\\nQuarterly GDP per capita growth in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\nGDP per capita of the UK 1999-2021, by country\\nGross domestic product per capita of the United Kingdom from 1999 to 2021, by country (in GBP)\\nGDP per capita of the UK 2021, by region\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\nGlobal Comparisons\\nGlobal Comparisons\\nCountries with the largest gross domestic product (GDP) 2022\\n Monthly GDP of the UK 2019-2023\\nMonthly index of gross domestic product in the United Kingdom from January 2019 to November 2023 (2019=100)\\nGVA of the UK 2022, by sector\\nGross value added of the United Kingdom in 2022, by industry sector (in million GBP)\\nGDP of the UK 2021, by country\\nGross domestic product of the United Kingdom in 2021, by country (in million GBP)\\nGDP of the UK 2021, by region\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\nGDP of Scotland 2021, by local area\\nGross domestic product of Scotland in 2021, by local (ITL 3) area (in million GBP)\\nGDP of Wales 2021, by local area\\nGross domestic product of Wales in 2021, by local (ITL 3) area (in million GBP)\\nGDP of Northern Ireland 2021, by local area\\nGross domestic product of Northern Ireland in 2021, by local (ITL 3) area (in million GBP)\\nGDP growth\\nGDP growth\\nGDP growth forecast for the UK 2000-2028\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\nAnnual GDP growth in the UK 1949-2022\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\nQuarterly GDP growth of the UK 2019-2023\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023\\nMonthly GDP growth of the UK 2019-2023\\n Transforming data into design:\\nStatista Content & Design\\nStrategy and business building for the data-driven economy:\\nUK GDP - Statistics & Facts\\nUK economy expected to shrink in 2023\\nCharacteristics of UK GDP\\nKey insights\\nDetailed statistics\\nGDP of the UK 1948-2022\\nDetailed statistics\\nAnnual GDP growth in the UK 1949-2022\\nDetailed statistics\\nGDP per capita in the UK 1955-2022\\nEditor\u2019s Picks\\nCurrent statistics on this topic\\nCurrent statistics on this topic\\nKey Economic Indicators\\nMonthly GDP growth of the UK 2019-2023\\nKey Economic Indicators\\nMonthly GDP of the UK 2019-2023\\nKey Economic Indicators\\nContribution to GDP growth in the UK 2023, by sector\\nRelated topics\\nRecommended\\nRecommended statistics\\nGDP\\nGDP\\nGDP of the UK 1948-2022\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\nQuarterly GDP of the UK 2019-2023\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in million GBP)\\n The 20 countries with the largest gross domestic product (GDP) in 2022 (in billion U.S. dollars)\\nGDP of European countries in 2022\\nGross domestic product at current market prices of selected European countries in 2022 (in million euros)\\nReal GDP growth rates in Europe 2023\\nAnnual real gross domestic product (GDP) growth rate in European countries in 2023\\nGross domestic product (GDP) of Europe\\'s largest economies 1980-2028\\nGross domestic product (GDP) at current prices of Europe\\'s largest economies from 1980 to 2028 (in billion U.S dollars)\\nUnited Kingdom\\'s share of global gross domestic product (GDP) 2028\\nUnited Kingdom (UK): Share of global gross domestic product (GDP) adjusted for Purchasing Power Parity (PPP) from 2018 to 2028\\nRelated topics\\nRecommended\\nReport on the topic\\nKey figures\\nThe most important key figures provide you with a compact summary of the topic of \"UK GDP\" and take you straight to the corresponding statistics.\\n Industry Overview\\nDigital & Trend reports\\nOverview and forecasts on trending topics\\nIndustry & Market reports\\nIndustry and market insights and forecasts\\nCompanies & Products reports\\nKey figures and rankings about companies and products\\nConsumer & Brand reports\\nConsumer and brand insights and preferences in various industries\\nPolitics & Society reports\\nDetailed information about political and social topics\\nCountry & Region reports\\nAll key figures about countries and regions\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\nInsights on consumer attitudes and behavior worldwide\\nBusiness information on 100m+ public and private companies\\nExplore Company Insights\\nDetailed information for 39,000+ online stores and marketplaces\\nDirectly accessible data for 170 industries from 150+ countries\\nand over 1\\xa0Mio. facts.\\n', 'score': 0.97943294, 'raw_content': None}, {'title': 'National accounts at a glance - Office for National Statistics', 'url': 'https://www.ons.gov.uk/economy/grossdomesticproductgdp/compendium/unitedkingdomnationalaccountsthebluebook/2024/nationalaccountsataglance', 'content': 'Real gross domestic product (GDP) is estimated to have increased by 0.3% in 2023, following a recovery from the impacts of the coronavirus (COVID-19) pandemic over the two previous years (Figure 1). Data for the UK are the Office for National Statistics (ONS) measure of real gross domestic product (GDP). Figure 9: Real GDP per head fell in 2023 when compared with 2022 in six G10 economies, including the UK Data for the UK are the Office for National Statistics (ONS) measure of real gross domestic product (GDP) per head. Download this chart Figure 9: Real GDP per head fell in 2023 when compared with 2022 in six G10 economies, including the UK', 'score': 0.975249, 'raw_content': None}, {'title': 'GDP growth (annual %) - United Kingdom | Data - World Bank Data', 'url': 'https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB&most_recent_value_desc=false', 'content': 'GDP growth (annual %) - United Kingdom | Data Data GDP growth (annual %)United Kingdom Data Catalog Data Programs International Debt Statistics Other Books and Reports For Developers GDP growth (annual %) - United Kingdom ====================================== Similar values Highest values Lowest values GDP (constant 2015 US$)  GDP (current US$)  GDP (constant LCU)  GDP: linked series (current LCU)  GDP, PPP (constant 2021 international $)  GDP (current LCU)  GDP, PPP (current international $)  GDP per capita growth (annual %)  Country Most Recent Value All Countries and Economies Country Most Recent Value This site uses cookies to optimize functionality and give you the best possible experience. If you continue to navigate this website beyond this page, cookies will be placed on your browser.', 'score': 0.83775276, 'raw_content': None}], 'response_time': 2.37}), HumanMessage(content='Based on the search results, I can provide the UK\\'s GDP values for the past 5 years (in billions of US dollars):\\n\\n2019: $2,851.54\\n2020: $2,697.81\\n2021: $3,141.51\\n2022: $3,088.84\\n2023: $3,340.03\\n\\nI\\'ll pass this data to my chart generator colleague to create a line chart. They should create a line chart with:\\n- Years 2019-2023 on the x-axis\\n- GDP values in billions USD on the y-axis\\n- Title: \"UK GDP 2019-2023\"\\n- Clear data points showing the values\\n\\nOver to you, chart generator colleague!', additional_kwargs={}, response_metadata={}, name='researcher', id='7e790b7a-7b06-4b45-a595-8736b53db844')]}}\n----\n``````output\nPython REPL can execute arbitrary code. Use with caution.\n{'chart_generator': {'messages': [HumanMessage(content=\"First, get the UK's GDP over the past 5 years, then make a line chart of it. Once you make the chart, finish.\", additional_kwargs={}, response_metadata={}, id='fa1f5e95-9e1a-47d4-b4b6-e93f345e339d'), AIMessage(content=[{'text': \"I'll help search for the UK's GDP data over the past 5 years. Then my colleague can help create the line chart.\", 'type': 'text'}, {'id': 'toolu_01Jd9dxa4Ss2NhzBhCuwUX3E', 'input': {'query': 'UK GDP annual data past 5 years 2019-2023'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_014nCkfVHnG6LAsiS6pY7zcd', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 555, 'output_tokens': 101}}, id='run-e2297529-9972-4de6-835d-23d920b0e29b-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'UK GDP annual data past 5 years 2019-2023'}, 'id': 'toolu_01Jd9dxa4Ss2NhzBhCuwUX3E', 'type': 'tool_call'}], usage_metadata={'input_tokens': 555, 'output_tokens': 101, 'total_tokens': 656, 'input_token_details': {}}), ToolMessage(content='[{\"url\": \"https://www.macrotrends.net/global-metrics/countries/GBR/united-kingdom/gdp-gross-domestic-product\", \"content\": \"Dollar figures for GDP are converted from domestic currencies using single year official exchange rates. For a few countries where the official exchange rate does not reflect the rate effectively applied to actual foreign exchange transactions, an alternative conversion factor is used. U.K. gdp for 2023 was $3,340.03B, a 8.13% increase from 2022.\"}, {\"url\": \"https://www.statista.com/topics/3795/gdp-of-the-uk/\", \"content\": \"Monthly growth of gross domestic product in the United Kingdom from January 2019 to November 2023\\\\nContribution to GDP growth in the UK 2023, by sector\\\\nContribution to gross domestic product growth in the United Kingdom in January 2023, by sector\\\\nGDP growth rate in the UK 1999-2021, by country\\\\nAnnual growth rates of gross domestic product in the United Kingdom from 1999 to 2021, by country\\\\nGDP growth rate in the UK 2021, by region\\\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\\\nGDP growth of Scotland 2021, by local area\\\\nAnnual growth rates of gross domestic product in Scotland in 2021, by local (ITL 3) area\\\\nGDP growth of Wales 2021, by local area\\\\nAnnual growth rates of gross domestic product in Wales in 2021, by local (ITL 3) area\\\\nGDP growth of Northern Ireland 2021, by local area\\\\nAnnual growth rates of gross domestic product in Northern Ireland in 2021, by local (ITL 3) area\\\\nGDP per capita\\\\nGDP per capita\\\\nGDP per capita in the UK 1955-2022\\\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\\\nAnnual GDP per capita growth in the UK 1956-2022\\\\nAnnual GDP per capita growth in the United Kingdom from 1956 to 2022\\\\nQuarterly GDP per capita in the UK 2019-2023\\\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\\\nQuarterly GDP per capita growth in the UK 2019-2023\\\\nQuarterly GDP per capita growth in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\\\nGDP per capita of the UK 1999-2021, by country\\\\nGross domestic product per capita of the United Kingdom from 1999 to 2021, by country (in GBP)\\\\nGDP per capita of the UK 2021, by region\\\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\\\nGlobal Comparisons\\\\nGlobal Comparisons\\\\nCountries with the largest gross domestic product (GDP) 2022\\\\n Monthly GDP of the UK 2019-2023\\\\nMonthly index of gross domestic product in the United Kingdom from January 2019 to November 2023 (2019=100)\\\\nGVA of the UK 2022, by sector\\\\nGross value added of the United Kingdom in 2022, by industry sector (in million GBP)\\\\nGDP of the UK 2021, by country\\\\nGross domestic product of the United Kingdom in 2021, by country (in million GBP)\\\\nGDP of the UK 2021, by region\\\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\\\nGDP of Scotland 2021, by local area\\\\nGross domestic product of Scotland in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP of Wales 2021, by local area\\\\nGross domestic product of Wales in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP of Northern Ireland 2021, by local area\\\\nGross domestic product of Northern Ireland in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP growth\\\\nGDP growth\\\\nGDP growth forecast for the UK 2000-2028\\\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\\\nAnnual GDP growth in the UK 1949-2022\\\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\\\nQuarterly GDP growth of the UK 2019-2023\\\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023\\\\nMonthly GDP growth of the UK 2019-2023\\\\n Transforming data into design:\\\\nStatista Content & Design\\\\nStrategy and business building for the data-driven economy:\\\\nUK GDP - Statistics & Facts\\\\nUK economy expected to shrink in 2023\\\\nCharacteristics of UK GDP\\\\nKey insights\\\\nDetailed statistics\\\\nGDP of the UK 1948-2022\\\\nDetailed statistics\\\\nAnnual GDP growth in the UK 1949-2022\\\\nDetailed statistics\\\\nGDP per capita in the UK 1955-2022\\\\nEditor\u2019s Picks\\\\nCurrent statistics on this topic\\\\nCurrent statistics on this topic\\\\nKey Economic Indicators\\\\nMonthly GDP growth of the UK 2019-2023\\\\nKey Economic Indicators\\\\nMonthly GDP of the UK 2019-2023\\\\nKey Economic Indicators\\\\nContribution to GDP growth in the UK 2023, by sector\\\\nRelated topics\\\\nRecommended\\\\nRecommended statistics\\\\nGDP\\\\nGDP\\\\nGDP of the UK 1948-2022\\\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\\\nQuarterly GDP of the UK 2019-2023\\\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in million GBP)\\\\n The 20 countries with the largest gross domestic product (GDP) in 2022 (in billion U.S. dollars)\\\\nGDP of European countries in 2022\\\\nGross domestic product at current market prices of selected European countries in 2022 (in million euros)\\\\nReal GDP growth rates in Europe 2023\\\\nAnnual real gross domestic product (GDP) growth rate in European countries in 2023\\\\nGross domestic product (GDP) of Europe\\'s largest economies 1980-2028\\\\nGross domestic product (GDP) at current prices of Europe\\'s largest economies from 1980 to 2028 (in billion U.S dollars)\\\\nUnited Kingdom\\'s share of global gross domestic product (GDP) 2028\\\\nUnited Kingdom (UK): Share of global gross domestic product (GDP) adjusted for Purchasing Power Parity (PPP) from 2018 to 2028\\\\nRelated topics\\\\nRecommended\\\\nReport on the topic\\\\nKey figures\\\\nThe most important key figures provide you with a compact summary of the topic of \\\\\"UK GDP\\\\\" and take you straight to the corresponding statistics.\\\\n Industry Overview\\\\nDigital & Trend reports\\\\nOverview and forecasts on trending topics\\\\nIndustry & Market reports\\\\nIndustry and market insights and forecasts\\\\nCompanies & Products reports\\\\nKey figures and rankings about companies and products\\\\nConsumer & Brand reports\\\\nConsumer and brand insights and preferences in various industries\\\\nPolitics & Society reports\\\\nDetailed information about political and social topics\\\\nCountry & Region reports\\\\nAll key figures about countries and regions\\\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\\\nInsights on consumer attitudes and behavior worldwide\\\\nBusiness information on 100m+ public and private companies\\\\nExplore Company Insights\\\\nDetailed information for 39,000+ online stores and marketplaces\\\\nDirectly accessible data for 170 industries from 150+ countries\\\\nand over 1\\xa0Mio. facts.\\\\n\"}, {\"url\": \"https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB\", \"content\": \"GDP growth (annual %) - United Kingdom | Data - World Bank Data\"}, {\"url\": \"https://www.statista.com/topics/6500/the-british-economy/\", \"content\": \"Output per hour worked in the UK 1971 to 2023\\\\nEconomic output per hour worked in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023 (2019=100)\\\\nAnnual unemployment rate in the UK 2000-2028\\\\nAnnual unemployment rate in the United Kingdom from 2000 to 2028\\\\nInflation\\\\nInflation\\\\nInflation rate in the UK 1989-2023\\\\nInflation rate for the Consumer Price Index (CPI) in the United Kingdom from January 1989 to October 2023\\\\nRPI inflation rate in the UK 1948-2023\\\\nInflation rate for the Retail Price Index (RPI) in the United Kingdom from June 1948 to October 2023\\\\nCPIH inflation rate in the UK 1989-2023\\\\nInflation rate for the Consumer Price Index including owner occupiers\\' housing costs (CPIH) in the United Kingdom from January 1989 to October 2023\\\\nPPI in the UK 2010-2023\\\\nProducer Price Index (PPI) in the United Kingdom from October 2010 to October 2023\\\\nCPI inflation rate in the UK 2023, by sector\\\\nInflation rate for the Consumer Price Index (CPI) in the United Kingdom in October 2023, by sector\\\\nConsumer Price Index in the UK 1988-2023\\\\nConsumer Price Index (CPI) in the United Kingdom from 1st quarter 1988 to 3rd quarter 2023\\\\nRetail Price Index in the UK 1987-2023\\\\nRetail Price Index (RPI) in the United Kingdom from 1st quarter 1987 to 3rd quarter 2023\\\\nConsumer Price Index including housing in the UK 1988-2023\\\\nConsumer Price Index including owner occupiers\\' housing costs (CPIH) in the United Kingdom from 1st quarter 1988 to 3rd quarter 2023\\\\nRPI annual inflation rate UK 2000-2028\\\\nAnnual inflation rate of the Retail Price Index in the United Kingdom from 2000 to 2028\\\\nCPI annual inflation rate UK 2000-2028\\\\nAnnual inflation rate of the Consumer Price Index in the United Kingdom from 2000 to 2028\\\\nGovernment finances\\\\nGovernment finances\\\\nGovernment spending as a percentage of GDP in the UK 1900-2029\\\\nTotal managed expenditure expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\\\nGovernment revenue as a percentage of GDP in the UK 1900-2029\\\\nTotal public sector current receipts expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29 (in million GBP)\\\\nGovernment borrowing as a percentage of GDP in the UK 1900-2029\\\\nPublic sector borrowing expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\\\nNational debt as a percentage of GDP in the UK 1900-2029\\\\nPublic sector net debt expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\\\nPublic sector spending in the United Kingdom 2023/24\\\\nBudgeted public sector expenditure on services in the United Kingdom in 2023/24, by function (in billion GBP)\\\\nGovernment revenue sources in the United Kingdom 2023/24\\\\nExpected public sector current receipts in the United Kingdom in 2023/24, by function (in billion GBP)\\\\nBusiness Enterprise\\\\nBusiness Enterprise\\\\nLargest companies in the United Kingdom based on revenue 2022\\\\nLargest companies in the United Kingdom based on revenue in 2022 (in billion US dollars)\\\\nLargest UK companies based on number of global employees 2020\\\\nLargest companies based in the United Kingdom on number of employees worldwide in 2020 (in 1,000s)\\\\nNumber of private sector businesses in the UK 2000-2023\\\\nNumber of private sector businesses in the United Kingdom from 2000 to 2023 (in millions)\\\\nNumber of private sector businesses in the UK 2023, by sector\\\\nNumber of private sector businesses in the United Kingdom in 2023, by sector\\\\nNumber of businesses by enterprise size in the UK 2023\\\\nNumber of private sector businesses in the United Kingdom in 2023, by employment size\\\\nNumber of private sector businesses in the UK 2023, by region\\\\nNumber of private sector businesses in the United Kingdom in 2023, by region\\\\nNumber of local business units in the UK 2012-2023\\\\nNumber of local units in VAT and/or PAYE based enterprises in the United Kingdom from 2012 to 2023 (in millions)\\\\nBusiness investment index in the UK 1997-2023\\\\nBusiness investment index in the United Kingdom from 1st quarter 1997 to 2nd quarter 2023 (Q1 1997=100)\\\\nBusiness confidence Index in the UK 1977-2023\\\\nBusiness confidence Index of the United Kingdom from March 1977 to November 2023 (100 = long-term average)\\\\nRelated topics\\\\nRecommended\\\\nReport on the topic\\\\nKey figures\\\\nThe most important key figures provide you with a compact summary of the topic of \\\\\"The UK economy\\\\\" and take you straight to the corresponding statistics.\\\\n Monthly GDP growth of the UK 2020-2023\\\\nMonthly growth of gross domestic product in the United Kingdom from January 2020 to September 2023\\\\nGDP of the UK 2021, by region\\\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\\\nGDP growth rate in the UK 2021, by region\\\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\\\nGDP per capita of the UK 2021, by region\\\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\\\nGDP growth forecast for the UK 2000-2028\\\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\\\nLabor Market\\\\nLabor Market\\\\nUnemployment rate of the UK 1971-2023\\\\nUnemployment rate in the United Kingdom from March 1971 to September 2023\\\\nEmployment rate in the UK 1971-2022\\\\nEmployment rate in the United Kingdom from March 1971 to July 2023\\\\nNumber of people unemployed in the UK 1971-2023\\\\nNumber of people unemployed in the United Kingdom from March 1971 to July 2023 (in 1,000s)\\\\nNumber of people employed in the UK 1971-2021\\\\nNumber of people employed in the United Kingdom from March 1971 to July 2023 (in 1,000s)\\\\nUnemployment rate in the UK 1971-2023, by gender\\\\nUnemployment rate in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023, by gender\\\\nUnemployment rate in the UK 1992-2023, by age group\\\\nUnemployment rate in the United Kingdom from May 1992 to July 2023, by age group\\\\nYouth unemployment rate in the UK 1992-2023\\\\nYouth unemployment rate in the United Kingdom from May 1992 to July 2023\\\\nAverage annual earnings for full-time employees in the UK 1999-2023\\\\nMedian annual earnings for full-time employees in the United Kingdom from 1999 to 2023 (in nominal GBP)\\\\nAverage weekly earning growth in the UK 2001-2023\\\\nAverage year-on-year growth of weekly earnings (3 month average) in the United Kingdom from March 2001 to October 2023\\\\nNumber of redundancies in the UK 1995-2023\\\\nAverage number of people made redundant in the United Kingdom from May 1995 to July 2023 (in 1,000s)\\\\nOverall weekly hours worked in the UK 1971-2023\\\\nOverall weekly hours worked for all employees in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023 (in million hours worked)\\\\n Transforming data into design:\\\\nStatista Content & Design\\\\nStrategy and business building for the data-driven economy:\\\\nThe UK economy - Statistics & Facts\\\\nUK households under pressure in 2023\\\\nCoronavirus devastates UK economy in 2020\\\\nKey insights\\\\nDetailed statistics\\\\nGDP of the UK 1948-2022\\\\nDetailed statistics\\\\nUnemployment rate of the UK 1971-2023\\\\nDetailed statistics\\\\nInflation rate in the UK 1989-2023\\\\nEditor\u2019s Picks\\\\nCurrent statistics on this topic\\\\nCurrent statistics on this topic\\\\nWages & Salaries\\\\nAverage weekly earning growth in the UK 2001-2023\\\\nIncome & Expenditure\\\\nPublic sector spending in the United Kingdom 2023/24\\\\nEmployment\\\\nNumber of people employed in the UK 1971-2021\\\\nRelated topics\\\\nRecommended\\\\nRecommended statistics\\\\nGross domestic product\\\\nGross domestic product\\\\nGDP of the UK 1948-2022\\\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\\\nAnnual GDP growth in the UK 1949-2022\\\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\\\nGDP per capita in the UK 1955-2022\\\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\\\nQuarterly GDP of the UK 1955-2023\\\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 1955 to 3rd quarter 2023 (in million GBP)\\\\nQuarterly GDP growth of the UK 2015-2023\\\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2015 to 3rd quarter 2023\\\\nQuarterly GDP per capita in the UK 1955-2023\\\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 1955 to 3rd quarter 2023 (in GBP)\\\\nMonthly GDP of the UK 1997-2023\\\\nMonthly index of gross domestic product in the United Kingdom from January 1997 to September 2023 (2019=100)\\\\n GDP\\\\nAnnual GDP growth in the UK 1949-2022\\\\nQuarterly GDP per capita growth in the UK 2015-2023\\\\nMonthly GDP growth of the UK 2020-2023\\\\nGDP per capita in the UK 1955-2022\\\\nLabor market\\\\nNumber of people employed in the UK 1971-2021\\\\nNumber of people unemployed in the UK 1971-2023\\\\nDaily number of jobs furloughed in the UK 2020-2021\\\\nAverage annual earnings for full-time employees in the UK 1999-2023\\\\nForecasts for 2023\\\\nGDP growth forecast for the UK 2000-2028\\\\nAnnual unemployment rate in the UK 2000-2028\\\\nCPI annual inflation rate UK 2000-2028\\\\nRPI annual inflation rate UK 2000-2028\\\\n Industry Overview\\\\nDigital & Trend reports\\\\nOverview and forecasts on trending topics\\\\nIndustry & Market reports\\\\nIndustry and market insights and forecasts\\\\nCompanies & Products reports\\\\nKey figures and rankings about companies and products\\\\nConsumer & Brand reports\\\\nConsumer and brand insights and preferences in various industries\\\\nPolitics & Society reports\\\\nDetailed information about political and social topics\\\\nCountry & Region reports\\\\nAll key figures about countries and regions\\\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\\\nInsights on consumer attitudes and behavior worldwide\\\\nBusiness information on 100m+ public and private companies\\\\nExplore Company Insights\\\\nDetailed information for 39,000+ online stores and marketplaces\\\\nDirectly accessible data for 170 industries from 150+ countries\\\\nand over 1\\xa0Mio. facts.\\\\n\"}, {\"url\": \"https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB&most_recent_value_desc=false\", \"content\": \"GDP growth (annual %) - United Kingdom | Data Data GDP growth (annual %)United Kingdom Data Catalog Data Programs International Debt Statistics Other Books and Reports For Developers GDP growth (annual %) - United Kingdom ====================================== Similar values Highest values Lowest values GDP (constant 2015 US$)  GDP (current US$)  GDP (constant LCU)  GDP: linked series (current LCU)  GDP, PPP (constant 2021 international $)  GDP (current LCU)  GDP, PPP (current international $)  GDP per capita growth (annual %)  Country Most Recent Value All Countries and Economies Country Most Recent Value This site uses cookies to optimize functionality and give you the best possible experience. If you continue to navigate this website beyond this page, cookies will be placed on your browser.\"}]', name='tavily_search_results_json', id='4c88089f-0ac4-4eeb-9141-722f0463b78d', tool_call_id='toolu_01Jd9dxa4Ss2NhzBhCuwUX3E', artifact={'query': 'UK GDP annual data past 5 years 2019-2023', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'U.K. GDP 1960-2024 - Macrotrends', 'url': 'https://www.macrotrends.net/global-metrics/countries/GBR/united-kingdom/gdp-gross-domestic-product', 'content': 'Dollar figures for GDP are converted from domestic currencies using single year official exchange rates. For a few countries where the official exchange rate does not reflect the rate effectively applied to actual foreign exchange transactions, an alternative conversion factor is used. U.K. gdp for 2023 was $3,340.03B, a 8.13% increase from 2022.', 'score': 0.97675806, 'raw_content': None}, {'title': 'UK GDP - Statistics & Facts | Statista', 'url': 'https://www.statista.com/topics/3795/gdp-of-the-uk/', 'content': 'Monthly growth of gross domestic product in the United Kingdom from January 2019 to November 2023\\nContribution to GDP growth in the UK 2023, by sector\\nContribution to gross domestic product growth in the United Kingdom in January 2023, by sector\\nGDP growth rate in the UK 1999-2021, by country\\nAnnual growth rates of gross domestic product in the United Kingdom from 1999 to 2021, by country\\nGDP growth rate in the UK 2021, by region\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\nGDP growth of Scotland 2021, by local area\\nAnnual growth rates of gross domestic product in Scotland in 2021, by local (ITL 3) area\\nGDP growth of Wales 2021, by local area\\nAnnual growth rates of gross domestic product in Wales in 2021, by local (ITL 3) area\\nGDP growth of Northern Ireland 2021, by local area\\nAnnual growth rates of gross domestic product in Northern Ireland in 2021, by local (ITL 3) area\\nGDP per capita\\nGDP per capita\\nGDP per capita in the UK 1955-2022\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\nAnnual GDP per capita growth in the UK 1956-2022\\nAnnual GDP per capita growth in the United Kingdom from 1956 to 2022\\nQuarterly GDP per capita in the UK 2019-2023\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\nQuarterly GDP per capita growth in the UK 2019-2023\\nQuarterly GDP per capita growth in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\nGDP per capita of the UK 1999-2021, by country\\nGross domestic product per capita of the United Kingdom from 1999 to 2021, by country (in GBP)\\nGDP per capita of the UK 2021, by region\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\nGlobal Comparisons\\nGlobal Comparisons\\nCountries with the largest gross domestic product (GDP) 2022\\n Monthly GDP of the UK 2019-2023\\nMonthly index of gross domestic product in the United Kingdom from January 2019 to November 2023 (2019=100)\\nGVA of the UK 2022, by sector\\nGross value added of the United Kingdom in 2022, by industry sector (in million GBP)\\nGDP of the UK 2021, by country\\nGross domestic product of the United Kingdom in 2021, by country (in million GBP)\\nGDP of the UK 2021, by region\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\nGDP of Scotland 2021, by local area\\nGross domestic product of Scotland in 2021, by local (ITL 3) area (in million GBP)\\nGDP of Wales 2021, by local area\\nGross domestic product of Wales in 2021, by local (ITL 3) area (in million GBP)\\nGDP of Northern Ireland 2021, by local area\\nGross domestic product of Northern Ireland in 2021, by local (ITL 3) area (in million GBP)\\nGDP growth\\nGDP growth\\nGDP growth forecast for the UK 2000-2028\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\nAnnual GDP growth in the UK 1949-2022\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\nQuarterly GDP growth of the UK 2019-2023\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023\\nMonthly GDP growth of the UK 2019-2023\\n Transforming data into design:\\nStatista Content & Design\\nStrategy and business building for the data-driven economy:\\nUK GDP - Statistics & Facts\\nUK economy expected to shrink in 2023\\nCharacteristics of UK GDP\\nKey insights\\nDetailed statistics\\nGDP of the UK 1948-2022\\nDetailed statistics\\nAnnual GDP growth in the UK 1949-2022\\nDetailed statistics\\nGDP per capita in the UK 1955-2022\\nEditor\u2019s Picks\\nCurrent statistics on this topic\\nCurrent statistics on this topic\\nKey Economic Indicators\\nMonthly GDP growth of the UK 2019-2023\\nKey Economic Indicators\\nMonthly GDP of the UK 2019-2023\\nKey Economic Indicators\\nContribution to GDP growth in the UK 2023, by sector\\nRelated topics\\nRecommended\\nRecommended statistics\\nGDP\\nGDP\\nGDP of the UK 1948-2022\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\nQuarterly GDP of the UK 2019-2023\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in million GBP)\\n The 20 countries with the largest gross domestic product (GDP) in 2022 (in billion U.S. dollars)\\nGDP of European countries in 2022\\nGross domestic product at current market prices of selected European countries in 2022 (in million euros)\\nReal GDP growth rates in Europe 2023\\nAnnual real gross domestic product (GDP) growth rate in European countries in 2023\\nGross domestic product (GDP) of Europe\\'s largest economies 1980-2028\\nGross domestic product (GDP) at current prices of Europe\\'s largest economies from 1980 to 2028 (in billion U.S dollars)\\nUnited Kingdom\\'s share of global gross domestic product (GDP) 2028\\nUnited Kingdom (UK): Share of global gross domestic product (GDP) adjusted for Purchasing Power Parity (PPP) from 2018 to 2028\\nRelated topics\\nRecommended\\nReport on the topic\\nKey figures\\nThe most important key figures provide you with a compact summary of the topic of \"UK GDP\" and take you straight to the corresponding statistics.\\n Industry Overview\\nDigital & Trend reports\\nOverview and forecasts on trending topics\\nIndustry & Market reports\\nIndustry and market insights and forecasts\\nCompanies & Products reports\\nKey figures and rankings about companies and products\\nConsumer & Brand reports\\nConsumer and brand insights and preferences in various industries\\nPolitics & Society reports\\nDetailed information about political and social topics\\nCountry & Region reports\\nAll key figures about countries and regions\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\nInsights on consumer attitudes and behavior worldwide\\nBusiness information on 100m+ public and private companies\\nExplore Company Insights\\nDetailed information for 39,000+ online stores and marketplaces\\nDirectly accessible data for 170 industries from 150+ countries\\nand over 1\\xa0Mio. facts.\\n', 'score': 0.97057647, 'raw_content': None}, {'title': 'GDP growth (annual %) - United Kingdom | Data - World Bank Data', 'url': 'https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB', 'content': 'GDP growth (annual %) - United Kingdom | Data - World Bank Data', 'score': 0.97052056, 'raw_content': None}, {'title': 'The UK economy - Statistics & Facts | Statista', 'url': 'https://www.statista.com/topics/6500/the-british-economy/', 'content': 'Output per hour worked in the UK 1971 to 2023\\nEconomic output per hour worked in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023 (2019=100)\\nAnnual unemployment rate in the UK 2000-2028\\nAnnual unemployment rate in the United Kingdom from 2000 to 2028\\nInflation\\nInflation\\nInflation rate in the UK 1989-2023\\nInflation rate for the Consumer Price Index (CPI) in the United Kingdom from January 1989 to October 2023\\nRPI inflation rate in the UK 1948-2023\\nInflation rate for the Retail Price Index (RPI) in the United Kingdom from June 1948 to October 2023\\nCPIH inflation rate in the UK 1989-2023\\nInflation rate for the Consumer Price Index including owner occupiers\\' housing costs (CPIH) in the United Kingdom from January 1989 to October 2023\\nPPI in the UK 2010-2023\\nProducer Price Index (PPI) in the United Kingdom from October 2010 to October 2023\\nCPI inflation rate in the UK 2023, by sector\\nInflation rate for the Consumer Price Index (CPI) in the United Kingdom in October 2023, by sector\\nConsumer Price Index in the UK 1988-2023\\nConsumer Price Index (CPI) in the United Kingdom from 1st quarter 1988 to 3rd quarter 2023\\nRetail Price Index in the UK 1987-2023\\nRetail Price Index (RPI) in the United Kingdom from 1st quarter 1987 to 3rd quarter 2023\\nConsumer Price Index including housing in the UK 1988-2023\\nConsumer Price Index including owner occupiers\\' housing costs (CPIH) in the United Kingdom from 1st quarter 1988 to 3rd quarter 2023\\nRPI annual inflation rate UK 2000-2028\\nAnnual inflation rate of the Retail Price Index in the United Kingdom from 2000 to 2028\\nCPI annual inflation rate UK 2000-2028\\nAnnual inflation rate of the Consumer Price Index in the United Kingdom from 2000 to 2028\\nGovernment finances\\nGovernment finances\\nGovernment spending as a percentage of GDP in the UK 1900-2029\\nTotal managed expenditure expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\nGovernment revenue as a percentage of GDP in the UK 1900-2029\\nTotal public sector current receipts expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29 (in million GBP)\\nGovernment borrowing as a percentage of GDP in the UK 1900-2029\\nPublic sector borrowing expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\nNational debt as a percentage of GDP in the UK 1900-2029\\nPublic sector net debt expressed as a percentage of GDP in the United Kingdom from 1900/01 to 2028/29\\nPublic sector spending in the United Kingdom 2023/24\\nBudgeted public sector expenditure on services in the United Kingdom in 2023/24, by function (in billion GBP)\\nGovernment revenue sources in the United Kingdom 2023/24\\nExpected public sector current receipts in the United Kingdom in 2023/24, by function (in billion GBP)\\nBusiness Enterprise\\nBusiness Enterprise\\nLargest companies in the United Kingdom based on revenue 2022\\nLargest companies in the United Kingdom based on revenue in 2022 (in billion US dollars)\\nLargest UK companies based on number of global employees 2020\\nLargest companies based in the United Kingdom on number of employees worldwide in 2020 (in 1,000s)\\nNumber of private sector businesses in the UK 2000-2023\\nNumber of private sector businesses in the United Kingdom from 2000 to 2023 (in millions)\\nNumber of private sector businesses in the UK 2023, by sector\\nNumber of private sector businesses in the United Kingdom in 2023, by sector\\nNumber of businesses by enterprise size in the UK 2023\\nNumber of private sector businesses in the United Kingdom in 2023, by employment size\\nNumber of private sector businesses in the UK 2023, by region\\nNumber of private sector businesses in the United Kingdom in 2023, by region\\nNumber of local business units in the UK 2012-2023\\nNumber of local units in VAT and/or PAYE based enterprises in the United Kingdom from 2012 to 2023 (in millions)\\nBusiness investment index in the UK 1997-2023\\nBusiness investment index in the United Kingdom from 1st quarter 1997 to 2nd quarter 2023 (Q1 1997=100)\\nBusiness confidence Index in the UK 1977-2023\\nBusiness confidence Index of the United Kingdom from March 1977 to November 2023 (100 = long-term average)\\nRelated topics\\nRecommended\\nReport on the topic\\nKey figures\\nThe most important key figures provide you with a compact summary of the topic of \"The UK economy\" and take you straight to the corresponding statistics.\\n Monthly GDP growth of the UK 2020-2023\\nMonthly growth of gross domestic product in the United Kingdom from January 2020 to September 2023\\nGDP of the UK 2021, by region\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\nGDP growth rate in the UK 2021, by region\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\nGDP per capita of the UK 2021, by region\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\nGDP growth forecast for the UK 2000-2028\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\nLabor Market\\nLabor Market\\nUnemployment rate of the UK 1971-2023\\nUnemployment rate in the United Kingdom from March 1971 to September 2023\\nEmployment rate in the UK 1971-2022\\nEmployment rate in the United Kingdom from March 1971 to July 2023\\nNumber of people unemployed in the UK 1971-2023\\nNumber of people unemployed in the United Kingdom from March 1971 to July 2023 (in 1,000s)\\nNumber of people employed in the UK 1971-2021\\nNumber of people employed in the United Kingdom from March 1971 to July 2023 (in 1,000s)\\nUnemployment rate in the UK 1971-2023, by gender\\nUnemployment rate in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023, by gender\\nUnemployment rate in the UK 1992-2023, by age group\\nUnemployment rate in the United Kingdom from May 1992 to July 2023, by age group\\nYouth unemployment rate in the UK 1992-2023\\nYouth unemployment rate in the United Kingdom from May 1992 to July 2023\\nAverage annual earnings for full-time employees in the UK 1999-2023\\nMedian annual earnings for full-time employees in the United Kingdom from 1999 to 2023 (in nominal GBP)\\nAverage weekly earning growth in the UK 2001-2023\\nAverage year-on-year growth of weekly earnings (3 month average) in the United Kingdom from March 2001 to October 2023\\nNumber of redundancies in the UK 1995-2023\\nAverage number of people made redundant in the United Kingdom from May 1995 to July 2023 (in 1,000s)\\nOverall weekly hours worked in the UK 1971-2023\\nOverall weekly hours worked for all employees in the United Kingdom from 1st quarter 1971 to 2nd quarter 2023 (in million hours worked)\\n Transforming data into design:\\nStatista Content & Design\\nStrategy and business building for the data-driven economy:\\nThe UK economy - Statistics & Facts\\nUK households under pressure in 2023\\nCoronavirus devastates UK economy in 2020\\nKey insights\\nDetailed statistics\\nGDP of the UK 1948-2022\\nDetailed statistics\\nUnemployment rate of the UK 1971-2023\\nDetailed statistics\\nInflation rate in the UK 1989-2023\\nEditor\u2019s Picks\\nCurrent statistics on this topic\\nCurrent statistics on this topic\\nWages & Salaries\\nAverage weekly earning growth in the UK 2001-2023\\nIncome & Expenditure\\nPublic sector spending in the United Kingdom 2023/24\\nEmployment\\nNumber of people employed in the UK 1971-2021\\nRelated topics\\nRecommended\\nRecommended statistics\\nGross domestic product\\nGross domestic product\\nGDP of the UK 1948-2022\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\nAnnual GDP growth in the UK 1949-2022\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\nGDP per capita in the UK 1955-2022\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\nQuarterly GDP of the UK 1955-2023\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 1955 to 3rd quarter 2023 (in million GBP)\\nQuarterly GDP growth of the UK 2015-2023\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2015 to 3rd quarter 2023\\nQuarterly GDP per capita in the UK 1955-2023\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 1955 to 3rd quarter 2023 (in GBP)\\nMonthly GDP of the UK 1997-2023\\nMonthly index of gross domestic product in the United Kingdom from January 1997 to September 2023 (2019=100)\\n GDP\\nAnnual GDP growth in the UK 1949-2022\\nQuarterly GDP per capita growth in the UK 2015-2023\\nMonthly GDP growth of the UK 2020-2023\\nGDP per capita in the UK 1955-2022\\nLabor market\\nNumber of people employed in the UK 1971-2021\\nNumber of people unemployed in the UK 1971-2023\\nDaily number of jobs furloughed in the UK 2020-2021\\nAverage annual earnings for full-time employees in the UK 1999-2023\\nForecasts for 2023\\nGDP growth forecast for the UK 2000-2028\\nAnnual unemployment rate in the UK 2000-2028\\nCPI annual inflation rate UK 2000-2028\\nRPI annual inflation rate UK 2000-2028\\n Industry Overview\\nDigital & Trend reports\\nOverview and forecasts on trending topics\\nIndustry & Market reports\\nIndustry and market insights and forecasts\\nCompanies & Products reports\\nKey figures and rankings about companies and products\\nConsumer & Brand reports\\nConsumer and brand insights and preferences in various industries\\nPolitics & Society reports\\nDetailed information about political and social topics\\nCountry & Region reports\\nAll key figures about countries and regions\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\nInsights on consumer attitudes and behavior worldwide\\nBusiness information on 100m+ public and private companies\\nExplore Company Insights\\nDetailed information for 39,000+ online stores and marketplaces\\nDirectly accessible data for 170 industries from 150+ countries\\nand over 1\\xa0Mio. facts.\\n', 'score': 0.95998776, 'raw_content': None}, {'title': 'GDP growth (annual %) - United Kingdom | Data - World Bank Data', 'url': 'https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB&most_recent_value_desc=false', 'content': 'GDP growth (annual %) - United Kingdom | Data Data GDP growth (annual %)United Kingdom Data Catalog Data Programs International Debt Statistics Other Books and Reports For Developers GDP growth (annual %) - United Kingdom ====================================== Similar values Highest values Lowest values GDP (constant 2015 US$)  GDP (current US$)  GDP (constant LCU)  GDP: linked series (current LCU)  GDP, PPP (constant 2021 international $)  GDP (current LCU)  GDP, PPP (current international $)  GDP per capita growth (annual %)  Country Most Recent Value All Countries and Economies Country Most Recent Value This site uses cookies to optimize functionality and give you the best possible experience. If you continue to navigate this website beyond this page, cookies will be placed on your browser.', 'score': 0.7892337, 'raw_content': None}], 'response_time': 2.3}), AIMessage(content=[{'text': 'Let me search for more specific data.', 'type': 'text'}, {'id': 'toolu_019dPRXojLJoVNYFLzzSWw4w', 'input': {'query': 'UK GDP values by year 2019 2020 2021 2022 2023'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01Ac9vcTFneb5dvcEYXJyf1P', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 5890, 'output_tokens': 87}}, id='run-3504417f-c0b5-4908-82e2-89a18abb1b8e-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'UK GDP values by year 2019 2020 2021 2022 2023'}, 'id': 'toolu_019dPRXojLJoVNYFLzzSWw4w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 5890, 'output_tokens': 87, 'total_tokens': 5977, 'input_token_details': {}}), ToolMessage(content='[{\"url\": \"https://www.macrotrends.net/global-metrics/countries/GBR/united-kingdom/gdp-gross-domestic-product\", \"content\": \"U.K. gdp for 2023 was $3,340.03B, a 8.13% increase from 2022. U.K. gdp for 2022 was $3,088.84B, a 1.68% decline from 2021. U.K. gdp for 2021 was $3,141.51B, a 16.45% increase from 2020. U.K. gdp for 2020 was $2,697.81B, a 5.39% decline from 2019.\"}, {\"url\": \"https://countryeconomy.com/gdp/uk?year=2023\", \"content\": \"Gross Domestic Product of United Kingdom grew 0.3% in 2023 compared to last year. This rate is 45 -tenths of one percent less than the figure of 4.8% published in 2022. The GDP figure in 2023 was $3,380,855 million, leaving United Kingdom placed 6th in the ranking of GDP of the 196 countries that we publish.\"}, {\"url\": \"https://www.statista.com/topics/3795/gdp-of-the-uk/\", \"content\": \"Monthly growth of gross domestic product in the United Kingdom from January 2019 to November 2023\\\\nContribution to GDP growth in the UK 2023, by sector\\\\nContribution to gross domestic product growth in the United Kingdom in January 2023, by sector\\\\nGDP growth rate in the UK 1999-2021, by country\\\\nAnnual growth rates of gross domestic product in the United Kingdom from 1999 to 2021, by country\\\\nGDP growth rate in the UK 2021, by region\\\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\\\nGDP growth of Scotland 2021, by local area\\\\nAnnual growth rates of gross domestic product in Scotland in 2021, by local (ITL 3) area\\\\nGDP growth of Wales 2021, by local area\\\\nAnnual growth rates of gross domestic product in Wales in 2021, by local (ITL 3) area\\\\nGDP growth of Northern Ireland 2021, by local area\\\\nAnnual growth rates of gross domestic product in Northern Ireland in 2021, by local (ITL 3) area\\\\nGDP per capita\\\\nGDP per capita\\\\nGDP per capita in the UK 1955-2022\\\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\\\nAnnual GDP per capita growth in the UK 1956-2022\\\\nAnnual GDP per capita growth in the United Kingdom from 1956 to 2022\\\\nQuarterly GDP per capita in the UK 2019-2023\\\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\\\nQuarterly GDP per capita growth in the UK 2019-2023\\\\nQuarterly GDP per capita growth in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\\\nGDP per capita of the UK 1999-2021, by country\\\\nGross domestic product per capita of the United Kingdom from 1999 to 2021, by country (in GBP)\\\\nGDP per capita of the UK 2021, by region\\\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\\\nGlobal Comparisons\\\\nGlobal Comparisons\\\\nCountries with the largest gross domestic product (GDP) 2022\\\\n Monthly GDP of the UK 2019-2023\\\\nMonthly index of gross domestic product in the United Kingdom from January 2019 to November 2023 (2019=100)\\\\nGVA of the UK 2022, by sector\\\\nGross value added of the United Kingdom in 2022, by industry sector (in million GBP)\\\\nGDP of the UK 2021, by country\\\\nGross domestic product of the United Kingdom in 2021, by country (in million GBP)\\\\nGDP of the UK 2021, by region\\\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\\\nGDP of Scotland 2021, by local area\\\\nGross domestic product of Scotland in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP of Wales 2021, by local area\\\\nGross domestic product of Wales in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP of Northern Ireland 2021, by local area\\\\nGross domestic product of Northern Ireland in 2021, by local (ITL 3) area (in million GBP)\\\\nGDP growth\\\\nGDP growth\\\\nGDP growth forecast for the UK 2000-2028\\\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\\\nAnnual GDP growth in the UK 1949-2022\\\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\\\nQuarterly GDP growth of the UK 2019-2023\\\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023\\\\nMonthly GDP growth of the UK 2019-2023\\\\n Transforming data into design:\\\\nStatista Content & Design\\\\nStrategy and business building for the data-driven economy:\\\\nUK GDP - Statistics & Facts\\\\nUK economy expected to shrink in 2023\\\\nCharacteristics of UK GDP\\\\nKey insights\\\\nDetailed statistics\\\\nGDP of the UK 1948-2022\\\\nDetailed statistics\\\\nAnnual GDP growth in the UK 1949-2022\\\\nDetailed statistics\\\\nGDP per capita in the UK 1955-2022\\\\nEditor\u2019s Picks\\\\nCurrent statistics on this topic\\\\nCurrent statistics on this topic\\\\nKey Economic Indicators\\\\nMonthly GDP growth of the UK 2019-2023\\\\nKey Economic Indicators\\\\nMonthly GDP of the UK 2019-2023\\\\nKey Economic Indicators\\\\nContribution to GDP growth in the UK 2023, by sector\\\\nRelated topics\\\\nRecommended\\\\nRecommended statistics\\\\nGDP\\\\nGDP\\\\nGDP of the UK 1948-2022\\\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\\\nQuarterly GDP of the UK 2019-2023\\\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in million GBP)\\\\n The 20 countries with the largest gross domestic product (GDP) in 2022 (in billion U.S. dollars)\\\\nGDP of European countries in 2022\\\\nGross domestic product at current market prices of selected European countries in 2022 (in million euros)\\\\nReal GDP growth rates in Europe 2023\\\\nAnnual real gross domestic product (GDP) growth rate in European countries in 2023\\\\nGross domestic product (GDP) of Europe\\'s largest economies 1980-2028\\\\nGross domestic product (GDP) at current prices of Europe\\'s largest economies from 1980 to 2028 (in billion U.S dollars)\\\\nUnited Kingdom\\'s share of global gross domestic product (GDP) 2028\\\\nUnited Kingdom (UK): Share of global gross domestic product (GDP) adjusted for Purchasing Power Parity (PPP) from 2018 to 2028\\\\nRelated topics\\\\nRecommended\\\\nReport on the topic\\\\nKey figures\\\\nThe most important key figures provide you with a compact summary of the topic of \\\\\"UK GDP\\\\\" and take you straight to the corresponding statistics.\\\\n Industry Overview\\\\nDigital & Trend reports\\\\nOverview and forecasts on trending topics\\\\nIndustry & Market reports\\\\nIndustry and market insights and forecasts\\\\nCompanies & Products reports\\\\nKey figures and rankings about companies and products\\\\nConsumer & Brand reports\\\\nConsumer and brand insights and preferences in various industries\\\\nPolitics & Society reports\\\\nDetailed information about political and social topics\\\\nCountry & Region reports\\\\nAll key figures about countries and regions\\\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\\\nInsights on consumer attitudes and behavior worldwide\\\\nBusiness information on 100m+ public and private companies\\\\nExplore Company Insights\\\\nDetailed information for 39,000+ online stores and marketplaces\\\\nDirectly accessible data for 170 industries from 150+ countries\\\\nand over 1\\xa0Mio. facts.\\\\n\"}, {\"url\": \"https://www.ons.gov.uk/economy/grossdomesticproductgdp/compendium/unitedkingdomnationalaccountsthebluebook/2024/nationalaccountsataglance\", \"content\": \"Real gross domestic product (GDP) is estimated to have increased by 0.3% in 2023, following a recovery from the impacts of the coronavirus (COVID-19) pandemic over the two previous years (Figure 1). Data for the UK are the Office for National Statistics (ONS) measure of real gross domestic product (GDP). Figure 9: Real GDP per head fell in 2023 when compared with 2022 in six G10 economies, including the UK Data for the UK are the Office for National Statistics (ONS) measure of real gross domestic product (GDP) per head. Download this chart Figure 9: Real GDP per head fell in 2023 when compared with 2022 in six G10 economies, including the UK\"}, {\"url\": \"https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB&most_recent_value_desc=false\", \"content\": \"GDP growth (annual %) - United Kingdom | Data Data GDP growth (annual %)United Kingdom Data Catalog Data Programs International Debt Statistics Other Books and Reports For Developers GDP growth (annual %) - United Kingdom ====================================== Similar values Highest values Lowest values GDP (constant 2015 US$)  GDP (current US$)  GDP (constant LCU)  GDP: linked series (current LCU)  GDP, PPP (constant 2021 international $)  GDP (current LCU)  GDP, PPP (current international $)  GDP per capita growth (annual %)  Country Most Recent Value All Countries and Economies Country Most Recent Value This site uses cookies to optimize functionality and give you the best possible experience. If you continue to navigate this website beyond this page, cookies will be placed on your browser.\"}]', name='tavily_search_results_json', id='84c571ca-27c6-4023-93a2-f0c2e8b6abb0', tool_call_id='toolu_019dPRXojLJoVNYFLzzSWw4w', artifact={'query': 'UK GDP values by year 2019 2020 2021 2022 2023', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'U.K. GDP 1960-2024 - Macrotrends', 'url': 'https://www.macrotrends.net/global-metrics/countries/GBR/united-kingdom/gdp-gross-domestic-product', 'content': 'U.K. gdp for 2023 was $3,340.03B, a 8.13% increase from 2022. U.K. gdp for 2022 was $3,088.84B, a 1.68% decline from 2021. U.K. gdp for 2021 was $3,141.51B, a 16.45% increase from 2020. U.K. gdp for 2020 was $2,697.81B, a 5.39% decline from 2019.', 'score': 0.9974491, 'raw_content': None}, {'title': 'United Kingdom (UK) GDP - Gross Domestic Product 2023', 'url': 'https://countryeconomy.com/gdp/uk?year=2023', 'content': 'Gross Domestic Product of United Kingdom grew 0.3% in 2023 compared to last year. This rate is 45 -tenths of one percent less than the figure of 4.8% published in 2022. The GDP figure in 2023 was $3,380,855 million, leaving United Kingdom placed 6th in the ranking of GDP of the 196 countries that we publish.', 'score': 0.9964064, 'raw_content': None}, {'title': 'UK GDP - Statistics & Facts | Statista', 'url': 'https://www.statista.com/topics/3795/gdp-of-the-uk/', 'content': 'Monthly growth of gross domestic product in the United Kingdom from January 2019 to November 2023\\nContribution to GDP growth in the UK 2023, by sector\\nContribution to gross domestic product growth in the United Kingdom in January 2023, by sector\\nGDP growth rate in the UK 1999-2021, by country\\nAnnual growth rates of gross domestic product in the United Kingdom from 1999 to 2021, by country\\nGDP growth rate in the UK 2021, by region\\nAnnual growth rates of gross domestic product in the United Kingdom in 2021, by region\\nGDP growth of Scotland 2021, by local area\\nAnnual growth rates of gross domestic product in Scotland in 2021, by local (ITL 3) area\\nGDP growth of Wales 2021, by local area\\nAnnual growth rates of gross domestic product in Wales in 2021, by local (ITL 3) area\\nGDP growth of Northern Ireland 2021, by local area\\nAnnual growth rates of gross domestic product in Northern Ireland in 2021, by local (ITL 3) area\\nGDP per capita\\nGDP per capita\\nGDP per capita in the UK 1955-2022\\nGross domestic product per capita in the United Kingdom from 1955 to 2022 (in GBP)\\nAnnual GDP per capita growth in the UK 1956-2022\\nAnnual GDP per capita growth in the United Kingdom from 1956 to 2022\\nQuarterly GDP per capita in the UK 2019-2023\\nQuarterly GDP per capita in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\nQuarterly GDP per capita growth in the UK 2019-2023\\nQuarterly GDP per capita growth in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in GBP)\\nGDP per capita of the UK 1999-2021, by country\\nGross domestic product per capita of the United Kingdom from 1999 to 2021, by country (in GBP)\\nGDP per capita of the UK 2021, by region\\nGross domestic product per capita of the United Kingdom in 2021, by region (in GBP)\\nGlobal Comparisons\\nGlobal Comparisons\\nCountries with the largest gross domestic product (GDP) 2022\\n Monthly GDP of the UK 2019-2023\\nMonthly index of gross domestic product in the United Kingdom from January 2019 to November 2023 (2019=100)\\nGVA of the UK 2022, by sector\\nGross value added of the United Kingdom in 2022, by industry sector (in million GBP)\\nGDP of the UK 2021, by country\\nGross domestic product of the United Kingdom in 2021, by country (in million GBP)\\nGDP of the UK 2021, by region\\nGross domestic product of the United Kingdom in 2021, by region (in million GBP)\\nGDP of Scotland 2021, by local area\\nGross domestic product of Scotland in 2021, by local (ITL 3) area (in million GBP)\\nGDP of Wales 2021, by local area\\nGross domestic product of Wales in 2021, by local (ITL 3) area (in million GBP)\\nGDP of Northern Ireland 2021, by local area\\nGross domestic product of Northern Ireland in 2021, by local (ITL 3) area (in million GBP)\\nGDP growth\\nGDP growth\\nGDP growth forecast for the UK 2000-2028\\nForecasted annual growth of gross domestic product in the United Kingdom from 2000 to 2028\\nAnnual GDP growth in the UK 1949-2022\\nAnnual growth of gross domestic product in the United Kingdom from 1949 to 2022\\nQuarterly GDP growth of the UK 2019-2023\\nQuarterly growth of gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023\\nMonthly GDP growth of the UK 2019-2023\\n Transforming data into design:\\nStatista Content & Design\\nStrategy and business building for the data-driven economy:\\nUK GDP - Statistics & Facts\\nUK economy expected to shrink in 2023\\nCharacteristics of UK GDP\\nKey insights\\nDetailed statistics\\nGDP of the UK 1948-2022\\nDetailed statistics\\nAnnual GDP growth in the UK 1949-2022\\nDetailed statistics\\nGDP per capita in the UK 1955-2022\\nEditor\u2019s Picks\\nCurrent statistics on this topic\\nCurrent statistics on this topic\\nKey Economic Indicators\\nMonthly GDP growth of the UK 2019-2023\\nKey Economic Indicators\\nMonthly GDP of the UK 2019-2023\\nKey Economic Indicators\\nContribution to GDP growth in the UK 2023, by sector\\nRelated topics\\nRecommended\\nRecommended statistics\\nGDP\\nGDP\\nGDP of the UK 1948-2022\\nGross domestic product of the United Kingdom from 1948 to 2022 (in million GBP)\\nQuarterly GDP of the UK 2019-2023\\nQuarterly gross domestic product in the United Kingdom from 1st quarter 2019 to 3rd quarter 2023 (in million GBP)\\n The 20 countries with the largest gross domestic product (GDP) in 2022 (in billion U.S. dollars)\\nGDP of European countries in 2022\\nGross domestic product at current market prices of selected European countries in 2022 (in million euros)\\nReal GDP growth rates in Europe 2023\\nAnnual real gross domestic product (GDP) growth rate in European countries in 2023\\nGross domestic product (GDP) of Europe\\'s largest economies 1980-2028\\nGross domestic product (GDP) at current prices of Europe\\'s largest economies from 1980 to 2028 (in billion U.S dollars)\\nUnited Kingdom\\'s share of global gross domestic product (GDP) 2028\\nUnited Kingdom (UK): Share of global gross domestic product (GDP) adjusted for Purchasing Power Parity (PPP) from 2018 to 2028\\nRelated topics\\nRecommended\\nReport on the topic\\nKey figures\\nThe most important key figures provide you with a compact summary of the topic of \"UK GDP\" and take you straight to the corresponding statistics.\\n Industry Overview\\nDigital & Trend reports\\nOverview and forecasts on trending topics\\nIndustry & Market reports\\nIndustry and market insights and forecasts\\nCompanies & Products reports\\nKey figures and rankings about companies and products\\nConsumer & Brand reports\\nConsumer and brand insights and preferences in various industries\\nPolitics & Society reports\\nDetailed information about political and social topics\\nCountry & Region reports\\nAll key figures about countries and regions\\nMarket forecast and expert KPIs for 1000+ markets in 190+ countries & territories\\nInsights on consumer attitudes and behavior worldwide\\nBusiness information on 100m+ public and private companies\\nExplore Company Insights\\nDetailed information for 39,000+ online stores and marketplaces\\nDirectly accessible data for 170 industries from 150+ countries\\nand over 1\\xa0Mio. facts.\\n', 'score': 0.97943294, 'raw_content': None}, {'title': 'National accounts at a glance - Office for National Statistics', 'url': 'https://www.ons.gov.uk/economy/grossdomesticproductgdp/compendium/unitedkingdomnationalaccountsthebluebook/2024/nationalaccountsataglance', 'content': 'Real gross domestic product (GDP) is estimated to have increased by 0.3% in 2023, following a recovery from the impacts of the coronavirus (COVID-19) pandemic over the two previous years (Figure 1). Data for the UK are the Office for National Statistics (ONS) measure of real gross domestic product (GDP). Figure 9: Real GDP per head fell in 2023 when compared with 2022 in six G10 economies, including the UK Data for the UK are the Office for National Statistics (ONS) measure of real gross domestic product (GDP) per head. Download this chart Figure 9: Real GDP per head fell in 2023 when compared with 2022 in six G10 economies, including the UK', 'score': 0.975249, 'raw_content': None}, {'title': 'GDP growth (annual %) - United Kingdom | Data - World Bank Data', 'url': 'https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?locations=GB&most_recent_value_desc=false', 'content': 'GDP growth (annual %) - United Kingdom | Data Data GDP growth (annual %)United Kingdom Data Catalog Data Programs International Debt Statistics Other Books and Reports For Developers GDP growth (annual %) - United Kingdom ====================================== Similar values Highest values Lowest values GDP (constant 2015 US$)  GDP (current US$)  GDP (constant LCU)  GDP: linked series (current LCU)  GDP, PPP (constant 2021 international $)  GDP (current LCU)  GDP, PPP (current international $)  GDP per capita growth (annual %)  Country Most Recent Value All Countries and Economies Country Most Recent Value This site uses cookies to optimize functionality and give you the best possible experience. If you continue to navigate this website beyond this page, cookies will be placed on your browser.', 'score': 0.83775276, 'raw_content': None}], 'response_time': 2.37}), HumanMessage(content='Based on the search results, I can provide the UK\\'s GDP values for the past 5 years (in billions of US dollars):\\n\\n2019: $2,851.54\\n2020: $2,697.81\\n2021: $3,141.51\\n2022: $3,088.84\\n2023: $3,340.03\\n\\nI\\'ll pass this data to my chart generator colleague to create a line chart. They should create a line chart with:\\n- Years 2019-2023 on the x-axis\\n- GDP values in billions USD on the y-axis\\n- Title: \"UK GDP 2019-2023\"\\n- Clear data points showing the values\\n\\nOver to you, chart generator colleague!', additional_kwargs={}, response_metadata={}, name='researcher', id='7e790b7a-7b06-4b45-a595-8736b53db844'), AIMessage(content=[{'text': \"I'll create a line chart with the specified GDP data and requirements using Python and matplotlib.\", 'type': 'text'}, {'id': 'toolu_017HmYWRMpnhPaw3SamZCQua', 'input': {'code': \"import matplotlib.pyplot as plt\\n\\nyears = [2019, 2020, 2021, 2022, 2023]\\ngdp = [2851.54, 2697.81, 3141.51, 3088.84, 3340.03]\\n\\nplt.figure(figsize=(10, 6))\\nplt.plot(years, gdp, marker='o', linewidth=2, markersize=8)\\n\\nplt.title('UK GDP 2019-2023', pad=15, size=14)\\nplt.xlabel('Year', labelpad=10)\\nplt.ylabel('GDP (Billions USD)', labelpad=10)\\n\\n# Add value labels above each point\\nfor i, value in enumerate(gdp):\\n    plt.text(years[i], value + 30, f'${value}B', ha='center')\\n\\nplt.grid(True, linestyle='--', alpha=0.7)\\nplt.show()\"}, 'name': 'python_repl_tool', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01Y29F46KJQzTmefwQL6s9Dp', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 8744, 'output_tokens': 295}}, id='run-e0ee838e-1c18-46d9-bed7-459330376276-0', tool_calls=[{'name': 'python_repl_tool', 'args': {'code': \"import matplotlib.pyplot as plt\\n\\nyears = [2019, 2020, 2021, 2022, 2023]\\ngdp = [2851.54, 2697.81, 3141.51, 3088.84, 3340.03]\\n\\nplt.figure(figsize=(10, 6))\\nplt.plot(years, gdp, marker='o', linewidth=2, markersize=8)\\n\\nplt.title('UK GDP 2019-2023', pad=15, size=14)\\nplt.xlabel('Year', labelpad=10)\\nplt.ylabel('GDP (Billions USD)', labelpad=10)\\n\\n# Add value labels above each point\\nfor i, value in enumerate(gdp):\\n    plt.text(years[i], value + 30, f'${value}B', ha='center')\\n\\nplt.grid(True, linestyle='--', alpha=0.7)\\nplt.show()\"}, 'id': 'toolu_017HmYWRMpnhPaw3SamZCQua', 'type': 'tool_call'}], usage_metadata={'input_tokens': 8744, 'output_tokens': 295, 'total_tokens': 9039, 'input_token_details': {}}), ToolMessage(content=\"Successfully executed:\\n\\`\\`\\`python\\nimport matplotlib.pyplot as plt\\n\\nyears = [2019, 2020, 2021, 2022, 2023]\\ngdp = [2851.54, 2697.81, 3141.51, 3088.84, 3340.03]\\n\\nplt.figure(figsize=(10, 6))\\nplt.plot(years, gdp, marker='o', linewidth=2, markersize=8)\\n\\nplt.title('UK GDP 2019-2023', pad=15, size=14)\\nplt.xlabel('Year', labelpad=10)\\nplt.ylabel('GDP (Billions USD)', labelpad=10)\\n\\n# Add value labels above each point\\nfor i, value in enumerate(gdp):\\n    plt.text(years[i], value + 30, f'${value}B', ha='center')\\n\\nplt.grid(True, linestyle='--', alpha=0.7)\\nplt.show()\\n\\`\\`\\`\\nStdout: \\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\", name='python_repl_tool', id='5421128d-9996-4dc5-b14c-77b862912d94', tool_call_id='toolu_017HmYWRMpnhPaw3SamZCQua'), HumanMessage(content='FINAL ANSWER: I have created a line chart showing the UK\\'s GDP from 2019 to 2023. The chart includes:\\n- A clear line with marked data points\\n- Years on the x-axis\\n- GDP values in billions USD on the y-axis\\n- Value labels above each data point\\n- A grid for better readability\\n- The title \"UK GDP 2019-2023\"\\n\\nThe chart clearly shows the GDP drop in 2020 due to the pandemic, followed by recovery and growth through 2023, with the most recent value reaching $3,340.03 billion.', additional_kwargs={}, response_metadata={}, name='chart_generator', id='4a649455-eed8-4b4f-a19f-c172140430c3')]}}\n----\nBack to top\nCopyright \u00a9 2025 LangChain, Inc |\nConsent Preferences\nMade with\nMaterial for MkDocs",
    "metadata": {
      "source": "https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration/",
      "title": "Multi-agent network",
      "heading": "Multi-agent network\u00b6"
    }
  },
  {
    "page_content": "Multi-agent - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nMulti-agent\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nWhy multi-agent?\nPatterns\nChoosing a pattern\nVisual overview\nPerformance comparison\nOne-shot request\nRepeat request\nMulti-domain\nSummary\nAdvanced usage\nMulti-agent\nMulti-agent\nCopy page\nCopy page\nMulti-agent systems coordinate specialized components to tackle complex workflows. However, not every complex task requires this approach \u2014 a single agent with the right (sometimes dynamic) tools and prompt can often achieve similar results.\n\u200b\nWhy multi-agent?\nWhen developers say they need \u201cmulti-agent,\u201d they\u2019re usually looking for one or more of these capabilities:\nContext management\n: Provide specialized knowledge without overwhelming the model\u2019s context window. If context were infinite and latency zero, you could dump all knowledge into a single prompt \u2014 but since it\u2019s not, you need patterns to selectively surface relevant information.\nDistributed development\n: Allow different teams to develop and maintain capabilities independently, composing them into a larger system with clear boundaries.\nParallelization\n: Spawn specialized workers for subtasks and execute them concurrently for faster results.\nMulti-agent patterns are particularly valuable when a single agent has too many\ntools\nand makes poor decisions about which to use, when tasks require specialized knowledge with extensive context (long prompts and domain-specific tools), or when you need to enforce sequential constraints that unlock capabilities only after certain conditions are met.\nAt the center of multi-agent design is\ncontext engineering\n\u2014deciding what information each agent sees. The quality of your system depends on ensuring each agent has access to the right data for its task.\n\u200b\nPatterns\nHere are the main patterns for building multi-agent systems, each suited to different use cases:\nPattern\nHow it works\nSubagents\nA main agent coordinates subagents as tools. All routing passes through the main agent, which decides when and how to invoke each subagent.\nHandoffs\nBehavior changes dynamically based on state. Tool calls update a state variable that triggers routing or configuration changes, switching agents or adjusting the current agent\u2019s tools and prompt.\nSkills\nSpecialized prompts and knowledge loaded on-demand. A single agent stays in control while loading context from skills as needed.\nRouter\nA routing step classifies input and directs it to one or more specialized agents. Results are synthesized into a combined response.\nCustom workflow\nBuild bespoke execution flows with\nLangGraph\n, mixing deterministic logic and agentic behavior. Embed other patterns as nodes in your workflow.\n\u200b\nChoosing a pattern\nUse this table to match your requirements to the right pattern:\nPattern\nDistributed development\nParallelization\nMulti-hop\nDirect user interaction\nSubagents\n\u2b50\u2b50\u2b50\u2b50\u2b50\n\u2b50\u2b50\u2b50\u2b50\u2b50\n\u2b50\u2b50\u2b50\u2b50\u2b50\n\u2b50\nHandoffs\n\u2014\n\u2014\n\u2b50\u2b50\u2b50\u2b50\u2b50\n\u2b50\u2b50\u2b50\u2b50\u2b50\nSkills\n\u2b50\u2b50\u2b50\u2b50\u2b50\n\u2b50\u2b50\u2b50\n\u2b50\u2b50\u2b50\u2b50\u2b50\n\u2b50\u2b50\u2b50\u2b50\u2b50\nRouter\n\u2b50\u2b50\u2b50\n\u2b50\u2b50\u2b50\u2b50\u2b50\n\u2014\n\u2b50\u2b50\u2b50\nDistributed development\n: Can different teams maintain components independently?\nParallelization\n: Can multiple agents execute concurrently?\nMulti-hop\n: Does the pattern support calling multiple subagents in series?\nDirect user interaction\n: Can subagents converse directly with the user?\nYou can mix patterns! For example, a\nsubagents\narchitecture can invoke tools that invoke custom workflows or router agents. Subagents can even use the\nskills\npattern to load context on-demand. The possibilities are endless!\n\u200b\nVisual overview\nSubagents\nHandoffs\nSkills\nRouter\nA main agent coordinates subagents as tools. All routing passes through the main agent.\nAgents transfer control to each other via tool calls. Each agent can hand off to others or respond directly to the user.\nA single agent loads specialized prompts and knowledge on-demand while staying in control.\nA routing step classifies input and directs it to specialized agents. Results are synthesized.\n\u200b\nPerformance comparison\nDifferent patterns have different performance characteristics. Understanding these tradeoffs helps you choose the right pattern for your latency and cost requirements.\nKey metrics:\nModel calls\n: Number of LLM invocations. More calls = higher latency (especially if sequential) and higher per-request API costs.\nTokens processed\n: Total\ncontext window\nusage across all calls. More tokens = higher processing costs and potential context limits.\n\u200b\nOne-shot request\nUser:\n\u201cBuy coffee\u201d\nA specialized coffee agent/skill can call a\nbuy_coffee\ntool.\nPattern\nModel calls\nBest fit\nSubagents\n4\nHandoffs\n3\n\u2705\nSkills\n3\n\u2705\nRouter\n3\n\u2705\nSubagents\nHandoffs\nSkills\nRouter\n4 model calls:\n3 model calls:\n3 model calls:\n3 model calls:\nKey insight:\nHandoffs, Skills, and Router are most efficient for single tasks (3 calls each). Subagents adds one extra call because results flow back through the main agent\u2014this overhead provides centralized control.\n\u200b\nRepeat request\nTurn 1:\n\u201cBuy coffee\u201d\nTurn 2:\n\u201cBuy coffee again\u201d\nThe user repeats the same request in the same conversation.\nPattern\nTurn 2 calls\nTotal (both turns)\nBest fit\nSubagents\n4\n8\nHandoffs\n2\n5\n\u2705\nSkills\n2\n5\n\u2705\nRouter\n3\n6\nSubagents\nHandoffs\nSkills\nRouter\n4 calls again \u2192 8 total\nSubagents are\nstateless by design\n\u2014each invocation follows the same flow\nThe main agent maintains conversation context, but subagents start fresh each time\nThis provides strong context isolation but repeats the full flow\n2 calls \u2192 5 total\nThe coffee agent is\nstill active\nfrom turn 1 (state persists)\nNo handoff needed\u2014agent directly calls\nbuy_coffee\ntool (call 1)\nAgent responds to user (call 2)\nSaves 1 call by skipping the handoff\n2 calls \u2192 5 total\nThe skill context is\nalready loaded\nin conversation history\nNo need to reload\u2014agent directly calls\nbuy_coffee\ntool (call 1)\nAgent responds to user (call 2)\nSaves 1 call by reusing loaded skill\n3 calls again \u2192 6 total\nRouters are\nstateless\n\u2014each request requires an LLM routing call\nTurn 2: Router LLM call (1) \u2192 Milk agent calls buy_coffee (2) \u2192 Milk agent responds (3)\nCan be optimized by wrapping as a tool in a stateful agent\nKey insight:\nStateful patterns (Handoffs, Skills) save 40-50% of calls on repeat requests. Subagents maintain consistent cost per request\u2014this stateless design provides strong context isolation but at the cost of repeated model calls.\n\u200b\nMulti-domain\nUser:\n\u201cCompare Python, JavaScript, and Rust for web development\u201d\nEach language agent/skill contains ~2000 tokens of documentation. All patterns can make parallel tool calls.\nPattern\nModel calls\nTotal tokens\nBest fit\nSubagents\n5\n~9K\n\u2705\nHandoffs\n7+\n~14K+\nSkills\n3\n~15K\nRouter\n5\n~9K\n\u2705\nSubagents\nHandoffs\nSkills\nRouter\n5 calls, ~9K tokens\nEach subagent works in\nisolation\nwith only its relevant context. Total:\n9K tokens\n.\n7+ calls, ~14K+ tokens\nHandoffs executes\nsequentially\n\u2014can\u2019t research all three languages in parallel. Growing conversation history adds overhead. Total:\n~14K+ tokens\n.\n3 calls, ~15K tokens\nAfter loading,\nevery subsequent call processes all 6K tokens of skill documentation\n. Subagents processes 67% fewer tokens overall due to context isolation. Total:\n15K tokens\n.\n5 calls, ~9K tokens\nRouter uses an\nLLM for routing\n, then invokes agents in parallel. Similar to Subagents but with explicit routing step. Total:\n9K tokens\n.\nKey insight:\nFor multi-domain tasks, patterns with parallel execution (Subagents, Router) are most efficient. Skills has fewer calls but high token usage due to context accumulation. Handoffs is inefficient here\u2014it must execute sequentially and can\u2019t leverage parallel tool calling for consulting multiple domains simultaneously.\n\u200b\nSummary\nHere\u2019s how patterns compare across all three scenarios:\nPattern\nOne-shot\nRepeat request\nMulti-domain\nSubagents\n4 calls\n8 calls (4+4)\n5 calls, 9K tokens\nHandoffs\n3 calls\n5 calls (3+2)\n7+ calls, 14K+ tokens\nSkills\n3 calls\n5 calls (3+2)\n3 calls, 15K tokens\nRouter\n3 calls\n6 calls (3+3)\n5 calls, 9K tokens\nChoosing a pattern:\nOptimize for\nSubagents\nHandoffs\nSkills\nRouter\nSingle requests\n\u2705\n\u2705\n\u2705\nRepeat requests\n\u2705\n\u2705\nParallel execution\n\u2705\n\u2705\nLarge-context domains\n\u2705\n\u2705\nSimple, focused tasks\n\u2705\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nHuman-in-the-loop\nPrevious\nSubagents\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/index",
      "title": "Multi-agent - Docs by LangChain",
      "heading": "Multi-agent"
    }
  },
  {
    "page_content": "Subagents - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nSubagents\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nDesign decisions\nSync vs. async\nSynchronous (default)\nAsynchronous\nTool patterns\nTool per agent\nSingle dispatch tool\nContext engineering\nSubagent specs\nSubagent inputs\nSubagent outputs\nAdvanced usage\nMulti-agent\nSubagents\nCopy page\nCopy page\nIn the\nsubagents\narchitecture, a central main\nagent\n(often referred to as a\nsupervisor\n) coordinates subagents by calling them as\ntools\n. The main agent decides which subagent to invoke, what input to provide, and how to combine results. Subagents are stateless\u2014they don\u2019t remember past interactions, with all conversation memory maintained by the main agent. This provides\ncontext\nisolation: each subagent invocation works in a clean context window, preventing context bloat in the main conversation.\n\u200b\nKey characteristics\nCentralized control: All routing passes through the main agent\nNo direct user interaction: Subagents return results to the main agent, not the user (though you can use\ninterrupts\nwithin a subagent to allow user interaction)\nSubagents via tools: Subagents are invoked via tools\nParallel execution: The main agent can invoke multiple subagents in a single turn\nSupervisor vs. Router\n: A supervisor agent (this pattern) is different from a\nrouter\n. The supervisor is a full agent that maintains conversation context and dynamically decides which subagents to call across multiple turns. A router is typically a single classification step that dispatches to agents without maintaining ongoing conversation state.\n\u200b\nWhen to use\nUse the subagents pattern when you have multiple distinct domains (e.g., calendar, email, CRM, database), subagents don\u2019t need to converse directly with users, or you want centralized workflow control. For simpler cases with just a few\ntools\n, use a\nsingle agent\n.\nNeed user interaction within a subagent?\nWhile subagents typically return results to the main agent rather than conversing directly with users, you can use\ninterrupts\nwithin a subagent to pause execution and gather user input. This is useful when a subagent needs clarification or approval before proceeding. The main agent remains the orchestrator, but the subagent can collect information from the user mid-task.\n\u200b\nBasic implementation\nThe core mechanism wraps a subagent as a tool that the main agent can call:\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.agents\nimport\ncreate_agent\n# Create a subagent\nsubagent\n=\ncreate_agent(\nmodel\n=\n\"anthropic:claude-sonnet-4-20250514\"\n,\ntools\n=\n[\n...\n])\n# Wrap it as a tool\n@tool\n(\n\"research\"\n,\ndescription\n=\n\"Research a topic and return findings\"\n)\ndef\ncall_research_agent\n(\nquery\n:\nstr\n):\nresult\n=\nsubagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].content\n# Main agent with subagent as a tool\nmain_agent\n=\ncreate_agent(\nmodel\n=\n\"anthropic:claude-sonnet-4-20250514\"\n,\ntools\n=\n[call_research_agent])\nTutorial: Build a personal assistant with subagents\nLearn how to build a personal assistant using the subagents pattern, where a central main agent (supervisor) coordinates specialized worker agents.\nLearn more\n\u200b\nDesign decisions\nWhen implementing the subagents pattern, you\u2019ll make several key design choices. This table summarizes the options\u2014each is covered in detail in the sections below.\nDecision\nOptions\nSync vs. async\nSync (blocking) vs. async (background)\nTool patterns\nTool per agent vs. single dispatch tool\nSubagent inputs\nQuery only vs. full context\nSubagent outputs\nSubagent result vs full conversation history\n\u200b\nSync vs. async\nSubagent execution can be\nsynchronous\n(blocking) or\nasynchronous\n(background). Your choice depends on whether the main agent needs the result to continue.\nMode\nMain agent behavior\nBest for\nTradeoff\nSync\nWaits for subagent to complete\nMain agent needs result to continue\nSimple, but blocks the conversation\nAsync\nContinues while subagent runs in background\nIndependent tasks, user shouldn\u2019t wait\nResponsive, but more complex\nNot to be confused with Python\u2019s\nasync\n/\nawait\n. Here, \u201casync\u201d means the main agent kicks off a background job (typically in a separate process or service) and continues without blocking.\n\u200b\nSynchronous (default)\nBy default, subagent calls are\nsynchronous\n\u2014the main agent waits for each subagent to complete before continuing. Use sync when the main agent\u2019s next action depends on the subagent\u2019s result.\nWhen to use sync:\nMain agent needs the subagent\u2019s result to formulate its response\nTasks have order dependencies (e.g., fetch data \u2192 analyze \u2192 respond)\nSubagent failures should block the main agent\u2019s response\nTradeoffs:\nSimple implementation\u2014just call and wait\nUser sees no response until all subagents complete\nLong-running tasks freeze the conversation\n\u200b\nAsynchronous\nUse\nasynchronous execution\nwhen the subagent\u2019s work is independent\u2014the main agent doesn\u2019t need the result to continue conversing with the user. The main agent kicks off a background job and remains responsive.\nWhen to use async:\nSubagent work is independent of the main conversation flow\nUsers should be able to continue chatting while work happens\nYou want to run multiple independent tasks in parallel\nThree-tool pattern:\nStart job\n: Kicks off the background task, returns a job ID\nCheck status\n: Returns current state (pending, running, completed, failed)\nGet result\n: Retrieves the completed result\nHandling job completion:\nWhen a job finishes, your application needs to notify the user. One approach: surface a notification that, when clicked, sends a\nHumanMessage\nlike \u201cCheck job_123 and summarize the results.\u201d\n\u200b\nTool patterns\nThere are two main ways to expose subagents as tools:\nPattern\nBest for\nTrade-off\nTool per agent\nFine-grained control over each subagent\u2019s input/output\nMore setup, but more customization\nSingle dispatch tool\nMany agents, distributed teams, convention over configuration\nSimpler composition, less per-agent customization\n\u200b\nTool per agent\nThe key idea is wrapping subagents as tools that the main agent can call:\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.agents\nimport\ncreate_agent\n# Create a sub-agent\nsubagent\n=\ncreate_agent(\nmodel\n=\n\"...\"\n,\ntools\n=\n[\n...\n])\n# Wrap it as a tool  #\n@tool\n(\n\"subagent_name\"\n,\ndescription\n=\n\"subagent_description\"\n)\ndef\ncall_subagent\n(\nquery\n:\nstr\n):\nresult\n=\nsubagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].content\n# Main agent with subagent as a tool  #\nmain_agent\n=\ncreate_agent(\nmodel\n=\n\"...\"\n,\ntools\n=\n[call_subagent])\nThe main agent invokes the subagent tool when it decides the task matches the subagent\u2019s description, receives the result, and continues orchestration. See\nContext engineering\nfor fine-grained control.\n\u200b\nSingle dispatch tool\nAn alternative approach uses a single parameterized tool to invoke ephemeral sub-agents for independent tasks. Unlike the\ntool per agent\napproach where each sub-agent is wrapped as a separate tool, this uses a convention-based approach with a single\ntask\ntool: the task description is passed as a human message to the sub-agent, and the sub-agent\u2019s final message is returned as the tool result.\nUse this approach when you want to distribute agent development across multiple teams, need to isolate complex tasks into separate context windows, need a scalable way to add new agents without modifying the coordinator, or prefer convention over customization. This approach trades flexibility in context engineering for simplicity in agent composition and strong context isolation.\nKey characteristics:\nSingle task tool: One parameterized tool that can invoke any registered sub-agent by name\nConvention-based invocation: Agent selected by name, task passed as human message, final message returned as tool result\nTeam distribution: Different teams can develop and deploy agents independently\nAgent discovery: Sub-agents can be discovered via system prompt (listing available agents) or through\nprogressive disclosure\n(loading agent information on-demand via tools)\nAn interesting aspect of this approach is that sub-agents may have the exact same capabilities as the main agent. In such cases, invoking a sub-agent is\nreally about context isolation\nas the primary reason\u2014allowing complex, multi-step tasks to run in isolated context windows without bloating the main agent\u2019s conversation history. The sub-agent completes its work autonomously and returns only a concise summary, keeping the main thread focused and efficient.\nAgent registry with task dispatcher\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.agents\nimport\ncreate_agent\n# Sub-agents developed by different teams\nresearch_agent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\nprompt\n=\n\"You are a research specialist...\"\n)\nwriter_agent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\nprompt\n=\n\"You are a writing specialist...\"\n)\n# Registry of available sub-agents\nSUBAGENTS\n=\n{\n\"research\"\n: research_agent,\n\"writer\"\n: writer_agent,\n}\n@tool\ndef\ntask\n(\nagent_name\n:\nstr\n,\ndescription\n:\nstr\n) ->\nstr\n:\n\"\"\"Launch an ephemeral subagent for a task.\nAvailable agents:\n- research: Research and fact-finding\n- writer: Content creation and editing\n\"\"\"\nagent\n=\nSUBAGENTS\n[agent_name]\nresult\n=\nagent.invoke({\n\"messages\"\n: [\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: description}\n]\n})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].content\n# Main coordinator agent\nmain_agent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[task],\nsystem_prompt\n=\n(\n\"You coordinate specialized sub-agents. \"\n\"Available: research (fact-finding), \"\n\"writer (content creation). \"\n\"Use the task tool to delegate work.\"\n),\n)\n\u200b\nContext engineering\nControl how context flows between the main agent and its subagents:\nCategory\nPurpose\nImpacts\nSubagent specs\nEnsure subagents are invoked when they should be\nMain agent routing decisions\nSubagent inputs\nEnsure subagents can execute well with optimized context\nSubagent performance\nSubagent outputs\nEnsure the supervisor can act on subagent results\nMain agent performance\nSee also our comprehensive guide on\ncontext engineering\nfor agents.\n\u200b\nSubagent specs\nThe\nnames\nand\ndescriptions\nassociated with subagents are the primary way the main agent knows which subagents to invoke.\nThese are prompting levers\u2014choose them carefully.\nName\n: How the main agent refers to the sub-agent. Keep it clear and action-oriented (e.g.,\nresearch_agent\n,\ncode_reviewer\n).\nDescription\n: What the main agent knows about the sub-agent\u2019s capabilities. Be specific about what tasks it handles and when to use it.\nFor the\nsingle dispatch tool\ndesign, the main agent needs to call the\ntask\ntool with the name of the subagent to invoke. The available tools can be provided to the main agent via one of the following methods:\nSystem prompt enumeration\n: List available agents in the system prompt.\nEnum constraint on dispatch tool\n: For small agent lists, add an enum to the\nagent_name\nfield.\nTool-based discovery\n: For large or dynamic agent registries, provide a separate tool (e.g.,\nlist_agents\nor\nsearch_agents\n) that returns available agents.\n\u200b\nSubagent inputs\nCustomize what context the subagent receives to execute its task. Add input that isn\u2019t practical to capture in a static prompt\u2014full message history, prior results, or task metadata\u2014by pulling from the agent\u2019s state.\nSubagent inputs example\nCopy\nfrom\nlangchain.agents\nimport\nAgentState\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nclass\nCustomState\n(\nAgentState\n):\nexample_state_key:\nstr\n@tool\n(\n\"subagent1_name\"\n,\ndescription\n=\n\"subagent1_description\"\n)\ndef\ncall_subagent1\n(\nquery\n:\nstr\n,\nruntime\n: ToolRuntime[\nNone\n, CustomState]):\n# Apply any logic needed to transform the messages into a suitable input\nsubagent_input\n=\nsome_logic(query, runtime.state[\n\"messages\"\n])\nresult\n=\nsubagent1.invoke({\n\"messages\"\n: subagent_input,\n# You could also pass other state keys here as needed.\n# Make sure to define these in both the main and subagent's\n# state schemas.\n\"example_state_key\"\n: runtime.state[\n\"example_state_key\"\n]\n})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].content\nSee all 21 lines\n\u200b\nSubagent outputs\nCustomize what the main agent receives back so it can make good decisions. Two strategies:\nPrompt the sub-agent\n: Specify exactly what should be returned. A common failure mode is that the sub-agent performs tool calls or reasoning but doesn\u2019t include results in its final message\u2014remind it that the supervisor only sees the final output.\nFormat in code\n: Adjust or enrich the response before returning it. For example, pass specific state keys back in addition to the final text using a\nCommand\n.\nSubagent outputs example\nCopy\nfrom\ntyping\nimport\nAnnotated\nfrom\nlangchain.agents\nimport\nAgentState\nfrom\nlangchain.tools\nimport\nInjectedToolCallId\nfrom\nlanggraph.types\nimport\nCommand\n@tool\n(\n\"subagent1_name\"\n,\ndescription\n=\n\"subagent1_description\"\n)\ndef\ncall_subagent1\n(\nquery\n:\nstr\n,\ntool_call_id\n: Annotated[\nstr\n, InjectedToolCallId],\n) -> Command:\nresult\n=\nsubagent1.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]\n})\nreturn\nCommand(\nupdate\n=\n{\n# Pass back additional state from the subagent\n\"example_state_key\"\n: result[\n\"example_state_key\"\n],\n\"messages\"\n: [\nToolMessage(\ncontent\n=\nresult[\n\"messages\"\n][\n-\n1\n].content,\ntool_call_id\n=\ntool_call_id\n)\n]\n})\nSee all 27 lines\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nMulti-agent\nPrevious\nHandoffs\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/subagents",
      "title": "Subagents - Docs by LangChain",
      "heading": "Subagents"
    }
  },
  {
    "page_content": "Handoffs - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nHandoffs\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nImplementation approaches\nSingle agent with middleware\nMultiple agent subgraphs\nContext engineering\nImplementation Considerations\nAdvanced usage\nMulti-agent\nHandoffs\nCopy page\nCopy page\nIn the\nhandoffs\narchitecture, behavior changes dynamically based on state. The core mechanism:\ntools\nupdate a state variable (e.g.,\ncurrent_step\nor\nactive_agent\n) that persists across turns, and the system reads this variable to adjust behavior\u2014either applying different configuration (system prompt, tools) or routing to a different\nagent\n. This pattern supports both handoffs between distinct agents and dynamic configuration changes within a single agent.\nThe term\nhandoffs\nwas coined by\nOpenAI\nfor using tool calls (e.g.,\ntransfer_to_sales_agent\n) to transfer control between agents or states.\n\u200b\nKey characteristics\nState-driven behavior: Behavior changes based on a state variable (e.g.,\ncurrent_step\nor\nactive_agent\n)\nTool-based transitions: Tools update the state variable to move between states\nDirect user interaction: Each state\u2019s configuration handles user messages directly\nPersistent state: State survives across conversation turns\n\u200b\nWhen to use\nUse the handoffs pattern when you need to enforce sequential constraints (unlock capabilities only after preconditions are met), the agent needs to converse directly with the user across different states, or you\u2019re building multi-stage conversational flows. This pattern is particularly valuable for customer support scenarios where you need to collect information in a specific sequence \u2014 for example, collecting a warranty ID before processing a refund.\n\u200b\nBasic implementation\nThe core mechanism is a\ntool\nthat returns a\nCommand\nto update state, triggering a transition to a new step or agent:\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.messages\nimport\nToolMessage\nfrom\nlanggraph.types\nimport\nCommand\n@tool\ndef\ntransfer_to_specialist\n(\nruntime\n) -> Command:\n\"\"\"Transfer to the specialist agent.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\ncontent\n=\n\"Transferred to specialist\"\n,\ntool_call_id\n=\nruntime.tool_call_id\n)\n],\n\"current_step\"\n:\n\"specialist\"\n# Triggers behavior change\n}\n)\nWhy include a\nToolMessage\n?\nWhen an LLM calls a tool, it expects a response. The\nToolMessage\nwith matching\ntool_call_id\ncompletes this request-response cycle\u2014without it, the conversation history becomes malformed. This is required whenever your handoff tool updates messages.\nFor a complete implementation, see the tutorial below.\nTutorial: Build customer support with handoffs\nLearn how to build a customer support agent using the handoffs pattern, where a single agent transitions between different configurations.\nLearn more\n\u200b\nImplementation approaches\nThere are two ways to implement handoffs:\nsingle agent with middleware\n(one agent with dynamic configuration) or\nmultiple agent subgraphs\n(distinct agents as graph nodes).\n\u200b\nSingle agent with middleware\nA single agent changes its behavior based on state. Middleware intercepts each model call and dynamically adjusts the system prompt and available tools. Tools update the state variable to trigger transitions:\nCopy\nfrom\nlangchain.tools\nimport\nToolRuntime, tool\nfrom\nlangchain.messages\nimport\nToolMessage\nfrom\nlanggraph.types\nimport\nCommand\n@tool\ndef\nrecord_warranty_status\n(\nstatus\n:\nstr\n,\nruntime\n: ToolRuntime[\nNone\n, SupportState]\n) -> Command:\n\"\"\"Record warranty status and transition to next step.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\ncontent\n=\nf\n\"Warranty status recorded:\n{\nstatus\n}\n\"\n,\ntool_call_id\n=\nruntime.tool_call_id\n)\n],\n\"warranty_status\"\n: status,\n\"current_step\"\n:\n\"specialist\"\n# Update state to trigger transition\n}\n)\nComplete example: Customer support with middleware\nCopy\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlangchain.messages\nimport\nToolMessage\nfrom\nlanggraph.types\nimport\nCommand\nfrom\ntyping\nimport\nCallable\n# 1. Define state with current_step tracker\nclass\nSupportState\n(\nAgentState\n):\n\"\"\"Track which step is currently active.\"\"\"\ncurrent_step:\nstr\n=\n\"triage\"\nwarranty_status:\nstr\n|\nNone\n=\nNone\n# 2. Tools update current_step via Command\n@tool\ndef\nrecord_warranty_status\n(\nstatus\n:\nstr\n,\nruntime\n: ToolRuntime[\nNone\n, SupportState]\n) -> Command:\n\"\"\"Record warranty status and transition to next step.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\ncontent\n=\nf\n\"Warranty status recorded:\n{\nstatus\n}\n\"\n,\ntool_call_id\n=\nruntime.tool_call_id\n)\n],\n\"warranty_status\"\n: status,\n# Transition to next step\n\"current_step\"\n:\n\"specialist\"\n})\n# 3. Middleware applies dynamic configuration based on current_step\n@wrap_model_call\ndef\napply_step_config\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n\"\"\"Configure agent behavior based on current_step.\"\"\"\nstep\n=\nrequest.state.get(\n\"current_step\"\n,\n\"triage\"\n)\n# Map steps to their configurations\nconfigs\n=\n{\n\"triage\"\n: {\n\"prompt\"\n:\n\"Collect warranty information...\"\n,\n\"tools\"\n: [record_warranty_status]\n},\n\"specialist\"\n: {\n\"prompt\"\n:\n\"Provide solutions based on warranty:\n{warranty_status}\n\"\n,\n\"tools\"\n: [provide_solution, escalate]\n}\n}\nconfig\n=\nconfigs[step]\nrequest\n=\nrequest.override(\nsystem_prompt\n=\nconfig[\n\"prompt\"\n].format(\n**\nrequest.state),\ntools\n=\nconfig[\n\"tools\"\n]\n)\nreturn\nhandler(request)\n# 4. Create agent with middleware\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[record_warranty_status, provide_solution, escalate],\nstate_schema\n=\nSupportState,\nmiddleware\n=\n[apply_step_config],\ncheckpointer\n=\nInMemorySaver()\n# Persist state across turns  #\n)\n\u200b\nMultiple agent subgraphs\nMultiple distinct agents exist as separate nodes in a graph. Handoff tools navigate between agent nodes using\nCommand.PARENT\nto specify which node to execute next.\nSubgraph handoffs require careful\ncontext engineering\n. Unlike single-agent middleware (where message history flows naturally), you must explicitly decide what messages pass between agents. Get this wrong and agents receive malformed conversation history or bloated context. See\nContext engineering\nbelow.\nCopy\nfrom\nlangchain.messages\nimport\nAIMessage, ToolMessage\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlanggraph.types\nimport\nCommand\n@tool\ndef\ntransfer_to_sales\n(\nruntime\n: ToolRuntime,\n) -> Command:\n\"\"\"Transfer to the sales agent.\"\"\"\nlast_ai_message\n=\nnext\n(\nmsg\nfor\nmsg\nin\nreversed\n(runtime.state[\n\"messages\"\n])\nif\nisinstance\n(msg, AIMessage)\n)\ntransfer_message\n=\nToolMessage(\ncontent\n=\n\"Transferred to sales agent\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\nreturn\nCommand(\ngoto\n=\n\"sales_agent\"\n,\nupdate\n=\n{\n\"active_agent\"\n:\n\"sales_agent\"\n,\n\"messages\"\n: [last_ai_message, transfer_message],\n},\ngraph\n=\nCommand.\nPARENT\n)\nComplete example: Sales and support with handoffs\nThis example shows a multi-agent system with separate sales and support agents. Each agent is a separate graph node, and handoff tools allow agents to transfer conversations to each other.\nCopy\nfrom\ntyping\nimport\nLiteral\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain.messages\nimport\nAIMessage, ToolMessage\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.types\nimport\nCommand\nfrom\ntyping_extensions\nimport\nNotRequired\n# 1. Define state with active_agent tracker\nclass\nMultiAgentState\n(\nAgentState\n):\nactive_agent: NotRequired[\nstr\n]\n# 2. Create handoff tools\n@tool\ndef\ntransfer_to_sales\n(\nruntime\n: ToolRuntime,\n) -> Command:\n\"\"\"Transfer to the sales agent.\"\"\"\nlast_ai_message\n=\nnext\n(\nmsg\nfor\nmsg\nin\nreversed\n(runtime.state[\n\"messages\"\n])\nif\nisinstance\n(msg, AIMessage)\n)\ntransfer_message\n=\nToolMessage(\ncontent\n=\n\"Transferred to sales agent from support agent\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\nreturn\nCommand(\ngoto\n=\n\"sales_agent\"\n,\nupdate\n=\n{\n\"active_agent\"\n:\n\"sales_agent\"\n,\n\"messages\"\n: [last_ai_message, transfer_message],\n},\ngraph\n=\nCommand.\nPARENT\n,\n)\n@tool\ndef\ntransfer_to_support\n(\nruntime\n: ToolRuntime,\n) -> Command:\n\"\"\"Transfer to the support agent.\"\"\"\nlast_ai_message\n=\nnext\n(\nmsg\nfor\nmsg\nin\nreversed\n(runtime.state[\n\"messages\"\n])\nif\nisinstance\n(msg, AIMessage)\n)\ntransfer_message\n=\nToolMessage(\ncontent\n=\n\"Transferred to support agent from sales agent\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\nreturn\nCommand(\ngoto\n=\n\"support_agent\"\n,\nupdate\n=\n{\n\"active_agent\"\n:\n\"support_agent\"\n,\n\"messages\"\n: [last_ai_message, transfer_message],\n},\ngraph\n=\nCommand.\nPARENT\n,\n)\n# 3. Create agents with handoff tools\nsales_agent\n=\ncreate_agent(\nmodel\n=\n\"anthropic:claude-sonnet-4-20250514\"\n,\ntools\n=\n[transfer_to_support],\nsystem_prompt\n=\n\"You are a sales agent. Help with sales inquiries. If asked about technical issues or support, transfer to the support agent.\"\n,\n)\nsupport_agent\n=\ncreate_agent(\nmodel\n=\n\"anthropic:claude-sonnet-4-20250514\"\n,\ntools\n=\n[transfer_to_sales],\nsystem_prompt\n=\n\"You are a support agent. Help with technical issues. If asked about pricing or purchasing, transfer to the sales agent.\"\n,\n)\n# 4. Create agent nodes that invoke the agents\ndef\ncall_sales_agent\n(\nstate\n: MultiAgentState) -> Command:\n\"\"\"Node that calls the sales agent.\"\"\"\nresponse\n=\nsales_agent.invoke(state)\nreturn\nresponse\ndef\ncall_support_agent\n(\nstate\n: MultiAgentState) -> Command:\n\"\"\"Node that calls the support agent.\"\"\"\nresponse\n=\nsupport_agent.invoke(state)\nreturn\nresponse\n# 5. Create router that checks if we should end or continue\ndef\nroute_after_agent\n(\nstate\n: MultiAgentState,\n) -> Literal[\n\"sales_agent\"\n,\n\"support_agent\"\n,\n\"__end__\"\n]:\n\"\"\"Route based on active_agent, or END if the agent finished without handoff.\"\"\"\nmessages\n=\nstate.get(\n\"messages\"\n, [])\n# Check the last message - if it's an AIMessage without tool calls, we're done\nif\nmessages:\nlast_msg\n=\nmessages[\n-\n1\n]\nif\nisinstance\n(last_msg, AIMessage)\nand\nnot\nlast_msg.tool_calls:\nreturn\n\"__end__\"\n# Otherwise route to the active agent\nactive\n=\nstate.get(\n\"active_agent\"\n,\n\"sales_agent\"\n)\nreturn\nactive\nif\nactive\nelse\n\"sales_agent\"\ndef\nroute_initial\n(\nstate\n: MultiAgentState,\n) -> Literal[\n\"sales_agent\"\n,\n\"support_agent\"\n]:\n\"\"\"Route to the active agent based on state, default to sales agent.\"\"\"\nreturn\nstate.get(\n\"active_agent\"\n)\nor\n\"sales_agent\"\n# 6. Build the graph\nbuilder\n=\nStateGraph(MultiAgentState)\nbuilder.add_node(\n\"sales_agent\"\n, call_sales_agent)\nbuilder.add_node(\n\"support_agent\"\n, call_support_agent)\n# Start with conditional routing based on initial active_agent\nbuilder.add_conditional_edges(\nSTART\n, route_initial, [\n\"sales_agent\"\n,\n\"support_agent\"\n])\n# After each agent, check if we should end or route to another agent\nbuilder.add_conditional_edges(\n\"sales_agent\"\n, route_after_agent, [\n\"sales_agent\"\n,\n\"support_agent\"\n,\nEND\n]\n)\nbuilder.add_conditional_edges(\n\"support_agent\"\n, route_after_agent, [\n\"sales_agent\"\n,\n\"support_agent\"\n,\nEND\n]\n)\ngraph\n=\nbuilder.compile()\nresult\n=\ngraph.invoke(\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi, I'm having trouble with my account login. Can you help?\"\n,\n}\n]\n}\n)\nfor\nmsg\nin\nresult[\n\"messages\"\n]:\nmsg.pretty_print()\nUse\nsingle agent with middleware\nfor most handoffs use cases\u2014it\u2019s simpler. Only use\nmultiple agent subgraphs\nwhen you need bespoke agent implementations (e.g., a node that\u2019s itself a complex graph with reflection or retrieval steps).\n\u200b\nContext engineering\nWith subgraph handoffs, you control exactly what messages flow between agents. This precision is essential for maintaining valid conversation history and avoiding context bloat that could confuse downstream agents. For more on this topic, see\ncontext engineering\n.\nHandling context during handoffs\nWhen handing off between agents, you need to ensure the conversation history remains valid. LLMs expect tool calls to be paired with their responses, so when using\nCommand.PARENT\nto hand off to another agent, you must include both:\nThe\nAIMessage\ncontaining the tool call\n(the message that triggered the handoff)\nA\nToolMessage\nacknowledging the handoff\n(the artificial response to that tool call)\nWithout this pairing, the receiving agent will see an incomplete conversation and may produce errors or unexpected behavior.\nThe example below assumes only the handoff tool was called (no parallel tool calls):\nCopy\n@tool\ndef\ntransfer_to_sales\n(\nruntime\n: ToolRuntime) -> Command:\n# Get the AI message that triggered this handoff\nlast_ai_message\n=\nruntime.state[\n\"messages\"\n][\n-\n1\n]\n# Create an artificial tool response to complete the pair\ntransfer_message\n=\nToolMessage(\ncontent\n=\n\"Transferred to sales agent\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\nreturn\nCommand(\ngoto\n=\n\"sales_agent\"\n,\nupdate\n=\n{\n\"active_agent\"\n:\n\"sales_agent\"\n,\n# Pass only these two messages, not the full subagent history\n\"messages\"\n: [last_ai_message, transfer_message],\n},\ngraph\n=\nCommand.\nPARENT\n,\n)\nWhy not pass all subagent messages?\nWhile you could include the full subagent conversation in the handoff, this often creates problems. The receiving agent may become confused by irrelevant internal reasoning, and token costs increase unnecessarily. By passing only the handoff pair, you keep the parent graph\u2019s context focused on high-level coordination. If the receiving agent needs additional context, consider summarizing the subagent\u2019s work in the ToolMessage content instead of passing raw message history.\nReturning control to the user\nWhen returning control to the user (ending the agent\u2019s turn), ensure the final message is an\nAIMessage\n. This maintains valid conversation history and signals to the user interface that the agent has finished its work.\n\u200b\nImplementation Considerations\nAs you design your multi-agent system, consider:\nContext filtering strategy\n: Will each agent receive full conversation history, filtered portions, or summaries? Different agents may need different context depending on their role.\nTool semantics\n: Clarify whether handoff tools only update routing state or also perform side effects. For example, should\ntransfer_to_sales()\nalso create a support ticket, or should that be a separate action?\nToken efficiency\n: Balance context completeness against token costs. Summarization and selective context passing become more important as conversations grow longer.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nSubagents\nPrevious\nSkills\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/handoffs",
      "title": "Handoffs - Docs by LangChain",
      "heading": "Handoffs"
    }
  },
  {
    "page_content": "Skills - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nSkills\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nExtending the pattern\nAdvanced usage\nMulti-agent\nSkills\nCopy page\nCopy page\nIn the\nskills\narchitecture, specialized capabilities are packaged as invokable \u201cskills\u201d that augment an\nagent\u2019s\nbehavior. Skills are primarily prompt-driven specializations that an agent can invoke on-demand.\nThis pattern is conceptually identical to\nllms.txt\n(introduced by Jeremy Howard), which uses tool calling for progressive disclosure of documentation. The skills pattern applies the same approach to specialized prompts and domain knowledge rather than just documentation pages.\n\u200b\nKey characteristics\nPrompt-driven specialization: Skills are primarily defined by specialized prompts\nProgressive disclosure: Skills become available based on context or user needs\nTeam distribution: Different teams can develop and maintain skills independently\nLightweight composition: Skills are simpler than full sub-agents\n\u200b\nWhen to use\nUse the skills pattern when you want a single\nagent\nwith many possible specializations, you don\u2019t need to enforce specific constraints between skills, or different teams need to develop capabilities independently. Common examples include coding assistants (skills for different languages or tasks), knowledge bases (skills for different domains), and creative assistants (skills for different formats).\n\u200b\nBasic implementation\nCopy\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.agents\nimport\ncreate_agent\n@tool\ndef\nload_skill\n(\nskill_name\n:\nstr\n) ->\nstr\n:\n\"\"\"Load a specialized skill prompt.\nAvailable skills:\n- write_sql: SQL query writing expert\n- review_legal_doc: Legal document reviewer\nReturns the skill's prompt and context.\n\"\"\"\n# Load skill content from file/database\n...\nagent\n=\ncreate_agent(\nmodel\n=\n\"gpt-5-mini\"\n,\ntools\n=\n[load_skill],\nsystem_prompt\n=\n(\n\"You are a helpful assistant. \"\n\"You have access to two skills: \"\n\"write_sql and review_legal_doc. \"\n\"Use load_skill to access them.\"\n),\n)\nFor a complete implementation, see the tutorial below.\nTutorial: Build a SQL assistant with on-demand skills\nLearn how to implement skills with progressive disclosure, where the agent loads specialized prompts and schemas on-demand rather than upfront.\nLearn more\n\u200b\nExtending the pattern\nWhen writing custom implementations, you can extend the basic skills pattern in several ways:\nDynamic tool registration\n: Combine progressive disclosure with state management to register new\ntools\nas skills load. For example, loading a \u201cdatabase_admin\u201d skill could both add specialized context and register database-specific tools (backup, restore, migrate). This uses the same tool-and-state mechanisms used across multi-agent patterns\u2014tools updating state to dynamically change agent capabilities.\nHierarchical skills\n: Skills can define other skills in a tree structure, creating nested specializations. For instance, loading a \u201cdata_science\u201d skill might make available sub-skills like \u201cpandas_expert\u201d, \u201cvisualization\u201d, and \u201cstatistical_analysis\u201d. Each sub-skill can be loaded independently as needed, allowing for fine-grained progressive disclosure of domain knowledge. This hierarchical approach helps manage large knowledge bases by organizing capabilities into logical groupings that can be discovered and loaded on-demand.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nHandoffs\nPrevious\nRouter\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/skills",
      "title": "Skills - Docs by LangChain",
      "heading": "Skills"
    }
  },
  {
    "page_content": "Router - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nRouter\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nStateless vs. stateful\nStateless\nStateful\nTool wrapper\nFull persistence\nAdvanced usage\nMulti-agent\nRouter\nCopy page\nCopy page\nIn the\nrouter\narchitecture, a routing step classifies input and directs it to specialized\nagents\n. This is useful when you have distinct\nverticals\n\u2014separate knowledge domains that each require their own agent.\n\u200b\nKey characteristics\nRouter decomposes the query\nZero or more specialized agents are invoked in parallel\nResults are synthesized into a coherent response\n\u200b\nWhen to use\nUse the router pattern when you have distinct verticals (separate knowledge domains that each require their own agent), need to query multiple sources in parallel, and want to synthesize results into a combined response.\n\u200b\nBasic implementation\nThe router classifies the query and directs it to the appropriate agent(s). Use\nCommand\nfor single-agent routing or\nSend\nfor parallel fan-out to multiple agents.\nSingle agent\nMultiple agents (parallel)\nUse\nCommand\nto route to a single specialized agent:\nCopy\nfrom\nlanggraph.types\nimport\nCommand\ndef\nclassify_query\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Use LLM to classify query and determine the appropriate agent.\"\"\"\n# Classification logic here\n...\ndef\nroute_query\n(\nstate\n: State) -> Command:\n\"\"\"Route to the appropriate agent based on query classification.\"\"\"\nactive_agent\n=\nclassify_query(state[\n\"query\"\n])\n# Route to the selected agent\nreturn\nCommand(\ngoto\n=\nactive_agent)\nUse\nSend\nto fan out to multiple specialized agents in parallel:\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\nlanggraph.types\nimport\nSend\nclass\nClassificationResult\n(\nTypedDict\n):\nquery:\nstr\nagent:\nstr\ndef\nclassify_query\n(\nquery\n:\nstr\n) -> list[ClassificationResult]:\n\"\"\"Use LLM to classify query and determine which agents to invoke.\"\"\"\n# Classification logic here\n...\ndef\nroute_query\n(\nstate\n: State):\n\"\"\"Route to relevant agents based on query classification.\"\"\"\nclassifications\n=\nclassify_query(state[\n\"query\"\n])\n# Fan out to selected agents in parallel\nreturn\n[\nSend(c[\n\"agent\"\n], {\n\"query\"\n: c[\n\"query\"\n]})\nfor\nc\nin\nclassifications\n]\nFor a complete implementation, see the tutorial below.\nTutorial: Build a multi-source knowledge base with routing\nBuild a router that queries GitHub, Notion, and Slack in parallel, then synthesizes results into a coherent answer. Covers state definition, specialized agents, parallel execution with\nSend\n, and result synthesis.\n\u200b\nStateless vs. stateful\nTwo approaches:\nStateless routers\naddress each request independently\nStateful routers\nmaintain conversation history across requests\n\u200b\nStateless\nEach request is routed independently\u2014no memory between calls. For multi-turn conversations, see\nStateful routers\n.\nRouter vs. Subagents\n: Both patterns can dispatch work to multiple agents, but they differ in how routing decisions are made:\nRouter\n: A dedicated routing step (often a single LLM call or rule-based logic) that classifies the input and dispatches to agents. The router itself typically doesn\u2019t maintain conversation history or perform multi-turn orchestration\u2014it\u2019s a preprocessing step.\nSubagents\n: An main supervisor agent dynamically decides which\nsubagents\nto call as part of an ongoing conversation. The main agent maintains context, can call multiple subagents across turns, and orchestrates complex multi-step workflows.\nUse a\nrouter\nwhen you have clear input categories and want deterministic or lightweight classification. Use a\nsupervisor\nwhen you need flexible, conversation-aware orchestration where the LLM decides what to do next based on evolving context.\n\u200b\nStateful\nFor multi-turn conversations, you need to maintain context across invocations.\n\u200b\nTool wrapper\nThe simplest approach: wrap the stateless router as a tool that a conversational agent can call. The conversational agent handles memory and context; the router stays stateless. This avoids the complexity of managing conversation history across multiple parallel agents.\nCopy\n@tool\ndef\nsearch_docs\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search across multiple documentation sources.\"\"\"\nresult\n=\nworkflow.invoke({\n\"query\"\n: query})\nreturn\nresult[\n\"final_answer\"\n]\n# Conversational agent uses the router as a tool\nconversational_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[search_docs],\nprompt\n=\n\"You are a helpful assistant. Use search_docs to answer questions.\"\n)\n\u200b\nFull persistence\nIf you need the router itself to maintain state, use\npersistence\nto store message history. When routing to an agent, fetch previous messages from state and selectively include them in the agent\u2019s context\u2014this is a lever for\ncontext engineering\n.\nStateful routers require custom history management.\nIf the router switches between agents across turns, conversations may not feel fluid to end users when agents have different tones or prompts. With parallel invocation, you\u2019ll need to maintain history at the router level (inputs and synthesized outputs) and leverage this history in routing logic. Consider the\nhandoffs pattern\nor\nsubagents pattern\ninstead\u2014both provide clearer semantics for multi-turn conversations.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nSkills\nPrevious\nCustom workflow\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/router",
      "title": "Router - Docs by LangChain",
      "heading": "Router"
    }
  },
  {
    "page_content": "Custom workflow - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nCustom workflow\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nExample: RAG pipeline\nAdvanced usage\nMulti-agent\nCustom workflow\nCopy page\nCopy page\nIn the\ncustom workflow\narchitecture, you define your own bespoke execution flow using\nLangGraph\n. You have complete control over the graph structure\u2014including sequential steps, conditional branches, loops, and parallel execution.\n\u200b\nKey characteristics\nComplete control over graph structure\nMix deterministic logic with agentic behavior\nSupport for sequential steps, conditional branches, loops, and parallel execution\nEmbed other patterns as nodes in your workflow\n\u200b\nWhen to use\nUse custom workflows when standard patterns (subagents, skills, etc.) don\u2019t fit your requirements, you need to mix deterministic logic with agentic behavior, or your use case requires complex routing or multi-stage processing.\nEach node in your workflow can be a simple function, an LLM call, or an entire\nagent\nwith\ntools\n. You can also compose other architectures within a custom workflow\u2014for example, embedding a multi-agent system as a single node.\nFor a complete example of a custom workflow, see the tutorial below.\nTutorial: Build a multi-source knowledge base with routing\nThe\nrouter pattern\nis an example of a custom workflow. This tutorial walks through building a router that queries GitHub, Notion, and Slack in parallel, then synthesizes results.\nLearn more\n\u200b\nBasic implementation\nThe core insight is that you can call a LangChain agent directly inside any LangGraph node, combining the flexibility of custom workflows with the convenience of pre-built agents:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nagent\n=\ncreate_agent(\nmodel\n=\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[\n...\n])\ndef\nagent_node\n(\nstate\n: State) ->\ndict\n:\n\"\"\"A LangGraph node that invokes a LangChain agent.\"\"\"\nresult\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"query\"\n]}]\n})\nreturn\n{\n\"answer\"\n: result[\n\"messages\"\n][\n-\n1\n].content}\n# Build a simple workflow\nworkflow\n=\n(\nStateGraph(State)\n.add_node(\n\"agent\"\n, agent_node)\n.add_edge(\nSTART\n,\n\"agent\"\n)\n.add_edge(\n\"agent\"\n,\nEND\n)\n.compile()\n)\n\u200b\nExample: RAG pipeline\nA common use case is combining\nretrieval\nwith an agent. This example builds a WNBA stats assistant that retrieves from a knowledge base and can fetch live news.\nCustom RAG workflow\nThe workflow demonstrates three types of nodes:\nModel node\n(Rewrite): Rewrites the user query for better retrieval using\nstructured output\n.\nDeterministic node\n(Retrieve): Performs vector similarity search \u2014 no LLM involved.\nAgent node\n(Agent): Reasons over retrieved context and can fetch additional information via tools.\nYou can use LangGraph state to pass information between workflow steps. This allows each part of your workflow to read and update structured fields, making it easy to share data and context across nodes.\nCopy\nfrom\ntyping\nimport\nTypedDict\nfrom\npydantic\nimport\nBaseModel\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain_openai\nimport\nChatOpenAI, OpenAIEmbeddings\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nclass\nState\n(\nTypedDict\n):\nquestion:\nstr\nrewritten_query:\nstr\ndocuments: list[\nstr\n]\nanswer:\nstr\n# WNBA knowledge base with rosters, game results, and player stats\nembeddings\n=\nOpenAIEmbeddings()\nvector_store\n=\nInMemoryVectorStore(embeddings)\nvector_store.add_texts([\n# Rosters\n\"New York Liberty 2024 roster: Breanna Stewart, Sabrina Ionescu, Jonquel Jones, Courtney Vandersloot.\"\n,\n\"Las Vegas Aces 2024 roster: A'ja Wilson, Kelsey Plum, Jackie Young, Chelsea Gray.\"\n,\n\"Indiana Fever 2024 roster: Caitlin Clark, Aliyah Boston, Kelsey Mitchell, NaLyssa Smith.\"\n,\n# Game results\n\"2024 WNBA Finals: New York Liberty defeated Minnesota Lynx 3-2 to win the championship.\"\n,\n\"June 15, 2024: Indiana Fever 85, Chicago Sky 79. Caitlin Clark had 23 points and 8 assists.\"\n,\n\"August 20, 2024: Las Vegas Aces 92, Phoenix Mercury 84. A'ja Wilson scored 35 points.\"\n,\n# Player stats\n\"A'ja Wilson 2024 season stats: 26.9 PPG, 11.9 RPG, 2.6 BPG. Won MVP award.\"\n,\n\"Caitlin Clark 2024 rookie stats: 19.2 PPG, 8.4 APG, 5.7 RPG. Won Rookie of the Year.\"\n,\n\"Breanna Stewart 2024 stats: 20.4 PPG, 8.5 RPG, 3.5 APG.\"\n,\n])\nretriever\n=\nvector_store.as_retriever(\nsearch_kwargs\n=\n{\n\"k\"\n:\n5\n})\n@tool\ndef\nget_latest_news\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Get the latest WNBA news and updates.\"\"\"\n# Your news API here\nreturn\n\"Latest: The WNBA announced expanded playoff format for 2025...\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"openai:gpt-5-mini\"\n,\ntools\n=\n[get_latest_news],\n)\nmodel\n=\nChatOpenAI(\nmodel\n=\n\"gpt-5-mini\"\n)\nclass\nRewrittenQuery\n(\nBaseModel\n):\nquery:\nstr\ndef\nrewrite_query\n(\nstate\n: State) ->\ndict\n:\n\"\"\"Rewrite the user query for better retrieval.\"\"\"\nsystem_prompt\n=\n\"\"\"Rewrite this query to retrieve relevant WNBA information.\nThe knowledge base contains: team rosters, game results with scores, and player statistics (PPG, RPG, APG).\nFocus on specific player names, team names, or stat categories mentioned.\"\"\"\nresponse\n=\nmodel.with_structured_output(RewrittenQuery).invoke([\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n: system_prompt},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"question\"\n]}\n])\nreturn\n{\n\"rewritten_query\"\n: response.query}\ndef\nretrieve\n(\nstate\n: State) ->\ndict\n:\n\"\"\"Retrieve documents based on the rewritten query.\"\"\"\ndocs\n=\nretriever.invoke(state[\n\"rewritten_query\"\n])\nreturn\n{\n\"documents\"\n: [doc.page_content\nfor\ndoc\nin\ndocs]}\ndef\ncall_agent\n(\nstate\n: State) ->\ndict\n:\n\"\"\"Generate answer using retrieved context.\"\"\"\ncontext\n=\n\"\n\\n\\n\n\"\n.join(state[\n\"documents\"\n])\nprompt\n=\nf\n\"Context:\n\\n\n{\ncontext\n}\n\\n\\n\nQuestion:\n{\nstate[\n'question'\n]\n}\n\"\nresponse\n=\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: prompt}]})\nreturn\n{\n\"answer\"\n: response[\n\"messages\"\n][\n-\n1\n].content_blocks}\nworkflow\n=\n(\nStateGraph(State)\n.add_node(\n\"rewrite\"\n, rewrite_query)\n.add_node(\n\"retrieve\"\n, retrieve)\n.add_node(\n\"agent\"\n, call_agent)\n.add_edge(\nSTART\n,\n\"rewrite\"\n)\n.add_edge(\n\"rewrite\"\n,\n\"retrieve\"\n)\n.add_edge(\n\"retrieve\"\n,\n\"agent\"\n)\n.add_edge(\n\"agent\"\n,\nEND\n)\n.compile()\n)\nresult\n=\nworkflow.invoke({\n\"question\"\n:\n\"Who won the 2024 WNBA Championship?\"\n})\nprint\n(result[\n\"answer\"\n])\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nRouter\nPrevious\nRetrieval\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/custom-workflow",
      "title": "Custom workflow - Docs by LangChain",
      "heading": "Custom workflow"
    }
  },
  {
    "page_content": "Build a personal assistant with subagents - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nBuild a personal assistant with subagents\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nWhy use a supervisor?\nConcepts\nSetup\nInstallation\nLangSmith\nComponents\n1. Define tools\n2. Create specialized sub-agents\nCreate a calendar agent\nCreate an email agent\n3. Wrap sub-agents as tools\n4. Create the supervisor agent\n5. Use the supervisor\nExample 1: Simple single-domain request\nExample 2: Complex multi-domain request\nComplete working example\nUnderstanding the architecture\n6. Add human-in-the-loop review\n7. Advanced: Control information flow\nPass additional conversational context to sub-agents\nControl what supervisor receives\n8. Key takeaways\nNext steps\nTutorials\nMulti-agent\nBuild a personal assistant with subagents\nCopy page\nCopy page\n\u200b\nOverview\nThe\nsupervisor pattern\nis a\nmulti-agent\narchitecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\nIn this tutorial, you\u2019ll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\nA\ncalendar agent\nthat handles scheduling, availability checking, and event management.\nAn\nemail agent\nthat manages communication, drafts messages, and sends notifications.\nWe will also incorporate\nhuman-in-the-loop review\nto allow users to approve, edit, and reject actions (such as outbound emails) as desired.\n\u200b\nWhy use a supervisor?\nMulti-agent architectures allow you to partition\ntools\nacross workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\n\u200b\nConcepts\nWe will cover the following concepts:\nMulti-agent systems\nHuman-in-the-loop review\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires the\nlangchain\npackage:\npip\nconda\nCopy\npip\ninstall\nlangchain\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nSet up\nLangSmith\nto inspect what is happening inside your agent. Then set the following environment variables:\nbash\npython\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\n\u200b\nComponents\nWe will need to select a chat model from LangChain\u2019s suite of integrations:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\n\u200b\n1. Define tools\nStart by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, you\u2019ll use stubs to demonstrate the pattern.\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\ndef\ncreate_calendar_event\n(\ntitle\n:\nstr\n,\nstart_time\n:\nstr\n,\n# ISO format: \"2024-01-15T14:00:00\"\nend_time\n:\nstr\n,\n# ISO format: \"2024-01-15T15:00:00\"\nattendees\n: list[\nstr\n],\n# email addresses\nlocation\n:\nstr\n=\n\"\"\n) ->\nstr\n:\n\"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\n# Stub: In practice, this would call Google Calendar API, Outlook API, etc.\nreturn\nf\n\"Event created:\n{\ntitle\n}\nfrom\n{\nstart_time\n}\nto\n{\nend_time\n}\nwith\n{\nlen\n(attendees)\n}\nattendees\"\n@tool\ndef\nsend_email\n(\nto\n: list[\nstr\n],\n# email addresses\nsubject\n:\nstr\n,\nbody\n:\nstr\n,\ncc\n: list[\nstr\n]\n=\n[]\n) ->\nstr\n:\n\"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n# Stub: In practice, this would call SendGrid, Gmail API, etc.\nreturn\nf\n\"Email sent to\n{\n', '\n.join(to)\n}\n- Subject:\n{\nsubject\n}\n\"\n@tool\ndef\nget_available_time_slots\n(\nattendees\n: list[\nstr\n],\ndate\n:\nstr\n,\n# ISO format: \"2024-01-15\"\nduration_minutes\n:\nint\n) -> list[\nstr\n]:\n\"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n# Stub: In practice, this would query calendar APIs\nreturn\n[\n\"09:00\"\n,\n\"14:00\"\n,\n\"16:00\"\n]\n\u200b\n2. Create specialized sub-agents\nNext, we\u2019ll create specialized sub-agents that handle each domain.\n\u200b\nCreate a calendar agent\nThe calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nCALENDAR_AGENT_PROMPT\n=\n(\n\"You are a calendar scheduling assistant. \"\n\"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n\"into proper ISO datetime formats. \"\n\"Use get_available_time_slots to check availability when needed. \"\n\"Use create_calendar_event to schedule events. \"\n\"Always confirm what was scheduled in your final response.\"\n)\ncalendar_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[create_calendar_event, get_available_time_slots],\nsystem_prompt\n=\nCALENDAR_AGENT_PROMPT\n,\n)\nTest the calendar agent to see how it handles natural language scheduling:\nCopy\nquery\n=\n\"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\nfor\nstep\nin\ncalendar_agent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]}\n):\nfor\nupdate\nin\nstep.values():\nfor\nmessage\nin\nupdate.get(\n\"messages\"\n, []):\nmessage.pretty_print()\nCopy\n================================== Ai Message ==================================\nTool Calls:\nget_available_time_slots (call_EIeoeIi1hE2VmwZSfHStGmXp)\nCall ID: call_EIeoeIi1hE2VmwZSfHStGmXp\nArgs:\nattendees: []\ndate: 2024-06-18\nduration_minutes: 60\n================================= Tool Message =================================\nName: get_available_time_slots\n[\"09:00\", \"14:00\", \"16:00\"]\n================================== Ai Message ==================================\nTool Calls:\ncreate_calendar_event (call_zgx3iJA66Ut0W8S3NpT93kEB)\nCall ID: call_zgx3iJA66Ut0W8S3NpT93kEB\nArgs:\ntitle: Team Meeting\nstart_time: 2024-06-18T14:00:00\nend_time: 2024-06-18T15:00:00\nattendees: []\n================================= Tool Message =================================\nName: create_calendar_event\nEvent created: Team Meeting from 2024-06-18T14:00:00 to 2024-06-18T15:00:00 with 0 attendees\n================================== Ai Message ==================================\nThe team meeting has been scheduled for next Tuesday, June 18th, at 2:00 PM and will last for 1 hour. If you need to add attendees or a location, please let me know!\nThe agent parses \u201cnext Tuesday at 2pm\u201d into ISO format (\u201c2024-01-16T14:00:00\u201d), calculates the end time, calls\ncreate_calendar_event\n, and returns a natural language confirmation.\n\u200b\nCreate an email agent\nThe email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.\nCopy\nEMAIL_AGENT_PROMPT\n=\n(\n\"You are an email assistant. \"\n\"Compose professional emails based on natural language requests. \"\n\"Extract recipient information and craft appropriate subject lines and body text. \"\n\"Use send_email to send the message. \"\n\"Always confirm what was sent in your final response.\"\n)\nemail_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[send_email],\nsystem_prompt\n=\nEMAIL_AGENT_PROMPT\n,\n)\nTest the email agent with a natural language request:\nCopy\nquery\n=\n\"Send the design team a reminder about reviewing the new mockups\"\nfor\nstep\nin\nemail_agent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]}\n):\nfor\nupdate\nin\nstep.values():\nfor\nmessage\nin\nupdate.get(\n\"messages\"\n, []):\nmessage.pretty_print()\nCopy\n================================== Ai Message ==================================\nTool Calls:\nsend_email (call_OMl51FziTVY6CRZvzYfjYOZr)\nCall ID: call_OMl51FziTVY6CRZvzYfjYOZr\nArgs:\nto: ['\n[email\u00a0protected]\n']\nsubject: Reminder: Please Review the New Mockups\nbody: Hi Design Team,\nThis is a friendly reminder to review the new mockups at your earliest convenience. Your feedback is important to ensure that we stay on track with our project timeline.\nPlease let me know if you have any questions or need additional information.\nThank you!\nBest regards,\n================================= Tool Message =================================\nName: send_email\nEmail sent to\n[email\u00a0protected]\n- Subject: Reminder: Please Review the New Mockups\n================================== Ai Message ==================================\nI've sent a reminder to the design team asking them to review the new mockups. If you need any further communication on this topic, just let me know!\nThe agent infers the recipient from the informal request, crafts a professional subject line and body, calls\nsend_email\n, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task.\n\u200b\n3. Wrap sub-agents as tools\nNow wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like \u201cschedule_event\u201d, not low-level tools like \u201ccreate_calendar_event\u201d.\nCopy\n@tool\ndef\nschedule_event\n(\nrequest\n:\nstr\n) ->\nstr\n:\n\"\"\"Schedule calendar events using natural language.\nUse this when the user wants to create, modify, or check calendar appointments.\nHandles date/time parsing, availability checking, and event creation.\nInput: Natural language scheduling request (e.g., 'meeting with design team\nnext Tuesday at 2pm')\n\"\"\"\nresult\n=\ncalendar_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: request}]\n})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].text\n@tool\ndef\nmanage_email\n(\nrequest\n:\nstr\n) ->\nstr\n:\n\"\"\"Send emails using natural language.\nUse this when the user wants to send notifications, reminders, or any email\ncommunication. Handles recipient extraction, subject generation, and email\ncomposition.\nInput: Natural language email request (e.g., 'send them a reminder about\nthe meeting')\n\"\"\"\nresult\n=\nemail_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: request}]\n})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].text\nThe tool descriptions help the supervisor decide when to use each tool, so make them clear and specific. We return only the sub-agent\u2019s final response, as the supervisor doesn\u2019t need to see intermediate reasoning or tool calls.\n\u200b\n4. Create the supervisor agent\nNow create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.\nCopy\nSUPERVISOR_PROMPT\n=\n(\n\"You are a helpful personal assistant. \"\n\"You can schedule calendar events and send emails. \"\n\"Break down user requests into appropriate tool calls and coordinate the results. \"\n\"When a request involves multiple actions, use multiple tools in sequence.\"\n)\nsupervisor_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[schedule_event, manage_email],\nsystem_prompt\n=\nSUPERVISOR_PROMPT\n,\n)\n\u200b\n5. Use the supervisor\nNow test your complete system with complex requests that require coordination across multiple domains:\n\u200b\nExample 1: Simple single-domain request\nCopy\nquery\n=\n\"Schedule a team standup for tomorrow at 9am\"\nfor\nstep\nin\nsupervisor_agent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]}\n):\nfor\nupdate\nin\nstep.values():\nfor\nmessage\nin\nupdate.get(\n\"messages\"\n, []):\nmessage.pretty_print()\nCopy\n================================== Ai Message ==================================\nTool Calls:\nschedule_event (call_mXFJJDU8bKZadNUZPaag8Lct)\nCall ID: call_mXFJJDU8bKZadNUZPaag8Lct\nArgs:\nrequest: Schedule a team standup for tomorrow at 9am with Alice and Bob.\n================================= Tool Message =================================\nName: schedule_event\nThe team standup has been scheduled for tomorrow at 9:00 AM with Alice and Bob. If you need to make any changes or add more details, just let me know!\n================================== Ai Message ==================================\nThe team standup with Alice and Bob is scheduled for tomorrow at 9:00 AM. If you need any further arrangements or adjustments, please let me know!\nThe supervisor identifies this as a calendar task, calls\nschedule_event\n, and the calendar agent handles date parsing and event creation.\nFor full transparency into the information flow, including prompts and responses for each chat model call, check out the\nLangSmith trace\nfor the above run.\n\u200b\nExample 2: Complex multi-domain request\nCopy\nquery\n=\n(\n\"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n\"and send them an email reminder about reviewing the new mockups.\"\n)\nfor\nstep\nin\nsupervisor_agent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]}\n):\nfor\nupdate\nin\nstep.values():\nfor\nmessage\nin\nupdate.get(\n\"messages\"\n, []):\nmessage.pretty_print()\nCopy\n================================== Ai Message ==================================\nTool Calls:\nschedule_event (call_YA68mqF0koZItCFPx0kGQfZi)\nCall ID: call_YA68mqF0koZItCFPx0kGQfZi\nArgs:\nrequest: meeting with the design team next Tuesday at 2pm for 1 hour\nmanage_email (call_XxqcJBvVIuKuRK794ZIzlLxx)\nCall ID: call_XxqcJBvVIuKuRK794ZIzlLxx\nArgs:\nrequest: send the design team an email reminder about reviewing the new mockups\n================================= Tool Message =================================\nName: schedule_event\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm. Let me know if you need to add more details or make any changes!\n================================= Tool Message =================================\nName: manage_email\nI've sent an email reminder to the design team requesting them to review the new mockups. If you need to include more information or recipients, just let me know!\n================================== Ai Message ==================================\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm.\nI've also sent an email reminder to the design team, asking them to review the new mockups.\nLet me know if you'd like to add more details to the meeting or include additional information in the email!\nThe supervisor recognizes this requires both calendar and email actions, calls\nschedule_event\nfor the meeting, then calls\nmanage_email\nfor the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\nRefer to the\nLangSmith trace\nto see the detailed information flow for the above run, including individual chat model prompts and responses.\n\u200b\nComplete working example\nHere\u2019s everything together in a runnable script:\nShow\nView complete code\nCopy\n\"\"\"\nPersonal Assistant Supervisor Example\nThis example demonstrates the tool calling pattern for multi-agent systems.\nA supervisor agent coordinates specialized sub-agents (calendar and email)\nthat are wrapped as tools.\n\"\"\"\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# ============================================================================\n# Step 1: Define low-level API tools (stubbed)\n# ============================================================================\n@tool\ndef\ncreate_calendar_event\n(\ntitle\n:\nstr\n,\nstart_time\n:\nstr\n,\n# ISO format: \"2024-01-15T14:00:00\"\nend_time\n:\nstr\n,\n# ISO format: \"2024-01-15T15:00:00\"\nattendees\n: list[\nstr\n],\n# email addresses\nlocation\n:\nstr\n=\n\"\"\n) ->\nstr\n:\n\"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\nreturn\nf\n\"Event created:\n{\ntitle\n}\nfrom\n{\nstart_time\n}\nto\n{\nend_time\n}\nwith\n{\nlen\n(attendees)\n}\nattendees\"\n@tool\ndef\nsend_email\n(\nto\n: list[\nstr\n],\n# email addresses\nsubject\n:\nstr\n,\nbody\n:\nstr\n,\ncc\n: list[\nstr\n]\n=\n[]\n) ->\nstr\n:\n\"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\nreturn\nf\n\"Email sent to\n{\n', '\n.join(to)\n}\n- Subject:\n{\nsubject\n}\n\"\n@tool\ndef\nget_available_time_slots\n(\nattendees\n: list[\nstr\n],\ndate\n:\nstr\n,\n# ISO format: \"2024-01-15\"\nduration_minutes\n:\nint\n) -> list[\nstr\n]:\n\"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\nreturn\n[\n\"09:00\"\n,\n\"14:00\"\n,\n\"16:00\"\n]\n# ============================================================================\n# Step 2: Create specialized sub-agents\n# ============================================================================\nmodel\n=\ninit_chat_model(\n\"claude-haiku-4-5-20251001\"\n)\n# for example\ncalendar_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[create_calendar_event, get_available_time_slots],\nsystem_prompt\n=\n(\n\"You are a calendar scheduling assistant. \"\n\"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n\"into proper ISO datetime formats. \"\n\"Use get_available_time_slots to check availability when needed. \"\n\"Use create_calendar_event to schedule events. \"\n\"Always confirm what was scheduled in your final response.\"\n)\n)\nemail_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[send_email],\nsystem_prompt\n=\n(\n\"You are an email assistant. \"\n\"Compose professional emails based on natural language requests. \"\n\"Extract recipient information and craft appropriate subject lines and body text. \"\n\"Use send_email to send the message. \"\n\"Always confirm what was sent in your final response.\"\n)\n)\n# ============================================================================\n# Step 3: Wrap sub-agents as tools for the supervisor\n# ============================================================================\n@tool\ndef\nschedule_event\n(\nrequest\n:\nstr\n) ->\nstr\n:\n\"\"\"Schedule calendar events using natural language.\nUse this when the user wants to create, modify, or check calendar appointments.\nHandles date/time parsing, availability checking, and event creation.\nInput: Natural language scheduling request (e.g., 'meeting with design team\nnext Tuesday at 2pm')\n\"\"\"\nresult\n=\ncalendar_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: request}]\n})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].text\n@tool\ndef\nmanage_email\n(\nrequest\n:\nstr\n) ->\nstr\n:\n\"\"\"Send emails using natural language.\nUse this when the user wants to send notifications, reminders, or any email\ncommunication. Handles recipient extraction, subject generation, and email\ncomposition.\nInput: Natural language email request (e.g., 'send them a reminder about\nthe meeting')\n\"\"\"\nresult\n=\nemail_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: request}]\n})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].text\n# ============================================================================\n# Step 4: Create the supervisor agent\n# ============================================================================\nsupervisor_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[schedule_event, manage_email],\nsystem_prompt\n=\n(\n\"You are a helpful personal assistant. \"\n\"You can schedule calendar events and send emails. \"\n\"Break down user requests into appropriate tool calls and coordinate the results. \"\n\"When a request involves multiple actions, use multiple tools in sequence.\"\n)\n)\n# ============================================================================\n# Step 5: Use the supervisor\n# ============================================================================\nif\n__name__\n==\n\"__main__\"\n:\n# Example: User request requiring both calendar and email coordination\nuser_request\n=\n(\n\"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n\"and send them an email reminder about reviewing the new mockups.\"\n)\nprint\n(\n\"User Request:\"\n, user_request)\nprint\n(\n\"\n\\n\n\"\n+\n\"=\"\n*\n80\n+\n\"\n\\n\n\"\n)\nfor\nstep\nin\nsupervisor_agent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: user_request}]}\n):\nfor\nupdate\nin\nstep.values():\nfor\nmessage\nin\nupdate.get(\n\"messages\"\n, []):\nmessage.pretty_print()\n\u200b\nUnderstanding the architecture\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\n\u200b\n6. Add human-in-the-loop review\nIt can be prudent to incorporate\nhuman-in-the-loop review\nof sensitive actions. LangChain includes\nbuilt-in middleware\nto review tool calls, in this case the tools invoked by sub-agents.\nLet\u2019s add human-in-the-loop review to both sub-agents:\nWe configure the\ncreate_calendar_event\nand\nsend_email\ntools to interrupt, permitting all\nresponse types\n(\napprove\n,\nedit\n,\nreject\n)\nWe add a\ncheckpointer\nonly to the top-level agent\n. This is required to pause and resume execution.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nHumanInTheLoopMiddleware\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\ncalendar_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[create_calendar_event, get_available_time_slots],\nsystem_prompt\n=\nCALENDAR_AGENT_PROMPT\n,\nmiddleware\n=\n[\nHumanInTheLoopMiddleware(\ninterrupt_on\n=\n{\n\"create_calendar_event\"\n:\nTrue\n},\ndescription_prefix\n=\n\"Calendar event pending approval\"\n,\n),\n],\n)\nemail_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[send_email],\nsystem_prompt\n=\nEMAIL_AGENT_PROMPT\n,\nmiddleware\n=\n[\nHumanInTheLoopMiddleware(\ninterrupt_on\n=\n{\n\"send_email\"\n:\nTrue\n},\ndescription_prefix\n=\n\"Outbound email pending approval\"\n,\n),\n],\n)\nsupervisor_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[schedule_event, manage_email],\nsystem_prompt\n=\nSUPERVISOR_PROMPT\n,\ncheckpointer\n=\nInMemorySaver(),\n)\nLet\u2019s repeat the query. Note that we gather interrupt events into a list to access downstream:\nCopy\nquery\n=\n(\n\"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n\"and send them an email reminder about reviewing the new mockups.\"\n)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"6\"\n}}\ninterrupts\n=\n[]\nfor\nstep\nin\nsupervisor_agent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nconfig,\n):\nfor\nupdate\nin\nstep.values():\nif\nisinstance\n(update,\ndict\n):\nfor\nmessage\nin\nupdate.get(\n\"messages\"\n, []):\nmessage.pretty_print()\nelse\n:\ninterrupt_\n=\nupdate[\n0\n]\ninterrupts.append(interrupt_)\nprint\n(\nf\n\"\n\\n\nINTERRUPTED:\n{\ninterrupt_.id\n}\n\"\n)\nCopy\n================================== Ai Message ==================================\nTool Calls:\nschedule_event (call_t4Wyn32ohaShpEZKuzZbl83z)\nCall ID: call_t4Wyn32ohaShpEZKuzZbl83z\nArgs:\nrequest: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.\nmanage_email (call_JWj4vDJ5VMnvkySymhCBm4IR)\nCall ID: call_JWj4vDJ5VMnvkySymhCBm4IR\nArgs:\nrequest: Send an email reminder to the design team about reviewing the new mockups before our meeting next Tuesday at 2pm.\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\nThis time we\u2019ve interrupted execution. Let\u2019s inspect the interrupt events:\nCopy\nfor\ninterrupt_\nin\ninterrupts:\nfor\nrequest\nin\ninterrupt_.value[\n\"action_requests\"\n]:\nprint\n(\nf\n\"INTERRUPTED:\n{\ninterrupt_.id\n}\n\"\n)\nprint\n(\nf\n\"\n{\nrequest[\n'description'\n]\n}\n\\n\n\"\n)\nCopy\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\nCalendar event pending approval\nTool: create_calendar_event\nArgs: {'title': 'Meeting with the Design Team', 'start_time': '2024-06-18T14:00:00', 'end_time': '2024-06-18T15:00:00', 'attendees': ['design team']}\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\nOutbound email pending approval\nTool: send_email\nArgs: {'to': ['\n[email\u00a0protected]\n'], 'subject': 'Reminder: Review New Mockups Before Meeting Next Tuesday at 2pm', 'body': \"Hello Team,\\n\\nThis is a reminder to review the new mockups ahead of our meeting scheduled for next Tuesday at 2pm. Your feedback and insights will be valuable for our discussion and next steps.\\n\\nPlease ensure you've gone through the designs and are ready to share your thoughts during the meeting.\\n\\nThank you!\\n\\nBest regards,\\n[Your Name]\"}\nWe can specify decisions for each interrupt by referring to its ID using a\nCommand\n. Refer to the\nhuman-in-the-loop guide\nfor additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email:\nCopy\nfrom\nlanggraph.types\nimport\nCommand\nresume\n=\n{}\nfor\ninterrupt_\nin\ninterrupts:\nif\ninterrupt_.id\n==\n\"2b56f299be313ad8bc689eff02973f16\"\n:\n# Edit email\nedited_action\n=\ninterrupt_.value[\n\"action_requests\"\n][\n0\n].copy()\nedited_action[\n\"arguments\"\n][\n\"subject\"\n]\n=\n\"Mockups reminder\"\nresume[interrupt_.id]\n=\n{\n\"decisions\"\n: [{\n\"type\"\n:\n\"edit\"\n,\n\"edited_action\"\n: edited_action}]\n}\nelse\n:\nresume[interrupt_.id]\n=\n{\n\"decisions\"\n: [{\n\"type\"\n:\n\"approve\"\n}]}\ninterrupts\n=\n[]\nfor\nstep\nin\nsupervisor_agent.stream(\nCommand(\nresume\n=\nresume),\nconfig,\n):\nfor\nupdate\nin\nstep.values():\nif\nisinstance\n(update,\ndict\n):\nfor\nmessage\nin\nupdate.get(\n\"messages\"\n, []):\nmessage.pretty_print()\nelse\n:\ninterrupt_\n=\nupdate[\n0\n]\ninterrupts.append(interrupt_)\nprint\n(\nf\n\"\n\\n\nINTERRUPTED:\n{\ninterrupt_.id\n}\n\"\n)\nCopy\n================================= Tool Message =================================\nName: schedule_event\nYour meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\n================================= Tool Message =================================\nName: manage_email\nYour email reminder to the design team has been sent. Here\u2019s what was sent:\n- Recipient:\n[email\u00a0protected]\n- Subject: Mockups reminder\n- Body: A reminder to review the new mockups before the meeting next Tuesday at 2pm, with a request for feedback and readiness for discussion.\nLet me know if you need any further assistance!\n================================== Ai Message ==================================\n- Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\n- An email reminder has been sent to the design team about reviewing the new mockups before the meeting.\nLet me know if you need any further assistance!\nThe run proceeds with our input.\n\u200b\n7. Advanced: Control information flow\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.\n\u200b\nPass additional conversational context to sub-agents\nCopy\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\n@tool\ndef\nschedule_event\n(\nrequest\n:\nstr\n,\nruntime\n: ToolRuntime\n) ->\nstr\n:\n\"\"\"Schedule calendar events using natural language.\"\"\"\n# Customize context received by sub-agent\noriginal_user_message\n=\nnext\n(\nmessage\nfor\nmessage\nin\nruntime.state[\n\"messages\"\n]\nif\nmessage.type\n==\n\"human\"\n)\nprompt\n=\n(\n\"You are assisting with the following user inquiry:\n\\n\\n\n\"\nf\n\"\n{\noriginal_user_message.text\n}\n\\n\\n\n\"\n\"You are tasked with the following sub-request:\n\\n\\n\n\"\nf\n\"\n{\nrequest\n}\n\"\n)\nresult\n=\ncalendar_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: prompt}],\n})\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].text\nThis allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like \u201cschedule it for the same time tomorrow\u201d (referencing a previous conversation).\nYou can see the full context received by the sub agent in the\nchat model call\nof the LangSmith trace.\n\u200b\nControl what supervisor receives\nYou can also customize what information flows back to the supervisor:\nCopy\nimport\njson\n@tool\ndef\nschedule_event\n(\nrequest\n:\nstr\n) ->\nstr\n:\n\"\"\"Schedule calendar events using natural language.\"\"\"\nresult\n=\ncalendar_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: request}]\n})\n# Option 1: Return just the confirmation message\nreturn\nresult[\n\"messages\"\n][\n-\n1\n].text\n# Option 2: Return structured data\n# return json.dumps({\n#     \"status\": \"success\",\n#     \"event_id\": \"evt_123\",\n#     \"summary\": result[\"messages\"][-1].text\n# })\nImportant:\nMake sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but don\u2019t include the results in their final response.\n\u200b\n8. Key takeaways\nThe supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.\nWhen to use the supervisor pattern\nUse the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents don\u2019t need to converse directly with users.\nFor simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use\nhandoffs\ninstead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.\n\u200b\nNext steps\nLearn about\nhandoffs\nfor agent-to-agent conversations, explore\ncontext engineering\nto fine-tune information flow, read the\nmulti-agent overview\nto compare different patterns, and use\nLangSmith\nto debug and monitor your multi-agent system.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a voice agent with LangChain\nPrevious\nBuild customer support with handoffs\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/subagents-personal-assistant",
      "title": "Build a personal assistant with subagents - Docs by LangChain",
      "heading": "Build a personal assistant with subagents"
    }
  },
  {
    "page_content": "Build customer support with handoffs - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nBuild customer support with handoffs\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nSetup\nInstallation\nLangSmith\nSelect an LLM\n1. Define custom state\n2. Create tools that manage workflow state\n3. Define step configurations\n4. Create step-based middleware\n5. Create the agent\n6. Test the workflow\n7. Understanding state transitions\nTurn 1: Initial message\nTurn 2: After warranty recorded\nTurn 3: After issue classified\n8. Manage message history\n9. Add flexibility: Go back\nComplete example\nNext steps\nTutorials\nMulti-agent\nBuild customer support with handoffs\nCopy page\nCopy page\nThe\nstate machine pattern\ndescribes workflows where an agent\u2019s behavior changes as it moves through different states of a task. This tutorial shows how to implement a state machine by using tool calls to dynamically change a single agent\u2019s configuration\u2014updating its available tools and instructions based on the current state. The state can be determined from multiple sources: the agent\u2019s past actions (tool calls), external state (such as API call results), or even initial user input (for example, by running a classifier to determine user intent).\nIn this tutorial, you\u2019ll build a customer support agent that does the following:\nCollects warranty information before proceeding.\nClassifies issues as hardware or software.\nProvides solutions or escalates to human support.\nMaintains conversation state across multiple turns.\nUnlike the\nsubagents pattern\nwhere sub-agents are called as tools, the\nstate machine pattern\nuses a single agent whose configuration changes based on workflow progress. Each \u201cstep\u201d is just a different configuration (system prompt + tools) of the same underlying agent, selected dynamically based on state.\nHere\u2019s the workflow we\u2019ll build:\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires the\nlangchain\npackage:\npip\nuv\nconda\nCopy\npip\ninstall\nlangchain\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nSet up\nLangSmith\nto inspect what is happening inside your agent. Then set the following environment variables:\nbash\npython\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\n\u200b\nSelect an LLM\nSelect a chat model from LangChain\u2019s suite of integrations:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\n\u200b\n1. Define custom state\nFirst, define a custom state schema that tracks which step is currently active:\nCopy\nfrom\nlangchain.agents\nimport\nAgentState\nfrom\ntyping_extensions\nimport\nNotRequired\nfrom\ntyping\nimport\nLiteral\n# Define the possible workflow steps\nSupportStep\n=\nLiteral[\n\"warranty_collector\"\n,\n\"issue_classifier\"\n,\n\"resolution_specialist\"\n]\nclass\nSupportState\n(\nAgentState\n):\n\"\"\"State for customer support workflow.\"\"\"\ncurrent_step: NotRequired[SupportStep]\nwarranty_status: NotRequired[Literal[\n\"in_warranty\"\n,\n\"out_of_warranty\"\n]]\nissue_type: NotRequired[Literal[\n\"hardware\"\n,\n\"software\"\n]]\nThe\ncurrent_step\nfield is the core of the state machine pattern - it determines which configuration (prompt + tools) is loaded on each turn.\n\u200b\n2. Create tools that manage workflow state\nCreate tools that update the workflow state. These tools allow the agent to record information and transition to the next step.\nThe key is using\nCommand\nto update state, including the\ncurrent_step\nfield:\nCopy\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlangchain.messages\nimport\nToolMessage\nfrom\nlanggraph.types\nimport\nCommand\n@tool\ndef\nrecord_warranty_status\n(\nstatus\n: Literal[\n\"in_warranty\"\n,\n\"out_of_warranty\"\n],\nruntime\n: ToolRuntime[\nNone\n, SupportState],\n) -> Command:\n\"\"\"Record the customer's warranty status and transition to issue classification.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\ncontent\n=\nf\n\"Warranty status recorded as:\n{\nstatus\n}\n\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\n],\n\"warranty_status\"\n: status,\n\"current_step\"\n:\n\"issue_classifier\"\n,\n}\n)\n@tool\ndef\nrecord_issue_type\n(\nissue_type\n: Literal[\n\"hardware\"\n,\n\"software\"\n],\nruntime\n: ToolRuntime[\nNone\n, SupportState],\n) -> Command:\n\"\"\"Record the type of issue and transition to resolution specialist.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\ncontent\n=\nf\n\"Issue type recorded as:\n{\nissue_type\n}\n\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\n],\n\"issue_type\"\n: issue_type,\n\"current_step\"\n:\n\"resolution_specialist\"\n,\n}\n)\n@tool\ndef\nescalate_to_human\n(\nreason\n:\nstr\n) ->\nstr\n:\n\"\"\"Escalate the case to a human support specialist.\"\"\"\n# In a real system, this would create a ticket, notify staff, etc.\nreturn\nf\n\"Escalating to human support. Reason:\n{\nreason\n}\n\"\n@tool\ndef\nprovide_solution\n(\nsolution\n:\nstr\n) ->\nstr\n:\n\"\"\"Provide a solution to the customer's issue.\"\"\"\nreturn\nf\n\"Solution provided:\n{\nsolution\n}\n\"\nNotice how\nrecord_warranty_status\nand\nrecord_issue_type\nreturn\nCommand\nobjects that update both the data (\nwarranty_status\n,\nissue_type\n) AND the\ncurrent_step\n. This is how the state machine works - tools control workflow progression.\n\u200b\n3. Define step configurations\nDefine prompts and tools for each step. First, define the prompts for each step:\nView complete prompt definitions\nCopy\n# Define prompts as constants for easy reference\nWARRANTY_COLLECTOR_PROMPT\n=\n\"\"\"You are a customer support agent helping with device issues.\nCURRENT STAGE: Warranty verification\nAt this step, you need to:\n1. Greet the customer warmly\n2. Ask if their device is under warranty\n3. Use record_warranty_status to record their response and move to the next step\nBe conversational and friendly. Don't ask multiple questions at once.\"\"\"\nISSUE_CLASSIFIER_PROMPT\n=\n\"\"\"You are a customer support agent helping with device issues.\nCURRENT STAGE: Issue classification\nCUSTOMER INFO: Warranty status is\n{warranty_status}\nAt this step, you need to:\n1. Ask the customer to describe their issue\n2. Determine if it's a hardware issue (physical damage, broken parts) or software issue (app crashes, performance)\n3. Use record_issue_type to record the classification and move to the next step\nIf unclear, ask clarifying questions before classifying.\"\"\"\nRESOLUTION_SPECIALIST_PROMPT\n=\n\"\"\"You are a customer support agent helping with device issues.\nCURRENT STAGE: Resolution\nCUSTOMER INFO: Warranty status is\n{warranty_status}\n, issue type is\n{issue_type}\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n- If IN WARRANTY: explain warranty repair process using provide_solution\n- If OUT OF WARRANTY: escalate_to_human for paid repair options\nBe specific and helpful in your solutions.\"\"\"\nThen map step names to their configurations using a dictionary:\nCopy\n# Step configuration: maps step name to (prompt, tools, required_state)\nSTEP_CONFIG\n=\n{\n\"warranty_collector\"\n: {\n\"prompt\"\n:\nWARRANTY_COLLECTOR_PROMPT\n,\n\"tools\"\n: [record_warranty_status],\n\"requires\"\n: [],\n},\n\"issue_classifier\"\n: {\n\"prompt\"\n:\nISSUE_CLASSIFIER_PROMPT\n,\n\"tools\"\n: [record_issue_type],\n\"requires\"\n: [\n\"warranty_status\"\n],\n},\n\"resolution_specialist\"\n: {\n\"prompt\"\n:\nRESOLUTION_SPECIALIST_PROMPT\n,\n\"tools\"\n: [provide_solution, escalate_to_human],\n\"requires\"\n: [\n\"warranty_status\"\n,\n\"issue_type\"\n],\n},\n}\nThis dictionary-based configuration makes it easy to:\nSee all steps at a glance\nAdd new steps (just add another entry)\nUnderstand the workflow dependencies (\nrequires\nfield)\nUse prompt templates with state variables (e.g.,\n{warranty_status}\n)\n\u200b\n4. Create step-based middleware\nCreate middleware that reads\ncurrent_step\nfrom state and applies the appropriate configuration. We\u2019ll use the\n@wrap_model_call\ndecorator for a clean implementation:\nCopy\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse\nfrom\ntyping\nimport\nCallable\n@wrap_model_call\ndef\napply_step_config\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n\"\"\"Configure agent behavior based on the current step.\"\"\"\n# Get current step (defaults to warranty_collector for first interaction)\ncurrent_step\n=\nrequest.state.get(\n\"current_step\"\n,\n\"warranty_collector\"\n)\n# Look up step configuration\nstage_config\n=\nSTEP_CONFIG\n[current_step]\n# Validate required state exists\nfor\nkey\nin\nstage_config[\n\"requires\"\n]:\nif\nrequest.state.get(key)\nis\nNone\n:\nraise\nValueError\n(\nf\n\"\n{\nkey\n}\nmust be set before reaching\n{\ncurrent_step\n}\n\"\n)\n# Format prompt with state values (supports {warranty_status}, {issue_type}, etc.)\nsystem_prompt\n=\nstage_config[\n\"prompt\"\n].format(\n**\nrequest.state)\n# Inject system prompt and step-specific tools\nrequest\n=\nrequest.override(\nsystem_prompt\n=\nsystem_prompt,\ntools\n=\nstage_config[\n\"tools\"\n],\n)\nreturn\nhandler(request)\nThis middleware:\nReads current step\n: Gets\ncurrent_step\nfrom state (defaults to \u201cwarranty_collector\u201d).\nLooks up configuration\n: Finds the matching entry in\nSTEP_CONFIG\n.\nValidates dependencies\n: Ensures required state fields exist.\nFormats prompt\n: Injects state values into the prompt template.\nApplies configuration\n: Overrides the system prompt and available tools.\nThe\nrequest.override()\nmethod is key - it allows us to dynamically change the agent\u2019s behavior based on state without creating separate agent instances.\n\u200b\n5. Create the agent\nNow create the agent with the step-based middleware and a checkpointer for state persistence:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\n# Collect all tools from all step configurations\nall_tools\n=\n[\nrecord_warranty_status,\nrecord_issue_type,\nprovide_solution,\nescalate_to_human,\n]\n# Create the agent with step-based configuration\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\nall_tools,\nstate_schema\n=\nSupportState,\nmiddleware\n=\n[apply_step_config],\ncheckpointer\n=\nInMemorySaver(),\n)\nWhy a checkpointer?\nThe checkpointer maintains state across conversation turns. Without it, the\ncurrent_step\nstate would be lost between user messages, breaking the workflow.\n\u200b\n6. Test the workflow\nTest the complete workflow:\nCopy\nfrom\nlangchain.messages\nimport\nHumanMessage\nimport\nuuid\n# Configuration for this conversation thread\nthread_id\n=\nstr\n(uuid.uuid4())\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n: thread_id}}\n# Turn 1: Initial message - starts with warranty_collector step\nprint\n(\n\"=== Turn 1: Warranty Collection ===\"\n)\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"Hi, my phone screen is cracked\"\n)]},\nconfig\n)\nfor\nmsg\nin\nresult[\n'messages'\n]:\nmsg.pretty_print()\n# Turn 2: User responds about warranty\nprint\n(\n\"\n\\n\n=== Turn 2: Warranty Response ===\"\n)\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"Yes, it's still under warranty\"\n)]},\nconfig\n)\nfor\nmsg\nin\nresult[\n'messages'\n]:\nmsg.pretty_print()\nprint\n(\nf\n\"Current step:\n{\nresult.get(\n'current_step'\n)\n}\n\"\n)\n# Turn 3: User describes the issue\nprint\n(\n\"\n\\n\n=== Turn 3: Issue Description ===\"\n)\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"The screen is physically cracked from dropping it\"\n)]},\nconfig\n)\nfor\nmsg\nin\nresult[\n'messages'\n]:\nmsg.pretty_print()\nprint\n(\nf\n\"Current step:\n{\nresult.get(\n'current_step'\n)\n}\n\"\n)\n# Turn 4: Resolution\nprint\n(\n\"\n\\n\n=== Turn 4: Resolution ===\"\n)\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"What should I do?\"\n)]},\nconfig\n)\nfor\nmsg\nin\nresult[\n'messages'\n]:\nmsg.pretty_print()\nExpected flow:\nWarranty verification step\n: Asks about warranty status\nIssue classification step\n: Asks about the problem, determines it\u2019s hardware\nResolution step\n: Provides warranty repair instructions\n\u200b\n7. Understanding state transitions\nLet\u2019s trace what happens at each turn:\n\u200b\nTurn 1: Initial message\nCopy\n{\n\"messages\"\n: [HumanMessage(\n\"Hi, my phone screen is cracked\"\n)],\n\"current_step\"\n:\n\"warranty_collector\"\n# Default value\n}\nMiddleware applies:\nSystem prompt:\nWARRANTY_COLLECTOR_PROMPT\nTools:\n[record_warranty_status]\n\u200b\nTurn 2: After warranty recorded\nTool call:\nrecord_warranty_status(\"in_warranty\")\nreturns:\nCopy\nCommand(\nupdate\n=\n{\n\"warranty_status\"\n:\n\"in_warranty\"\n,\n\"current_step\"\n:\n\"issue_classifier\"\n# State transition!\n})\nNext turn, middleware applies:\nSystem prompt:\nISSUE_CLASSIFIER_PROMPT\n(formatted with\nwarranty_status=\"in_warranty\"\n)\nTools:\n[record_issue_type]\n\u200b\nTurn 3: After issue classified\nTool call:\nrecord_issue_type(\"hardware\")\nreturns:\nCopy\nCommand(\nupdate\n=\n{\n\"issue_type\"\n:\n\"hardware\"\n,\n\"current_step\"\n:\n\"resolution_specialist\"\n# State transition!\n})\nNext turn, middleware applies:\nSystem prompt:\nRESOLUTION_SPECIALIST_PROMPT\n(formatted with\nwarranty_status\nand\nissue_type\n)\nTools:\n[provide_solution, escalate_to_human]\nThe key insight:\nTools drive the workflow\nby updating\ncurrent_step\n, and\nmiddleware responds\nby applying the appropriate configuration on the next turn.\n\u200b\n8. Manage message history\nAs the agent progresses through steps, message history grows. Use\nsummarization middleware\nto compress earlier messages while preserving conversational context:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nSummarizationMiddleware\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\nall_tools,\nstate_schema\n=\nSupportState,\nmiddleware\n=\n[\napply_step_config,\nSummarizationMiddleware(\nmodel\n=\n\"gpt-5-mini\"\n,\ntrigger\n=\n(\n\"tokens\"\n,\n4000\n),\nkeep\n=\n(\n\"messages\"\n,\n10\n)\n)\n],\ncheckpointer\n=\nInMemorySaver(),\n)\nSee the\nshort-term memory guide\nfor other memory management techniques.\n\u200b\n9. Add flexibility: Go back\nSome workflows need to allow users to return to previous steps to correct information (e.g., changing warranty status or issue classification). However, not all transitions make sense\u2014for example, you typically can\u2019t go back once a refund has been processed. For this support workflow, we\u2019ll add tools to return to the warranty verification and issue classification steps.\nIf your workflow requires arbitrary transitions between most steps, consider whether you need a structured workflow at all. This pattern works best when steps follow a clear sequential progression with occasional backwards transitions for corrections.\nAdd \u201cgo back\u201d tools to the resolution step:\nCopy\n@tool\ndef\ngo_back_to_warranty\n() -> Command:\n\"\"\"Go back to warranty verification step.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"current_step\"\n:\n\"warranty_collector\"\n})\n@tool\ndef\ngo_back_to_classification\n() -> Command:\n\"\"\"Go back to issue classification step.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"current_step\"\n:\n\"issue_classifier\"\n})\n# Update the resolution_specialist configuration to include these tools\nSTEP_CONFIG\n[\n\"resolution_specialist\"\n][\n\"tools\"\n].extend([\ngo_back_to_warranty,\ngo_back_to_classification\n])\nUpdate the resolution specialist\u2019s prompt to mention these tools:\nCopy\nRESOLUTION_SPECIALIST_PROMPT\n=\n\"\"\"You are a customer support agent helping with device issues.\nCURRENT STAGE: Resolution\nCUSTOMER INFO: Warranty status is\n{warranty_status}\n, issue type is\n{issue_type}\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n- If IN WARRANTY: explain warranty repair process using provide_solution\n- If OUT OF WARRANTY: escalate_to_human for paid repair options\nIf the customer indicates any information was wrong, use:\n- go_back_to_warranty to correct warranty status\n- go_back_to_classification to correct issue type\nBe specific and helpful in your solutions.\"\"\"\nNow the agent can handle corrections:\nCopy\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"Actually, I made a mistake - my device is out of warranty\"\n)]},\nconfig\n)\n# Agent will call go_back_to_warranty and restart the warranty verification step\n\u200b\nComplete example\nHere\u2019s everything together in a runnable script:\nShow\nComplete code\nCopy\n\"\"\"\nCustomer Support State Machine Example\nThis example demonstrates the state machine pattern.\nA single agent dynamically changes its behavior based on the current_step state,\ncreating a state machine for sequential information collection.\n\"\"\"\nimport\nuuid\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.types\nimport\nCommand\nfrom\ntyping\nimport\nCallable, Literal\nfrom\ntyping_extensions\nimport\nNotRequired\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain.agents.middleware\nimport\nwrap_model_call, ModelRequest, ModelResponse, SummarizationMiddleware\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain.messages\nimport\nHumanMessage, ToolMessage\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nmodel\n=\ninit_chat_model(\n\"anthropic:claude-3-5-sonnet-latest\"\n)\n# Define the possible workflow steps\nSupportStep\n=\nLiteral[\n\"warranty_collector\"\n,\n\"issue_classifier\"\n,\n\"resolution_specialist\"\n]\nclass\nSupportState\n(\nAgentState\n):\n\"\"\"State for customer support workflow.\"\"\"\ncurrent_step: NotRequired[SupportStep]\nwarranty_status: NotRequired[Literal[\n\"in_warranty\"\n,\n\"out_of_warranty\"\n]]\nissue_type: NotRequired[Literal[\n\"hardware\"\n,\n\"software\"\n]]\n@tool\ndef\nrecord_warranty_status\n(\nstatus\n: Literal[\n\"in_warranty\"\n,\n\"out_of_warranty\"\n],\nruntime\n: ToolRuntime[\nNone\n, SupportState],\n) -> Command:\n\"\"\"Record the customer's warranty status and transition to issue classification.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\ncontent\n=\nf\n\"Warranty status recorded as:\n{\nstatus\n}\n\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\n],\n\"warranty_status\"\n: status,\n\"current_step\"\n:\n\"issue_classifier\"\n,\n}\n)\n@tool\ndef\nrecord_issue_type\n(\nissue_type\n: Literal[\n\"hardware\"\n,\n\"software\"\n],\nruntime\n: ToolRuntime[\nNone\n, SupportState],\n) -> Command:\n\"\"\"Record the type of issue and transition to resolution specialist.\"\"\"\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\ncontent\n=\nf\n\"Issue type recorded as:\n{\nissue_type\n}\n\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\n],\n\"issue_type\"\n: issue_type,\n\"current_step\"\n:\n\"resolution_specialist\"\n,\n}\n)\n@tool\ndef\nescalate_to_human\n(\nreason\n:\nstr\n) ->\nstr\n:\n\"\"\"Escalate the case to a human support specialist.\"\"\"\n# In a real system, this would create a ticket, notify staff, etc.\nreturn\nf\n\"Escalating to human support. Reason:\n{\nreason\n}\n\"\n@tool\ndef\nprovide_solution\n(\nsolution\n:\nstr\n) ->\nstr\n:\n\"\"\"Provide a solution to the customer's issue.\"\"\"\nreturn\nf\n\"Solution provided:\n{\nsolution\n}\n\"\n# Define prompts as constants\nWARRANTY_COLLECTOR_PROMPT\n=\n\"\"\"You are a customer support agent helping with device issues.\nCURRENT STEP: Warranty verification\nAt this step, you need to:\n1. Greet the customer warmly\n2. Ask if their device is under warranty\n3. Use record_warranty_status to record their response and move to the next step\nBe conversational and friendly. Don't ask multiple questions at once.\"\"\"\nISSUE_CLASSIFIER_PROMPT\n=\n\"\"\"You are a customer support agent helping with device issues.\nCURRENT STEP: Issue classification\nCUSTOMER INFO: Warranty status is\n{warranty_status}\nAt this step, you need to:\n1. Ask the customer to describe their issue\n2. Determine if it's a hardware issue (physical damage, broken parts) or software issue (app crashes, performance)\n3. Use record_issue_type to record the classification and move to the next step\nIf unclear, ask clarifying questions before classifying.\"\"\"\nRESOLUTION_SPECIALIST_PROMPT\n=\n\"\"\"You are a customer support agent helping with device issues.\nCURRENT STEP: Resolution\nCUSTOMER INFO: Warranty status is\n{warranty_status}\n, issue type is\n{issue_type}\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n- If IN WARRANTY: explain warranty repair process using provide_solution\n- If OUT OF WARRANTY: escalate_to_human for paid repair options\nBe specific and helpful in your solutions.\"\"\"\n# Step configuration: maps step name to (prompt, tools, required_state)\nSTEP_CONFIG\n=\n{\n\"warranty_collector\"\n: {\n\"prompt\"\n:\nWARRANTY_COLLECTOR_PROMPT\n,\n\"tools\"\n: [record_warranty_status],\n\"requires\"\n: [],\n},\n\"issue_classifier\"\n: {\n\"prompt\"\n:\nISSUE_CLASSIFIER_PROMPT\n,\n\"tools\"\n: [record_issue_type],\n\"requires\"\n: [\n\"warranty_status\"\n],\n},\n\"resolution_specialist\"\n: {\n\"prompt\"\n:\nRESOLUTION_SPECIALIST_PROMPT\n,\n\"tools\"\n: [provide_solution, escalate_to_human],\n\"requires\"\n: [\n\"warranty_status\"\n,\n\"issue_type\"\n],\n},\n}\n@wrap_model_call\ndef\napply_step_config\n(\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n\"\"\"Configure agent behavior based on the current step.\"\"\"\n# Get current step (defaults to warranty_collector for first interaction)\ncurrent_step\n=\nrequest.state.get(\n\"current_step\"\n,\n\"warranty_collector\"\n)\n# Look up step configuration\nstep_config\n=\nSTEP_CONFIG\n[current_step]\n# Validate required state exists\nfor\nkey\nin\nstep_config[\n\"requires\"\n]:\nif\nrequest.state.get(key)\nis\nNone\n:\nraise\nValueError\n(\nf\n\"\n{\nkey\n}\nmust be set before reaching\n{\ncurrent_step\n}\n\"\n)\n# Format prompt with state values\nsystem_prompt\n=\nstep_config[\n\"prompt\"\n].format(\n**\nrequest.state)\n# Inject system prompt and step-specific tools\nrequest\n=\nrequest.override(\nsystem_prompt\n=\nsystem_prompt,\ntools\n=\nstep_config[\n\"tools\"\n],\n)\nreturn\nhandler(request)\n# Collect all tools from all step configurations\nall_tools\n=\n[\nrecord_warranty_status,\nrecord_issue_type,\nprovide_solution,\nescalate_to_human,\n]\n# Create the agent with step-based configuration and summarization\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\nall_tools,\nstate_schema\n=\nSupportState,\nmiddleware\n=\n[\napply_step_config,\nSummarizationMiddleware(\nmodel\n=\n\"gpt-5-mini\"\n,\ntrigger\n=\n(\n\"tokens\"\n,\n4000\n),\nkeep\n=\n(\n\"messages\"\n,\n10\n)\n)\n],\ncheckpointer\n=\nInMemorySaver(),\n)\n# ============================================================================\n# Test the workflow\n# ============================================================================\nif\n__name__\n==\n\"__main__\"\n:\nthread_id\n=\nstr\n(uuid.uuid4())\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n: thread_id}}\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"Hi, my phone screen is cracked\"\n)]},\nconfig\n)\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"Yes, it's still under warranty\"\n)]},\nconfig\n)\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"The screen is physically cracked from dropping it\"\n)]},\nconfig\n)\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [HumanMessage(\n\"What should I do?\"\n)]},\nconfig\n)\nfor\nmsg\nin\nresult[\n'messages'\n]:\nmsg.pretty_print()\n\u200b\nNext steps\nLearn about the\nsubagents pattern\nfor centralized orchestration\nExplore\nmiddleware\nfor more dynamic behaviors\nRead the\nmulti-agent overview\nto compare patterns\nUse\nLangSmith\nto debug and monitor your multi-agent system\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a personal assistant with subagents\nPrevious\nBuild a multi-source knowledge base with routing\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/handoffs-customer-support",
      "title": "Build customer support with handoffs - Docs by LangChain",
      "heading": "Build customer support with handoffs"
    }
  },
  {
    "page_content": "Build a multi-source knowledge base with routing - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nBuild a multi-source knowledge base with routing\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nWhy use a router?\nConcepts\nSetup\nInstallation\nLangSmith\nSelect an LLM\n1. Define state\n2. Define tools for each vertical\n3. Create specialized agents\n4. Build the router workflow\n5. Compile the workflow\n6. Use the router\n7. Understanding the architecture\nClassification phase\nParallel execution with Send\nResult collection with reducers\nSynthesis phase\n8. Complete working example\n9. Advanced: Stateful routers\nTool wrapper approach\nFull persistence approach\n10. Key takeaways\nNext steps\nTutorials\nMulti-agent\nBuild a multi-source knowledge base with routing\nCopy page\nCopy page\n\u200b\nOverview\nThe\nrouter pattern\nis a\nmulti-agent\narchitecture where a routing step classifies input and directs it to specialized agents, with results synthesized into a combined response. This pattern excels when your organization\u2019s knowledge lives across distinct\nverticals\n\u2014separate knowledge domains that each require their own agent with specialized tools and prompts.\nIn this tutorial, you\u2019ll build a multi-source knowledge base router that demonstrates these benefits through a realistic enterprise scenario. The system will coordinate three specialists:\nA\nGitHub agent\nthat searches code, issues, and pull requests.\nA\nNotion agent\nthat searches internal documentation and wikis.\nA\nSlack agent\nthat searches relevant threads and discussions.\nWhen a user asks \u201cHow do I authenticate API requests?\u201d, the router decomposes the query into source-specific sub-questions, routes them to the relevant agents in parallel, and synthesizes results into a coherent answer.\n\u200b\nWhy use a router?\nThe router pattern provides several advantages:\nParallel execution\n: Query multiple sources simultaneously, reducing latency compared to sequential approaches.\nSpecialized agents\n: Each vertical has focused tools and prompts optimized for its domain.\nSelective routing\n: Not every query needs every source\u2014the router intelligently selects relevant verticals.\nTargeted sub-questions\n: Each agent receives a question tailored to its domain, improving result quality.\nClean synthesis\n: Results from multiple sources are combined into a single, coherent response.\n\u200b\nConcepts\nWe will cover the following concepts:\nMulti-agent systems\nStateGraph\nfor workflow orchestration\nSend API\nfor parallel execution\nRouter vs. Subagents\n: The\nsubagents pattern\ncan also route to multiple agents. Use the router pattern when you need specialized preprocessing, custom routing logic, or want explicit control over parallel execution. Use the subagents pattern when you want the LLM to decide which agents to call dynamically.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires the\nlangchain\nand\nlanggraph\npackages:\npip\nuv\nconda\nCopy\npip\ninstall\nlangchain\nlanggraph\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nSet up\nLangSmith\nto inspect what is happening inside your agent. Then set the following environment variables:\nbash\npython\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\n\u200b\nSelect an LLM\nSelect a chat model from LangChain\u2019s suite of integrations:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\n\u200b\n1. Define state\nFirst, define the state schemas. We use three types:\nAgentInput\n: Simple state passed to each subagent (just a query)\nAgentOutput\n: Result returned by each subagent (source name + result)\nRouterState\n: Main workflow state tracking the query, classifications, results, and final answer\nCopy\nfrom\ntyping\nimport\nAnnotated, Literal, TypedDict\nimport\noperator\nclass\nAgentInput\n(\nTypedDict\n):\n\"\"\"Simple input state for each subagent.\"\"\"\nquery:\nstr\nclass\nAgentOutput\n(\nTypedDict\n):\n\"\"\"Output from each subagent.\"\"\"\nsource:\nstr\nresult:\nstr\nclass\nClassification\n(\nTypedDict\n):\n\"\"\"A single routing decision: which agent to call with what query.\"\"\"\nsource: Literal[\n\"github\"\n,\n\"notion\"\n,\n\"slack\"\n]\nquery:\nstr\nclass\nRouterState\n(\nTypedDict\n):\nquery:\nstr\nclassifications: list[Classification]\nresults: Annotated[list[AgentOutput], operator.add]\n# Reducer collects parallel results\nfinal_answer:\nstr\nThe\nresults\nfield uses a\nreducer\n(\noperator.add\nin Python, a concat function in JS) to collect outputs from parallel agent executions into a single list.\n\u200b\n2. Define tools for each vertical\nCreate tools for each knowledge domain. In a production system, these would call actual APIs. For this tutorial, we use stub implementations that return mock data. We define 7 tools across 3 verticals: GitHub (search code, issues, PRs), Notion (search docs, get page), and Slack (search messages, get thread).\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\ndef\nsearch_code\n(\nquery\n:\nstr\n,\nrepo\n:\nstr\n=\n\"main\"\n) ->\nstr\n:\n\"\"\"Search code in GitHub repositories.\"\"\"\nreturn\nf\n\"Found code matching '\n{\nquery\n}\n' in\n{\nrepo\n}\n: authentication middleware in src/auth.py\"\n@tool\ndef\nsearch_issues\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search GitHub issues and pull requests.\"\"\"\nreturn\nf\n\"Found 3 issues matching '\n{\nquery\n}\n': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)\"\n@tool\ndef\nsearch_prs\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search pull requests for implementation details.\"\"\"\nreturn\nf\n\"PR #156 added JWT authentication, PR #178 updated OAuth scopes\"\n@tool\ndef\nsearch_notion\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search Notion workspace for documentation.\"\"\"\nreturn\nf\n\"Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens\"\n@tool\ndef\nget_page\n(\npage_id\n:\nstr\n) ->\nstr\n:\n\"\"\"Get a specific Notion page by ID.\"\"\"\nreturn\nf\n\"Page content: Step-by-step authentication setup instructions\"\n@tool\ndef\nsearch_slack\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search Slack messages and threads.\"\"\"\nreturn\nf\n\"Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'\"\n@tool\ndef\nget_thread\n(\nthread_id\n:\nstr\n) ->\nstr\n:\n\"\"\"Get a specific Slack thread.\"\"\"\nreturn\nf\n\"Thread discusses best practices for API key rotation\"\nSee all 43 lines\n\u200b\n3. Create specialized agents\nCreate an agent for each vertical. Each agent has domain-specific tools and a prompt optimized for its knowledge source. All three follow the same pattern\u2014only the tools and system prompt differ.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nmodel\n=\ninit_chat_model(\n\"openai:gpt-5-mini\"\n)\ngithub_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[search_code, search_issues, search_prs],\nsystem_prompt\n=\n(\n\"You are a GitHub expert. Answer questions about code, \"\n\"API references, and implementation details by searching \"\n\"repositories, issues, and pull requests.\"\n),\n)\nnotion_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[search_notion, get_page],\nsystem_prompt\n=\n(\n\"You are a Notion expert. Answer questions about internal \"\n\"processes, policies, and team documentation by searching \"\n\"the organization's Notion workspace.\"\n),\n)\nslack_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[search_slack, get_thread],\nsystem_prompt\n=\n(\n\"You are a Slack expert. Answer questions by searching \"\n\"relevant threads and discussions where team members have \"\n\"shared knowledge and solutions.\"\n),\n)\nSee all 34 lines\n\u200b\n4. Build the router workflow\nNow build the router workflow using a StateGraph. The workflow has four main steps:\nClassify\n: Analyze the query and determine which agents to invoke with what sub-questions\nRoute\n: Fan out to selected agents in parallel using\nSend\nQuery agents\n: Each agent receives a simple\nAgentInput\nand returns an\nAgentOutput\nSynthesize\n: Combine collected results into a coherent response\nCopy\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.types\nimport\nSend\nrouter_llm\n=\ninit_chat_model(\n\"openai:gpt-5-mini\"\n)\n# Define structured output schema for the classifier\nclass\nClassificationResult\n(\nBaseModel\n):\n\"\"\"Result of classifying a user query into agent-specific sub-questions.\"\"\"\nclassifications: list[Classification]\n=\nField(\ndescription\n=\n\"List of agents to invoke with their targeted sub-questions\"\n)\ndef\nclassify_query\n(\nstate\n: RouterState) ->\ndict\n:\n\"\"\"Classify query and determine which agents to invoke.\"\"\"\nstructured_llm\n=\nrouter_llm.with_structured_output(ClassificationResult)\nresult\n=\nstructured_llm.invoke([\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"\"\"Analyze this query and determine which knowledge bases to consult.\nFor each relevant source, generate a targeted sub-question optimized for that source.\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\nReturn ONLY the sources that are relevant to the query. Each source should have\na targeted sub-question optimized for that specific knowledge domain.\nExample for \"How do I authenticate API requests?\":\n- github: \"What authentication code exists? Search for auth middleware, JWT handling\"\n- notion: \"What authentication documentation exists? Look for API auth guides\"\n(slack omitted because it's not relevant for this technical question)\"\"\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"query\"\n]}\n])\nreturn\n{\n\"classifications\"\n: result.classifications}\ndef\nroute_to_agents\n(\nstate\n: RouterState) -> list[Send]:\n\"\"\"Fan out to agents based on classifications.\"\"\"\nreturn\n[\nSend(c[\n\"source\"\n], {\n\"query\"\n: c[\n\"query\"\n]})\nfor\nc\nin\nstate[\n\"classifications\"\n]\n]\ndef\nquery_github\n(\nstate\n: AgentInput) ->\ndict\n:\n\"\"\"Query the GitHub agent.\"\"\"\nresult\n=\ngithub_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"query\"\n]}]\n})\nreturn\n{\n\"results\"\n: [{\n\"source\"\n:\n\"github\"\n,\n\"result\"\n: result[\n\"messages\"\n][\n-\n1\n].content}]}\ndef\nquery_notion\n(\nstate\n: AgentInput) ->\ndict\n:\n\"\"\"Query the Notion agent.\"\"\"\nresult\n=\nnotion_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"query\"\n]}]\n})\nreturn\n{\n\"results\"\n: [{\n\"source\"\n:\n\"notion\"\n,\n\"result\"\n: result[\n\"messages\"\n][\n-\n1\n].content}]}\ndef\nquery_slack\n(\nstate\n: AgentInput) ->\ndict\n:\n\"\"\"Query the Slack agent.\"\"\"\nresult\n=\nslack_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"query\"\n]}]\n})\nreturn\n{\n\"results\"\n: [{\n\"source\"\n:\n\"slack\"\n,\n\"result\"\n: result[\n\"messages\"\n][\n-\n1\n].content}]}\ndef\nsynthesize_results\n(\nstate\n: RouterState) ->\ndict\n:\n\"\"\"Combine results from all agents into a coherent answer.\"\"\"\nif\nnot\nstate[\n\"results\"\n]:\nreturn\n{\n\"final_answer\"\n:\n\"No results found from any knowledge source.\"\n}\n# Format results for synthesis\nformatted\n=\n[\nf\n\"**From\n{\nr[\n'source'\n].title()\n}\n:**\n\\n\n{\nr[\n'result'\n]\n}\n\"\nfor\nr\nin\nstate[\n\"results\"\n]\n]\nsynthesis_response\n=\nrouter_llm.invoke([\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\nf\n\"\"\"Synthesize these search results to answer the original question: \"\n{\nstate[\n'query'\n]\n}\n\"\n- Combine information from multiple sources without redundancy\n- Highlight the most relevant and actionable information\n- Note any discrepancies between sources\n- Keep the response concise and well-organized\"\"\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"\n\\n\\n\n\"\n.join(formatted)}\n])\nreturn\n{\n\"final_answer\"\n: synthesis_response.content}\n\u200b\n5. Compile the workflow\nNow assemble the workflow by connecting nodes with edges. The key is using\nadd_conditional_edges\nwith the routing function to enable parallel execution:\nCopy\nworkflow\n=\n(\nStateGraph(RouterState)\n.add_node(\n\"classify\"\n, classify_query)\n.add_node(\n\"github\"\n, query_github)\n.add_node(\n\"notion\"\n, query_notion)\n.add_node(\n\"slack\"\n, query_slack)\n.add_node(\n\"synthesize\"\n, synthesize_results)\n.add_edge(\nSTART\n,\n\"classify\"\n)\n.add_conditional_edges(\n\"classify\"\n, route_to_agents, [\n\"github\"\n,\n\"notion\"\n,\n\"slack\"\n])\n.add_edge(\n\"github\"\n,\n\"synthesize\"\n)\n.add_edge(\n\"notion\"\n,\n\"synthesize\"\n)\n.add_edge(\n\"slack\"\n,\n\"synthesize\"\n)\n.add_edge(\n\"synthesize\"\n,\nEND\n)\n.compile()\n)\nThe\nadd_conditional_edges\ncall connects the classify node to the agent nodes through the\nroute_to_agents\nfunction. When\nroute_to_agents\nreturns multiple\nSend\nobjects, those nodes execute in parallel.\n\u200b\n6. Use the router\nTest your router with queries that span multiple knowledge domains:\nCopy\nresult\n=\nworkflow.invoke({\n\"query\"\n:\n\"How do I authenticate API requests?\"\n})\nprint\n(\n\"Original query:\"\n, result[\n\"query\"\n])\nprint\n(\n\"\n\\n\nClassifications:\"\n)\nfor\nc\nin\nresult[\n\"classifications\"\n]:\nprint\n(\nf\n\"\n{\nc[\n'source'\n]\n}\n:\n{\nc[\n'query'\n]\n}\n\"\n)\nprint\n(\n\"\n\\n\n\"\n+\n\"=\"\n*\n60\n+\n\"\n\\n\n\"\n)\nprint\n(\n\"Final Answer:\"\n)\nprint\n(result[\n\"final_answer\"\n])\nExpected output:\nCopy\nOriginal query: How do I authenticate API requests?\nClassifications:\ngithub: What authentication code exists? Search for auth middleware, JWT handling\nnotion: What authentication documentation exists? Look for API auth guides\n============================================================\nFinal Answer:\nTo authenticate API requests, you have several options:\n1. **JWT Tokens**: The recommended approach for most use cases.\nImplementation details are in `src/auth.py` (PR #156).\n2. **OAuth2 Flow**: For third-party integrations, follow the OAuth2\nflow documented in Notion's 'API Authentication Guide'.\n3. **API Keys**: For server-to-server communication, use Bearer tokens\nin the Authorization header.\nFor token refresh handling, see issue #203 and PR #178 for the latest\nOAuth scope updates.\nThe router analyzed the query, classified it to determine which agents to invoke (GitHub and Notion, but not Slack for this technical question), queried both agents in parallel, and synthesized the results into a coherent answer.\n\u200b\n7. Understanding the architecture\nThe router workflow follows a clear pattern:\n\u200b\nClassification phase\nThe\nclassify_query\nfunction uses\nstructured output\nto analyze the user\u2019s query and determine which agents to invoke. This is where the routing intelligence lives:\nUses a Pydantic model (Python) or Zod schema (JS) to ensure valid output\nReturns a list of\nClassification\nobjects, each with a\nsource\nand targeted\nquery\nOnly includes relevant sources\u2014irrelevant ones are simply omitted\nThis structured approach is more reliable than free-form JSON parsing and makes the routing logic explicit.\n\u200b\nParallel execution with Send\nThe\nroute_to_agents\nfunction maps classifications to\nSend\nobjects. Each\nSend\nspecifies the target node and the state to pass:\nCopy\n# Classifications: [{\"source\": \"github\", \"query\": \"...\"}, {\"source\": \"notion\", \"query\": \"...\"}]\n# Becomes:\n[Send(\n\"github\"\n, {\n\"query\"\n:\n\"...\"\n}), Send(\n\"notion\"\n, {\n\"query\"\n:\n\"...\"\n})]\n# Both agents execute simultaneously, each receiving only the query it needs\nEach agent node receives a simple\nAgentInput\nwith just a\nquery\nfield\u2014not the full router state. This keeps the interface clean and explicit.\n\u200b\nResult collection with reducers\nAgent results flow back to the main state via a\nreducer\n. Each agent returns:\nCopy\n{\n\"results\"\n: [{\n\"source\"\n:\n\"github\"\n,\n\"result\"\n:\n\"...\"\n}]}\nThe reducer (\noperator.add\nin Python) concatenates these lists, collecting all parallel results into\nstate[\"results\"]\n.\n\u200b\nSynthesis phase\nAfter all agents complete, the\nsynthesize_results\nfunction iterates over the collected results:\nWaits for all parallel branches to complete (LangGraph handles this automatically)\nReferences the original query to ensure the answer addresses what the user asked\nCombines information from all sources without redundancy\nPartial results\n: In this tutorial, all selected agents must complete before synthesis. For more advanced patterns where you want to handle partial results or timeouts, see the\nmap-reduce guide\n.\n\u200b\n8. Complete working example\nHere\u2019s everything together in a runnable script:\nShow\nView complete code\nCopy\n\"\"\"\nMulti-Source Knowledge Router Example\nThis example demonstrates the router pattern for multi-agent systems.\nA router classifies queries, routes them to specialized agents in parallel,\nand synthesizes results into a combined response.\n\"\"\"\nimport\noperator\nfrom\ntyping\nimport\nAnnotated, Literal, TypedDict\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.types\nimport\nSend\nfrom\npydantic\nimport\nBaseModel, Field\n# State definitions\nclass\nAgentInput\n(\nTypedDict\n):\n\"\"\"Simple input state for each subagent.\"\"\"\nquery:\nstr\nclass\nAgentOutput\n(\nTypedDict\n):\n\"\"\"Output from each subagent.\"\"\"\nsource:\nstr\nresult:\nstr\nclass\nClassification\n(\nTypedDict\n):\n\"\"\"A single routing decision: which agent to call with what query.\"\"\"\nsource: Literal[\n\"github\"\n,\n\"notion\"\n,\n\"slack\"\n]\nquery:\nstr\nclass\nRouterState\n(\nTypedDict\n):\nquery:\nstr\nclassifications: list[Classification]\nresults: Annotated[list[AgentOutput], operator.add]\nfinal_answer:\nstr\n# Structured output schema for classifier\nclass\nClassificationResult\n(\nBaseModel\n):\n\"\"\"Result of classifying a user query into agent-specific sub-questions.\"\"\"\nclassifications: list[Classification]\n=\nField(\ndescription\n=\n\"List of agents to invoke with their targeted sub-questions\"\n)\n# Tools\n@tool\ndef\nsearch_code\n(\nquery\n:\nstr\n,\nrepo\n:\nstr\n=\n\"main\"\n) ->\nstr\n:\n\"\"\"Search code in GitHub repositories.\"\"\"\nreturn\nf\n\"Found code matching '\n{\nquery\n}\n' in\n{\nrepo\n}\n: authentication middleware in src/auth.py\"\n@tool\ndef\nsearch_issues\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search GitHub issues and pull requests.\"\"\"\nreturn\nf\n\"Found 3 issues matching '\n{\nquery\n}\n': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)\"\n@tool\ndef\nsearch_prs\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search pull requests for implementation details.\"\"\"\nreturn\nf\n\"PR #156 added JWT authentication, PR #178 updated OAuth scopes\"\n@tool\ndef\nsearch_notion\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search Notion workspace for documentation.\"\"\"\nreturn\nf\n\"Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens\"\n@tool\ndef\nget_page\n(\npage_id\n:\nstr\n) ->\nstr\n:\n\"\"\"Get a specific Notion page by ID.\"\"\"\nreturn\nf\n\"Page content: Step-by-step authentication setup instructions\"\n@tool\ndef\nsearch_slack\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search Slack messages and threads.\"\"\"\nreturn\nf\n\"Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'\"\n@tool\ndef\nget_thread\n(\nthread_id\n:\nstr\n) ->\nstr\n:\n\"\"\"Get a specific Slack thread.\"\"\"\nreturn\nf\n\"Thread discusses best practices for API key rotation\"\n# Models and agents\nmodel\n=\ninit_chat_model(\n\"openai:gpt-5-mini\"\n)\nrouter_llm\n=\ninit_chat_model(\n\"openai:gpt-5-mini\"\n)\ngithub_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[search_code, search_issues, search_prs],\nsystem_prompt\n=\n(\n\"You are a GitHub expert. Answer questions about code, \"\n\"API references, and implementation details by searching \"\n\"repositories, issues, and pull requests.\"\n),\n)\nnotion_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[search_notion, get_page],\nsystem_prompt\n=\n(\n\"You are a Notion expert. Answer questions about internal \"\n\"processes, policies, and team documentation by searching \"\n\"the organization's Notion workspace.\"\n),\n)\nslack_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[search_slack, get_thread],\nsystem_prompt\n=\n(\n\"You are a Slack expert. Answer questions by searching \"\n\"relevant threads and discussions where team members have \"\n\"shared knowledge and solutions.\"\n),\n)\n# Workflow nodes\ndef\nclassify_query\n(\nstate\n: RouterState) ->\ndict\n:\n\"\"\"Classify query and determine which agents to invoke.\"\"\"\nstructured_llm\n=\nrouter_llm.with_structured_output(ClassificationResult)\nresult\n=\nstructured_llm.invoke([\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"\"\"Analyze this query and determine which knowledge bases to consult.\nFor each relevant source, generate a targeted sub-question optimized for that source.\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\nReturn ONLY the sources that are relevant to the query.\"\"\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"query\"\n]}\n])\nreturn\n{\n\"classifications\"\n: result.classifications}\ndef\nroute_to_agents\n(\nstate\n: RouterState) -> list[Send]:\n\"\"\"Fan out to agents based on classifications.\"\"\"\nreturn\n[\nSend(c[\n\"source\"\n], {\n\"query\"\n: c[\n\"query\"\n]})\nfor\nc\nin\nstate[\n\"classifications\"\n]\n]\ndef\nquery_github\n(\nstate\n: AgentInput) ->\ndict\n:\n\"\"\"Query the GitHub agent.\"\"\"\nresult\n=\ngithub_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"query\"\n]}]\n})\nreturn\n{\n\"results\"\n: [{\n\"source\"\n:\n\"github\"\n,\n\"result\"\n: result[\n\"messages\"\n][\n-\n1\n].content}]}\ndef\nquery_notion\n(\nstate\n: AgentInput) ->\ndict\n:\n\"\"\"Query the Notion agent.\"\"\"\nresult\n=\nnotion_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"query\"\n]}]\n})\nreturn\n{\n\"results\"\n: [{\n\"source\"\n:\n\"notion\"\n,\n\"result\"\n: result[\n\"messages\"\n][\n-\n1\n].content}]}\ndef\nquery_slack\n(\nstate\n: AgentInput) ->\ndict\n:\n\"\"\"Query the Slack agent.\"\"\"\nresult\n=\nslack_agent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: state[\n\"query\"\n]}]\n})\nreturn\n{\n\"results\"\n: [{\n\"source\"\n:\n\"slack\"\n,\n\"result\"\n: result[\n\"messages\"\n][\n-\n1\n].content}]}\ndef\nsynthesize_results\n(\nstate\n: RouterState) ->\ndict\n:\n\"\"\"Combine results from all agents into a coherent answer.\"\"\"\nif\nnot\nstate[\n\"results\"\n]:\nreturn\n{\n\"final_answer\"\n:\n\"No results found from any knowledge source.\"\n}\nformatted\n=\n[\nf\n\"**From\n{\nr[\n'source'\n].title()\n}\n:**\n\\n\n{\nr[\n'result'\n]\n}\n\"\nfor\nr\nin\nstate[\n\"results\"\n]\n]\nsynthesis_response\n=\nrouter_llm.invoke([\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\nf\n\"\"\"Synthesize these search results to answer the original question: \"\n{\nstate[\n'query'\n]\n}\n\"\n- Combine information from multiple sources without redundancy\n- Highlight the most relevant and actionable information\n- Note any discrepancies between sources\n- Keep the response concise and well-organized\"\"\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"\n\\n\\n\n\"\n.join(formatted)}\n])\nreturn\n{\n\"final_answer\"\n: synthesis_response.content}\n# Build workflow\nworkflow\n=\n(\nStateGraph(RouterState)\n.add_node(\n\"classify\"\n, classify_query)\n.add_node(\n\"github\"\n, query_github)\n.add_node(\n\"notion\"\n, query_notion)\n.add_node(\n\"slack\"\n, query_slack)\n.add_node(\n\"synthesize\"\n, synthesize_results)\n.add_edge(\nSTART\n,\n\"classify\"\n)\n.add_conditional_edges(\n\"classify\"\n, route_to_agents, [\n\"github\"\n,\n\"notion\"\n,\n\"slack\"\n])\n.add_edge(\n\"github\"\n,\n\"synthesize\"\n)\n.add_edge(\n\"notion\"\n,\n\"synthesize\"\n)\n.add_edge(\n\"slack\"\n,\n\"synthesize\"\n)\n.add_edge(\n\"synthesize\"\n,\nEND\n)\n.compile()\n)\nif\n__name__\n==\n\"__main__\"\n:\nresult\n=\nworkflow.invoke({\n\"query\"\n:\n\"How do I authenticate API requests?\"\n})\nprint\n(\n\"Original query:\"\n, result[\n\"query\"\n])\nprint\n(\n\"\n\\n\nClassifications:\"\n)\nfor\nc\nin\nresult[\n\"classifications\"\n]:\nprint\n(\nf\n\"\n{\nc[\n'source'\n]\n}\n:\n{\nc[\n'query'\n]\n}\n\"\n)\nprint\n(\n\"\n\\n\n\"\n+\n\"=\"\n*\n60\n+\n\"\n\\n\n\"\n)\nprint\n(\n\"Final Answer:\"\n)\nprint\n(result[\n\"final_answer\"\n])\n\u200b\n9. Advanced: Stateful routers\nThe router we\u2019ve built so far is\nstateless\n\u2014each request is handled independently with no memory between calls. For multi-turn conversations, you need a\nstateful\napproach.\n\u200b\nTool wrapper approach\nThe simplest way to add conversation memory is to wrap the stateless router as a tool that a conversational agent can call:\nCopy\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\n@tool\ndef\nsearch_knowledge_base\n(\nquery\n:\nstr\n) ->\nstr\n:\n\"\"\"Search across multiple knowledge sources (GitHub, Notion, Slack).\nUse this to find information about code, documentation, or team discussions.\n\"\"\"\nresult\n=\nworkflow.invoke({\n\"query\"\n: query})\nreturn\nresult[\n\"final_answer\"\n]\nconversational_agent\n=\ncreate_agent(\nmodel,\ntools\n=\n[search_knowledge_base],\nsystem_prompt\n=\n(\n\"You are a helpful assistant that answers questions about our organization. \"\n\"Use the search_knowledge_base tool to find information across our code, \"\n\"documentation, and team discussions.\"\n),\ncheckpointer\n=\nInMemorySaver(),\n)\nThis approach keeps the router stateless while the conversational agent handles memory and context. The user can have a multi-turn conversation, and the agent will call the router tool as needed.\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"user-123\"\n}}\nresult\n=\nconversational_agent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"How do I authenticate API requests?\"\n}]},\nconfig\n)\nprint\n(result[\n\"messages\"\n][\n-\n1\n].content)\nresult\n=\nconversational_agent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What about rate limiting for those endpoints?\"\n}]},\nconfig\n)\nprint\n(result[\n\"messages\"\n][\n-\n1\n].content)\nThe tool wrapper approach is recommended for most use cases. It provides clean separation: the router handles multi-source querying, while the conversational agent handles context and memory.\n\u200b\nFull persistence approach\nIf you need the router itself to maintain state\u2014for example, to use previous search results in routing decisions\u2014use\npersistence\nto store message history at the router level.\nStateful routers add complexity.\nWhen routing to different agents across turns, conversations may feel inconsistent if agents have different tones or prompts. Consider the\nhandoffs pattern\nor\nsubagents pattern\ninstead\u2014both provide clearer semantics for multi-turn conversations with different agents.\n\u200b\n10. Key takeaways\nThe router pattern excels when you have:\nDistinct verticals\n: Separate knowledge domains that each require specialized tools and prompts\nParallel query needs\n: Questions that benefit from querying multiple sources simultaneously\nSynthesis requirements\n: Results from multiple sources need to be combined into a coherent response\nThe pattern has three phases:\ndecompose\n(analyze the query and generate targeted sub-questions),\nroute\n(execute queries in parallel), and\nsynthesize\n(combine results).\nWhen to use the router pattern\nUse the router pattern when you have multiple independent knowledge sources, need low-latency parallel queries, and want explicit control over routing logic.\nFor simpler cases with dynamic tool selection, consider the\nsubagents pattern\n. For workflows where agents need to converse with users sequentially, consider\nhandoffs\n.\n\u200b\nNext steps\nLearn about\nhandoffs\nfor agent-to-agent conversations\nExplore the\nsubagents pattern\nfor centralized orchestration\nRead the\nmulti-agent overview\nto compare different patterns\nUse\nLangSmith\nto debug and monitor your router\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild customer support with handoffs\nPrevious\nBuild a SQL assistant with on-demand skills\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/router-knowledge-base",
      "title": "Build a multi-source knowledge base with routing - Docs by LangChain",
      "heading": "Build a multi-source knowledge base with routing"
    }
  },
  {
    "page_content": "Build a SQL assistant with on-demand skills - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nMulti-agent\nBuild a SQL assistant with on-demand skills\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nHow it works\nSetup\nInstallation\nLangSmith\nSelect an LLM\n1. Define skills\n2. Create skill loading tool\n3. Build skill middleware\n4. Create the agent with skill support\n5. Test progressive disclosure\n6. Advanced: Add constraints with custom state\nComplete example\nImplementation variations\nProgressive disclosure and context engineering\nNext steps\nTutorials\nMulti-agent\nBuild a SQL assistant with on-demand skills\nCopy page\nCopy page\nThis tutorial shows how to use\nprogressive disclosure\n- a context management technique where the agent loads information on-demand rather than upfront - to implement\nskills\n(specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task.\nUse case:\nImagine building an agent to help write SQL queries across different business verticals in a large enterprise. Your organization might have separate datastores for each vertical, or a single monolithic database with thousands of tables. Either way, loading all schemas upfront would overwhelm the context window. Progressive disclosure solves this by loading only the relevant schema when needed. This architecture also enables different product owners and stakeholders to independently contribute and maintain skills for their specific business verticals.\nWhat you\u2019ll build:\nA SQL query assistant with two skills (sales analytics and inventory management). The agent sees lightweight skill descriptions in its system prompt, then loads full database schemas and business logic through tool calls only when relevant to the user\u2019s query.\nFor a more complete example of a SQL agent with query execution, error correction, and validation, see our\nSQL Agent tutorial\n. This tutorial focuses on the progressive disclosure pattern which can be applied to any domain.\nProgressive disclosure was popularized by Anthropic as a technique for building scalable agent skills systems. This approach uses a three-level architecture (metadata \u2192 core content \u2192 detailed resources) where agents load information only as needed. For more on this technique, see\nEquipping agents for the real world with Agent Skills\n.\n\u200b\nHow it works\nHere\u2019s the flow when a user asks for a SQL query:\nWhy progressive disclosure:\nReduces context usage\n- load only the 2-3 skills needed for a task, not all available skills\nEnables team autonomy\n- different teams can develop specialized skills independently (similar to other multi-agent architectures)\nScales efficiently\n- add dozens or hundreds of skills without overwhelming context\nSimplifies conversation history\n- single agent with one conversation thread\nWhat are skills:\nSkills, as popularized by Claude Code, are primarily prompt-based: self-contained units of specialized instructions for specific business tasks. In Claude Code, skills are exposed as directories with files on the file system, discovered through file operations. Skills guide behavior through prompts and can provide information about tool usage or include sample code for a coding agent to execute.\nSkills with progressive disclosure can be viewed as a form of\nRAG (Retrieval-Augmented Generation)\n, where each skill is a retrieval unit\u2014though not necessarily backed by embeddings or keyword search, but by tools for browsing content (like file operations or, in this tutorial, direct lookup).\nTrade-offs:\nLatency\n: Loading skills on-demand requires additional tool calls, which adds latency to the first request that needs each skill\nWorkflow control\n: Basic implementations rely on prompting to guide skill usage - you cannot enforce hard constraints like \u201calways try skill A before skill B\u201d without custom logic\nImplementing your own skills system\nWhen building your own skills implementation (as we do in this tutorial), the core concept is progressive disclosure - loading information on-demand. Beyond that, you have full flexibility in implementation:\nStorage\n: databases, S3, in-memory data structures, or any backend\nDiscovery\n: direct lookup (this tutorial), RAG for large skill collections, file system scanning, or API calls\nLoading logic\n: customize latency characteristics and add logic to search through skill content or rank relevance\nSide effects\n: define what happens when a skill loads, such as exposing tools associated with that skill (covered in section 8)\nThis flexibility lets you optimize for your specific requirements around performance, storage, and workflow control.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires the\nlangchain\npackage:\npip\nuv\nconda\nCopy\npip\ninstall\nlangchain\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nSet up\nLangSmith\nto inspect what is happening inside your agent. Then set the following environment variables:\nbash\npython\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\n\u200b\nSelect an LLM\nSelect a chat model from LangChain\u2019s suite of integrations:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\n\u200b\n1. Define skills\nFirst, define the structure for skills. Each skill has a name, a brief description (shown in the system prompt), and full content (loaded on-demand):\nCopy\nfrom\ntyping\nimport\nTypedDict\nclass\nSkill\n(\nTypedDict\n):\n\"\"\"A skill that can be progressively disclosed to the agent.\"\"\"\nname:\nstr\n# Unique identifier for the skill\ndescription:\nstr\n# 1-2 sentence description to show in system prompt\ncontent:\nstr\n# Full skill content with detailed instructions\nNow define example skills for a SQL query assistant. The skills are designed to be\nlightweight in description\n(shown to the agent upfront) but\ndetailed in content\n(loaded only when needed):\nView complete skill definitions\nCopy\nSKILLS\n: list[Skill]\n=\n[\n{\n\"name\"\n:\n\"sales_analytics\"\n,\n\"description\"\n:\n\"Database schema and business logic for sales data analysis including customers, orders, and revenue.\"\n,\n\"content\"\n:\n\"\"\"# Sales Analytics Schema\n## Tables\n### customers\n- customer_id (PRIMARY KEY)\n- name\n- email\n- signup_date\n- status (active/inactive)\n- customer_tier (bronze/silver/gold/platinum)\n### orders\n- order_id (PRIMARY KEY)\n- customer_id (FOREIGN KEY -> customers)\n- order_date\n- status (pending/completed/cancelled/refunded)\n- total_amount\n- sales_region (north/south/east/west)\n### order_items\n- item_id (PRIMARY KEY)\n- order_id (FOREIGN KEY -> orders)\n- product_id\n- quantity\n- unit_price\n- discount_percent\n## Business Logic\n**Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\n**Revenue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.\n**Customer lifetime value (CLV)**: Sum of all completed order amounts for a customer.\n**High-value orders**: Orders with total_amount > 1000\n## Example Query\n-- Get top 10 customers by revenue in the last quarter\nSELECT\nc.customer_id,\nc.name,\nc.customer_tier,\nSUM(o.total_amount) as total_revenue\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nWHERE o.status = 'completed'\nAND o.order_date >= CURRENT_DATE - INTERVAL '3 months'\nGROUP BY c.customer_id, c.name, c.customer_tier\nORDER BY total_revenue DESC\nLIMIT 10;\n\"\"\"\n,\n},\n{\n\"name\"\n:\n\"inventory_management\"\n,\n\"description\"\n:\n\"Database schema and business logic for inventory tracking including products, warehouses, and stock levels.\"\n,\n\"content\"\n:\n\"\"\"# Inventory Management Schema\n## Tables\n### products\n- product_id (PRIMARY KEY)\n- product_name\n- sku\n- category\n- unit_cost\n- reorder_point (minimum stock level before reordering)\n- discontinued (boolean)\n### warehouses\n- warehouse_id (PRIMARY KEY)\n- warehouse_name\n- location\n- capacity\n### inventory\n- inventory_id (PRIMARY KEY)\n- product_id (FOREIGN KEY -> products)\n- warehouse_id (FOREIGN KEY -> warehouses)\n- quantity_on_hand\n- last_updated\n### stock_movements\n- movement_id (PRIMARY KEY)\n- product_id (FOREIGN KEY -> products)\n- warehouse_id (FOREIGN KEY -> warehouses)\n- movement_type (inbound/outbound/transfer/adjustment)\n- quantity (positive for inbound, negative for outbound)\n- movement_date\n- reference_number\n## Business Logic\n**Available stock**: quantity_on_hand from inventory table where quantity_on_hand > 0\n**Products needing reorder**: Products where total quantity_on_hand across all warehouses is less than or equal to the product's reorder_point\n**Active products only**: Exclude products where discontinued = true unless specifically analyzing discontinued items\n**Stock valuation**: quantity_on_hand * unit_cost for each product\n## Example Query\n-- Find products below reorder point across all warehouses\nSELECT\np.product_id,\np.product_name,\np.reorder_point,\nSUM(i.quantity_on_hand) as total_stock,\np.unit_cost,\n(p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\nFROM products p\nJOIN inventory i ON p.product_id = i.product_id\nWHERE p.discontinued = false\nGROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost\nHAVING SUM(i.quantity_on_hand) <= p.reorder_point\nORDER BY units_to_reorder DESC;\n\"\"\"\n,\n},\n]\n\u200b\n2. Create skill loading tool\nCreate a tool to load full skill content on-demand:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\ndef\nload_skill\n(\nskill_name\n:\nstr\n) ->\nstr\n:\n\"\"\"Load the full content of a skill into the agent's context.\nUse this when you need detailed information about how to handle a specific\ntype of request. This will provide you with comprehensive instructions,\npolicies, and guidelines for the skill area.\nArgs:\nskill_name: The name of the skill to load (e.g., \"expense_reporting\", \"travel_booking\")\n\"\"\"\n# Find and return the requested skill\nfor\nskill\nin\nSKILLS\n:\nif\nskill[\n\"name\"\n]\n==\nskill_name:\nreturn\nf\n\"Loaded skill:\n{\nskill_name\n}\n\\n\\n\n{\nskill[\n'content'\n]\n}\n\"\n# Skill not found\navailable\n=\n\", \"\n.join(s[\n\"name\"\n]\nfor\ns\nin\nSKILLS\n)\nreturn\nf\n\"Skill '\n{\nskill_name\n}\n' not found. Available skills:\n{\navailable\n}\n\"\nThe\nload_skill\ntool returns the full skill content as a string, which becomes part of the conversation as a ToolMessage. For more details on creating and using tools, see the\nTools guide\n.\n\u200b\n3. Build skill middleware\nCreate custom middleware that injects skill descriptions into the system prompt. This middleware makes skills discoverable without loading their full content upfront.\nThis guide demonstrates creating custom middleware. For a comprehensive guide on middleware concepts and patterns, see the\ncustom middleware documentation\n.\nCopy\nfrom\nlangchain.agents.middleware\nimport\nModelRequest, ModelResponse, AgentMiddleware\nfrom\nlangchain.messages\nimport\nSystemMessage\nfrom\ntyping\nimport\nCallable\nclass\nSkillMiddleware\n(\nAgentMiddleware\n):\n\"\"\"Middleware that injects skill descriptions into the system prompt.\"\"\"\n# Register the load_skill tool as a class variable\ntools\n=\n[load_skill]\ndef\n__init__\n(\nself\n):\n\"\"\"Initialize and generate the skills prompt from SKILLS.\"\"\"\n# Build skills prompt from the SKILLS list\nskills_list\n=\n[]\nfor\nskill\nin\nSKILLS\n:\nskills_list.append(\nf\n\"- **\n{\nskill[\n'name'\n]\n}\n**:\n{\nskill[\n'description'\n]\n}\n\"\n)\nself\n.skills_prompt\n=\n\"\n\\n\n\"\n.join(skills_list)\ndef\nwrap_model_call\n(\nself\n,\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n\"\"\"Sync: Inject skill descriptions into system prompt.\"\"\"\n# Build the skills addendum\nskills_addendum\n=\n(\nf\n\"\n\\n\\n\n## Available Skills\n\\n\\n\n{\nself\n.skills_prompt\n}\n\\n\\n\n\"\n\"Use the load_skill tool when you need detailed information \"\n\"about handling a specific type of request.\"\n)\n# Append to system message content blocks\nnew_content\n=\nlist\n(request.system_message.content_blocks)\n+\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n: skills_addendum}\n]\nnew_system_message\n=\nSystemMessage(\ncontent\n=\nnew_content)\nmodified_request\n=\nrequest.override(\nsystem_message\n=\nnew_system_message)\nreturn\nhandler(modified_request)\nThe middleware appends skill descriptions to the system prompt, making the agent aware of available skills without loading their full content. The\nload_skill\ntool is registered as a class variable, making it available to the agent.\nProduction consideration\n: This tutorial loads the skill list in\n__init__\nfor simplicity. In a production system, you may want to load skills in the\nbefore_agent\nhook instead, allowing them to be refreshed periodically to reflect up-to-date changes (e.g., when new skills are added or existing ones are modified). See the\nbefore_agent hook documentation\nfor details.\n\u200b\n4. Create the agent with skill support\nNow create the agent with the skill middleware and a checkpointer for state persistence:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\n# Create the agent with skill support\nagent\n=\ncreate_agent(\nmodel,\nsystem_prompt\n=\n(\n\"You are a SQL query assistant that helps users \"\n\"write queries against business databases.\"\n),\nmiddleware\n=\n[SkillMiddleware()],\ncheckpointer\n=\nInMemorySaver(),\n)\nThe agent now has access to skill descriptions in its system prompt and can call\nload_skill\nto retrieve full skill content when needed. The checkpointer maintains conversation history across turns.\n\u200b\n5. Test progressive disclosure\nTest the agent with a question that requires skill-specific knowledge:\nCopy\nimport\nuuid\n# Configuration for this conversation thread\nthread_id\n=\nstr\n(uuid.uuid4())\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n: thread_id}}\n# Ask for a SQL query\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: (\n\"Write a SQL query to find all customers \"\n\"who made orders over $1000 in the last month\"\n),\n}\n]\n},\nconfig\n)\n# Print the conversation\nfor\nmessage\nin\nresult[\n\"messages\"\n]:\nif\nhasattr\n(message,\n'pretty_print'\n):\nmessage.pretty_print()\nelse\n:\nprint\n(\nf\n\"\n{\nmessage.type\n}\n:\n{\nmessage.content\n}\n\"\n)\nExpected output:\nCopy\n================================ Human Message =================================\nWrite a SQL query to find all customers who made orders over $1000 in the last month\n================================== Ai Message ==================================\nTool Calls:\nload_skill (call_abc123)\nCall ID: call_abc123\nArgs:\nskill_name: sales_analytics\n================================= Tool Message =================================\nName: load_skill\nLoaded skill: sales_analytics\n# Sales Analytics Schema\n## Tables\n### customers\n- customer_id (PRIMARY KEY)\n- name\n- email\n- signup_date\n- status (active/inactive)\n- customer_tier (bronze/silver/gold/platinum)\n### orders\n- order_id (PRIMARY KEY)\n- customer_id (FOREIGN KEY -> customers)\n- order_date\n- status (pending/completed/cancelled/refunded)\n- total_amount\n- sales_region (north/south/east/west)\n[... rest of schema ...]\n## Business Logic\n**High-value orders**: Orders with `total_amount > 1000`\n**Revenue calculation**: Only count orders with `status = 'completed'`\n================================== Ai Message ==================================\nHere's a SQL query to find all customers who made orders over $1000 in the last month:\n\\`\\`\\`sql\nSELECT DISTINCT\nc.customer_id,\nc.name,\nc.email,\nc.customer_tier\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nWHERE o.total_amount > 1000\nAND o.status = 'completed'\nAND o.order_date >= CURRENT_DATE - INTERVAL '1 month'\nORDER BY c.customer_id;\n\\`\\`\\`\nThis query:\n- Joins customers with their orders\n- Filters for high-value orders (>$1000) using the total_amount field\n- Only includes completed orders (as per the business logic)\n- Restricts to orders from the last month\n- Returns distinct customers to avoid duplicates if they made multiple qualifying orders\nThe agent saw the lightweight skill description in its system prompt, recognized the question required sales database knowledge, called\nload_skill(\"sales_analytics\")\nto get the full schema and business logic, and then used that information to write a correct query following the database conventions.\n\u200b\n6. Advanced: Add constraints with custom state\nOptional: Track loaded skills and enforce tool constraints\nYou can add constraints to enforce that certain tools are only available after specific skills have been loaded. This requires tracking which skills have been loaded in custom agent state.\n\u200b\nDefine custom state\nFirst, extend the agent state to track loaded skills:\nCopy\nfrom\nlangchain.agents.middleware\nimport\nAgentState\nclass\nCustomState\n(\nAgentState\n):\nskills_loaded: NotRequired[list[\nstr\n]]\n# Track which skills have been loaded  #\n\u200b\nUpdate load_skill to modify state\nModify the\nload_skill\ntool to update state when a skill is loaded:\nCopy\nfrom\nlanggraph.types\nimport\nCommand\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\nfrom\nlangchain.messages\nimport\nToolMessage\n@tool\ndef\nload_skill\n(\nskill_name\n:\nstr\n,\nruntime\n: ToolRuntime) -> Command:\n\"\"\"Load the full content of a skill into the agent's context.\nUse this when you need detailed information about how to handle a specific\ntype of request. This will provide you with comprehensive instructions,\npolicies, and guidelines for the skill area.\nArgs:\nskill_name: The name of the skill to load\n\"\"\"\n# Find and return the requested skill\nfor\nskill\nin\nSKILLS\n:\nif\nskill[\n\"name\"\n]\n==\nskill_name:\nskill_content\n=\nf\n\"Loaded skill:\n{\nskill_name\n}\n\\n\\n\n{\nskill[\n'content'\n]\n}\n\"\n# Update state to track loaded skill\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\ncontent\n=\nskill_content,\ntool_call_id\n=\nruntime.tool_call_id,\n)\n],\n\"skills_loaded\"\n: [skill_name],\n}\n)\n# Skill not found\navailable\n=\n\", \"\n.join(s[\n\"name\"\n]\nfor\ns\nin\nSKILLS\n)\nreturn\nCommand(\nupdate\n=\n{\n\"messages\"\n: [\nToolMessage(\ncontent\n=\nf\n\"Skill '\n{\nskill_name\n}\n' not found. Available skills:\n{\navailable\n}\n\"\n,\ntool_call_id\n=\nruntime.tool_call_id,\n)\n]\n}\n)\n\u200b\nCreate constrained tool\nCreate a tool that\u2019s only usable after a specific skill has been loaded:\nCopy\n@tool\ndef\nwrite_sql_query\n(\nquery\n:\nstr\n,\nvertical\n:\nstr\n,\nruntime\n: ToolRuntime,\n) ->\nstr\n:\n\"\"\"Write and validate a SQL query for a specific business vertical.\nThis tool helps format and validate SQL queries. You must load the\nappropriate skill first to understand the database schema.\nArgs:\nquery: The SQL query to write\nvertical: The business vertical (sales_analytics or inventory_management)\n\"\"\"\n# Check if the required skill has been loaded\nskills_loaded\n=\nruntime.state.get(\n\"skills_loaded\"\n, [])\nif\nvertical\nnot\nin\nskills_loaded:\nreturn\n(\nf\n\"Error: You must load the '\n{\nvertical\n}\n' skill first \"\nf\n\"to understand the database schema before writing queries. \"\nf\n\"Use load_skill('\n{\nvertical\n}\n') to load the schema.\"\n)\n# Validate and format the query\nreturn\n(\nf\n\"SQL Query for\n{\nvertical\n}\n:\n\\n\\n\n\"\nf\n\"```sql\n\\n\n{\nquery\n}\n\\n\n```\n\\n\\n\n\"\nf\n\"\u2713 Query validated against\n{\nvertical\n}\nschema\n\\n\n\"\nf\n\"Ready to execute against the database.\"\n)\n\u200b\nUpdate middleware and agent\nUpdate the middleware to use the custom state schema:\nCopy\nclass\nSkillMiddleware\n(AgentMiddleware[CustomState]):\n\"\"\"Middleware that injects skill descriptions into the system prompt.\"\"\"\nstate_schema\n=\nCustomState\ntools\n=\n[load_skill, write_sql_query]\n# ... rest of the middleware implementation stays the same\nCreate the agent with the middleware that registers the constrained tool:\nCopy\nagent\n=\ncreate_agent(\nmodel,\nsystem_prompt\n=\n(\n\"You are a SQL query assistant that helps users \"\n\"write queries against business databases.\"\n),\nmiddleware\n=\n[SkillMiddleware()],\ncheckpointer\n=\nInMemorySaver(),\n)\nNow if the agent tries to use\nwrite_sql_query\nbefore loading the required skill, it will receive an error message prompting it to load the appropriate skill (e.g.,\nsales_analytics\nor\ninventory_management\n) first. This ensures the agent has the necessary schema knowledge before attempting to validate queries.\n\u200b\nComplete example\nView complete runnable script\nHere\u2019s a complete, runnable implementation combining all the pieces from this tutorial:\nCopy\nimport\nuuid\nfrom\ntyping\nimport\nTypedDict, NotRequired\nfrom\nlangchain.tools\nimport\ntool\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\nModelRequest, ModelResponse, AgentMiddleware\nfrom\nlangchain.messages\nimport\nSystemMessage\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\ntyping\nimport\nCallable\n# Define skill structure\nclass\nSkill\n(\nTypedDict\n):\n\"\"\"A skill that can be progressively disclosed to the agent.\"\"\"\nname:\nstr\ndescription:\nstr\ncontent:\nstr\n# Define skills with schemas and business logic\nSKILLS\n: list[Skill]\n=\n[\n{\n\"name\"\n:\n\"sales_analytics\"\n,\n\"description\"\n:\n\"Database schema and business logic for sales data analysis including customers, orders, and revenue.\"\n,\n\"content\"\n:\n\"\"\"# Sales Analytics Schema\n## Tables\n### customers\n- customer_id (PRIMARY KEY)\n- name\n- email\n- signup_date\n- status (active/inactive)\n- customer_tier (bronze/silver/gold/platinum)\n### orders\n- order_id (PRIMARY KEY)\n- customer_id (FOREIGN KEY -> customers)\n- order_date\n- status (pending/completed/cancelled/refunded)\n- total_amount\n- sales_region (north/south/east/west)\n### order_items\n- item_id (PRIMARY KEY)\n- order_id (FOREIGN KEY -> orders)\n- product_id\n- quantity\n- unit_price\n- discount_percent\n## Business Logic\n**Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\n**Revenue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.\n**Customer lifetime value (CLV)**: Sum of all completed order amounts for a customer.\n**High-value orders**: Orders with total_amount > 1000\n## Example Query\n-- Get top 10 customers by revenue in the last quarter\nSELECT\nc.customer_id,\nc.name,\nc.customer_tier,\nSUM(o.total_amount) as total_revenue\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nWHERE o.status = 'completed'\nAND o.order_date >= CURRENT_DATE - INTERVAL '3 months'\nGROUP BY c.customer_id, c.name, c.customer_tier\nORDER BY total_revenue DESC\nLIMIT 10;\n\"\"\"\n,\n},\n{\n\"name\"\n:\n\"inventory_management\"\n,\n\"description\"\n:\n\"Database schema and business logic for inventory tracking including products, warehouses, and stock levels.\"\n,\n\"content\"\n:\n\"\"\"# Inventory Management Schema\n## Tables\n### products\n- product_id (PRIMARY KEY)\n- product_name\n- sku\n- category\n- unit_cost\n- reorder_point (minimum stock level before reordering)\n- discontinued (boolean)\n### warehouses\n- warehouse_id (PRIMARY KEY)\n- warehouse_name\n- location\n- capacity\n### inventory\n- inventory_id (PRIMARY KEY)\n- product_id (FOREIGN KEY -> products)\n- warehouse_id (FOREIGN KEY -> warehouses)\n- quantity_on_hand\n- last_updated\n### stock_movements\n- movement_id (PRIMARY KEY)\n- product_id (FOREIGN KEY -> products)\n- warehouse_id (FOREIGN KEY -> warehouses)\n- movement_type (inbound/outbound/transfer/adjustment)\n- quantity (positive for inbound, negative for outbound)\n- movement_date\n- reference_number\n## Business Logic\n**Available stock**: quantity_on_hand from inventory table where quantity_on_hand > 0\n**Products needing reorder**: Products where total quantity_on_hand across all warehouses is less than or equal to the product's reorder_point\n**Active products only**: Exclude products where discontinued = true unless specifically analyzing discontinued items\n**Stock valuation**: quantity_on_hand * unit_cost for each product\n## Example Query\n-- Find products below reorder point across all warehouses\nSELECT\np.product_id,\np.product_name,\np.reorder_point,\nSUM(i.quantity_on_hand) as total_stock,\np.unit_cost,\n(p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\nFROM products p\nJOIN inventory i ON p.product_id = i.product_id\nWHERE p.discontinued = false\nGROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost\nHAVING SUM(i.quantity_on_hand) <= p.reorder_point\nORDER BY units_to_reorder DESC;\n\"\"\"\n,\n},\n]\n# Create skill loading tool\n@tool\ndef\nload_skill\n(\nskill_name\n:\nstr\n) ->\nstr\n:\n\"\"\"Load the full content of a skill into the agent's context.\nUse this when you need detailed information about how to handle a specific\ntype of request. This will provide you with comprehensive instructions,\npolicies, and guidelines for the skill area.\nArgs:\nskill_name: The name of the skill to load (e.g., \"sales_analytics\", \"inventory_management\")\n\"\"\"\n# Find and return the requested skill\nfor\nskill\nin\nSKILLS\n:\nif\nskill[\n\"name\"\n]\n==\nskill_name:\nreturn\nf\n\"Loaded skill:\n{\nskill_name\n}\n\\n\\n\n{\nskill[\n'content'\n]\n}\n\"\n# Skill not found\navailable\n=\n\", \"\n.join(s[\n\"name\"\n]\nfor\ns\nin\nSKILLS\n)\nreturn\nf\n\"Skill '\n{\nskill_name\n}\n' not found. Available skills:\n{\navailable\n}\n\"\n# Create skill middleware\nclass\nSkillMiddleware\n(\nAgentMiddleware\n):\n\"\"\"Middleware that injects skill descriptions into the system prompt.\"\"\"\n# Register the load_skill tool as a class variable\ntools\n=\n[load_skill]\ndef\n__init__\n(\nself\n):\n\"\"\"Initialize and generate the skills prompt from SKILLS.\"\"\"\n# Build skills prompt from the SKILLS list\nskills_list\n=\n[]\nfor\nskill\nin\nSKILLS\n:\nskills_list.append(\nf\n\"- **\n{\nskill[\n'name'\n]\n}\n**:\n{\nskill[\n'description'\n]\n}\n\"\n)\nself\n.skills_prompt\n=\n\"\n\\n\n\"\n.join(skills_list)\ndef\nwrap_model_call\n(\nself\n,\nrequest\n: ModelRequest,\nhandler\n: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n\"\"\"Sync: Inject skill descriptions into system prompt.\"\"\"\n# Build the skills addendum\nskills_addendum\n=\n(\nf\n\"\n\\n\\n\n## Available Skills\n\\n\\n\n{\nself\n.skills_prompt\n}\n\\n\\n\n\"\n\"Use the load_skill tool when you need detailed information \"\n\"about handling a specific type of request.\"\n)\n# Append to system message content blocks\nnew_content\n=\nlist\n(request.system_message.content_blocks)\n+\n[\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n: skills_addendum}\n]\nnew_system_message\n=\nSystemMessage(\ncontent\n=\nnew_content)\nmodified_request\n=\nrequest.override(\nsystem_message\n=\nnew_system_message)\nreturn\nhandler(modified_request)\n# Initialize your chat model (replace with your model)\n# Example: from langchain_anthropic import ChatAnthropic\n# model = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\nfrom\nlangchain_openai\nimport\nChatOpenAI\nmodel\n=\nChatOpenAI(\nmodel\n=\n\"gpt-4\"\n)\n# Create the agent with skill support\nagent\n=\ncreate_agent(\nmodel,\nsystem_prompt\n=\n(\n\"You are a SQL query assistant that helps users \"\n\"write queries against business databases.\"\n),\nmiddleware\n=\n[SkillMiddleware()],\ncheckpointer\n=\nInMemorySaver(),\n)\n# Example usage\nif\n__name__\n==\n\"__main__\"\n:\n# Configuration for this conversation thread\nthread_id\n=\nstr\n(uuid.uuid4())\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n: thread_id}}\n# Ask for a SQL query\nresult\n=\nagent.invoke(\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: (\n\"Write a SQL query to find all customers \"\n\"who made orders over $1000 in the last month\"\n),\n}\n]\n},\nconfig\n)\n# Print the conversation\nfor\nmessage\nin\nresult[\n\"messages\"\n]:\nif\nhasattr\n(message,\n'pretty_print'\n):\nmessage.pretty_print()\nelse\n:\nprint\n(\nf\n\"\n{\nmessage.type\n}\n:\n{\nmessage.content\n}\n\"\n)\nThis complete example includes:\nSkill definitions with full database schemas\nThe\nload_skill\ntool for on-demand loading\nSkillMiddleware\nthat injects skill descriptions into the system prompt\nAgent creation with middleware and checkpointer\nExample usage showing how the agent loads skills and writes SQL queries\nTo run this, you\u2019ll need to:\nInstall required packages:\npip install langchain langchain-openai langgraph\nSet your API key (e.g.,\nexport OPENAI_API_KEY=...\n)\nReplace the model initialization with your preferred LLM provider\n\u200b\nImplementation variations\nView implementation options and trade-offs\nThis tutorial implemented skills as in-memory Python dictionaries loaded through tool calls. However, there are several ways to implement progressive disclosure with skills:\nStorage backends:\nIn-memory\n(this tutorial): Skills defined as Python data structures, fast access, no I/O overhead\nFile system\n(Claude Code approach): Skills as directories with files, discovered via file operations like\nread_file\nRemote storage\n: Skills in S3, databases, Notion, or APIs, fetched on-demand\nSkill discovery\n(how the agent learns which skills exist):\nSystem prompt listing\n: Skill descriptions in system prompt (used in this tutorial)\nFile-based\n: Discover skills by scanning directories (Claude Code approach)\nRegistry-based\n: Query a skill registry service or API for available skills\nDynamic lookup\n: List available skills via a tool call\nProgressive disclosure strategies\n(how skill content is loaded):\nSingle load\n: Load entire skill content in one tool call (used in this tutorial)\nPaginated\n: Load skill content in multiple pages/chunks for large skills\nSearch-based\n: Search within a specific skill\u2019s content for relevant sections (e.g., using grep/read operations on skill files)\nHierarchical\n: Load skill overview first, then drill into specific subsections\nSize considerations\n(uncalibrated mental model - optimize for your system):\nSmall skills\n(< 1K tokens / ~750 words): Can be included directly in system prompt and cached with prompt caching for cost savings and faster responses\nMedium skills\n(1-10K tokens / ~750-7.5K words): Benefit from on-demand loading to avoid context overhead (this tutorial)\nLarge skills\n(> 10K tokens / ~7.5K words, or > 5-10% of context window): Should use progressive disclosure techniques like pagination, search-based loading, or hierarchical exploration to avoid consuming excessive context\nThe choice depends on your requirements: in-memory is fastest but requires redeployment for skill updates, while file-based or remote storage enables dynamic skill management without code changes.\n\u200b\nProgressive disclosure and context engineering\nCombining with few-shot prompting and other techniques\nProgressive disclosure is fundamentally a\ncontext engineering\ntechnique\n- you\u2019re managing what information is available to the agent and when. This tutorial focused on loading database schemas, but the same principles apply to other types of context.\n\u200b\nCombining with few-shot prompting\nFor the SQL query use case, you could extend progressive disclosure to dynamically load\nfew-shot examples\nthat match the user\u2019s query:\nExample approach:\nUser asks: \u201cFind customers who haven\u2019t ordered in 6 months\u201d\nAgent loads\nsales_analytics\nschema (as shown in this tutorial)\nAgent also loads 2-3 relevant example queries (via semantic search or tag-based lookup):\nQuery for finding inactive customers\nQuery with date-based filtering\nQuery joining customers and orders tables\nAgent writes query using both schema knowledge AND example patterns\nThis combination of progressive disclosure (loading schemas on-demand) and dynamic few-shot prompting (loading relevant examples) creates a powerful context engineering pattern that scales to large knowledge bases while providing high-quality, grounded outputs.\n\u200b\nNext steps\nLearn about\nmiddleware\nfor more dynamic agent behaviors\nExplore\ncontext engineering\ntechniques for managing agent context\nExplore the\nhandoffs pattern\nfor sequential workflows\nRead the\nsubagents pattern\nfor parallel task routing\nSee\nmulti-agent patterns\nfor other approaches to specialized agents\nUse\nLangSmith\nto debug and monitor skill loading\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a multi-source knowledge base with routing\nPrevious\nBuild a custom RAG agent with LangGraph\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant",
      "title": "Build a SQL assistant with on-demand skills - Docs by LangChain",
      "heading": "Build a SQL assistant with on-demand skills"
    }
  },
  {
    "page_content": "Workflows and agents - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nGet started\nWorkflows and agents\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nSetup\nLLMs and augmentations\nPrompt chaining\nParallelization\nRouting\nOrchestrator-worker\nCreating workers in LangGraph\nEvaluator-optimizer\nAgents\nGet started\nWorkflows and agents\nCopy page\nCopy page\nThis guide reviews common workflow and agent patterns.\nWorkflows have predetermined code paths and are designed to operate in a certain order.\nAgents are dynamic and define their own processes and tool usage.\nLangGraph offers several benefits when building agents and workflows, including\npersistence\n,\nstreaming\n, and support for debugging as well as\ndeployment\n.\n\u200b\nSetup\nTo build a workflow or agent, you can use\nany chat model\nthat supports structured outputs and tool calling. The following example uses Anthropic:\nInstall dependencies:\nCopy\npip\ninstall\nlangchain_core\nlangchain-anthropic\nlanggraph\nInitialize the LLM:\nCopy\nimport\nos\nimport\ngetpass\nfrom\nlangchain_anthropic\nimport\nChatAnthropic\ndef\n_set_env\n(\nvar\n:\nstr\n):\nif\nnot\nos.environ.get(var):\nos.environ[var]\n=\ngetpass.getpass(\nf\n\"\n{\nvar\n}\n: \"\n)\n_set_env(\n\"ANTHROPIC_API_KEY\"\n)\nllm\n=\nChatAnthropic(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n)\n\u200b\nLLMs and augmentations\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them.\nTool calling\n,\nstructured outputs\n, and\nshort term memory\nare a few options for tailoring LLMs to your needs.\nCopy\n# Schema for structured output\nfrom\npydantic\nimport\nBaseModel, Field\nclass\nSearchQuery\n(\nBaseModel\n):\nsearch_query:\nstr\n=\nField(\nNone\n,\ndescription\n=\n\"Query that is optimized web search.\"\n)\njustification:\nstr\n=\nField(\nNone\n,\ndescription\n=\n\"Why this query is relevant to the user's request.\"\n)\n# Augment the LLM with schema for structured output\nstructured_llm\n=\nllm.with_structured_output(SearchQuery)\n# Invoke the augmented LLM\noutput\n=\nstructured_llm.invoke(\n\"How does Calcium CT score relate to high cholesterol?\"\n)\n# Define a tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n) ->\nint\n:\nreturn\na\n*\nb\n# Augment the LLM with tools\nllm_with_tools\n=\nllm.bind_tools([multiply])\n# Invoke the LLM with input that triggers the tool call\nmsg\n=\nllm_with_tools.invoke(\n\"What is 2 times 3?\"\n)\n# Get the tool call\nmsg.tool_calls\n\u200b\nPrompt chaining\nPrompt chaining is when each LLM call processes the output of the previous call. It\u2019s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\nTranslating documents into different languages\nVerifying generated content for consistency\nGraph API\nFunctional API\nCopy\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nIPython.display\nimport\nImage, display\n# Graph state\nclass\nState\n(\nTypedDict\n):\ntopic:\nstr\njoke:\nstr\nimproved_joke:\nstr\nfinal_joke:\nstr\n# Nodes\ndef\ngenerate_joke\n(\nstate\n: State):\n\"\"\"First LLM call to generate initial joke\"\"\"\nmsg\n=\nllm.invoke(\nf\n\"Write a short joke about\n{\nstate[\n'topic'\n]\n}\n\"\n)\nreturn\n{\n\"joke\"\n: msg.content}\ndef\ncheck_punchline\n(\nstate\n: State):\n\"\"\"Gate function to check if the joke has a punchline\"\"\"\n# Simple check - does the joke contain \"?\" or \"!\"\nif\n\"?\"\nin\nstate[\n\"joke\"\n]\nor\n\"!\"\nin\nstate[\n\"joke\"\n]:\nreturn\n\"Pass\"\nreturn\n\"Fail\"\ndef\nimprove_joke\n(\nstate\n: State):\n\"\"\"Second LLM call to improve the joke\"\"\"\nmsg\n=\nllm.invoke(\nf\n\"Make this joke funnier by adding wordplay:\n{\nstate[\n'joke'\n]\n}\n\"\n)\nreturn\n{\n\"improved_joke\"\n: msg.content}\ndef\npolish_joke\n(\nstate\n: State):\n\"\"\"Third LLM call for final polish\"\"\"\nmsg\n=\nllm.invoke(\nf\n\"Add a surprising twist to this joke:\n{\nstate[\n'improved_joke'\n]\n}\n\"\n)\nreturn\n{\n\"final_joke\"\n: msg.content}\n# Build workflow\nworkflow\n=\nStateGraph(State)\n# Add nodes\nworkflow.add_node(\n\"generate_joke\"\n, generate_joke)\nworkflow.add_node(\n\"improve_joke\"\n, improve_joke)\nworkflow.add_node(\n\"polish_joke\"\n, polish_joke)\n# Add edges to connect nodes\nworkflow.add_edge(\nSTART\n,\n\"generate_joke\"\n)\nworkflow.add_conditional_edges(\n\"generate_joke\"\n, check_punchline, {\n\"Fail\"\n:\n\"improve_joke\"\n,\n\"Pass\"\n:\nEND\n}\n)\nworkflow.add_edge(\n\"improve_joke\"\n,\n\"polish_joke\"\n)\nworkflow.add_edge(\n\"polish_joke\"\n,\nEND\n)\n# Compile\nchain\n=\nworkflow.compile()\n# Show workflow\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\n# Invoke\nstate\n=\nchain.invoke({\n\"topic\"\n:\n\"cats\"\n})\nprint\n(\n\"Initial joke:\"\n)\nprint\n(state[\n\"joke\"\n])\nprint\n(\n\"\n\\n\n--- --- ---\n\\n\n\"\n)\nif\n\"improved_joke\"\nin\nstate:\nprint\n(\n\"Improved joke:\"\n)\nprint\n(state[\n\"improved_joke\"\n])\nprint\n(\n\"\n\\n\n--- --- ---\n\\n\n\"\n)\nprint\n(\n\"Final joke:\"\n)\nprint\n(state[\n\"final_joke\"\n])\nelse\n:\nprint\n(\n\"Final joke:\"\n)\nprint\n(state[\n\"joke\"\n])\n\u200b\nParallelization\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\nSplit up subtasks and run them in parallel, which increases speed\nRun tasks multiple times to check for different outputs, which increases confidence\nSome examples include:\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\nGraph API\nFunctional API\nCopy\n# Graph state\nclass\nState\n(\nTypedDict\n):\ntopic:\nstr\njoke:\nstr\nstory:\nstr\npoem:\nstr\ncombined_output:\nstr\n# Nodes\ndef\ncall_llm_1\n(\nstate\n: State):\n\"\"\"First LLM call to generate initial joke\"\"\"\nmsg\n=\nllm.invoke(\nf\n\"Write a joke about\n{\nstate[\n'topic'\n]\n}\n\"\n)\nreturn\n{\n\"joke\"\n: msg.content}\ndef\ncall_llm_2\n(\nstate\n: State):\n\"\"\"Second LLM call to generate story\"\"\"\nmsg\n=\nllm.invoke(\nf\n\"Write a story about\n{\nstate[\n'topic'\n]\n}\n\"\n)\nreturn\n{\n\"story\"\n: msg.content}\ndef\ncall_llm_3\n(\nstate\n: State):\n\"\"\"Third LLM call to generate poem\"\"\"\nmsg\n=\nllm.invoke(\nf\n\"Write a poem about\n{\nstate[\n'topic'\n]\n}\n\"\n)\nreturn\n{\n\"poem\"\n: msg.content}\ndef\naggregator\n(\nstate\n: State):\n\"\"\"Combine the joke, story and poem into a single output\"\"\"\ncombined\n=\nf\n\"Here's a story, joke, and poem about\n{\nstate[\n'topic'\n]\n}\n!\n\\n\\n\n\"\ncombined\n+=\nf\n\"STORY:\n\\n\n{\nstate[\n'story'\n]\n}\n\\n\\n\n\"\ncombined\n+=\nf\n\"JOKE:\n\\n\n{\nstate[\n'joke'\n]\n}\n\\n\\n\n\"\ncombined\n+=\nf\n\"POEM:\n\\n\n{\nstate[\n'poem'\n]\n}\n\"\nreturn\n{\n\"combined_output\"\n: combined}\n# Build workflow\nparallel_builder\n=\nStateGraph(State)\n# Add nodes\nparallel_builder.add_node(\n\"call_llm_1\"\n, call_llm_1)\nparallel_builder.add_node(\n\"call_llm_2\"\n, call_llm_2)\nparallel_builder.add_node(\n\"call_llm_3\"\n, call_llm_3)\nparallel_builder.add_node(\n\"aggregator\"\n, aggregator)\n# Add edges to connect nodes\nparallel_builder.add_edge(\nSTART\n,\n\"call_llm_1\"\n)\nparallel_builder.add_edge(\nSTART\n,\n\"call_llm_2\"\n)\nparallel_builder.add_edge(\nSTART\n,\n\"call_llm_3\"\n)\nparallel_builder.add_edge(\n\"call_llm_1\"\n,\n\"aggregator\"\n)\nparallel_builder.add_edge(\n\"call_llm_2\"\n,\n\"aggregator\"\n)\nparallel_builder.add_edge(\n\"call_llm_3\"\n,\n\"aggregator\"\n)\nparallel_builder.add_edge(\n\"aggregator\"\n,\nEND\n)\nparallel_workflow\n=\nparallel_builder.compile()\n# Show workflow\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n# Invoke\nstate\n=\nparallel_workflow.invoke({\n\"topic\"\n:\n\"cats\"\n})\nprint\n(state[\n\"combined_output\"\n])\n\u200b\nRouting\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\nGraph API\nFunctional API\nCopy\nfrom\ntyping_extensions\nimport\nLiteral\nfrom\nlangchain.messages\nimport\nHumanMessage, SystemMessage\n# Schema for structured output to use as routing logic\nclass\nRoute\n(\nBaseModel\n):\nstep: Literal[\n\"poem\"\n,\n\"story\"\n,\n\"joke\"\n]\n=\nField(\nNone\n,\ndescription\n=\n\"The next step in the routing process\"\n)\n# Augment the LLM with schema for structured output\nrouter\n=\nllm.with_structured_output(Route)\n# State\nclass\nState\n(\nTypedDict\n):\ninput\n:\nstr\ndecision:\nstr\noutput:\nstr\n# Nodes\ndef\nllm_call_1\n(\nstate\n: State):\n\"\"\"Write a story\"\"\"\nresult\n=\nllm.invoke(state[\n\"input\"\n])\nreturn\n{\n\"output\"\n: result.content}\ndef\nllm_call_2\n(\nstate\n: State):\n\"\"\"Write a joke\"\"\"\nresult\n=\nllm.invoke(state[\n\"input\"\n])\nreturn\n{\n\"output\"\n: result.content}\ndef\nllm_call_3\n(\nstate\n: State):\n\"\"\"Write a poem\"\"\"\nresult\n=\nllm.invoke(state[\n\"input\"\n])\nreturn\n{\n\"output\"\n: result.content}\ndef\nllm_call_router\n(\nstate\n: State):\n\"\"\"Route the input to the appropriate node\"\"\"\n# Run the augmented LLM with structured output to serve as routing logic\ndecision\n=\nrouter.invoke(\n[\nSystemMessage(\ncontent\n=\n\"Route the input to story, joke, or poem based on the user's request.\"\n),\nHumanMessage(\ncontent\n=\nstate[\n\"input\"\n]),\n]\n)\nreturn\n{\n\"decision\"\n: decision.step}\n# Conditional edge function to route to the appropriate node\ndef\nroute_decision\n(\nstate\n: State):\n# Return the node name you want to visit next\nif\nstate[\n\"decision\"\n]\n==\n\"story\"\n:\nreturn\n\"llm_call_1\"\nelif\nstate[\n\"decision\"\n]\n==\n\"joke\"\n:\nreturn\n\"llm_call_2\"\nelif\nstate[\n\"decision\"\n]\n==\n\"poem\"\n:\nreturn\n\"llm_call_3\"\n# Build workflow\nrouter_builder\n=\nStateGraph(State)\n# Add nodes\nrouter_builder.add_node(\n\"llm_call_1\"\n, llm_call_1)\nrouter_builder.add_node(\n\"llm_call_2\"\n, llm_call_2)\nrouter_builder.add_node(\n\"llm_call_3\"\n, llm_call_3)\nrouter_builder.add_node(\n\"llm_call_router\"\n, llm_call_router)\n# Add edges to connect nodes\nrouter_builder.add_edge(\nSTART\n,\n\"llm_call_router\"\n)\nrouter_builder.add_conditional_edges(\n\"llm_call_router\"\n,\nroute_decision,\n{\n# Name returned by route_decision : Name of next node to visit\n\"llm_call_1\"\n:\n\"llm_call_1\"\n,\n\"llm_call_2\"\n:\n\"llm_call_2\"\n,\n\"llm_call_3\"\n:\n\"llm_call_3\"\n,\n},\n)\nrouter_builder.add_edge(\n\"llm_call_1\"\n,\nEND\n)\nrouter_builder.add_edge(\n\"llm_call_2\"\n,\nEND\n)\nrouter_builder.add_edge(\n\"llm_call_3\"\n,\nEND\n)\n# Compile workflow\nrouter_workflow\n=\nrouter_builder.compile()\n# Show the workflow\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\n# Invoke\nstate\n=\nrouter_workflow.invoke({\n\"input\"\n:\n\"Write me a joke about cats\"\n})\nprint\n(state[\n\"output\"\n])\n\u200b\nOrchestrator-worker\nIn an orchestrator-worker configuration, the orchestrator:\nBreaks down tasks into subtasks\nDelegates subtasks to workers\nSynthesizes worker outputs into a final result\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with\nparallelization\n. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\nGraph API\nFunctional API\nCopy\nfrom\ntyping\nimport\nAnnotated, List\nimport\noperator\n# Schema for structured output to use in planning\nclass\nSection\n(\nBaseModel\n):\nname:\nstr\n=\nField(\ndescription\n=\n\"Name for this section of the report.\"\n,\n)\ndescription:\nstr\n=\nField(\ndescription\n=\n\"Brief overview of the main topics and concepts to be covered in this section.\"\n,\n)\nclass\nSections\n(\nBaseModel\n):\nsections: List[Section]\n=\nField(\ndescription\n=\n\"Sections of the report.\"\n,\n)\n# Augment the LLM with schema for structured output\nplanner\n=\nllm.with_structured_output(Sections)\n\u200b\nCreating workers in LangGraph\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The\nSend\nAPI lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the\nSend\nAPI to send a section to each worker.\nCopy\nfrom\nlanggraph.types\nimport\nSend\n# Graph state\nclass\nState\n(\nTypedDict\n):\ntopic:\nstr\n# Report topic\nsections: list[Section]\n# List of report sections\ncompleted_sections: Annotated[\nlist\n, operator.add\n]\n# All workers write to this key in parallel\nfinal_report:\nstr\n# Final report\n# Worker state\nclass\nWorkerState\n(\nTypedDict\n):\nsection: Section\ncompleted_sections: Annotated[\nlist\n, operator.add]\n# Nodes\ndef\norchestrator\n(\nstate\n: State):\n\"\"\"Orchestrator that generates a plan for the report\"\"\"\n# Generate queries\nreport_sections\n=\nplanner.invoke(\n[\nSystemMessage(\ncontent\n=\n\"Generate a plan for the report.\"\n),\nHumanMessage(\ncontent\n=\nf\n\"Here is the report topic:\n{\nstate[\n'topic'\n]\n}\n\"\n),\n]\n)\nreturn\n{\n\"sections\"\n: report_sections.sections}\ndef\nllm_call\n(\nstate\n: WorkerState):\n\"\"\"Worker writes a section of the report\"\"\"\n# Generate section\nsection\n=\nllm.invoke(\n[\nSystemMessage(\ncontent\n=\n\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n),\nHumanMessage(\ncontent\n=\nf\n\"Here is the section name:\n{\nstate[\n'section'\n].name\n}\nand description:\n{\nstate[\n'section'\n].description\n}\n\"\n),\n]\n)\n# Write the updated section to completed sections\nreturn\n{\n\"completed_sections\"\n: [section.content]}\ndef\nsynthesizer\n(\nstate\n: State):\n\"\"\"Synthesize full report from sections\"\"\"\n# List of completed sections\ncompleted_sections\n=\nstate[\n\"completed_sections\"\n]\n# Format completed section to str to use as context for final sections\ncompleted_report_sections\n=\n\"\n\\n\\n\n---\n\\n\\n\n\"\n.join(completed_sections)\nreturn\n{\n\"final_report\"\n: completed_report_sections}\n# Conditional edge function to create llm_call workers that each write a section of the report\ndef\nassign_workers\n(\nstate\n: State):\n\"\"\"Assign a worker to each section in the plan\"\"\"\n# Kick off section writing in parallel via Send() API\nreturn\n[Send(\n\"llm_call\"\n, {\n\"section\"\n: s})\nfor\ns\nin\nstate[\n\"sections\"\n]]\n# Build workflow\norchestrator_worker_builder\n=\nStateGraph(State)\n# Add the nodes\norchestrator_worker_builder.add_node(\n\"orchestrator\"\n, orchestrator)\norchestrator_worker_builder.add_node(\n\"llm_call\"\n, llm_call)\norchestrator_worker_builder.add_node(\n\"synthesizer\"\n, synthesizer)\n# Add edges to connect nodes\norchestrator_worker_builder.add_edge(\nSTART\n,\n\"orchestrator\"\n)\norchestrator_worker_builder.add_conditional_edges(\n\"orchestrator\"\n, assign_workers, [\n\"llm_call\"\n]\n)\norchestrator_worker_builder.add_edge(\n\"llm_call\"\n,\n\"synthesizer\"\n)\norchestrator_worker_builder.add_edge(\n\"synthesizer\"\n,\nEND\n)\n# Compile the workflow\norchestrator_worker\n=\norchestrator_worker_builder.compile()\n# Show the workflow\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\n# Invoke\nstate\n=\norchestrator_worker.invoke({\n\"topic\"\n:\n\"Create a report on LLM scaling laws\"\n})\nfrom\nIPython.display\nimport\nMarkdown\nMarkdown(state[\n\"final_report\"\n])\n\u200b\nEvaluator-optimizer\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a\nhuman-in-the-loop\ndetermines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\nEvaluator-optimizer workflows are commonly used when there\u2019s particular success criteria for a task, but iteration is required to meet that criteria. For example, there\u2019s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\nGraph API\nFunctional API\nCopy\n# Graph state\nclass\nState\n(\nTypedDict\n):\njoke:\nstr\ntopic:\nstr\nfeedback:\nstr\nfunny_or_not:\nstr\n# Schema for structured output to use in evaluation\nclass\nFeedback\n(\nBaseModel\n):\ngrade: Literal[\n\"funny\"\n,\n\"not funny\"\n]\n=\nField(\ndescription\n=\n\"Decide if the joke is funny or not.\"\n,\n)\nfeedback:\nstr\n=\nField(\ndescription\n=\n\"If the joke is not funny, provide feedback on how to improve it.\"\n,\n)\n# Augment the LLM with schema for structured output\nevaluator\n=\nllm.with_structured_output(Feedback)\n# Nodes\ndef\nllm_call_generator\n(\nstate\n: State):\n\"\"\"LLM generates a joke\"\"\"\nif\nstate.get(\n\"feedback\"\n):\nmsg\n=\nllm.invoke(\nf\n\"Write a joke about\n{\nstate[\n'topic'\n]\n}\nbut take into account the feedback:\n{\nstate[\n'feedback'\n]\n}\n\"\n)\nelse\n:\nmsg\n=\nllm.invoke(\nf\n\"Write a joke about\n{\nstate[\n'topic'\n]\n}\n\"\n)\nreturn\n{\n\"joke\"\n: msg.content}\ndef\nllm_call_evaluator\n(\nstate\n: State):\n\"\"\"LLM evaluates the joke\"\"\"\ngrade\n=\nevaluator.invoke(\nf\n\"Grade the joke\n{\nstate[\n'joke'\n]\n}\n\"\n)\nreturn\n{\n\"funny_or_not\"\n: grade.grade,\n\"feedback\"\n: grade.feedback}\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\ndef\nroute_joke\n(\nstate\n: State):\n\"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\nif\nstate[\n\"funny_or_not\"\n]\n==\n\"funny\"\n:\nreturn\n\"Accepted\"\nelif\nstate[\n\"funny_or_not\"\n]\n==\n\"not funny\"\n:\nreturn\n\"Rejected + Feedback\"\n# Build workflow\noptimizer_builder\n=\nStateGraph(State)\n# Add the nodes\noptimizer_builder.add_node(\n\"llm_call_generator\"\n, llm_call_generator)\noptimizer_builder.add_node(\n\"llm_call_evaluator\"\n, llm_call_evaluator)\n# Add edges to connect nodes\noptimizer_builder.add_edge(\nSTART\n,\n\"llm_call_generator\"\n)\noptimizer_builder.add_edge(\n\"llm_call_generator\"\n,\n\"llm_call_evaluator\"\n)\noptimizer_builder.add_conditional_edges(\n\"llm_call_evaluator\"\n,\nroute_joke,\n{\n# Name returned by route_joke : Name of next node to visit\n\"Accepted\"\n:\nEND\n,\n\"Rejected + Feedback\"\n:\n\"llm_call_generator\"\n,\n},\n)\n# Compile the workflow\noptimizer_workflow\n=\noptimizer_builder.compile()\n# Show the workflow\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\n# Invoke\nstate\n=\noptimizer_workflow.invoke({\n\"topic\"\n:\n\"Cats\"\n})\nprint\n(state[\n\"joke\"\n])\n\u200b\nAgents\nAgents are typically implemented as an LLM performing actions using\ntools\n. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\nTo get started with agents, see the\nquickstart\nor read more about\nhow they work\nin LangChain.\nUsing tools\nCopy\nfrom\nlangchain.tools\nimport\ntool\n# Define tools\n@tool\ndef\nmultiply\n(\na\n:\nint\n,\nb\n:\nint\n) ->\nint\n:\n\"\"\"Multiply `a` and `b`.\nArgs:\na: First int\nb: Second int\n\"\"\"\nreturn\na\n*\nb\n@tool\ndef\nadd\n(\na\n:\nint\n,\nb\n:\nint\n) ->\nint\n:\n\"\"\"Adds `a` and `b`.\nArgs:\na: First int\nb: Second int\n\"\"\"\nreturn\na\n+\nb\n@tool\ndef\ndivide\n(\na\n:\nint\n,\nb\n:\nint\n) ->\nfloat\n:\n\"\"\"Divide `a` and `b`.\nArgs:\na: First int\nb: Second int\n\"\"\"\nreturn\na\n/\nb\n# Augment the LLM with tools\ntools\n=\n[add, multiply, divide]\ntools_by_name\n=\n{tool.name: tool\nfor\ntool\nin\ntools}\nllm_with_tools\n=\nllm.bind_tools(tools)\nGraph API\nFunctional API\nCopy\nfrom\nlanggraph.graph\nimport\nMessagesState\nfrom\nlangchain.messages\nimport\nSystemMessage, HumanMessage, ToolMessage\n# Nodes\ndef\nllm_call\n(\nstate\n: MessagesState):\n\"\"\"LLM decides whether to call a tool or not\"\"\"\nreturn\n{\n\"messages\"\n: [\nllm_with_tools.invoke(\n[\nSystemMessage(\ncontent\n=\n\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n)\n]\n+\nstate[\n\"messages\"\n]\n)\n]\n}\ndef\ntool_node\n(\nstate\n:\ndict\n):\n\"\"\"Performs the tool call\"\"\"\nresult\n=\n[]\nfor\ntool_call\nin\nstate[\n\"messages\"\n][\n-\n1\n].tool_calls:\ntool\n=\ntools_by_name[tool_call[\n\"name\"\n]]\nobservation\n=\ntool.invoke(tool_call[\n\"args\"\n])\nresult.append(ToolMessage(\ncontent\n=\nobservation,\ntool_call_id\n=\ntool_call[\n\"id\"\n]))\nreturn\n{\n\"messages\"\n: result}\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\ndef\nshould_continue\n(\nstate\n: MessagesState) -> Literal[\n\"tool_node\"\n,\nEND\n]:\n\"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\nmessages\n=\nstate[\n\"messages\"\n]\nlast_message\n=\nmessages[\n-\n1\n]\n# If the LLM makes a tool call, then perform an action\nif\nlast_message.tool_calls:\nreturn\n\"tool_node\"\n# Otherwise, we stop (reply to the user)\nreturn\nEND\n# Build workflow\nagent_builder\n=\nStateGraph(MessagesState)\n# Add nodes\nagent_builder.add_node(\n\"llm_call\"\n, llm_call)\nagent_builder.add_node(\n\"tool_node\"\n, tool_node)\n# Add edges to connect nodes\nagent_builder.add_edge(\nSTART\n,\n\"llm_call\"\n)\nagent_builder.add_conditional_edges(\n\"llm_call\"\n,\nshould_continue,\n[\n\"tool_node\"\n,\nEND\n]\n)\nagent_builder.add_edge(\n\"tool_node\"\n,\n\"llm_call\"\n)\n# Compile the agent\nagent\n=\nagent_builder.compile()\n# Show the agent\ndisplay(Image(agent.get_graph(\nxray\n=\nTrue\n).draw_mermaid_png()))\n# Invoke\nmessages\n=\n[HumanMessage(\ncontent\n=\n\"Add 3 and 4.\"\n)]\nmessages\n=\nagent.invoke({\n\"messages\"\n: messages})\nfor\nm\nin\nmessages[\n\"messages\"\n]:\nm.pretty_print()\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nThinking in LangGraph\nPrevious\nPersistence\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
      "title": "Workflows and agents - Docs by LangChain",
      "heading": "Workflows and agents"
    }
  },
  {
    "page_content": "Memory overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nConceptual overviews\nMemory overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nShort-term memory\nManage short-term memory\nLong-term memory\nSemantic memory\nProfile\nCollection\nEpisodic memory\nProcedural memory\nWriting memories\nIn the hot path\nIn the background\nMemory storage\nConceptual overviews\nMemory overview\nCopy page\nCopy page\nMemory\nis a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\nThis conceptual guide covers two types of memory, based on their recall scope:\nShort-term memory\n, or\nthread\n-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent\u2019s\nstate\n. State is persisted to a database using a\ncheckpointer\nso the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.\nLong-term memory\nstores user-specific or application-level data across sessions and is shared\nacross\nconversational threads. It can be recalled\nat any time\nand\nin any thread\n. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides\nstores\n(\nreference doc\n) to let you save and recall long-term memories.\n\u200b\nShort-term memory\nShort-term memory\nlets your application remember previous interactions within a single\nthread\nor conversation. A\nthread\norganizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\nLangGraph manages short-term memory as part of the agent\u2019s state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph\u2019s state, the bot can access the full context for a given conversation while maintaining separation between different threads.\n\u200b\nManage short-term memory\nConversation history is the most common form of short-term memory, and long conversations pose a challenge to today\u2019s LLMs. A full history may not fit inside an LLM\u2019s context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get \u201cdistracted\u201d by stale or off-topic content, all while suffering from slower response times and higher costs.\nChat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.\nFor more information on common techniques for managing messages, see the\nAdd and manage memory\nguide.\n\u200b\nLong-term memory\nLong-term memory\nin LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is\nthread-scoped\n, long-term memory is saved within custom \u201cnamespaces.\u201d\nLong-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:\nWhat is the type of memory? Humans use memories to remember facts (\nsemantic memory\n), experiences (\nepisodic memory\n), and rules (\nprocedural memory\n). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task.\nWhen do you want to update memories?\nMemory can be updated as part of an agent\u2019s application logic (e.g., \u201con the hot path\u201d). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the\nsection below\n.\nDifferent applications require various types of memory. Although the analogy isn\u2019t perfect, examining\nhuman memory types\ncan be insightful. Some research (e.g., the\nCoALA paper\n) have even mapped these human memory types to those used in AI agents.\nMemory Type\nWhat is Stored\nHuman Example\nAgent Example\nSemantic\nFacts\nThings I learned in school\nFacts about a user\nEpisodic\nExperiences\nThings I did\nPast agent actions\nProcedural\nInstructions\nInstincts or motor skills\nAgent system prompt\n\u200b\nSemantic memory\nSemantic memory\n, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.\nSemantic memory is different from \u201csemantic search,\u201d which is a technique for finding similar content using \u201cmeaning\u201d (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.\nSemantic memories can be managed in different ways:\n\u200b\nProfile\nMemories can be a single, continuously updated \u201cprofile\u201d of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you\u2019ve selected to represent your domain.\nWhen remembering a profile, you will want to make sure that you are\nupdating\nthe profile each time. As a result, you will want to pass in the previous profile and\nask the model to generate a new profile\n(or some\nJSON patch\nto apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or\nstrict\ndecoding when generating documents to ensure the memory schemas remains valid.\n\u200b\nCollection\nAlternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you\u2019re less likely to\nlose\ninformation over time. It\u2019s easier for an LLM to generate\nnew\nobjects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to\nhigher recall downstream\n.\nHowever, this shifts some complexity memory updating. The model must now\ndelete\nor\nupdate\nexisting items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the\nTrustcall\npackage for one way to manage this and consider evaluation (e.g., with a tool like\nLangSmith\n) to help you tune the behavior.\nWorking with document collections also shifts complexity to memory\nsearch\nover the list. The\nStore\ncurrently supports both\nsemantic search\nand\nfiltering by content\n.\nFinally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.\nRegardless of memory management approach, the central point is that the agent will use the semantic memories to\nground its responses\n, which often leads to more personalized and relevant interactions.\n\u200b\nEpisodic memory\nEpisodic memory\n, in both humans and AI agents, involves recalling past events or actions. The\nCoALA paper\nframes this well: facts can be written to semantic memory, whereas\nexperiences\ncan be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task.\nIn practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it\u2019s easier to \u201cshow\u201d than \u201ctell\u201d and LLMs learn well from examples. Few-shot learning lets you\n\u201cprogram\u201d\nyour LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input.\nNote that the memory\nstore\nis just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a\nLangSmith Dataset\nto store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity (\nusing a BM25-like algorithm\nfor keyword based similarity).\nSee this how-to\nvideo\nfor example usage of dynamic few-shot example selection in LangSmith. Also, see this\nblog post\nshowcasing few-shot prompting to improve tool calling performance and this\nblog post\nusing few-shot example to align an LLMs to human preferences.\n\u200b\nProcedural memory\nProcedural memory\n, in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent\u2019s prompt that collectively determine the agent\u2019s functionality.\nIn practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts.\nOne effective approach to refining an agent\u2019s instructions is through\n\u201cReflection\u201d\nor meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions.\nFor example, we built a\nTweet generator\nusing external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify\na priori\n, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process.\nThe below pseudo-code shows how you might implement this with the LangGraph memory\nstore\n, using the store to save a prompt, the\nupdate_instructions\nnode to get the current prompt (as well as feedback from the conversation with the user captured in\nstate[\"messages\"]\n), update the prompt, and save the new prompt back to the store. Then, the\ncall_model\nget the updated prompt from the store and uses it to generate a response.\nCopy\n# Node that *uses* the instructions\ndef\ncall_model\n(\nstate\n: State,\nstore\n: BaseStore):\nnamespace\n=\n(\n\"agent_instructions\"\n, )\ninstructions\n=\nstore.get(namespace,\nkey\n=\n\"agent_a\"\n)[\n0\n]\n# Application logic\nprompt\n=\nprompt_template.format(\ninstructions\n=\ninstructions.value[\n\"instructions\"\n])\n...\n# Node that updates instructions\ndef\nupdate_instructions\n(\nstate\n: State,\nstore\n: BaseStore):\nnamespace\n=\n(\n\"instructions\"\n,)\ninstructions\n=\nstore.search(namespace)[\n0\n]\n# Memory logic\nprompt\n=\nprompt_template.format(\ninstructions\n=\ninstructions.value[\n\"instructions\"\n],\nconversation\n=\nstate[\n\"messages\"\n])\noutput\n=\nllm.invoke(prompt)\nnew_instructions\n=\noutput[\n'new_instructions'\n]\nstore.put((\n\"agent_instructions\"\n,),\n\"agent_a\"\n, {\n\"instructions\"\n: new_instructions})\n...\n\u200b\nWriting memories\nThere are two primary methods for agents to write memories:\n\u201cin the hot path\u201d\nand\n\u201cin the background\u201d\n.\n\u200b\nIn the hot path\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a\nsave_memories\ntool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our\nmemory-agent\ntemplate as an reference implementation.\n\u200b\nIn the background\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our\nmemory-service\ntemplate as an reference implementation.\n\u200b\nMemory storage\nLangGraph stores long-term memories as JSON documents in a\nstore\n. Each memory is organized under a custom\nnamespace\n(similar to a folder) and a distinct\nkey\n(like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\nCopy\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\ndef\nembed\n(\ntexts\n: list[\nstr\n]) -> list[list[\nfloat\n]]:\n# Replace with an actual embedding function or LangChain embeddings object\nreturn\n[[\n1.0\n,\n2.0\n]\n*\nlen\n(texts)]\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore\n=\nInMemoryStore(\nindex\n=\n{\n\"embed\"\n: embed,\n\"dims\"\n:\n2\n})\nuser_id\n=\n\"my-user\"\napplication_context\n=\n\"chitchat\"\nnamespace\n=\n(user_id, application_context)\nstore.put(\nnamespace,\n\"a-memory\"\n,\n{\n\"rules\"\n: [\n\"User likes short, direct language\"\n,\n\"User only speaks English & python\"\n,\n],\n\"my-key\"\n:\n\"my-value\"\n,\n},\n)\n# get the \"memory\" by ID\nitem\n=\nstore.get(namespace,\n\"a-memory\"\n)\n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems\n=\nstore.search(\nnamespace,\nfilter\n=\n{\n\"my-key\"\n:\n\"my-value\"\n},\nquery\n=\n\"language preferences\"\n)\nFor more information about the memory store, see the\nPersistence\nguide.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nComponent architecture\nPrevious\nContext overview\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/concepts/memory",
      "title": "Memory overview - Docs by LangChain",
      "heading": "Memory overview"
    }
  },
  {
    "page_content": "Context overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nConceptual overviews\nContext overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nStatic runtime context\nDynamic runtime context\nDynamic cross-conversation context\nSee also\nConceptual overviews\nContext overview\nCopy page\nCopy page\nContext engineering\nis the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:\nBy\nmutability\n:\nStatic context\n: Immutable data that doesn\u2019t change during execution (e.g., user metadata, database connections, tools)\nDynamic context\n: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\nBy\nlifetime\n:\nRuntime context\n: Data scoped to a single run or invocation\nCross-conversation context\n: Data that persists across multiple conversations or sessions\nRuntime context refers to local context: data and dependencies your code needs to run. It does\nnot\nrefer to:\nThe LLM context, which is the data passed into the LLM\u2019s prompt.\nThe \u201ccontext window\u201d, which is the maximum number of tokens that can be passed to the LLM.\nRuntime context is a form of dependency injection and can be used to optimize the LLM context. It lets to provide dependencies (like database connections, user IDs, or API clients) to your tools and nodes at runtime rather than hardcoding them. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.\nLangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:\nContext type\nDescription\nMutability\nLifetime\nAccess method\nStatic runtime context\nUser metadata, tools, db connections passed at startup\nStatic\nSingle run\ncontext\nargument to\ninvoke\n/\nstream\nDynamic runtime context (state)\nMutable data that evolves during a single run\nDynamic\nSingle run\nLangGraph state object\nDynamic cross-conversation context (store)\nPersistent data shared across conversations\nDynamic\nCross-conversation\nLangGraph store\n\u200b\nStatic runtime context\nStatic runtime context\nrepresents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the\ncontext\nargument to\ninvoke\n/\nstream\n. This data does not change during execution.\nCopy\n@dataclass\nclass\nContextSchema\n:\nuser_name:\nstr\ngraph.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi!\"\n}]},\ncontext\n=\n{\n\"user_name\"\n:\n\"John Smith\"\n}\n)\nAgent prompt\nWorkflow node\nIn a tool\nCopy\nfrom\ndataclasses\nimport\ndataclass\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dataclass\nclass\nContextSchema\n:\nuser_name:\nstr\n@dynamic_prompt\ndef\npersonalized_prompt\n(\nrequest\n: ModelRequest) ->\nstr\n:\nuser_name\n=\nrequest.runtime.context.user_name\nreturn\nf\n\"You are a helpful assistant. Address the user as\n{\nuser_name\n}\n.\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nmiddleware\n=\n[personalized_prompt],\ncontext_schema\n=\nContextSchema\n)\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]},\ncontext\n=\nContextSchema(\nuser_name\n=\n\"John Smith\"\n)\n)\nSee\nAgents\nfor details.\nCopy\nfrom\nlanggraph.runtime\nimport\nRuntime\ndef\nnode\n(\nstate\n: State,\nruntime\n: Runtime[ContextSchema]):\nuser_name\n=\nruntime.context.user_name\n...\nSee\nthe Graph API\nfor details.\nCopy\nfrom\nlangchain.tools\nimport\ntool, ToolRuntime\n@tool\ndef\nget_user_email\n(\nruntime\n: ToolRuntime[ContextSchema]) ->\nstr\n:\n\"\"\"Retrieve user information based on user ID.\"\"\"\n# simulate fetching user info from a database\nemail\n=\nget_user_email_from_db(runtime.context.user_name)\nreturn\nemail\nSee the\ntool calling guide\nfor details.\nThe\nRuntime\nobject can be used to access static context and other utilities like the active store and stream writer.\nSee the\nRuntime\ndocumentation for details.\n\u200b\nDynamic runtime context\nDynamic runtime context\nrepresents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as\nshort-term memory\nduring a run.\nIn an agent\nIn a workflow\nExample shows how to incorporate state into an agent\nprompt\n.\nState can also be accessed by the agent\u2019s\ntools\n, which can read or update the state as needed. See\ntool calling guide\nfor details.\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\nfrom\nlangchain.agents\nimport\nAgentState\nclass\nCustomState\n(\nAgentState\n):\nuser_name:\nstr\n@dynamic_prompt\ndef\npersonalized_prompt\n(\nrequest\n: ModelRequest) ->\nstr\n:\nuser_name\n=\nrequest.state.get(\n\"user_name\"\n,\n\"User\"\n)\nreturn\nf\n\"You are a helpful assistant. User's name is\n{\nuser_name\n}\n\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[\n...\n],\nstate_schema\n=\nCustomState,\nmiddleware\n=\n[personalized_prompt],\n)\nagent.invoke({\n\"messages\"\n:\n\"hi!\"\n,\n\"user_name\"\n:\n\"John Smith\"\n})\nCopy\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\nlangchain.messages\nimport\nAnyMessage\nfrom\nlanggraph.graph\nimport\nStateGraph\nclass\nCustomState\n(\nTypedDict\n):\nmessages: list[AnyMessage]\nextra_field:\nint\ndef\nnode\n(\nstate\n: CustomState):\nmessages\n=\nstate[\n\"messages\"\n]\n...\nreturn\n{\n\"extra_field\"\n: state[\n\"extra_field\"\n]\n+\n1\n}\nbuilder\n=\nStateGraph(State)\nbuilder.add_node(node)\nbuilder.set_entry_point(\n\"node\"\n)\ngraph\n=\nbuilder.compile()\nTurning on memory\nPlease see the\nmemory guide\nfor more details on how to enable memory. This is a powerful feature that allows you to persist the agent\u2019s state across multiple invocations. Otherwise, the state is scoped only to a single run.\n\u200b\nDynamic cross-conversation context\nDynamic cross-conversation context\nrepresents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as\nlong-term memory\nacross multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).\n\u200b\nSee also\nMemory conceptual overview\nShort-term memory in LangChain\nLong-term memory in LangChain\nMemory in LangGraph\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nMemory overview\nPrevious\nGraph API overview\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/concepts/context",
      "title": "Context overview - Docs by LangChain",
      "heading": "Context overview"
    }
  },
  {
    "page_content": "Long-term memory - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCore capabilities\nLong-term memory\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nQuickstart\nCustomization\nCore capabilities\nAgent harness\nBackends\nSubagents\nHuman-in-the-loop\nLong-term memory\nMiddleware\nCommand line interface\nUse the CLI\nOn this page\nSetup\nHow it works\n1. Short-term (transient) filesystem\n2. Long-term (persistent) filesystem\nPath routing\nCross-thread persistence\nUse cases\nUser preferences\nSelf-improving instructions\nKnowledge base\nResearch projects\nStore implementations\nInMemoryStore (development)\nPostgresStore (production)\nBest practices\nUse descriptive paths\nDocument the memory structure\nPrune old data\nChoose the right storage\nCore capabilities\nLong-term memory\nCopy page\nLearn how to extend deep agents with persistent memory across threads\nCopy page\nDeep agents come with a local filesystem to offload memory. By default, this filesystem is stored in agent state and is\ntransient to a single thread\n\u2014files are lost when the conversation ends.\nYou can extend deep agents with\nlong-term memory\nby using a\nCompositeBackend\nthat routes specific paths to persistent storage. This enables hybrid storage where some files persist across threads while others remain ephemeral.\n\u200b\nSetup\nConfigure long-term memory by using a\nCompositeBackend\nthat routes the\n/memories/\npath to a\nStoreBackend\n:\nCopy\nfrom\ndeepagents\nimport\ncreate_deep_agent\nfrom\ndeepagents.backends\nimport\nCompositeBackend, StateBackend, StoreBackend\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nfrom\nlanggraph.checkpoint.memory\nimport\nMemorySaver\ncheckpointer\n=\nMemorySaver()\ndef\nmake_backend\n(\nruntime\n):\nreturn\nCompositeBackend(\ndefault\n=\nStateBackend(runtime),\n# Ephemeral storage\nroutes\n=\n{\n\"/memories/\"\n: StoreBackend(runtime)\n# Persistent storage\n}\n)\nagent\n=\ncreate_deep_agent(\nstore\n=\nInMemoryStore(),\n# Required for StoreBackend\nbackend\n=\nmake_backend,\ncheckpointer\n=\ncheckpointer\n)\n\u200b\nHow it works\nWhen using\nCompositeBackend\n, deep agents maintain\ntwo separate filesystems\n:\n\u200b\n1. Short-term (transient) filesystem\nStored in the agent\u2019s state (via\nStateBackend\n)\nPersists only within a single thread\nFiles are lost when the thread ends\nAccessed through standard paths:\n/notes.txt\n,\n/workspace/draft.md\n\u200b\n2. Long-term (persistent) filesystem\nStored in a LangGraph Store (via\nStoreBackend\n)\nPersists across all threads and conversations\nSurvives agent restarts\nAccessed through paths prefixed with\n/memories/\n:\n/memories/preferences.txt\n\u200b\nPath routing\nThe\nCompositeBackend\nroutes file operations based on path prefixes:\nFiles with paths starting with\n/memories/\nare stored in the Store (persistent)\nFiles without this prefix remain in transient state\nAll filesystem tools (\nls\n,\nread_file\n,\nwrite_file\n,\nedit_file\n) work with both\nCopy\n# Transient file (lost after thread ends)\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Write draft to /draft.txt\"\n}]\n})\n# Persistent file (survives across threads)\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Save final report to /memories/report.txt\"\n}]\n})\n\u200b\nCross-thread persistence\nFiles in\n/memories/\ncan be accessed from any thread:\nCopy\nimport\nuuid\n# Thread 1: Write to long-term memory\nconfig1\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\nstr\n(uuid.uuid4())}}\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Save my preferences to /memories/preferences.txt\"\n}]\n},\nconfig\n=\nconfig1)\n# Thread 2: Read from long-term memory (different conversation!)\nconfig2\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\nstr\n(uuid.uuid4())}}\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What are my preferences?\"\n}]\n},\nconfig\n=\nconfig2)\n# Agent can read /memories/preferences.txt from the first thread\n\u200b\nUse cases\n\u200b\nUser preferences\nStore user preferences that persist across sessions:\nCopy\nagent\n=\ncreate_deep_agent(\nstore\n=\nInMemoryStore(),\nbackend\n=\nlambda\nrt\n: CompositeBackend(\ndefault\n=\nStateBackend(rt),\nroutes\n=\n{\n\"/memories/\"\n: StoreBackend(rt)}\n),\nsystem_prompt\n=\n\"\"\"When users tell you their preferences, save them to\n/memories/user_preferences.txt so you remember them in future conversations.\"\"\"\n)\n\u200b\nSelf-improving instructions\nAn agent can update its own instructions based on feedback:\nCopy\nagent\n=\ncreate_deep_agent(\nstore\n=\nInMemoryStore(),\nbackend\n=\nlambda\nrt\n: CompositeBackend(\ndefault\n=\nStateBackend(rt),\nroutes\n=\n{\n\"/memories/\"\n: StoreBackend(rt)}\n),\nsystem_prompt\n=\n\"\"\"You have a file at /memories/instructions.txt with additional\ninstructions and preferences.\nRead this file at the start of conversations to understand user preferences.\nWhen users provide feedback like \"please always do X\" or \"I prefer Y\",\nupdate /memories/instructions.txt using the edit_file tool.\"\"\"\n)\nOver time, the instructions file accumulates user preferences, helping the agent improve.\n\u200b\nKnowledge base\nBuild up knowledge over multiple conversations:\nCopy\n# Conversation 1: Learn about a project\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"We're building a web app with React. Save project notes.\"\n}]\n})\n# Conversation 2: Use that knowledge\nagent.invoke({\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What framework are we using?\"\n}]\n})\n# Agent reads /memories/project_notes.txt from previous conversation\n\u200b\nResearch projects\nMaintain research state across sessions:\nCopy\nresearch_agent\n=\ncreate_deep_agent(\nstore\n=\nInMemoryStore(),\nbackend\n=\nlambda\nrt\n: CompositeBackend(\ndefault\n=\nStateBackend(rt),\nroutes\n=\n{\n\"/memories/\"\n: StoreBackend(rt)}\n),\nsystem_prompt\n=\n\"\"\"You are a research assistant.\nSave your research progress to /memories/research/:\n- /memories/research/sources.txt - List of sources found\n- /memories/research/notes.txt - Key findings and notes\n- /memories/research/report.md - Final report draft\nThis allows research to continue across multiple sessions.\"\"\"\n)\n\u200b\nStore implementations\nAny LangGraph\nBaseStore\nimplementation works:\n\u200b\nInMemoryStore (development)\nGood for testing and development, but data is lost on restart:\nCopy\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nstore\n=\nInMemoryStore()\nagent\n=\ncreate_deep_agent(\nstore\n=\nstore,\nbackend\n=\nlambda\nrt\n: CompositeBackend(\ndefault\n=\nStateBackend(rt),\nroutes\n=\n{\n\"/memories/\"\n: StoreBackend(rt)}\n)\n)\n\u200b\nPostgresStore (production)\nFor production, use a persistent store:\nCopy\nfrom\nlanggraph.store.postgres\nimport\nPostgresStore\nimport\nos\nstore\n=\nPostgresStore(\nconnection_string\n=\nos.environ[\n\"DATABASE_URL\"\n])\nagent\n=\ncreate_deep_agent(\nstore\n=\nstore,\nbackend\n=\nlambda\nrt\n: CompositeBackend(\ndefault\n=\nStateBackend(rt),\nroutes\n=\n{\n\"/memories/\"\n: StoreBackend(rt)}\n)\n)\n\u200b\nBest practices\n\u200b\nUse descriptive paths\nOrganize persistent files with clear paths:\nCopy\n/memories/user_preferences.txt\n/memories/research/topic_a/sources.txt\n/memories/research/topic_a/notes.txt\n/memories/project/requirements.md\n\u200b\nDocument the memory structure\nTell the agent what\u2019s stored where in your system prompt:\nCopy\nYour persistent memory structure:\n- /memories/preferences.txt: User preferences and settings\n- /memories/context/: Long-term context about the user\n- /memories/knowledge/: Facts and information learned over time\n\u200b\nPrune old data\nImplement periodic cleanup of outdated persistent files to keep storage manageable.\n\u200b\nChoose the right storage\nDevelopment\n: Use\nInMemoryStore\nfor quick iteration\nProduction\n: Use\nPostgresStore\nor other persistent stores\nMulti-tenant\n: Consider using assistant_id-based namespacing in your store\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nHuman-in-the-loop\nPrevious\nDeep Agents Middleware\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/deepagents/long-term-memory",
      "title": "Long-term memory - Docs by LangChain",
      "heading": "Long-term memory"
    }
  },
  {
    "page_content": "Persistence - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCapabilities\nPersistence\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nThreads\nCheckpoints\nGet state\nGet state history\nReplay\nUpdate state\nconfig\nvalues\nas_node\nMemory Store\nBasic Usage\nSemantic Search\nUsing in LangGraph\nCheckpointer libraries\nCheckpointer interface\nSerializer\nSerialization with pickle\nEncryption\nCapabilities\nHuman-in-the-loop\nMemory\nTime Travel\nFault-tolerance\nPending writes\nCapabilities\nPersistence\nCopy page\nCopy page\nLangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a\ncheckpoint\nof the graph state at every super-step. Those checkpoints are saved to a\nthread\n, which can be accessed after graph execution. Because\nthreads\nallow access to graph\u2019s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we\u2019ll discuss each of these concepts in more detail.\nAgent Server handles checkpointing automatically\nWhen using the\nAgent Server\n, you don\u2019t need to implement or configure checkpointers manually. The server handles all persistence infrastructure for you behind the scenes.\n\u200b\nThreads\nA thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of\nruns\n. When a run is executed, the\nstate\nof the underlying graph of the assistant will be persisted to the thread.\nWhen invoking a graph with a checkpointer, you\nmust\nspecify a\nthread_id\nas part of the\nconfigurable\nportion of the config:\nCopy\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\nA thread\u2019s current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the\nAPI reference\nfor more details.\nThe checkpointer uses\nthread_id\nas the primary key for storing and retrieving checkpoints. Without it, the checkpointer cannot save state or resume execution after an\ninterrupt\n, since the checkpointer uses\nthread_id\nto load the saved state.\n\u200b\nCheckpoints\nThe state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by\nStateSnapshot\nobject with the following key properties:\nconfig\n: Config associated with this checkpoint.\nmetadata\n: Metadata associated with this checkpoint.\nvalues\n: Values of the state channels at this point in time.\nnext\nA tuple of the node names to execute next in the graph.\ntasks\n: A tuple of\nPregelTask\nobjects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted\ndynamically\nfrom within a node, tasks will contain additional data associated with interrupts.\nCheckpoints are persisted and can be used to restore the state of a thread at a later time.\nLet\u2019s see what checkpoints are saved when a simple graph is invoked as follows:\nCopy\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\ntyping\nimport\nAnnotated\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\noperator\nimport\nadd\nclass\nState\n(\nTypedDict\n):\nfoo:\nstr\nbar: Annotated[list[\nstr\n], add]\ndef\nnode_a\n(\nstate\n: State):\nreturn\n{\n\"foo\"\n:\n\"a\"\n,\n\"bar\"\n: [\n\"a\"\n]}\ndef\nnode_b\n(\nstate\n: State):\nreturn\n{\n\"foo\"\n:\n\"b\"\n,\n\"bar\"\n: [\n\"b\"\n]}\nworkflow\n=\nStateGraph(State)\nworkflow.add_node(node_a)\nworkflow.add_node(node_b)\nworkflow.add_edge(\nSTART\n,\n\"node_a\"\n)\nworkflow.add_edge(\n\"node_a\"\n,\n\"node_b\"\n)\nworkflow.add_edge(\n\"node_b\"\n,\nEND\n)\ncheckpointer\n=\nInMemorySaver()\ngraph\n=\nworkflow.compile(\ncheckpointer\n=\ncheckpointer)\nconfig: RunnableConfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\ngraph.invoke({\n\"foo\"\n:\n\"\"\n,\n\"bar\"\n:[]}, config)\nAfter we run the graph, we expect to see exactly 4 checkpoints:\nEmpty checkpoint with\nSTART\nas the next node to be executed\nCheckpoint with the user input\n{'foo': '', 'bar': []}\nand\nnode_a\nas the next node to be executed\nCheckpoint with the outputs of\nnode_a\n{'foo': 'a', 'bar': ['a']}\nand\nnode_b\nas the next node to be executed\nCheckpoint with the outputs of\nnode_b\n{'foo': 'b', 'bar': ['a', 'b']}\nand no next nodes to be executed\nNote that we\nbar\nchannel values contain outputs from both nodes as we have a reducer for\nbar\nchannel.\n\u200b\nGet state\nWhen interacting with the saved graph state, you\nmust\nspecify a\nthread identifier\n. You can view the\nlatest\nstate of the graph by calling\ngraph.get_state(config)\n. This will return a\nStateSnapshot\nobject that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.\nCopy\n# get the latest state snapshot\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\ngraph.get_state(config)\n# get a state snapshot for a specific checkpoint_id\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n,\n\"checkpoint_id\"\n:\n\"1ef663ba-28fe-6528-8002-5a559208592c\"\n}}\ngraph.get_state(config)\nIn our example, the output of\nget_state\nwill look like this:\nCopy\nStateSnapshot(\nvalues={'foo': 'b', 'bar': ['a', 'b']},\nnext=(),\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\nmetadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\ncreated_at='2024-08-29T19:19:38.821749+00:00',\nparent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()\n)\n\u200b\nGet state history\nYou can get the full history of the graph execution for a given thread by calling\ngraph.get_state_history(config)\n. This will return a list of\nStateSnapshot\nobjects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint /\nStateSnapshot\nbeing the first in the list.\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\nlist\n(graph.get_state_history(config))\nIn our example, the output of\nget_state_history\nwill look like this:\nCopy\n[\nStateSnapshot(\nvalues={'foo': 'b', 'bar': ['a', 'b']},\nnext=(),\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\nmetadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\ncreated_at='2024-08-29T19:19:38.821749+00:00',\nparent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\ntasks=(),\n),\nStateSnapshot(\nvalues={'foo': 'a', 'bar': ['a']},\nnext=('node_b',),\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\nmetadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},\ncreated_at='2024-08-29T19:19:38.819946+00:00',\nparent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\ntasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),\n),\nStateSnapshot(\nvalues={'foo': '', 'bar': []},\nnext=('node_a',),\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\nmetadata={'source': 'loop', 'writes': None, 'step': 0},\ncreated_at='2024-08-29T19:19:38.817813+00:00',\nparent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\ntasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),\n),\nStateSnapshot(\nvalues={'bar': []},\nnext=('__start__',),\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\nmetadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},\ncreated_at='2024-08-29T19:19:38.816205+00:00',\nparent_config=None,\ntasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),\n)\n]\n\u200b\nReplay\nIt\u2019s also possible to play-back a prior graph execution. If we\ninvoke\na graph with a\nthread_id\nand a\ncheckpoint_id\n, then we will\nre-play\nthe previously executed steps\nbefore\na checkpoint that corresponds to the\ncheckpoint_id\n, and only execute the steps\nafter\nthe checkpoint.\nthread_id\nis the ID of a thread.\ncheckpoint_id\nis an identifier that refers to a specific checkpoint within a thread.\nYou must pass these when invoking the graph as part of the\nconfigurable\nportion of the config:\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n,\n\"checkpoint_id\"\n:\n\"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"\n}}\ngraph.invoke(\nNone\n,\nconfig\n=\nconfig)\nImportantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply\nre-plays\nthat particular step in the graph and does not re-execute the step, but only for the steps\nbefore\nthe provided\ncheckpoint_id\n. All of the steps\nafter\ncheckpoint_id\nwill be executed (i.e., a new fork), even if they have been executed previously. See this\nhow to guide on time-travel to learn more about replaying\n.\n\u200b\nUpdate state\nIn addition to re-playing the graph from specific\ncheckpoints\n, we can also\nedit\nthe graph state. We do this using\nupdate_state\n. This method accepts three different arguments:\n\u200b\nconfig\nThe config should contain\nthread_id\nspecifying which thread to update. When only the\nthread_id\nis passed, we update (or fork) the current state. Optionally, if we include\ncheckpoint_id\nfield, then we fork that selected checkpoint.\n\u200b\nvalues\nThese are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the\nreducer\nfunctions, if they are defined for some of the channels in the graph state. This means that\nupdate_state\ndoes NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let\u2019s walk through an example.\nLet\u2019s assume you have defined the state of your graph with the following schema (see full example above):\nCopy\nfrom\ntyping\nimport\nAnnotated\nfrom\ntyping_extensions\nimport\nTypedDict\nfrom\noperator\nimport\nadd\nclass\nState\n(\nTypedDict\n):\nfoo:\nint\nbar: Annotated[list[\nstr\n], add]\nLet\u2019s now assume the current state of the graph is\nCopy\n{\"foo\": 1, \"bar\": [\"a\"]}\nIf you update the state as below:\nCopy\ngraph.update_state(config, {\n\"foo\"\n:\n2\n,\n\"bar\"\n: [\n\"b\"\n]})\nThen the new state of the graph will be:\nCopy\n{\"foo\": 2, \"bar\": [\"a\", \"b\"]}\nThe\nfoo\nkey (channel) is completely changed (because there is no reducer specified for that channel, so\nupdate_state\noverwrites it). However, there is a reducer specified for the\nbar\nkey, and so it appends\n\"b\"\nto the state of\nbar\n.\n\u200b\nas_node\nThe final thing you can optionally specify when calling\nupdate_state\nis\nas_node\n. If you provided it, the update will be applied as if it came from node\nas_node\n. If\nas_node\nis not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this\nhow to guide on time-travel to learn more about forking state\n.\n\u200b\nMemory Store\nA\nstate schema\nspecifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.\nBut, what if we want to retain some information\nacross threads\n? Consider the case of a chatbot where we want to retain specific information about the user across\nall\nchat conversations (e.g., threads) with that user!\nWith checkpointers alone, we cannot share information across threads. This motivates the need for the\nStore\ninterface. As an illustration, we can define an\nInMemoryStore\nto store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new\nin_memory_store\nvariable.\nLangGraph API handles stores automatically\nWhen using the LangGraph API, you don\u2019t need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.\n\u200b\nBasic Usage\nFirst, let\u2019s showcase this in isolation without using LangGraph.\nCopy\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nin_memory_store\n=\nInMemoryStore()\nMemories are namespaced by a\ntuple\n, which in this specific example will be\n(<user_id>, \"memories\")\n. The namespace can be any length and represent anything, does not have to be user specific.\nCopy\nuser_id\n=\n\"1\"\nnamespace_for_memory\n=\n(user_id,\n\"memories\"\n)\nWe use the\nstore.put\nmethod to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (\nmemory_id\n) and the value (a dictionary) is the memory itself.\nCopy\nmemory_id\n=\nstr\n(uuid.uuid4())\nmemory\n=\n{\n\"food_preference\"\n:\n\"I like pizza\"\n}\nin_memory_store.put(namespace_for_memory, memory_id, memory)\nWe can read out memories in our namespace using the\nstore.search\nmethod, which will return all memories for a given user as a list. The most recent memory is the last in the list.\nCopy\nmemories\n=\nin_memory_store.search(namespace_for_memory)\nmemories[\n-\n1\n].dict()\n{\n'value'\n: {\n'food_preference'\n:\n'I like pizza'\n},\n'key'\n:\n'07e0caf4-1631-47b7-b15f-65515d4c1843'\n,\n'namespace'\n: [\n'1'\n,\n'memories'\n],\n'created_at'\n:\n'2024-10-02T17:22:31.590602+00:00'\n,\n'updated_at'\n:\n'2024-10-02T17:22:31.590605+00:00'\n}\nEach memory type is a Python class (\nItem\n) with certain attributes. We can access it as a dictionary by converting via\n.dict\nas above.\nThe attributes it has are:\nvalue\n: The value (itself a dictionary) of this memory\nkey\n: A unique key for this memory in this namespace\nnamespace\n: A list of strings, the namespace of this memory type\ncreated_at\n: Timestamp for when this memory was created\nupdated_at\n: Timestamp for when this memory was updated\n\u200b\nSemantic Search\nBeyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:\nCopy\nfrom\nlangchain.embeddings\nimport\ninit_embeddings\nstore\n=\nInMemoryStore(\nindex\n=\n{\n\"embed\"\n: init_embeddings(\n\"openai:text-embedding-3-small\"\n),\n# Embedding provider\n\"dims\"\n:\n1536\n,\n# Embedding dimensions\n\"fields\"\n: [\n\"food_preference\"\n,\n\"$\"\n]\n# Fields to embed\n}\n)\nNow when searching, you can use natural language queries to find relevant memories:\nCopy\n# Find memories about food preferences\n# (This can be done after putting memories into the store)\nmemories\n=\nstore.search(\nnamespace_for_memory,\nquery\n=\n\"What does the user like to eat?\"\n,\nlimit\n=\n3\n# Return top 3 matches\n)\nYou can control which parts of your memories get embedded by configuring the\nfields\nparameter or by specifying the\nindex\nparameter when storing memories:\nCopy\n# Store with specific fields to embed\nstore.put(\nnamespace_for_memory,\nstr\n(uuid.uuid4()),\n{\n\"food_preference\"\n:\n\"I love Italian cuisine\"\n,\n\"context\"\n:\n\"Discussing dinner plans\"\n},\nindex\n=\n[\n\"food_preference\"\n]\n# Only embed \"food_preferences\" field\n)\n# Store without embedding (still retrievable, but not searchable)\nstore.put(\nnamespace_for_memory,\nstr\n(uuid.uuid4()),\n{\n\"system_info\"\n:\n\"Last updated: 2024-01-01\"\n},\nindex\n=\nFalse\n)\n\u200b\nUsing in LangGraph\nWith this all in place, we use the\nin_memory_store\nin LangGraph. The\nin_memory_store\nworks hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the\nin_memory_store\nallows us to store arbitrary information for access\nacross\nthreads. We compile the graph with both the checkpointer and the\nin_memory_store\nas follows.\nCopy\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\n# We need this because we want to enable threads (conversations)\ncheckpointer\n=\nInMemorySaver()\n# ... Define the graph ...\n# Compile the graph with the checkpointer and store\ngraph\n=\ngraph.compile(\ncheckpointer\n=\ncheckpointer,\nstore\n=\nin_memory_store)\nWe invoke the graph with a\nthread_id\n, as before, and also with a\nuser_id\n, which we\u2019ll use to namespace our memories to this particular user as we showed above.\nCopy\n# Invoke the graph\nuser_id\n=\n\"1\"\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n,\n\"user_id\"\n: user_id}}\n# First let's just say hi to the AI\nfor\nupdate\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi\"\n}]}, config,\nstream_mode\n=\n\"updates\"\n):\nprint\n(update)\nWe can access the\nin_memory_store\nand the\nuser_id\nin\nany node\nby passing\nstore: BaseStore\nand\nconfig: RunnableConfig\nas node arguments. Here\u2019s how we might use semantic search in a node to find relevant memories:\nCopy\ndef\nupdate_memory\n(\nstate\n: MessagesState,\nconfig\n: RunnableConfig,\n*\n,\nstore\n: BaseStore):\n# Get the user id from the config\nuser_id\n=\nconfig[\n\"configurable\"\n][\n\"user_id\"\n]\n# Namespace the memory\nnamespace\n=\n(user_id,\n\"memories\"\n)\n# ... Analyze conversation and create a new memory\n# Create a new memory ID\nmemory_id\n=\nstr\n(uuid.uuid4())\n# We create a new memory\nstore.put(namespace, memory_id, {\n\"memory\"\n: memory})\nAs we showed above, we can also access the store in any node and use the\nstore.search\nmethod to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.\nCopy\nmemories[\n-\n1\n].dict()\n{\n'value'\n: {\n'food_preference'\n:\n'I like pizza'\n},\n'key'\n:\n'07e0caf4-1631-47b7-b15f-65515d4c1843'\n,\n'namespace'\n: [\n'1'\n,\n'memories'\n],\n'created_at'\n:\n'2024-10-02T17:22:31.590602+00:00'\n,\n'updated_at'\n:\n'2024-10-02T17:22:31.590605+00:00'\n}\nWe can access the memories and use them in our model call.\nCopy\ndef\ncall_model\n(\nstate\n: MessagesState,\nconfig\n: RunnableConfig,\n*\n,\nstore\n: BaseStore):\n# Get the user id from the config\nuser_id\n=\nconfig[\n\"configurable\"\n][\n\"user_id\"\n]\n# Namespace the memory\nnamespace\n=\n(user_id,\n\"memories\"\n)\n# Search based on the most recent message\nmemories\n=\nstore.search(\nnamespace,\nquery\n=\nstate[\n\"messages\"\n][\n-\n1\n].content,\nlimit\n=\n3\n)\ninfo\n=\n\"\n\\n\n\"\n.join([d.value[\n\"memory\"\n]\nfor\nd\nin\nmemories])\n# ... Use memories in the model call\nIf we create a new thread, we can still access the same memories so long as the\nuser_id\nis the same.\nCopy\n# Invoke the graph\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"2\"\n,\n\"user_id\"\n:\n\"1\"\n}}\n# Let's say hi again\nfor\nupdate\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi, tell me about my memories\"\n}]}, config,\nstream_mode\n=\n\"updates\"\n):\nprint\n(update)\nWhen we use the LangSmith, either locally (e.g., in\nStudio\n) or\nhosted with LangSmith\n, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you\ndo\nneed to configure the indexing settings in your\nlanggraph.json\nfile. For example:\nCopy\n{\n...\n\"store\"\n: {\n\"index\"\n: {\n\"embed\"\n:\n\"openai:text-embeddings-3-small\"\n,\n\"dims\"\n:\n1536\n,\n\"fields\"\n: [\n\"$\"\n]\n}\n}\n}\nSee the\ndeployment guide\nfor more details and configuration options.\n\u200b\nCheckpointer libraries\nUnder the hood, checkpointing is powered by checkpointer objects that conform to\nBaseCheckpointSaver\ninterface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:\nlanggraph-checkpoint\n: The base interface for checkpointer savers (\nBaseCheckpointSaver\n) and serialization/deserialization interface (\nSerializerProtocol\n). Includes in-memory checkpointer implementation (\nInMemorySaver\n) for experimentation. LangGraph comes with\nlanggraph-checkpoint\nincluded.\nlanggraph-checkpoint-sqlite\n: An implementation of LangGraph checkpointer that uses SQLite database (\nSqliteSaver\n/\nAsyncSqliteSaver\n). Ideal for experimentation and local workflows. Needs to be installed separately.\nlanggraph-checkpoint-postgres\n: An advanced checkpointer that uses Postgres database (\nPostgresSaver\n/\nAsyncPostgresSaver\n), used in LangSmith. Ideal for using in production. Needs to be installed separately.\n\u200b\nCheckpointer interface\nEach checkpointer conforms to\nBaseCheckpointSaver\ninterface and implements the following methods:\n.put\n- Store a checkpoint with its configuration and metadata.\n.put_writes\n- Store intermediate writes linked to a checkpoint (i.e.\npending writes\n).\n.get_tuple\n- Fetch a checkpoint tuple using for a given configuration (\nthread_id\nand\ncheckpoint_id\n). This is used to populate\nStateSnapshot\nin\ngraph.get_state()\n.\n.list\n- List checkpoints that match a given configuration and filter criteria. This is used to populate state history in\ngraph.get_state_history()\nIf the checkpointer is used with asynchronous graph execution (i.e. executing the graph via\n.ainvoke\n,\n.astream\n,\n.abatch\n), asynchronous versions of the above methods will be used (\n.aput\n,\n.aput_writes\n,\n.aget_tuple\n,\n.alist\n).\nFor running your graph asynchronously, you can use\nInMemorySaver\n, or async versions of Sqlite/Postgres checkpointers \u2014\nAsyncSqliteSaver\n/\nAsyncPostgresSaver\ncheckpointers.\n\u200b\nSerializer\nWhen checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.\nlanggraph_checkpoint\ndefines\nprotocol\nfor implementing serializers provides a default implementation (\nJsonPlusSerializer\n) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.\n\u200b\nSerialization with\npickle\nThe default serializer,\nJsonPlusSerializer\n, uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.\nIf you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),\nyou can use the\npickle_fallback\nargument of the\nJsonPlusSerializer\n:\nCopy\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.checkpoint.serde.jsonplus\nimport\nJsonPlusSerializer\n# ... Define the graph ...\ngraph.compile(\ncheckpointer\n=\nInMemorySaver(\nserde\n=\nJsonPlusSerializer(\npickle_fallback\n=\nTrue\n))\n)\n\u200b\nEncryption\nCheckpointers can optionally encrypt all persisted state. To enable this, pass an instance of\nEncryptedSerializer\nto the\nserde\nargument of any\nBaseCheckpointSaver\nimplementation. The easiest way to create an encrypted serializer is via\nfrom_pycryptodome_aes\n, which reads the AES key from the\nLANGGRAPH_AES_KEY\nenvironment variable (or accepts a\nkey\nargument):\nCopy\nimport\nsqlite3\nfrom\nlanggraph.checkpoint.serde.encrypted\nimport\nEncryptedSerializer\nfrom\nlanggraph.checkpoint.sqlite\nimport\nSqliteSaver\nserde\n=\nEncryptedSerializer.from_pycryptodome_aes()\n# reads LANGGRAPH_AES_KEY\ncheckpointer\n=\nSqliteSaver(sqlite3.connect(\n\"checkpoint.db\"\n),\nserde\n=\nserde)\nCopy\nfrom\nlanggraph.checkpoint.serde.encrypted\nimport\nEncryptedSerializer\nfrom\nlanggraph.checkpoint.postgres\nimport\nPostgresSaver\nserde\n=\nEncryptedSerializer.from_pycryptodome_aes()\ncheckpointer\n=\nPostgresSaver.from_conn_string(\n\"postgresql://...\"\n,\nserde\n=\nserde)\ncheckpointer.setup()\nWhen running on LangSmith, encryption is automatically enabled whenever\nLANGGRAPH_AES_KEY\nis present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing\nCipherProtocol\nand supplying it to\nEncryptedSerializer\n.\n\u200b\nCapabilities\n\u200b\nHuman-in-the-loop\nFirst, checkpointers facilitate\nhuman-in-the-loop workflows\nby allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See\nthe how-to guides\nfor examples.\n\u200b\nMemory\nSecond, checkpointers allow for\n\u201cmemory\u201d\nbetween interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See\nAdd memory\nfor information on how to add and manage conversation memory using checkpointers.\n\u200b\nTime Travel\nThird, checkpointers allow for\n\u201ctime travel\u201d\n, allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.\n\u200b\nFault-tolerance\nLastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don\u2019t re-run the successful nodes.\n\u200b\nPending writes\nAdditionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don\u2019t re-run the successful nodes.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nWorkflows and agents\nPrevious\nDurable execution\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/persistence",
      "title": "Persistence - Docs by LangChain",
      "heading": "Persistence"
    }
  },
  {
    "page_content": "Use time-travel - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCapabilities\nUse time-travel\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nIn a workflow\nSetup\n1. Run the graph\n2. Identify a checkpoint\n3. Update the state\n4. Resume execution from the checkpoint\nCapabilities\nUse time-travel\nCopy page\nCopy page\nWhen working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:\nUnderstand reasoning\n: Analyze the steps that led to a successful result.\nDebug mistakes\n: Identify where and why errors occurred.\nExplore alternatives\n: Test different paths to uncover better solutions.\nLangGraph provides\ntime travel\nfunctionality to support these use cases. Specifically, you can resume execution from a prior checkpoint \u2014 either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.\nTo use\ntime-travel\nin LangGraph:\nRun the graph\nwith initial inputs using\ninvoke\nor\nstream\nmethods.\nIdentify a checkpoint in an existing thread\n: Use the\nget_state_history\nmethod to retrieve the execution history for a specific\nthread_id\nand locate the desired\ncheckpoint_id\n.\nAlternatively, set an\ninterrupt\nbefore the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that interrupt.\nUpdate the graph state (optional)\n: Use the\nupdate_state\nmethod to modify the graph\u2019s state at the checkpoint and resume execution from alternative state.\nResume execution from the checkpoint\n: Use the\ninvoke\nor\nstream\nmethods with an input of\nNone\nand a configuration containing the appropriate\nthread_id\nand\ncheckpoint_id\n.\nFor a conceptual overview of time-travel, see\nTime travel\n.\n\u200b\nIn a workflow\nThis example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.\n\u200b\nSetup\nFirst we need to install the packages required\nCopy\n%%\ncapture\n--\nno\n-\nstderr\npip install\n--\nquiet\n-\nU langgraph langchain_anthropic\nNext, we need to set API keys for Anthropic (the LLM we will use)\nCopy\nimport\ngetpass\nimport\nos\ndef\n_set_env\n(\nvar\n:\nstr\n):\nif\nnot\nos.environ.get(var):\nos.environ[var]\n=\ngetpass.getpass(\nf\n\"\n{\nvar\n}\n: \"\n)\n_set_env(\n\"ANTHROPIC_API_KEY\"\n)\nSign up for\nLangSmith\nto quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.\nCopy\nimport\nuuid\nfrom\ntyping_extensions\nimport\nTypedDict, NotRequired\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n,\nEND\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nclass\nState\n(\nTypedDict\n):\ntopic: NotRequired[\nstr\n]\njoke: NotRequired[\nstr\n]\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n,\ntemperature\n=\n0\n,\n)\ndef\ngenerate_topic\n(\nstate\n: State):\n\"\"\"LLM call to generate a topic for the joke\"\"\"\nmsg\n=\nmodel.invoke(\n\"Give me a funny topic for a joke\"\n)\nreturn\n{\n\"topic\"\n: msg.content}\ndef\nwrite_joke\n(\nstate\n: State):\n\"\"\"LLM call to write a joke based on the topic\"\"\"\nmsg\n=\nmodel.invoke(\nf\n\"Write a short joke about\n{\nstate[\n'topic'\n]\n}\n\"\n)\nreturn\n{\n\"joke\"\n: msg.content}\n# Build workflow\nworkflow\n=\nStateGraph(State)\n# Add nodes\nworkflow.add_node(\n\"generate_topic\"\n, generate_topic)\nworkflow.add_node(\n\"write_joke\"\n, write_joke)\n# Add edges to connect nodes\nworkflow.add_edge(\nSTART\n,\n\"generate_topic\"\n)\nworkflow.add_edge(\n\"generate_topic\"\n,\n\"write_joke\"\n)\nworkflow.add_edge(\n\"write_joke\"\n,\nEND\n)\n# Compile\ncheckpointer\n=\nInMemorySaver()\ngraph\n=\nworkflow.compile(\ncheckpointer\n=\ncheckpointer)\ngraph\n\u200b\n1. Run the graph\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n: uuid.uuid4(),\n}\n}\nstate\n=\ngraph.invoke({}, config)\nprint\n(state[\n\"topic\"\n])\nprint\n()\nprint\n(state[\n\"joke\"\n])\nOutput:\nCopy\nHow about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!\n# The Secret Life of Socks in the Dryer\nI finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all\u2014they've just eloped with someone else's socks from the laundromat to start new lives together.\nMy blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.\n\u200b\n2. Identify a checkpoint\nCopy\n# The states are returned in reverse chronological order.\nstates\n=\nlist\n(graph.get_state_history(config))\nfor\nstate\nin\nstates:\nprint\n(state.next)\nprint\n(state.config[\n\"configurable\"\n][\n\"checkpoint_id\"\n])\nprint\n()\nOutput:\nCopy\n()\n1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e\n('write_joke',)\n1f02ac4a-ce2a-6494-8001-cb2e2d651227\n('generate_topic',)\n1f02ac4a-a4e0-630d-8000-b73c254ba748\n('__start__',)\n1f02ac4a-a4dd-665e-bfff-e6c8c44315d9\nCopy\n# This is the state before last (states are listed in chronological order)\nselected_state\n=\nstates[\n1\n]\nprint\n(selected_state.next)\nprint\n(selected_state.values)\nOutput:\nCopy\n('write_joke',)\n{'topic': 'How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\\\'t know about? There\\\\'s a lot of comedic potential in the everyday mystery that unites us all!'}\n\u200b\n3. Update the state\nupdate_state\nwill create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.\nCopy\nnew_config\n=\ngraph.update_state(selected_state.config,\nvalues\n=\n{\n\"topic\"\n:\n\"chickens\"\n})\nprint\n(new_config)\nOutput:\nCopy\n{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}\n\u200b\n4. Resume execution from the checkpoint\nCopy\ngraph.invoke(\nNone\n, new_config)\nOutput:\nCopy\n{\n'topic'\n:\n'chickens'\n,\n'joke'\n:\n'Why did the chicken join a band?\n\\n\\n\nBecause it had excellent drumsticks!'\n}\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInterrupts\nPrevious\nMemory\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/use-time-travel",
      "title": "Use time-travel - Docs by LangChain",
      "heading": "Use time-travel"
    }
  },
  {
    "page_content": "Memory - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nCapabilities\nMemory\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nRuntime\nOn this page\nAdd short-term memory\nUse in production\nUse in subgraphs\nAdd long-term memory\nUse in production\nUse semantic search\nManage short-term memory\nTrim messages\nDelete messages\nSummarize messages\nManage checkpoints\nView thread state\nView the history of the thread\nDelete all checkpoints for a thread\nPrebuilt memory tools\nDatabase management\nCapabilities\nMemory\nCopy page\nCopy page\nAI applications need\nmemory\nto share context across multiple interactions. In LangGraph, you can add two types of memory:\nAdd short-term memory\nas a part of your agent\u2019s\nstate\nto enable multi-turn conversations.\nAdd long-term memory\nto store user-specific or application-level data across sessions.\n\u200b\nAdd short-term memory\nShort-term\nmemory (thread-level\npersistence\n) enables agents to track multi-turn conversations. To add short-term memory:\nCopy\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlanggraph.graph\nimport\nStateGraph\ncheckpointer\n=\nInMemorySaver()\nbuilder\n=\nStateGraph(\n...\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\ngraph.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi! i am Bob\"\n}]},\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}},\n)\n\u200b\nUse in production\nIn production, use a checkpointer backed by a database:\nCopy\nfrom\nlanggraph.checkpoint.postgres\nimport\nPostgresSaver\nDB_URI\n=\n\"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith\nPostgresSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer:\nbuilder\n=\nStateGraph(\n...\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nExample: using Postgres checkpointer\nCopy\npip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\nYou need to call\ncheckpointer.setup()\nthe first time you\u2019re using Postgres checkpointer\nSync\nAsync\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.postgres\nimport\nPostgresSaver\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith\nPostgresSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer:\n# checkpointer.setup()\ndef\ncall_model\n(\nstate\n: MessagesState):\nresponse\n=\nmodel.invoke(state[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}\n}\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi! I'm bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.postgres.aio\nimport\nAsyncPostgresSaver\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nasync\nwith\nAsyncPostgresSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer:\n# await checkpointer.setup()\nasync\ndef\ncall_model\n(\nstate\n: MessagesState):\nresponse\n=\nawait\nmodel.ainvoke(state[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}\n}\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi! I'm bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nExample: using MongoDB checkpointer\nCopy\npip install -U pymongo langgraph langgraph-checkpoint-mongodb\nSetup\nTo use the\nMongoDB checkpointer\n, you will need a MongoDB cluster. Follow\nthis guide\nto create a cluster if you don\u2019t already have one.\nSync\nAsync\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.mongodb\nimport\nMongoDBSaver\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"localhost:27017\"\nwith\nMongoDBSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer:\ndef\ncall_model\n(\nstate\n: MessagesState):\nresponse\n=\nmodel.invoke(state[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}\n}\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi! I'm bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.mongodb.aio\nimport\nAsyncMongoDBSaver\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"localhost:27017\"\nasync\nwith\nAsyncMongoDBSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer:\nasync\ndef\ncall_model\n(\nstate\n: MessagesState):\nresponse\n=\nawait\nmodel.ainvoke(state[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}\n}\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi! I'm bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nExample: using Redis checkpointer\nCopy\npip install -U langgraph langgraph-checkpoint-redis\nYou need to call\ncheckpointer.setup()\nthe first time you\u2019re using Redis checkpointer.\nSync\nAsync\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.redis\nimport\nRedisSaver\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"redis://localhost:6379\"\nwith\nRedisSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer:\n# checkpointer.setup()\ndef\ncall_model\n(\nstate\n: MessagesState):\nresponse\n=\nmodel.invoke(state[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}\n}\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi! I'm bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.redis.aio\nimport\nAsyncRedisSaver\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"redis://localhost:6379\"\nasync\nwith\nAsyncRedisSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer:\n# await checkpointer.asetup()\nasync\ndef\ncall_model\n(\nstate\n: MessagesState):\nresponse\n=\nawait\nmodel.ainvoke(state[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}\n}\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi! I'm bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\n\u200b\nUse in subgraphs\nIf your graph contains\nsubgraphs\n, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.\nCopy\nfrom\nlanggraph.graph\nimport\nSTART\n, StateGraph\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\ntyping\nimport\nTypedDict\nclass\nState\n(\nTypedDict\n):\nfoo:\nstr\n# Subgraph\ndef\nsubgraph_node_1\n(\nstate\n: State):\nreturn\n{\n\"foo\"\n: state[\n\"foo\"\n]\n+\n\"bar\"\n}\nsubgraph_builder\n=\nStateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(\nSTART\n,\n\"subgraph_node_1\"\n)\nsubgraph\n=\nsubgraph_builder.compile()\n# Parent graph\nbuilder\n=\nStateGraph(State)\nbuilder.add_node(\n\"node_1\"\n, subgraph)\nbuilder.add_edge(\nSTART\n,\n\"node_1\"\n)\ncheckpointer\n=\nInMemorySaver()\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nIf you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in\nmulti-agent\nsystems, if you want agents to keep track of their internal message histories.\nCopy\nsubgraph_builder\n=\nStateGraph(\n...\n)\nsubgraph\n=\nsubgraph_builder.compile(\ncheckpointer\n=\nTrue\n)\n\u200b\nAdd long-term memory\nUse long-term memory to store user-specific or application-specific data across conversations.\nCopy\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nfrom\nlanggraph.graph\nimport\nStateGraph\nstore\n=\nInMemoryStore()\nbuilder\n=\nStateGraph(\n...\n)\ngraph\n=\nbuilder.compile(\nstore\n=\nstore)\n\u200b\nUse in production\nIn production, use a store backed by a database:\nCopy\nfrom\nlanggraph.store.postgres\nimport\nPostgresStore\nDB_URI\n=\n\"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith\nPostgresStore.from_conn_string(\nDB_URI\n)\nas\nstore:\nbuilder\n=\nStateGraph(\n...\n)\ngraph\n=\nbuilder.compile(\nstore\n=\nstore)\nExample: using Postgres store\nCopy\npip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\nYou need to call\nstore.setup()\nthe first time you\u2019re using Postgres store\nSync\nAsync\nCopy\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.postgres\nimport\nPostgresSaver\nfrom\nlanggraph.store.postgres\nimport\nPostgresStore\nfrom\nlanggraph.store.base\nimport\nBaseStore\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith\n(\nPostgresStore.from_conn_string(\nDB_URI\n)\nas\nstore,\nPostgresSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer,\n):\n# store.setup()\n# checkpointer.setup()\ndef\ncall_model\n(\nstate\n: MessagesState,\nconfig\n: RunnableConfig,\n*\n,\nstore\n: BaseStore,\n):\nuser_id\n=\nconfig[\n\"configurable\"\n][\n\"user_id\"\n]\nnamespace\n=\n(\n\"memories\"\n, user_id)\nmemories\n=\nstore.search(namespace,\nquery\n=\nstr\n(state[\n\"messages\"\n][\n-\n1\n].content))\ninfo\n=\n\"\n\\n\n\"\n.join([d.value[\n\"data\"\n]\nfor\nd\nin\nmemories])\nsystem_msg\n=\nf\n\"You are a helpful assistant talking to the user. User info:\n{\ninfo\n}\n\"\n# Store new memories if the user asks the model to remember\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nif\n\"remember\"\nin\nlast_message.content.lower():\nmemory\n=\n\"User name is Bob\"\nstore.put(namespace,\nstr\n(uuid.uuid4()), {\n\"data\"\n: memory})\nresponse\n=\nmodel.invoke(\n[{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n: system_msg}]\n+\nstate[\n\"messages\"\n]\n)\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer,\nstore\n=\nstore,\n)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n,\n\"user_id\"\n:\n\"1\"\n,\n}\n}\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi! Remember: my name is Bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"2\"\n,\n\"user_id\"\n:\n\"1\"\n,\n}\n}\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.postgres.aio\nimport\nAsyncPostgresSaver\nfrom\nlanggraph.store.postgres.aio\nimport\nAsyncPostgresStore\nfrom\nlanggraph.store.base\nimport\nBaseStore\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nasync\nwith\n(\nAsyncPostgresStore.from_conn_string(\nDB_URI\n)\nas\nstore,\nAsyncPostgresSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer,\n):\n# await store.setup()\n# await checkpointer.setup()\nasync\ndef\ncall_model\n(\nstate\n: MessagesState,\nconfig\n: RunnableConfig,\n*\n,\nstore\n: BaseStore,\n):\nuser_id\n=\nconfig[\n\"configurable\"\n][\n\"user_id\"\n]\nnamespace\n=\n(\n\"memories\"\n, user_id)\nmemories\n=\nawait\nstore.asearch(namespace,\nquery\n=\nstr\n(state[\n\"messages\"\n][\n-\n1\n].content))\ninfo\n=\n\"\n\\n\n\"\n.join([d.value[\n\"data\"\n]\nfor\nd\nin\nmemories])\nsystem_msg\n=\nf\n\"You are a helpful assistant talking to the user. User info:\n{\ninfo\n}\n\"\n# Store new memories if the user asks the model to remember\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nif\n\"remember\"\nin\nlast_message.content.lower():\nmemory\n=\n\"User name is Bob\"\nawait\nstore.aput(namespace,\nstr\n(uuid.uuid4()), {\n\"data\"\n: memory})\nresponse\n=\nawait\nmodel.ainvoke(\n[{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n: system_msg}]\n+\nstate[\n\"messages\"\n]\n)\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer,\nstore\n=\nstore,\n)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n,\n\"user_id\"\n:\n\"1\"\n,\n}\n}\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi! Remember: my name is Bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"2\"\n,\n\"user_id\"\n:\n\"1\"\n,\n}\n}\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nExample: using Redis store\nCopy\npip install -U langgraph langgraph-checkpoint-redis\nYou need to call\nstore.setup()\nthe first time you\u2019re using\nRedis store\n.\nSync\nAsync\nCopy\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.redis\nimport\nRedisSaver\nfrom\nlanggraph.store.redis\nimport\nRedisStore\nfrom\nlanggraph.store.base\nimport\nBaseStore\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"redis://localhost:6379\"\nwith\n(\nRedisStore.from_conn_string(\nDB_URI\n)\nas\nstore,\nRedisSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer,\n):\nstore.setup()\ncheckpointer.setup()\ndef\ncall_model\n(\nstate\n: MessagesState,\nconfig\n: RunnableConfig,\n*\n,\nstore\n: BaseStore,\n):\nuser_id\n=\nconfig[\n\"configurable\"\n][\n\"user_id\"\n]\nnamespace\n=\n(\n\"memories\"\n, user_id)\nmemories\n=\nstore.search(namespace,\nquery\n=\nstr\n(state[\n\"messages\"\n][\n-\n1\n].content))\ninfo\n=\n\"\n\\n\n\"\n.join([d.value[\n\"data\"\n]\nfor\nd\nin\nmemories])\nsystem_msg\n=\nf\n\"You are a helpful assistant talking to the user. User info:\n{\ninfo\n}\n\"\n# Store new memories if the user asks the model to remember\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nif\n\"remember\"\nin\nlast_message.content.lower():\nmemory\n=\n\"User name is Bob\"\nstore.put(namespace,\nstr\n(uuid.uuid4()), {\n\"data\"\n: memory})\nresponse\n=\nmodel.invoke(\n[{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n: system_msg}]\n+\nstate[\n\"messages\"\n]\n)\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer,\nstore\n=\nstore,\n)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n,\n\"user_id\"\n:\n\"1\"\n,\n}\n}\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi! Remember: my name is Bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"2\"\n,\n\"user_id\"\n:\n\"1\"\n,\n}\n}\nfor\nchunk\nin\ngraph.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\nfrom\nlangchain_core.runnables\nimport\nRunnableConfig\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph, MessagesState,\nSTART\nfrom\nlanggraph.checkpoint.redis.aio\nimport\nAsyncRedisSaver\nfrom\nlanggraph.store.redis.aio\nimport\nAsyncRedisStore\nfrom\nlanggraph.store.base\nimport\nBaseStore\nmodel\n=\ninit_chat_model(\nmodel\n=\n\"claude-haiku-4-5-20251001\"\n)\nDB_URI\n=\n\"redis://localhost:6379\"\nasync\nwith\n(\nAsyncRedisStore.from_conn_string(\nDB_URI\n)\nas\nstore,\nAsyncRedisSaver.from_conn_string(\nDB_URI\n)\nas\ncheckpointer,\n):\n# await store.setup()\n# await checkpointer.asetup()\nasync\ndef\ncall_model\n(\nstate\n: MessagesState,\nconfig\n: RunnableConfig,\n*\n,\nstore\n: BaseStore,\n):\nuser_id\n=\nconfig[\n\"configurable\"\n][\n\"user_id\"\n]\nnamespace\n=\n(\n\"memories\"\n, user_id)\nmemories\n=\nawait\nstore.asearch(namespace,\nquery\n=\nstr\n(state[\n\"messages\"\n][\n-\n1\n].content))\ninfo\n=\n\"\n\\n\n\"\n.join([d.value[\n\"data\"\n]\nfor\nd\nin\nmemories])\nsystem_msg\n=\nf\n\"You are a helpful assistant talking to the user. User info:\n{\ninfo\n}\n\"\n# Store new memories if the user asks the model to remember\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nif\n\"remember\"\nin\nlast_message.content.lower():\nmemory\n=\n\"User name is Bob\"\nawait\nstore.aput(namespace,\nstr\n(uuid.uuid4()), {\n\"data\"\n: memory})\nresponse\n=\nawait\nmodel.ainvoke(\n[{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n: system_msg}]\n+\nstate[\n\"messages\"\n]\n)\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer,\nstore\n=\nstore,\n)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n,\n\"user_id\"\n:\n\"1\"\n,\n}\n}\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Hi! Remember: my name is Bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"2\"\n,\n\"user_id\"\n:\n\"1\"\n,\n}\n}\nasync\nfor\nchunk\nin\ngraph.astream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n,\n):\nchunk[\n\"messages\"\n][\n-\n1\n].pretty_print()\n\u200b\nUse semantic search\nEnable semantic search in your graph\u2019s memory store to let graph agents search for items in the store by semantic similarity.\nCopy\nfrom\nlangchain.embeddings\nimport\ninit_embeddings\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\n# Create store with semantic search enabled\nembeddings\n=\ninit_embeddings(\n\"openai:text-embedding-3-small\"\n)\nstore\n=\nInMemoryStore(\nindex\n=\n{\n\"embed\"\n: embeddings,\n\"dims\"\n:\n1536\n,\n}\n)\nstore.put((\n\"user_123\"\n,\n\"memories\"\n),\n\"1\"\n, {\n\"text\"\n:\n\"I love pizza\"\n})\nstore.put((\n\"user_123\"\n,\n\"memories\"\n),\n\"2\"\n, {\n\"text\"\n:\n\"I am a plumber\"\n})\nitems\n=\nstore.search(\n(\n\"user_123\"\n,\n\"memories\"\n),\nquery\n=\n\"I'm hungry\"\n,\nlimit\n=\n1\n)\nLong-term memory with semantic search\nCopy\nfrom\nlangchain.embeddings\nimport\ninit_embeddings\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.store.base\nimport\nBaseStore\nfrom\nlanggraph.store.memory\nimport\nInMemoryStore\nfrom\nlanggraph.graph\nimport\nSTART\n, MessagesState, StateGraph\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n# Create store with semantic search enabled\nembeddings\n=\ninit_embeddings(\n\"openai:text-embedding-3-small\"\n)\nstore\n=\nInMemoryStore(\nindex\n=\n{\n\"embed\"\n: embeddings,\n\"dims\"\n:\n1536\n,\n}\n)\nstore.put((\n\"user_123\"\n,\n\"memories\"\n),\n\"1\"\n, {\n\"text\"\n:\n\"I love pizza\"\n})\nstore.put((\n\"user_123\"\n,\n\"memories\"\n),\n\"2\"\n, {\n\"text\"\n:\n\"I am a plumber\"\n})\ndef\nchat\n(\nstate\n,\n*\n,\nstore\n: BaseStore):\n# Search based on user's last message\nitems\n=\nstore.search(\n(\n\"user_123\"\n,\n\"memories\"\n),\nquery\n=\nstate[\n\"messages\"\n][\n-\n1\n].content,\nlimit\n=\n2\n)\nmemories\n=\n\"\n\\n\n\"\n.join(item.value[\n\"text\"\n]\nfor\nitem\nin\nitems)\nmemories\n=\nf\n\"## Memories of user\n\\n\n{\nmemories\n}\n\"\nif\nmemories\nelse\n\"\"\nresponse\n=\nmodel.invoke(\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\nf\n\"You are a helpful assistant.\n\\n\n{\nmemories\n}\n\"\n},\n*\nstate[\n\"messages\"\n],\n]\n)\nreturn\n{\n\"messages\"\n: [response]}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(chat)\nbuilder.add_edge(\nSTART\n,\n\"chat\"\n)\ngraph\n=\nbuilder.compile(\nstore\n=\nstore)\nfor\nmessage, metadata\nin\ngraph.stream(\ninput\n=\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"I'm hungry\"\n}]},\nstream_mode\n=\n\"messages\"\n,\n):\nprint\n(message.content,\nend\n=\n\"\"\n)\n\u200b\nManage short-term memory\nWith\nshort-term memory\nenabled, long conversations can exceed the LLM\u2019s context window. Common solutions are:\nTrim messages\n: Remove first or last N messages (before calling LLM)\nDelete messages\nfrom LangGraph state permanently\nSummarize messages\n: Summarize earlier messages in the history and replace them with a summary\nManage checkpoints\nto store and retrieve message history\nCustom strategies (e.g., message filtering, etc.)\nThis allows the agent to keep track of the conversation without exceeding the LLM\u2019s context window.\n\u200b\nTrim messages\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you\u2019re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the\nstrategy\n(e.g., keep the last\nmax_tokens\n) to use for handling the boundary.\nTo trim message history, use the\ntrim_messages\nfunction:\nCopy\nfrom\nlangchain_core.messages.utils\nimport\n(\ntrim_messages,\ncount_tokens_approximately\n)\ndef\ncall_model\n(\nstate\n: MessagesState):\nmessages\n=\ntrim_messages(\nstate[\n\"messages\"\n],\nstrategy\n=\n\"last\"\n,\ntoken_counter\n=\ncount_tokens_approximately,\nmax_tokens\n=\n128\n,\nstart_on\n=\n\"human\"\n,\nend_on\n=\n(\n\"human\"\n,\n\"tool\"\n),\n)\nresponse\n=\nmodel.invoke(messages)\nreturn\n{\n\"messages\"\n: [response]}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\n...\nFull example: trim messages\nCopy\nfrom\nlangchain_core.messages.utils\nimport\n(\ntrim_messages,\ncount_tokens_approximately\n)\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n, MessagesState\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\nsummarization_model\n=\nmodel.bind(\nmax_tokens\n=\n128\n)\ndef\ncall_model\n(\nstate\n: MessagesState):\nmessages\n=\ntrim_messages(\nstate[\n\"messages\"\n],\nstrategy\n=\n\"last\"\n,\ntoken_counter\n=\ncount_tokens_approximately,\nmax_tokens\n=\n128\n,\nstart_on\n=\n\"human\"\n,\nend_on\n=\n(\n\"human\"\n,\n\"tool\"\n),\n)\nresponse\n=\nmodel.invoke(messages)\nreturn\n{\n\"messages\"\n: [response]}\ncheckpointer\n=\nInMemorySaver()\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\ngraph.invoke({\n\"messages\"\n:\n\"hi, my name is bob\"\n}, config)\ngraph.invoke({\n\"messages\"\n:\n\"write a short poem about cats\"\n}, config)\ngraph.invoke({\n\"messages\"\n:\n\"now do the same but for dogs\"\n}, config)\nfinal_response\n=\ngraph.invoke({\n\"messages\"\n:\n\"what's my name?\"\n}, config)\nfinal_response[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================== Ai Message ==================================\nYour name is Bob, as you mentioned when you first introduced yourself.\n\u200b\nDelete messages\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\nTo delete messages from the graph state, you can use the\nRemoveMessage\n. For\nRemoveMessage\nto work, you need to use a state key with\nadd_messages\nreducer\n, like\nMessagesState\n.\nTo remove specific messages:\nCopy\nfrom\nlangchain.messages\nimport\nRemoveMessage\ndef\ndelete_messages\n(\nstate\n):\nmessages\n=\nstate[\n\"messages\"\n]\nif\nlen\n(messages)\n>\n2\n:\n# remove the earliest two messages\nreturn\n{\n\"messages\"\n: [RemoveMessage(\nid\n=\nm.id)\nfor\nm\nin\nmessages[:\n2\n]]}\nTo remove\nall\nmessages:\nCopy\nfrom\nlanggraph.graph.message\nimport\nREMOVE_ALL_MESSAGES\ndef\ndelete_messages\n(\nstate\n):\nreturn\n{\n\"messages\"\n: [RemoveMessage(\nid\n=\nREMOVE_ALL_MESSAGES\n)]}\nWhen deleting messages,\nmake sure\nthat the resulting message history is valid. Check the limitations of the LLM provider you\u2019re using. For example:\nSome providers expect message history to start with a\nuser\nmessage\nMost providers require\nassistant\nmessages with tool calls to be followed by corresponding\ntool\nresult messages.\nFull example: delete messages\nCopy\nfrom\nlangchain.messages\nimport\nRemoveMessage\ndef\ndelete_messages\n(\nstate\n):\nmessages\n=\nstate[\n\"messages\"\n]\nif\nlen\n(messages)\n>\n2\n:\n# remove the earliest two messages\nreturn\n{\n\"messages\"\n: [RemoveMessage(\nid\n=\nm.id)\nfor\nm\nin\nmessages[:\n2\n]]}\ndef\ncall_model\n(\nstate\n: MessagesState):\nresponse\n=\nmodel.invoke(state[\n\"messages\"\n])\nreturn\n{\n\"messages\"\n: response}\nbuilder\n=\nStateGraph(MessagesState)\nbuilder.add_sequence([call_model, delete_messages])\nbuilder.add_edge(\nSTART\n,\n\"call_model\"\n)\ncheckpointer\n=\nInMemorySaver()\napp\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\nfor\nevent\nin\napp.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"hi! I'm bob\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nprint\n([(message.type, message.content)\nfor\nmessage\nin\nevent[\n\"messages\"\n]])\nfor\nevent\nin\napp.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what's my name?\"\n}]},\nconfig,\nstream_mode\n=\n\"values\"\n):\nprint\n([(message.type, message.content)\nfor\nmessage\nin\nevent[\n\"messages\"\n]])\nCopy\n[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n[('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n\u200b\nSummarize messages\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the\nMessagesState\nto include a\nsummary\nkey:\nCopy\nfrom\nlanggraph.graph\nimport\nMessagesState\nclass\nState\n(\nMessagesState\n):\nsummary:\nstr\nThen, you can generate a summary of the chat history, using any existing summary as context for the next summary. This\nsummarize_conversation\nnode can be called after some number of messages have accumulated in the\nmessages\nstate key.\nCopy\ndef\nsummarize_conversation\n(\nstate\n: State):\n# First, we get any existing summary\nsummary\n=\nstate.get(\n\"summary\"\n,\n\"\"\n)\n# Create our summarization prompt\nif\nsummary:\n# A summary already exists\nsummary_message\n=\n(\nf\n\"This is a summary of the conversation to date:\n{\nsummary\n}\n\\n\\n\n\"\n\"Extend the summary by taking into account the new messages above:\"\n)\nelse\n:\nsummary_message\n=\n\"Create a summary of the conversation above:\"\n# Add prompt to our history\nmessages\n=\nstate[\n\"messages\"\n]\n+\n[HumanMessage(\ncontent\n=\nsummary_message)]\nresponse\n=\nmodel.invoke(messages)\n# Delete all but the 2 most recent messages\ndelete_messages\n=\n[RemoveMessage(\nid\n=\nm.id)\nfor\nm\nin\nstate[\n\"messages\"\n][:\n-\n2\n]]\nreturn\n{\n\"summary\"\n: response.content,\n\"messages\"\n: delete_messages}\nFull example: summarize messages\nCopy\nfrom\ntyping\nimport\nAny, TypedDict\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nfrom\nlangchain.messages\nimport\nAnyMessage\nfrom\nlangchain_core.messages.utils\nimport\ncount_tokens_approximately\nfrom\nlanggraph.graph\nimport\nStateGraph,\nSTART\n, MessagesState\nfrom\nlanggraph.checkpoint.memory\nimport\nInMemorySaver\nfrom\nlangmem.short_term\nimport\nSummarizationNode, RunningSummary\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\nsummarization_model\n=\nmodel.bind(\nmax_tokens\n=\n128\n)\nclass\nState\n(\nMessagesState\n):\ncontext: dict[\nstr\n, RunningSummary]\nclass\nLLMInputState\n(\nTypedDict\n):\nsummarized_messages: list[AnyMessage]\ncontext: dict[\nstr\n, RunningSummary]\nsummarization_node\n=\nSummarizationNode(\ntoken_counter\n=\ncount_tokens_approximately,\nmodel\n=\nsummarization_model,\nmax_tokens\n=\n256\n,\nmax_tokens_before_summary\n=\n256\n,\nmax_summary_tokens\n=\n128\n,\n)\ndef\ncall_model\n(\nstate\n: LLMInputState):\nresponse\n=\nmodel.invoke(state[\n\"summarized_messages\"\n])\nreturn\n{\n\"messages\"\n: [response]}\ncheckpointer\n=\nInMemorySaver()\nbuilder\n=\nStateGraph(State)\nbuilder.add_node(call_model)\nbuilder.add_node(\n\"summarize\"\n, summarization_node)\nbuilder.add_edge(\nSTART\n,\n\"summarize\"\n)\nbuilder.add_edge(\n\"summarize\"\n,\n\"call_model\"\n)\ngraph\n=\nbuilder.compile(\ncheckpointer\n=\ncheckpointer)\n# Invoke the graph\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}}\ngraph.invoke({\n\"messages\"\n:\n\"hi, my name is bob\"\n}, config)\ngraph.invoke({\n\"messages\"\n:\n\"write a short poem about cats\"\n}, config)\ngraph.invoke({\n\"messages\"\n:\n\"now do the same but for dogs\"\n}, config)\nfinal_response\n=\ngraph.invoke({\n\"messages\"\n:\n\"what's my name?\"\n}, config)\nfinal_response[\n\"messages\"\n][\n-\n1\n].pretty_print()\nprint\n(\n\"\n\\n\nSummary:\"\n, final_response[\n\"context\"\n][\n\"running_summary\"\n].summary)\nWe will keep track of our running summary in the\ncontext\nfield\n(expected by the\nSummarizationNode\n).\nDefine private state that will be used only for filtering\nthe inputs to\ncall_model\nnode.\nWe\u2019re passing a private input state here to isolate the messages returned by the summarization node\nCopy\n================================== Ai Message ==================================\nFrom our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\nSummary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n\u200b\nManage checkpoints\nYou can view and delete the information stored by the checkpointer.\n\u200b\nView thread state\nGraph/Functional API\nCheckpointer API\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n,\n# optionally provide an ID for a specific checkpoint,\n# otherwise the latest checkpoint is shown\n# \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  #\n}\n}\ngraph.get_state(config)\nCopy\nStateSnapshot(\nvalues={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\nmetadata={\n'source': 'loop',\n'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n'step': 4,\n'parents': {},\n'thread_id': '1'\n},\ncreated_at='2025-05-05T16:01:24.680462+00:00',\nparent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\ntasks=(),\ninterrupts=()\n)\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n,\n# optionally provide an ID for a specific checkpoint,\n# otherwise the latest checkpoint is shown\n# \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  #\n}\n}\ncheckpointer.get_tuple(config)\nCopy\nCheckpointTuple(\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\ncheckpoint={\n'v': 3,\n'ts': '2025-05-05T16:01:24.680462+00:00',\n'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',\n'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n},\nmetadata={\n'source': 'loop',\n'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n'step': 4,\n'parents': {},\n'thread_id': '1'\n},\nparent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\npending_writes=[]\n)\n\u200b\nView the history of the thread\nGraph/Functional API\nCheckpointer API\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}\n}\nlist\n(graph.get_state_history(config))\nCopy\n[\nStateSnapshot(\nvalues={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\nnext=(),\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\nmetadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\ncreated_at='2025-05-05T16:01:24.680462+00:00',\nparent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\ntasks=(),\ninterrupts=()\n),\nStateSnapshot(\nvalues={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]},\nnext=('call_model',),\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\nmetadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\ncreated_at='2025-05-05T16:01:23.863421+00:00',\nparent_config={...}\ntasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),\ninterrupts=()\n),\nStateSnapshot(\nvalues={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\nnext=('__start__',),\nconfig={...},\nmetadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\ncreated_at='2025-05-05T16:01:23.863173+00:00',\nparent_config={...}\ntasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),),\ninterrupts=()\n),\nStateSnapshot(\nvalues={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\nnext=(),\nconfig={...},\nmetadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\ncreated_at='2025-05-05T16:01:23.862295+00:00',\nparent_config={...}\ntasks=(),\ninterrupts=()\n),\nStateSnapshot(\nvalues={'messages': [HumanMessage(content=\"hi! I'm bob\")]},\nnext=('call_model',),\nconfig={...},\nmetadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\ncreated_at='2025-05-05T16:01:22.278960+00:00',\nparent_config={...}\ntasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),\ninterrupts=()\n),\nStateSnapshot(\nvalues={'messages': []},\nnext=('__start__',),\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\nmetadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\ncreated_at='2025-05-05T16:01:22.277497+00:00',\nparent_config=None,\ntasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),),\ninterrupts=()\n)\n]\nCopy\nconfig\n=\n{\n\"configurable\"\n: {\n\"thread_id\"\n:\n\"1\"\n}\n}\nlist\n(checkpointer.list(config))\nCopy\n[\nCheckpointTuple(\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\ncheckpoint={\n'v': 3,\n'ts': '2025-05-05T16:01:24.680462+00:00',\n'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',\n'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\n'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n},\nmetadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\nparent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\npending_writes=[]\n),\nCheckpointTuple(\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\ncheckpoint={\n'v': 3,\n'ts': '2025-05-05T16:01:23.863421+00:00',\n'id': '1f029ca3-1790-6b0a-8003-baf965b6a38f',\n'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\n'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")], 'branch:to:call_model': None}\n},\nmetadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\nparent_config={...},\npending_writes=[('8ab4155e-6b15-b885-9ce5-bed69a2c305c', 'messages', AIMessage(content='Your name is Bob.'))]\n),\nCheckpointTuple(\nconfig={...},\ncheckpoint={\n'v': 3,\n'ts': '2025-05-05T16:01:23.863173+00:00',\n'id': '1f029ca3-1790-616e-8002-9e021694a0cd',\n'channel_versions': {'__start__': '00000000000000000000000000000004.0.5736472536395331', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\n'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\n'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}, 'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\n},\nmetadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\nparent_config={...},\npending_writes=[('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'messages', [{'role': 'user', 'content': \"what's my name?\"}]), ('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'branch:to:call_model', None)]\n),\nCheckpointTuple(\nconfig={...},\ncheckpoint={\n'v': 3,\n'ts': '2025-05-05T16:01:23.862295+00:00',\n'id': '1f029ca3-178d-6f54-8001-d7b180db0c89',\n'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\n'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\n'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\n},\nmetadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\nparent_config={...},\npending_writes=[]\n),\nCheckpointTuple(\nconfig={...},\ncheckpoint={\n'v': 3,\n'ts': '2025-05-05T16:01:22.278960+00:00',\n'id': '1f029ca3-0874-6612-8000-339f2abc83b1',\n'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000002.0.30296526818059655', 'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'},\n'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}},\n'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\")], 'branch:to:call_model': None}\n},\nmetadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\nparent_config={...},\npending_writes=[('8cbd75e0-3720-b056-04f7-71ac805140a0', 'messages', AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'))]\n),\nCheckpointTuple(\nconfig={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\ncheckpoint={\n'v': 3,\n'ts': '2025-05-05T16:01:22.277497+00:00',\n'id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565',\n'channel_versions': {'__start__': '00000000000000000000000000000001.0.7040775356287469'},\n'versions_seen': {'__input__': {}},\n'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}\n},\nmetadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\nparent_config=None,\npending_writes=[('d458367b-8265-812c-18e2-33001d199ce6', 'messages', [{'role': 'user', 'content': \"hi! I'm bob\"}]), ('d458367b-8265-812c-18e2-33001d199ce6', 'branch:to:call_model', None)]\n)\n]\n\u200b\nDelete all checkpoints for a thread\nCopy\nthread_id\n=\n\"1\"\ncheckpointer.delete_thread(thread_id)\n\u200b\nPrebuilt memory tools\nLangMem\nis a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the\nLangMem documentation\nfor usage examples.\n\u200b\nDatabase management\nIf you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database.\nBy convention, most database-specific libraries define a\nsetup()\nmethod on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of\nBaseCheckpointSaver\nor\nBaseStore\nto confirm the exact method name and usage.\nWe recommend running migrations as a dedicated deployment step, or you can ensure they\u2019re run as part of server startup.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nUse time-travel\nPrevious\nSubgraphs\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://docs.langchain.com/oss/python/langgraph/add-memory",
      "title": "Memory - Docs by LangChain",
      "heading": "Memory"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/introduction/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/concepts/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\nTutorials\nLangChain\nBuild a RAG agent with LangChain\nCopy page\nCopy page\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/tutorials/",
      "title": "Build a RAG agent with LangChain - Docs by LangChain",
      "heading": "Build a RAG agent with LangChain"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/how_to/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/concepts/agents/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\nTutorials\nLangChain\nBuild a RAG agent with LangChain\nCopy page\nCopy page\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/tutorials/agents/",
      "title": "Build a RAG agent with LangChain - Docs by LangChain",
      "heading": "Build a RAG agent with LangChain"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/how_to/agent_executor/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/how_to/custom_tools/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "LangChain integrations packages - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain integrations packages\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic (Claude)\nGoogle\nAWS (Amazon)\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nLangChain integrations packages\nCopy page\nCopy page\nLangChain offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more.\nChat models\nEmbedding models\nTools and toolkits\nTo see a full list of integrations by component type, refer to the categories in the sidebar.\n\u200b\nPopular providers\nProvider\nPackage\nDownloads\nLatest version\nJS/TS support\nOpenAI\nlangchain-openai\n\u2705\nGoogle (Vertex AI)\nlangchain-google-vertexai\n\u2705\nAnthropic (Claude)\nlangchain-anthropic\n\u2705\nAWS\nlangchain-aws\n\u2705\nGoogle (GenAI)\nlangchain-google-genai\n\u2705\nOllama\nlangchain-ollama\n\u2705\nChroma\nlangchain-chroma\n\u2705\nGroq\nlangchain-groq\n\u2705\nHuggingface\nlangchain-huggingface\n\u2705\nPinecone\nlangchain-pinecone\n\u2705\nCohere\nlangchain-cohere\n\u2705\nPostgres\nlangchain-postgres\n\u2705\nMistralAI\nlangchain-mistralai\n\u2705\nDatabricks\ndatabricks-langchain\n\u2705\nFireworks\nlangchain-fireworks\n\u2705\nMongoDB\nlangchain-mongodb\n\u2705\nPerplexity\nlangchain-perplexity\n\u2705\nDeepseek\nlangchain-deepseek\n\u2705\nIBM\nlangchain-ibm\n\u2705\nNvidia AI Endpoints\nlangchain-nvidia-ai-endpoints\n\u274c\nxAI (Grok)\nlangchain-xai\n\u2705\nQdrant\nlangchain-qdrant\n\u2705\nTavily\nlangchain-tavily\n\u2705\nMilvus\nlangchain-milvus\n\u2705\nElasticsearch\nlangchain-elasticsearch\n\u2705\nLiteLLM\nlangchain-litellm\nN/A\nAzure AI\nlangchain-azure-ai\n\u2705\nDataStax Astra DB\nlangchain-astradb\n\u2705\nRedis\nlangchain-redis\n\u2705\nTogether\nlangchain-together\n\u2705\nMCP Toolbox (Google)\ntoolbox-langchain\n\u274c\nGoogle (Community)\nlangchain-google-community\n\u274c\nUnstructured\nlangchain-unstructured\n\u2705\nNeo4J\nlangchain-neo4j\n\u2705\nGraph RAG\nlangchain-graph-retriever\n\u274c\nSambanova\nlangchain-sambanova\n\u274c\n\u200b\nAll providers\nSee all providers\nor search for a provider using the search field.\nCommunity integrations can be found in\nlangchain-community\n.\nIf you\u2019d like to contribute an integration, see the\ncontributing guide\n.\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nAll integration providers\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/integrations/tools/",
      "title": "LangChain integrations packages - Docs by LangChain",
      "heading": "LangChain integrations packages"
    }
  },
  {
    "page_content": "Build a RAG agent with LangChain - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain\nBuild a RAG agent with LangChain\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLearn\nTutorials\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nLangGraph\nConceptual overviews\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and Generation\nRAG agents\nRAG chains\nNext steps\nTutorials\nLangChain\nBuild a RAG agent with LangChain\nCopy page\nCopy page\n\u200b\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or\nRAG\n.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nConcepts\nWe will cover the following concepts:\nIndexing\n: a pipeline for ingesting data from a source and indexing it.\nThis usually happens in a separate process.\nRetrieval and generation\n: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce we\u2019ve indexed our data, we will use an\nagent\nas our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the\nsemantic search tutorial\n.\nIf your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on\nretrieval and generation\n\u200b\nPreview\nIn this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the\nLLM Powered Autonomous Agents\nblog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\nExpand for full code snippet\nCopy\nimport\nbs4\nfrom\nlangchain.agents\nimport\nAgentState, create_agent\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\nfrom\nlangchain.messages\nimport\nMessageLikeRepresentation\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\n# Load and chunk contents of the blog\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\ndict\n(\nparse_only\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-content\"\n,\n\"post-title\"\n,\n\"post-header\"\n)\n)\n),\n)\ndocs\n=\nloader.load()\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\nchunk_overlap\n=\n200\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\n# Index chunks\n_\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\n# Construct a tool for retrieving context\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\nCall ID: call_xTkJr8njRY0geNz43ZvGkX0R\nArgs:\nquery: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTask decomposition refers to...\nCheck out the\nLangSmith trace\n.\n\u200b\nSetup\n\u200b\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip\ninstall\nlangchain\nlangchain-text-splitters\nlangchain-community\nbs4\nFor more details, see our\nInstallation guide\n.\n\u200b\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with\nLangSmith\n.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport\nLANGSMITH_TRACING\n=\n\"true\"\nexport\nLANGSMITH_API_KEY\n=\n\"...\"\nOr, set them in Python:\nCopy\nimport\ngetpass\nimport\nos\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\nos.environ[\n\"LANGSMITH_API_KEY\"\n]\n=\ngetpass.getpass()\n\u200b\nComponents\nWe will need to select three components from LangChain\u2019s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\n\ud83d\udc49 Read the\nOpenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"gpt-5-mini\"\n)\n\ud83d\udc49 Read the\nAnthropic chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[anthropic]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"ANTHROPIC_API_KEY\"\n]\n=\n\"sk-...\"\nmodel\n=\ninit_chat_model(\n\"claude-sonnet-4-5-20250929\"\n)\n\ud83d\udc49 Read the\nAzure chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[openai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\n\"...\"\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n]\n=\n\"...\"\nos.environ[\n\"OPENAI_API_VERSION\"\n]\n=\n\"2025-03-01-preview\"\nmodel\n=\ninit_chat_model(\n\"azure_openai:gpt-5-mini\"\n,\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\n)\n\ud83d\udc49 Read the\nGoogle GenAI chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[google-genai]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\n\"...\"\nmodel\n=\ninit_chat_model(\n\"google_genai:gemini-2.5-flash-lite\"\n)\n\ud83d\udc49 Read the\nAWS Bedrock chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[aws]\"\ninit_chat_model\nModel Class\nCopy\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\nmodel\n=\ninit_chat_model(\n\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n,\nmodel_provider\n=\n\"bedrock_converse\"\n,\n)\n\ud83d\udc49 Read the\nHuggingFace chat model integration docs\nCopy\npip\ninstall\n-U\n\"langchain[huggingface]\"\ninit_chat_model\nModel Class\nCopy\nimport\nos\nfrom\nlangchain.chat_models\nimport\ninit_chat_model\nos.environ[\n\"HUGGINGFACEHUB_API_TOKEN\"\n]\n=\n\"hf_...\"\nmodel\n=\ninit_chat_model(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\nmodel_provider\n=\n\"huggingface\"\n,\ntemperature\n=\n0.7\n,\nmax_tokens\n=\n1024\n,\n)\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"OPENAI_API_KEY\"\n):\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for OpenAI: \"\n)\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\npip\ninstall\n-U\n\"langchain-openai\"\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"AZURE_OPENAI_API_KEY\"\n):\nos.environ[\n\"AZURE_OPENAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Azure: \"\n)\nfrom\nlangchain_openai\nimport\nAzureOpenAIEmbeddings\nembeddings\n=\nAzureOpenAIEmbeddings(\nazure_endpoint\n=\nos.environ[\n\"AZURE_OPENAI_ENDPOINT\"\n],\nazure_deployment\n=\nos.environ[\n\"AZURE_OPENAI_DEPLOYMENT_NAME\"\n],\nopenai_api_version\n=\nos.environ[\n\"AZURE_OPENAI_API_VERSION\"\n],\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-genai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"GOOGLE_API_KEY\"\n):\nos.environ[\n\"GOOGLE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Google Gemini: \"\n)\nfrom\nlangchain_google_genai\nimport\nGoogleGenerativeAIEmbeddings\nembeddings\n=\nGoogleGenerativeAIEmbeddings(\nmodel\n=\n\"models/gemini-embedding-001\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-google-vertexai\nCopy\nfrom\nlangchain_google_vertexai\nimport\nVertexAIEmbeddings\nembeddings\n=\nVertexAIEmbeddings(\nmodel\n=\n\"text-embedding-005\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-aws\nCopy\nfrom\nlangchain_aws\nimport\nBedrockEmbeddings\nembeddings\n=\nBedrockEmbeddings(\nmodel_id\n=\n\"amazon.titan-embed-text-v2:0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-huggingface\nCopy\nfrom\nlangchain_huggingface\nimport\nHuggingFaceEmbeddings\nembeddings\n=\nHuggingFaceEmbeddings(\nmodel_name\n=\n\"sentence-transformers/all-mpnet-base-v2\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ollama\nCopy\nfrom\nlangchain_ollama\nimport\nOllamaEmbeddings\nembeddings\n=\nOllamaEmbeddings(\nmodel\n=\n\"llama3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-cohere\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"COHERE_API_KEY\"\n):\nos.environ[\n\"COHERE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Cohere: \"\n)\nfrom\nlangchain_cohere\nimport\nCohereEmbeddings\nembeddings\n=\nCohereEmbeddings(\nmodel\n=\n\"embed-english-v3.0\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-mistralai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"MISTRALAI_API_KEY\"\n):\nos.environ[\n\"MISTRALAI_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for MistralAI: \"\n)\nfrom\nlangchain_mistralai\nimport\nMistralAIEmbeddings\nembeddings\n=\nMistralAIEmbeddings(\nmodel\n=\n\"mistral-embed\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nomic\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NOMIC_API_KEY\"\n):\nos.environ[\n\"NOMIC_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Nomic: \"\n)\nfrom\nlangchain_nomic\nimport\nNomicEmbeddings\nembeddings\n=\nNomicEmbeddings(\nmodel\n=\n\"nomic-embed-text-v1.5\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-nvidia-ai-endpoints\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"NVIDIA_API_KEY\"\n):\nos.environ[\n\"NVIDIA_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for NVIDIA: \"\n)\nfrom\nlangchain_nvidia_ai_endpoints\nimport\nNVIDIAEmbeddings\nembeddings\n=\nNVIDIAEmbeddings(\nmodel\n=\n\"NV-Embed-QA\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-voyageai\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"VOYAGE_API_KEY\"\n):\nos.environ[\n\"VOYAGE_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Voyage AI: \"\n)\nfrom\nlangchain\n-\nvoyageai\nimport\nVoyageAIEmbeddings\nembeddings\n=\nVoyageAIEmbeddings(\nmodel\n=\n\"voyage-3\"\n)\nCopy\npip\ninstall\n-qU\nlangchain-ibm\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"WATSONX_APIKEY\"\n):\nos.environ[\n\"WATSONX_APIKEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for IBM watsonx: \"\n)\nfrom\nlangchain_ibm\nimport\nWatsonxEmbeddings\nembeddings\n=\nWatsonxEmbeddings(\nmodel_id\n=\n\"ibm/slate-125m-english-rtrvr\"\n,\nurl\n=\n\"https://us-south.ml.cloud.ibm.com\"\n,\nproject_id\n=\n\"<WATSONX PROJECT_ID>\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-core\nCopy\nfrom\nlangchain_core.embeddings\nimport\nDeterministicFakeEmbedding\nembeddings\n=\nDeterministicFakeEmbedding(\nsize\n=\n4096\n)\nCopy\npip\ninstall\n-qU\nlangchain-isaacus\nCopy\nimport\ngetpass\nimport\nos\nif\nnot\nos.environ.get(\n\"ISAACUS_API_KEY\"\n):\nos.environ[\n\"ISAACUS_API_KEY\"\n]\n=\ngetpass.getpass(\n\"Enter API key for Isaacus: \"\n)\nfrom\nlangchain_isaacus\nimport\nIsaacusEmbeddings\nembeddings\n=\nIsaacusEmbeddings(\nmodel\n=\n\"kanon-2-embedder\"\n)\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip\ninstall\n-U\n\"langchain-core\"\nCopy\nfrom\nlangchain_core.vectorstores\nimport\nInMemoryVectorStore\nvector_store\n=\nInMemoryVectorStore(embeddings)\nCopy\npip\ninstall\n-qU\nboto3\nCopy\nfrom\nopensearchpy\nimport\nRequestsHttpConnection\nservice\n=\n\"es\"\n# must set the service as 'es'\nregion\n=\n\"us-east-2\"\ncredentials\n=\nboto3.Session(\naws_access_key_id\n=\n\"xxxxxx\"\n,\naws_secret_access_key\n=\n\"xxxxx\"\n).get_credentials()\nawsauth\n=\nAWS4Auth(\n\"xxxxx\"\n,\n\"xxxxxx\"\n, region, service,\nsession_token\n=\ncredentials.token)\nvector_store\n=\nOpenSearchVectorSearch.from_documents(\ndocs,\nembeddings,\nopensearch_url\n=\n\"host url\"\n,\nhttp_auth\n=\nawsauth,\ntimeout\n=\n300\n,\nuse_ssl\n=\nTrue\n,\nverify_certs\n=\nTrue\n,\nconnection_class\n=\nRequestsHttpConnection,\nindex_name\n=\n\"test-index\"\n,\n)\nCopy\npip\ninstall\n-U\n\"langchain-astradb\"\nCopy\nfrom\nlangchain_astradb\nimport\nAstraDBVectorStore\nvector_store\n=\nAstraDBVectorStore(\nembedding\n=\nembeddings,\napi_endpoint\n=\nASTRA_DB_API_ENDPOINT\n,\ncollection_name\n=\n\"astra_vector_langchain\"\n,\ntoken\n=\nASTRA_DB_APPLICATION_TOKEN\n,\nnamespace\n=\nASTRA_DB_NAMESPACE\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-chroma\nCopy\nfrom\nlangchain_chroma\nimport\nChroma\nvector_store\n=\nChroma(\ncollection_name\n=\n\"example_collection\"\n,\nembedding_function\n=\nembeddings,\npersist_directory\n=\n\"./chroma_langchain_db\"\n,\n# Where to save data locally, remove if not necessary\n)\nCopy\npip\ninstall\n-qU\nlangchain-community\nfaiss-cpu\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nembedding_dim\n=\nlen\n(embeddings.embed_query(\n\"hello world\"\n))\nindex\n=\nfaiss.IndexFlatL2(embedding_dim)\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\nCopy\npip\ninstall\n-qU\nlangchain-milvus\nCopy\nfrom\nlangchain_milvus\nimport\nMilvus\nURI\n=\n\"./milvus_example.db\"\nvector_store\n=\nMilvus(\nembedding_function\n=\nembeddings,\nconnection_args\n=\n{\n\"uri\"\n:\nURI\n},\nindex_params\n=\n{\n\"index_type\"\n:\n\"FLAT\"\n,\n\"metric_type\"\n:\n\"L2\"\n},\n)\nCopy\npip\ninstall\n-qU\nlangchain-mongodb\nCopy\nfrom\nlangchain_mongodb\nimport\nMongoDBAtlasVectorSearch\nvector_store\n=\nMongoDBAtlasVectorSearch(\nembedding\n=\nembeddings,\ncollection\n=\nMONGODB_COLLECTION\n,\nindex_name\n=\nATLAS_VECTOR_SEARCH_INDEX_NAME\n,\nrelevance_score_fn\n=\n\"cosine\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGVector\nvector_store\n=\nPGVector(\nembeddings\n=\nembeddings,\ncollection_name\n=\n\"my_docs\"\n,\nconnection\n=\n\"postgresql+psycopg://...\"\n,\n)\nCopy\npip\ninstall\n-qU\nlangchain-postgres\nCopy\nfrom\nlangchain_postgres\nimport\nPGEngine, PGVectorStore\npg_engine\n=\nPGEngine.from_connection_string(\nurl\n=\n\"postgresql+psycopg://...\"\n)\nvector_store\n=\nPGVectorStore.create_sync(\nengine\n=\npg_engine,\ntable_name\n=\n'test_table'\n,\nembedding_service\n=\nembedding\n)\nCopy\npip\ninstall\n-qU\nlangchain-pinecone\nCopy\nfrom\nlangchain_pinecone\nimport\nPineconeVectorStore\nfrom\npinecone\nimport\nPinecone\npc\n=\nPinecone(\napi_key\n=\n...\n)\nindex\n=\npc.Index(index_name)\nvector_store\n=\nPineconeVectorStore(\nembedding\n=\nembeddings,\nindex\n=\nindex)\nCopy\npip\ninstall\n-qU\nlangchain-qdrant\nCopy\nfrom\nqdrant_client.models\nimport\nDistance, VectorParams\nfrom\nlangchain_qdrant\nimport\nQdrantVectorStore\nfrom\nqdrant_client\nimport\nQdrantClient\nclient\n=\nQdrantClient(\n\":memory:\"\n)\nvector_size\n=\nlen\n(embeddings.embed_query(\n\"sample text\"\n))\nif\nnot\nclient.collection_exists(\n\"test\"\n):\nclient.create_collection(\ncollection_name\n=\n\"test\"\n,\nvectors_config\n=\nVectorParams(\nsize\n=\nvector_size,\ndistance\n=\nDistance.\nCOSINE\n)\n)\nvector_store\n=\nQdrantVectorStore(\nclient\n=\nclient,\ncollection_name\n=\n\"test\"\n,\nembedding\n=\nembeddings,\n)\n\u200b\n1. Indexing\nThis section is an abbreviated version of the content in the\nsemantic search tutorial\n.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with\ndocument loaders\n,\nembeddings\n, and\nvector stores\n, feel free to skip to the next section on\nretrieval and generation\n.\nIndexing commonly works as follows:\nLoad\n: First we need to load our data. This is done with\nDocument Loaders\n.\nSplit\n:\nText splitters\nbreak large\nDocuments\ninto smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window.\nStore\n: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a\nVectorStore\nand\nEmbeddings\nmodel.\n\u200b\nLoading documents\nWe need to first load the blog post contents. We can use\nDocumentLoaders\nfor this, which are objects that load in data from a source and return a list of\nDocument\nobjects.\nIn this case we\u2019ll use the\nWebBaseLoader\n, which uses\nurllib\nto load HTML from web URLs and\nBeautifulSoup\nto parse it to text. We can customize the HTML -> text parsing by passing in parameters into the\nBeautifulSoup\nparser via\nbs_kwargs\n(see\nBeautifulSoup docs\n). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others.\nCopy\nimport\nbs4\nfrom\nlangchain_community.document_loaders\nimport\nWebBaseLoader\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer\n=\nbs4.SoupStrainer(\nclass_\n=\n(\n\"post-title\"\n,\n\"post-header\"\n,\n\"post-content\"\n))\nloader\n=\nWebBaseLoader(\nweb_paths\n=\n(\n\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n,),\nbs_kwargs\n=\n{\n\"parse_only\"\n: bs4_strainer},\n)\ndocs\n=\nloader.load()\nassert\nlen\n(docs)\n==\n1\nprint\n(\nf\n\"Total characters:\n{\nlen\n(docs[\n0\n].page_content)\n}\n\"\n)\nCopy\nTotal characters: 43131\nCopy\nprint\n(docs[\n0\n].page_content[:\n500\n])\nCopy\nLLM Powered Autonomous Agents\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\nGo deeper\nDocumentLoader\n: Object that loads data from a source as list of\nDocuments\n.\nIntegrations\n: 160+ integrations to choose from.\nBaseLoader\n: API reference for the base interface.\n\u200b\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this we\u2019ll split the\nDocument\ninto chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the\nsemantic search tutorial\n, we use a\nRecursiveCharacterTextSplitter\n, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom\nlangchain_text_splitters\nimport\nRecursiveCharacterTextSplitter\ntext_splitter\n=\nRecursiveCharacterTextSplitter(\nchunk_size\n=\n1000\n,\n# chunk size (characters)\nchunk_overlap\n=\n200\n,\n# chunk overlap (characters)\nadd_start_index\n=\nTrue\n,\n# track index in original document\n)\nall_splits\n=\ntext_splitter.split_documents(docs)\nprint\n(\nf\n\"Split blog post into\n{\nlen\n(all_splits)\n}\nsub-documents.\"\n)\nCopy\nSplit blog post into 66 sub-documents.\nGo deeper\nTextSplitter\n: Object that splits a list of\nDocument\nobjects into smaller\nchunks for storage and retrieval.\nIntegrations\nInterface\n: API reference for the base interface.\n\u200b\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the\nsemantic search tutorial\n, our approach is to\nembed\nthe contents of each document split and insert these embeddings into a\nvector store\n. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the\nstart of the tutorial\n.\nCopy\ndocument_ids\n=\nvector_store.add_documents(\ndocuments\n=\nall_splits)\nprint\n(document_ids[:\n3\n])\nCopy\n[\n'07c18af6-ad58-479a-bfb1-d508033f9c64'\n,\n'9000bf8e-1993-446f-8d4d-f4e507ba4b8f'\n,\n'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'\n]\nGo deeper\nEmbeddings\n: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations\n: 30+ integrations to choose from.\nInterface\n: API reference for the base interface.\nVectorStore\n: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations\n: 40+ integrations to choose from.\nInterface\n: API reference for the base interface.\nThis completes the\nIndexing\nportion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\u200b\n2. Retrieval and Generation\nRAG applications commonly work as follows:\nRetrieve\n: Given a user input, relevant splits are retrieved from storage using a\nRetriever\n.\nGenerate\n: A\nmodel\nproduces an answer using a prompt that includes both the question with the retrieved data\nNow let\u2019s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG\nagent\nthat executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG\nchain\nthat uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\u200b\nRAG agents\nOne formulation of a RAG application is as a simple\nagent\nwith a tool that retrieves information. We can assemble a minimal RAG agent by implementing a\ntool\nthat wraps our vector store:\nCopy\nfrom\nlangchain.tools\nimport\ntool\n@tool\n(\nresponse_format\n=\n\"content_and_artifact\"\n)\ndef\nretrieve_context\n(\nquery\n:\nstr\n):\n\"\"\"Retrieve information to help answer a query.\"\"\"\nretrieved_docs\n=\nvector_store.similarity_search(query,\nk\n=\n2\n)\nserialized\n=\n\"\n\\n\\n\n\"\n.join(\n(\nf\n\"Source:\n{\ndoc.metadata\n}\n\\n\nContent:\n{\ndoc.page_content\n}\n\"\n)\nfor\ndoc\nin\nretrieved_docs\n)\nreturn\nserialized, retrieved_docs\nHere we use the\ntool decorator\nto configure the tool to attach raw documents as\nartifacts\nto each\nToolMessage\n. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string\nquery\nargument, as in the above example. You can\nforce the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\nCopy\nfrom\ntyping\nimport\nLiteral\ndef\nretrieve_context\n(\nquery\n:\nstr\n,\nsection\n: Literal[\n\"beginning\"\n,\n\"middle\"\n,\n\"end\"\n]):\nGiven our tool, we can construct the agent:\nCopy\nfrom\nlangchain.agents\nimport\ncreate_agent\ntools\n=\n[retrieve_context]\n# If desired, specify custom instructions\nprompt\n=\n(\n\"You have access to a tool that retrieves context from a blog post. \"\n\"Use the tool to help answer user queries.\"\n)\nagent\n=\ncreate_agent(model, tools,\nsystem_prompt\n=\nprompt)\nLet\u2019s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery\n=\n(\n\"What is the standard method for Task Decomposition?\n\\n\\n\n\"\n\"Once you get the answer, look up common extensions of that method.\"\n)\nfor\nevent\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nevent[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\nCall ID: call_d6AVxICMPQYwAKj9lgH4E337\nArgs:\nquery: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\nretrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\nCall ID: call_0dbMOw7266jvETbXWn4JqWpR\nArgs:\nquery: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the\nLangSmith trace\n.\nYou can add a deeper level of control and customization using the\nLangGraph\nframework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s\nAgentic RAG tutorial\nfor more advanced formulations.\n\u200b\nRAG chains\nIn the above\nagentic RAG\nformulation we allow the LLM to use its discretion in generating a\ntool call\nto help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\u2705 Benefits\n\u26a0\ufe0f Drawbacks\nSearch only when needed\n\u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\nTwo inference calls\n\u2013 When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries\n\u2013 By treating search as a tool with a\nquery\ninput, the LLM crafts its own queries that incorporate conversational context.\nReduced control\n\u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed\n\u2013 The LLM can execute several searches in support of a single user query.\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom\nlangchain.agents.middleware\nimport\ndynamic_prompt, ModelRequest\n@dynamic_prompt\ndef\nprompt_with_context\n(\nrequest\n: ModelRequest) ->\nstr\n:\n\"\"\"Inject context into state messages.\"\"\"\nlast_query\n=\nrequest.state[\n\"messages\"\n][\n-\n1\n].text\nretrieved_docs\n=\nvector_store.similarity_search(last_query)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\nsystem_message\n=\n(\n\"You are a helpful assistant. Use the following context in your response:\"\nf\n\"\n\\n\\n\n{\ndocs_content\n}\n\"\n)\nreturn\nsystem_message\nagent\n=\ncreate_agent(model,\ntools\n=\n[],\nmiddleware\n=\n[prompt_with_context])\nLet\u2019s try this out:\nCopy\nquery\n=\n\"What is task decomposition?\"\nfor\nstep\nin\nagent.stream(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: query}]},\nstream_mode\n=\n\"values\"\n,\n):\nstep[\n\"messages\"\n][\n-\n1\n].pretty_print()\nCopy\n================================ Human Message =================================\nWhat is task decomposition?\n================================== Ai Message ==================================\nTask decomposition is...\nIn the\nLangSmith trace\nwe can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\nReturning source documents\nThe above\nRAG chain\nincorporates retrieved context into a single system message for that run.\nAs in the\nagentic RAG\nformulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\nAdding a key to the state to store the retrieved documents\nAdding a new node via a\npre-model hook\nto populate that key (as well as inject the context).\nCopy\nfrom\ntyping\nimport\nAny\nfrom\nlangchain_core.documents\nimport\nDocument\nfrom\nlangchain.agents.middleware\nimport\nAgentMiddleware, AgentState\nclass\nState\n(\nAgentState\n):\ncontext: list[Document]\nclass\nRetrieveDocumentsMiddleware\n(AgentMiddleware[State]):\nstate_schema\n=\nState\ndef\nbefore_model\n(\nself\n,\nstate\n: AgentState) -> dict[\nstr\n, Any]\n|\nNone\n:\nlast_message\n=\nstate[\n\"messages\"\n][\n-\n1\n]\nretrieved_docs\n=\nvector_store.similarity_search(last_message.text)\ndocs_content\n=\n\"\n\\n\\n\n\"\n.join(doc.page_content\nfor\ndoc\nin\nretrieved_docs)\naugmented_message_content\n=\n(\nf\n\"\n{\nlast_message.text\n}\n\\n\\n\n\"\n\"Use the following context to answer the query:\n\\n\n\"\nf\n\"\n{\ndocs_content\n}\n\"\n)\nreturn\n{\n\"messages\"\n: [last_message.model_copy(\nupdate\n=\n{\n\"content\"\n: augmented_message_content})],\n\"context\"\n: retrieved_docs,\n}\nagent\n=\ncreate_agent(\nmodel,\ntools\n=\n[],\nmiddleware\n=\n[RetrieveDocumentsMiddleware()],\n)\n\u200b\nNext steps\nNow that we\u2019ve implemented a simple RAG application via\ncreate_agent\n, we can easily incorporate new features and go deeper:\nStream\ntokens and other information for responsive user experiences\nAdd\nconversational memory\nto support multi-turn interactions\nAdd\nlong-term memory\nto support memory across conversational threads\nAdd\nstructured responses\nDeploy your application with\nLangSmith Deployment\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nBuild a semantic search engine with LangChain\nPrevious\nBuild a SQL agent\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/tutorials/rag/",
      "title": "Build a RAG agent with LangChain - Docs by LangChain",
      "heading": "Build a RAG agent with LangChain"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/how_to/vectorstores/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/how_to/embed_text/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "Faiss - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nFaiss\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic (Claude)\nGoogle\nAWS (Amazon)\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nSetup\nInitialization\nManage vector store\nAdd items to vector store\nDelete items from vector store\nQuery vector store\nQuery directly\nSimilarity search\nSimilarity search with score\nOther search methods\nQuery by turning into retriever\nUsage for retrieval-augmented generation\nSaving and loading\nMerging\nAPI reference\nFaiss\nCopy page\nCopy page\nFacebook AI Similarity Search (FAISS)\nis a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.\nSee\nThe FAISS Library\npaper.\nYou can find the FAISS documentation at\nthis page\n.\nThis notebook shows how to use functionality related to the\nFAISS\nvector database. It will show functionality specific to this integration. After going through, it may be useful to explore\nrelevant use-case pages\nto learn how to use this vectorstore as part of a larger chain.\n\u200b\nSetup\nThe integration lives in the\nlangchain-community\npackage. We also need to install the\nfaiss\npackage itself. We can install these with:\nNote that you can also install\nfaiss-gpu\nif you want to use the GPU enabled version\nCopy\npip install\n-\nqU langchain\n-\ncommunity faiss\n-\ncpu\nIf you want to get best in-class automated tracing of your model calls you can also set your\nLangSmith\nAPI key by uncommenting below:\nCopy\nos.environ[\n\"LANGSMITH_TRACING\"\n]\n=\n\"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n\u200b\nInitialization\nCopy\n# | output: false\n# | echo: false\nfrom\nlangchain_openai\nimport\nOpenAIEmbeddings\nembeddings\n=\nOpenAIEmbeddings(\nmodel\n=\n\"text-embedding-3-large\"\n)\nCopy\nimport\nfaiss\nfrom\nlangchain_community.docstore.in_memory\nimport\nInMemoryDocstore\nfrom\nlangchain_community.vectorstores\nimport\nFAISS\nindex\n=\nfaiss.IndexFlatL2(\nlen\n(embeddings.embed_query(\n\"hello world\"\n)))\nvector_store\n=\nFAISS(\nembedding_function\n=\nembeddings,\nindex\n=\nindex,\ndocstore\n=\nInMemoryDocstore(),\nindex_to_docstore_id\n=\n{},\n)\n\u200b\nManage vector store\n\u200b\nAdd items to vector store\nCopy\nfrom\nuuid\nimport\nuuid4\nfrom\nlangchain_core.documents\nimport\nDocument\ndocument_1\n=\nDocument(\npage_content\n=\n\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"tweet\"\n},\n)\ndocument_2\n=\nDocument(\npage_content\n=\n\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"news\"\n},\n)\ndocument_3\n=\nDocument(\npage_content\n=\n\"Building an exciting new project with LangChain - come check it out!\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"tweet\"\n},\n)\ndocument_4\n=\nDocument(\npage_content\n=\n\"Robbers broke into the city bank and stole $1 million in cash.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"news\"\n},\n)\ndocument_5\n=\nDocument(\npage_content\n=\n\"Wow! That was an amazing movie. I can't wait to see it again.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"tweet\"\n},\n)\ndocument_6\n=\nDocument(\npage_content\n=\n\"Is the new iPhone worth the price? Read this review to find out.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"website\"\n},\n)\ndocument_7\n=\nDocument(\npage_content\n=\n\"The top 10 soccer players in the world right now.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"website\"\n},\n)\ndocument_8\n=\nDocument(\npage_content\n=\n\"LangGraph is the best framework for building stateful, agentic applications!\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"tweet\"\n},\n)\ndocument_9\n=\nDocument(\npage_content\n=\n\"The stock market is down 500 points today due to fears of a recession.\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"news\"\n},\n)\ndocument_10\n=\nDocument(\npage_content\n=\n\"I have a bad feeling I am going to get deleted :(\"\n,\nmetadata\n=\n{\n\"source\"\n:\n\"tweet\"\n},\n)\ndocuments\n=\n[\ndocument_1,\ndocument_2,\ndocument_3,\ndocument_4,\ndocument_5,\ndocument_6,\ndocument_7,\ndocument_8,\ndocument_9,\ndocument_10,\n]\nuuids\n=\n[\nstr\n(uuid4())\nfor\n_\nin\nrange\n(\nlen\n(documents))]\nvector_store.add_documents(\ndocuments\n=\ndocuments,\nids\n=\nuuids)\nCopy\n[\n'22f5ce99-cd6f-4e0c-8dab-664128307c72'\n,\n'dc3f061b-5f88-4fa1-a966-413550c51891'\n,\n'd33d890b-baad-47f7-b7c1-175f5f7b4e59'\n,\n'6e6c01d2-6020-4a7b-95da-ef43d43f01b5'\n,\n'e677223d-ad75-4c1a-bef6-b5912bd1de03'\n,\n'47e2a168-6462-4ed2-b1d9-d9edfd7391d6'\n,\n'1e4d66d6-e155-4891-9212-f7be97f36c6a'\n,\n'c0663096-e1a5-4665-b245-1c2e6c4fb653'\n,\n'8297474a-7f7c-4006-9865-398c1781b1bc'\n,\n'44e4be03-0a8d-4316-b3c4-f35f4bb2b532'\n]\n\u200b\nDelete items from vector store\nCopy\nvector_store.delete(\nids\n=\n[uuids[\n-\n1\n]])\nCopy\nTrue\n\u200b\nQuery vector store\nOnce your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent.\n\u200b\nQuery directly\n\u200b\nSimilarity search\nPerforming a simple similarity search with filtering on metadata can be done as follows:\nCopy\nresults\n=\nvector_store.similarity_search(\n\"LangChain provides abstractions to make working with LLMs easy\"\n,\nk\n=\n2\n,\nfilter\n=\n{\n\"source\"\n:\n\"tweet\"\n},\n)\nfor\nres\nin\nresults:\nprint\n(\nf\n\"*\n{\nres.page_content\n}\n[\n{\nres.metadata\n}\n]\"\n)\nCopy\n* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\nSome\nMongoDB query and projection operators\nare supported for more advanced metadata filtering. The current list of supported operators are as follows:\n$eq\n(equals)\n$neq\n(not equals)\n$gt\n(greater than)\n$lt\n(less than)\n$gte\n(greater than or equal)\n$lte\n(less than or equal)\n$in\n(membership in list)\n$nin\n(not in list)\n$and\n(all conditions must match)\n$or\n(any condition must match)\n$not\n(negation of condition)\nPerforming the same above similarity search with advanced metadata filtering can be done as follows:\nCopy\nresults\n=\nvector_store.similarity_search(\n\"LangChain provides abstractions to make working with LLMs easy\"\n,\nk\n=\n2\n,\nfilter\n=\n{\n\"source\"\n: {\n\"$eq\"\n:\n\"tweet\"\n}},\n)\nfor\nres\nin\nresults:\nprint\n(\nf\n\"*\n{\nres.page_content\n}\n[\n{\nres.metadata\n}\n]\"\n)\nCopy\n* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n\u200b\nSimilarity search with score\nYou can also search with score:\nCopy\nresults\n=\nvector_store.similarity_search_with_score(\n\"Will it be hot tomorrow?\"\n,\nk\n=\n1\n,\nfilter\n=\n{\n\"source\"\n:\n\"news\"\n}\n)\nfor\nres, score\nin\nresults:\nprint\n(\nf\n\"* [SIM=\n{\nscore\n:3f}\n]\n{\nres.page_content\n}\n[\n{\nres.metadata\n}\n]\"\n)\nCopy\n* [SIM=0.893688] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n\u200b\nOther search methods\nThere are a variety of other ways to search a FAISS vector store. For a complete list of those methods, please refer to the\nAPI Reference\n\u200b\nQuery by turning into retriever\nYou can also transform the vector store into a retriever for easier usage in your chains.\nCopy\nretriever\n=\nvector_store.as_retriever(\nsearch_type\n=\n\"mmr\"\n,\nsearch_kwargs\n=\n{\n\"k\"\n:\n1\n})\nretriever.invoke(\n\"Stealing from the bank is a crime\"\n,\nfilter\n=\n{\n\"source\"\n:\n\"news\"\n})\nCopy\n[Document(metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]\n\u200b\nUsage for retrieval-augmented generation\nFor guides on how to use this vector store for retrieval-augmented generation (RAG), see the following sections:\nTutorials\nHow-to: Question and answer with RAG\nRetrieval conceptual docs\n\u200b\nSaving and loading\nYou can also save and load a FAISS index. This is useful so you don\u2019t have to recreate it everytime you use it.\nCopy\nvector_store.save_local(\n\"faiss_index\"\n)\nnew_vector_store\n=\nFAISS\n.load_local(\n\"faiss_index\"\n, embeddings,\nallow_dangerous_deserialization\n=\nTrue\n)\ndocs\n=\nnew_vector_store.similarity_search(\n\"qux\"\n)\nCopy\ndocs[\n0\n]\nCopy\nDocument(metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!')\n\u200b\nMerging\nYou can also merge two FAISS vectorstores\nCopy\ndb1\n=\nFAISS\n.from_texts([\n\"foo\"\n], embeddings)\ndb2\n=\nFAISS\n.from_texts([\n\"bar\"\n], embeddings)\ndb1.docstore._dict\nCopy\n{\n'b752e805-350e-4cf5-ba54-0883d46a3a44'\n: Document(\npage_content\n=\n'foo'\n)}\nCopy\ndb2.docstore._dict\nCopy\n{\n'08192d92-746d-4cd1-b681-bdfba411f459'\n: Document(\npage_content\n=\n'bar'\n)}\nCopy\ndb1.merge_from(db2)\nCopy\ndb1.docstore._dict\nCopy\n{'b752e805-350e-4cf5-ba54-0883d46a3a44': Document(page_content='foo'),\n'08192d92-746d-4cd1-b681-bdfba411f459': Document(page_content='bar')}\n\u200b\nAPI reference\nFor detailed documentation of all\nFAISS\nvector store features and configurations head to the API reference:\npython.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/integrations/vectorstores/faiss/",
      "title": "Faiss - Docs by LangChain",
      "heading": "Faiss"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/concepts/chains/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/concepts/prompts/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/how_to/prompts/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  },
  {
    "page_content": "LangChain overview - Docs by LangChain\nSkip to main content\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nAsk AI\nGitHub\nTry LangSmith\nTry LangSmith\nSearch...\nNavigation\nLangChain overview\nLangChain\nLangGraph\nDeep Agents\nIntegrations\nLearn\nReference\nContribute\nPython\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nBuilt-in middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate an agent\nCore benefits\nLangChain overview\nCopy page\nLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\nCopy page\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and\nmore\n. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use\nLangGraph\n, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\nLangChain\nagents\nare built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\u200b\nCreate an agent\nCopy\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom\nlangchain.agents\nimport\ncreate_agent\ndef\nget_weather\n(\ncity\n:\nstr\n) ->\nstr\n:\n\"\"\"Get weather for a given city.\"\"\"\nreturn\nf\n\"It's always sunny in\n{\ncity\n}\n!\"\nagent\n=\ncreate_agent(\nmodel\n=\n\"claude-sonnet-4-5-20250929\"\n,\ntools\n=\n[get_weather],\nsystem_prompt\n=\n\"You are a helpful assistant\"\n,\n)\n# Run the agent\nagent.invoke(\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"what is the weather in sf\"\n}]}\n)\nSee the\nInstallation instructions\nand\nQuickstart guide\nto get started building your own agents and applications with LangChain.\n\u200b\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nLearn more\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nLearn more\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nLearn more\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\nLearn more\nEdit this page on GitHub\nor\nfile an issue\n.\nConnect these docs\nto Claude, VSCode, and more via MCP for real-time answers.\nWas this page helpful?\nYes\nNo\nInstall LangChain\nNext\n\u2318\nI\nDocs by LangChain\nhome page\ngithub\nx\nlinkedin\nyoutube\nResources\nForum\nChangelog\nLangChain Academy\nTrust Center\nCompany\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by",
    "metadata": {
      "source": "https://python.langchain.com/docs/how_to/sequence/",
      "title": "LangChain overview - Docs by LangChain",
      "heading": "LangChain overview"
    }
  }
]