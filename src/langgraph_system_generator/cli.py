"""Command-line interface for LangGraph Notebook Foundry.

Provides lightweight commands to generate notebook artifacts (stub by default)
and to build the documentation index from cached docs.
"""

from __future__ import annotations

import argparse
import asyncio
import json
import os
from pathlib import Path
from typing import Any, Dict, List, Literal, TypedDict

from langchain_community.embeddings import FakeEmbeddings

from langgraph_system_generator.generator.graph import create_generator_graph
from langgraph_system_generator.generator.state import CellSpec, Constraint, NotebookPlan
from langgraph_system_generator.rag.indexer import build_index_from_cache
from langgraph_system_generator.utils.config import settings

BASE_DIR = Path(__file__).resolve().parents[2]
DEFAULT_CACHE_PATH = (BASE_DIR / "data" / "cached_docs").resolve()

GenerationMode = Literal["stub", "live"]


class GenerationArtifacts(TypedDict):
    """Serialized generation results written by the CLI/API."""

    mode: GenerationMode
    prompt: str
    manifest: Dict[str, Any]
    manifest_path: str
    output_dir: str
    result: Dict[str, Any]


def _default_state(prompt: str) -> Dict[str, Any]:
    """Return a baseline GeneratorState payload."""

    return {
        "user_prompt": prompt,
        "uploaded_files": None,
        "constraints": [],
        "selected_patterns": {},
        "docs_context": [],
        "notebook_plan": None,
        "architecture_justification": "",
        "architecture_type": None,
        "workflow_design": None,
        "tools_plan": None,
        "generated_cells": [],
        "qa_reports": [],
        "repair_attempts": 0,
        "artifacts_manifest": {},
        "generation_complete": False,
        "error_message": None,
    }


def _serialize(obj: Any) -> Any:
    """Recursively convert Pydantic models and objects into plain dicts/lists."""

    if hasattr(obj, "model_dump"):
        return obj.model_dump()
    if isinstance(obj, list):
        return [_serialize(item) for item in obj]
    if isinstance(obj, dict):
        return {key: _serialize(val) for key, val in obj.items()}
    return obj


def _write_json(path: Path, payload: Any) -> None:
    path.write_text(json.dumps(payload, indent=2), encoding="utf-8")


def _infer_stub_architecture(prompt: str) -> tuple[str, str]:
    """Lightweight heuristic to pick an architecture in stub mode."""

    text = prompt.lower()
    if any(keyword in text for keyword in ["delegate", "supervisor", "team", "subagent"]):
        return (
            "subagents",
            "Subagents pattern selected based on collaborative/delegation cues in the prompt.",
        )
    if any(keyword in text for keyword in ["hybrid", "combined", "mix", "multi-stage"]):
        return (
            "hybrid",
            "Hybrid pattern selected for mixed or multi-stage requirements detected in the prompt.",
        )
    if any(keyword in text for keyword in ["router", "route", "triage", "dispatch", "classification"]):
        return (
            "router",
            "Router pattern selected for routing/triage style requests in the prompt.",
        )
    return ("router", "Router pattern selected as a sensible default for general workflows.")


def _build_stub_result(prompt: str) -> Dict[str, Any]:
    """Create a deterministic, offline-friendly generation result."""

    architecture_type, justification = _infer_stub_architecture(prompt)

    constraints = [
        Constraint(type="goal", value=f"Deliver a notebook for: {prompt}", priority=5),
        Constraint(
            type="environment",
            value="Designed to run in Jupyter/Colab without extra setup",
            priority=3,
        ),
    ]

    plan = NotebookPlan(
        title=f"LangGraph Workflow: {prompt[:48]}",
        sections=[
            "Setup",
            "State Definition",
            "Tools",
            "Nodes",
            "Graph Construction",
            "Execution",
        ],
        cell_count_estimate=12,
        patterns_used=[architecture_type],
        architecture_type=architecture_type,
    )

    cells: List[CellSpec] = [
        CellSpec(
            cell_type="markdown",
            content=f"# {plan.title}\nGenerated by LangGraph Notebook Foundry",
            section="intro",
        ),
        CellSpec(
            cell_type="code",
            content="!pip install -q langgraph langchain-openai",
            section="setup",
        ),
        CellSpec(
            cell_type="code",
            content="from langgraph.graph import StateGraph\n\n# Define your workflow here",
            section="graph",
        ),
    ]

    return {
        "constraints": constraints,
        "selected_patterns": {"primary": architecture_type},
        "docs_context": [],
        "notebook_plan": plan,
        "architecture_type": plan.architecture_type,
        "architecture_justification": justification,
        "workflow_design": {
            "entry_point": architecture_type,
            "nodes": [
                {
                    "name": architecture_type,
                    "purpose": "Dispatch to specialists" if architecture_type == "router" else "Coordinate sub-agents",
                }
            ],
        },
        "tools_plan": [],
        "generated_cells": cells,
        "qa_reports": [],
        "repair_attempts": 0,
        "artifacts_manifest": {},
        "generation_complete": True,
        "error_message": None,
    }


async def generate_artifacts(
    prompt: str,
    *,
    output_dir: str | Path,
    mode: GenerationMode = "stub",
) -> GenerationArtifacts:
    """Generate notebook artifacts either in stub or live mode.

    Stub mode produces deterministic outputs without external API calls.
    Live mode invokes the generator graph and requires configured LLM credentials.
    """

    target = Path(output_dir)
    target.mkdir(parents=True, exist_ok=True)

    if mode == "live":
        if not os.environ.get("OPENAI_API_KEY"):
            raise RuntimeError("LLM API credentials are required for live generation mode.")
        graph = create_generator_graph()
        result = await graph.ainvoke(_default_state(prompt))
    else:
        result = _build_stub_result(prompt)

    serialized = _serialize(result)
    if "architecture_type" in serialized and serialized.get("architecture_type"):
        architecture_type = serialized.get("architecture_type")
    else:
        selected_patterns = serialized.get("selected_patterns") or {}
        architecture_type = selected_patterns.get("primary") or "router"
    manifest: Dict[str, Any] = {
        "prompt": prompt,
        "mode": mode,
        "architecture_type": architecture_type,
        "cell_count": len(serialized.get("generated_cells", []) or []),
        "plan_title": serialized.get("notebook_plan", {}).get("title"),
    }

    # Persist helpful artifacts for downstream consumers
    plan = serialized.get("notebook_plan")
    if plan:
        plan_path = target / "notebook_plan.json"
        _write_json(plan_path, plan)
        manifest["plan_path"] = str(plan_path)

    cells = serialized.get("generated_cells")
    if isinstance(cells, list):
        cells_path = target / "generated_cells.json"
        _write_json(cells_path, cells)
        manifest["cells_path"] = str(cells_path)

    manifest_path = target / "manifest.json"
    _write_json(manifest_path, manifest)

    return GenerationArtifacts(
        mode=mode,
        prompt=prompt,
        manifest=manifest,
        manifest_path=str(manifest_path),
        output_dir=str(target),
        result=serialized,
    )


async def _handle_build_index(
    cache_path: str, store_path: str, use_openai: bool, chunk_size: int, chunk_overlap: int
) -> str:
    """Build a documentation index from cached docs."""

    cache = str(Path(cache_path).resolve())
    store = str(Path(store_path).resolve())
    embeddings = None if use_openai else FakeEmbeddings(size=32)
    manager = await build_index_from_cache(
        cache_path=cache,
        store_path=store,
        embeddings=embeddings,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    return manager.store_path


def _run_generate(args: argparse.Namespace) -> int:
    artifacts = asyncio.run(
        generate_artifacts(
            args.prompt,
            output_dir=args.output,
            mode=args.mode,
        )
    )

    print(f"✓ Generated artifacts in {artifacts['output_dir']}")
    print(f"  Manifest: {artifacts['manifest_path']}")
    if artifacts["manifest"].get("plan_path"):
        print(f"  Plan: {artifacts['manifest']['plan_path']}")
    if artifacts["manifest"].get("cells_path"):
        print(f"  Cells: {artifacts['manifest']['cells_path']}")
    return 0


def _run_build_index(args: argparse.Namespace) -> int:
    try:
        path = asyncio.run(
            _handle_build_index(
                cache_path=args.cache,
                store_path=args.store,
                use_openai=args.use_openai,
                chunk_size=args.chunk_size,
                chunk_overlap=args.chunk_overlap,
            )
        )
        print(f"✓ Vector index written to {path}")
        return 0
    except (FileNotFoundError, RuntimeError, ValueError) as exc:  # pragma: no cover - defensive
        print(f"✗ Failed to build index: {exc}")
        return 1


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="LangGraph Notebook Foundry CLI")
    subparsers = parser.add_subparsers(dest="command", required=True)

    gen = subparsers.add_parser("generate", help="Generate notebook artifacts")
    gen.add_argument("prompt", type=str, help="User prompt describing the system to build")
    gen.add_argument(
        "-o",
        "--output",
        default=str((BASE_DIR / "output").resolve()),
        help="Directory to write artifacts (default: <project>/output)",
    )
    gen.add_argument(
        "--mode",
        choices=["stub", "live"],
        default="stub",
        help="Generation mode. 'stub' avoids external API calls (default).",
    )
    gen.set_defaults(func=_run_generate)

    idx = subparsers.add_parser("build-index", help="Build vector index from cached docs")
    idx.add_argument(
        "--cache",
        default=str(DEFAULT_CACHE_PATH),
        help="Path to cached docs directory (defaults to package data/cached_docs)",
    )
    idx.add_argument(
        "--store",
        default=str(Path(settings.vector_store_path).resolve()),
        help="Path to save the vector index",
    )
    idx.add_argument(
        "--use-openai",
        action="store_true",
        help="Use OpenAI embeddings instead of local fake embeddings.",
    )
    idx.add_argument("--chunk-size", type=int, default=500, help="Chunk size for document splitting")
    idx.add_argument("--chunk-overlap", type=int, default=50, help="Chunk overlap for document splitting")
    idx.set_defaults(func=_run_build_index)

    return parser


def main(argv: list[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    return args.func(args)


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
