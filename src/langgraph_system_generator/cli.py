"""Command-line interface for LangGraph Notebook Foundry.

Provides lightweight commands to generate notebook artifacts (stub by default)
and to build the documentation index from cached docs.
"""

from __future__ import annotations

import argparse
import asyncio
import json
import os
from pathlib import Path
from typing import Any, Dict, Iterable, List, Literal, TypedDict

from langchain_community.embeddings import FakeEmbeddings

from langgraph_system_generator.generator.graph import create_generator_graph
from langgraph_system_generator.generator.state import CellSpec, Constraint, NotebookPlan
from langgraph_system_generator.rag.indexer import build_index_from_cache
from langgraph_system_generator.utils.config import settings

GenerationMode = Literal["stub", "live"]


class GenerationArtifacts(TypedDict):
    """Serialized generation results written by the CLI/API."""

    mode: GenerationMode
    prompt: str
    manifest: Dict[str, Any]
    manifest_path: str
    output_dir: str
    result: Dict[str, Any]


def _default_state(prompt: str) -> Dict[str, Any]:
    """Return a baseline GeneratorState payload."""

    return {
        "user_prompt": prompt,
        "uploaded_files": None,
        "constraints": [],
        "selected_patterns": {},
        "docs_context": [],
        "notebook_plan": None,
        "architecture_justification": "",
        "workflow_design": None,
        "tools_plan": None,
        "generated_cells": [],
        "qa_reports": [],
        "repair_attempts": 0,
        "artifacts_manifest": {},
        "generation_complete": False,
        "error_message": None,
    }


def _serialize(obj: Any) -> Any:
    """Recursively convert Pydantic models and objects into plain dicts/lists."""

    if hasattr(obj, "model_dump"):
        return obj.model_dump()
    if isinstance(obj, list):
        return [_serialize(item) for item in obj]
    if isinstance(obj, dict):
        return {key: _serialize(val) for key, val in obj.items()}
    return obj


def _write_json(path: Path, payload: Any) -> None:
    path.write_text(json.dumps(payload, indent=2), encoding="utf-8")


def _build_stub_result(prompt: str) -> Dict[str, Any]:
    """Create a deterministic, offline-friendly generation result."""

    constraints = [
        Constraint(type="goal", value=f"Deliver a notebook for: {prompt}", priority=5),
        Constraint(
            type="environment",
            value="Designed to run in Jupyter/Colab without extra setup",
            priority=3,
        ),
    ]

    plan = NotebookPlan(
        title=f"LangGraph Workflow: {prompt[:48]}",
        sections=[
            "Setup",
            "State Definition",
            "Tools",
            "Nodes",
            "Graph Construction",
            "Execution",
        ],
        cell_count_estimate=12,
        patterns_used=["router"],
        architecture_type="router",
    )

    cells: List[CellSpec] = [
        CellSpec(
            cell_type="markdown",
            content=f"# {plan.title}\nGenerated by LangGraph Notebook Foundry",
            section="intro",
        ),
        CellSpec(
            cell_type="code",
            content="!pip install -q langgraph langchain-openai",
            section="setup",
        ),
        CellSpec(
            cell_type="code",
            content="from langgraph.graph import StateGraph\n\n# Define your workflow here",
            section="graph",
        ),
    ]

    return {
        "constraints": constraints,
        "selected_patterns": {"primary": "router"},
        "docs_context": [],
        "notebook_plan": plan,
        "architecture_type": plan.architecture_type,
        "architecture_justification": "Router pattern selected for fast routing.",
        "workflow_design": {
            "entry_point": "router",
            "nodes": [{"name": "router", "purpose": "Dispatch to specialists"}],
        },
        "tools_plan": [],
        "generated_cells": cells,
        "qa_reports": [],
        "repair_attempts": 0,
        "artifacts_manifest": {},
        "generation_complete": True,
        "error_message": None,
    }


async def generate_artifacts(
    prompt: str,
    *,
    output_dir: str | Path,
    mode: GenerationMode = "stub",
) -> GenerationArtifacts:
    """Generate notebook artifacts either in stub or live mode.

    Stub mode produces deterministic outputs without external API calls.
    Live mode invokes the generator graph and requires configured LLM credentials.
    """

    target = Path(output_dir)
    target.mkdir(parents=True, exist_ok=True)

    if mode == "live":
        if not os.environ.get("OPENAI_API_KEY"):
            raise RuntimeError("OPENAI_API_KEY is required for live generation mode.")
        graph = create_generator_graph()
        result = await graph.ainvoke(_default_state(prompt))
    else:
        result = _build_stub_result(prompt)

    serialized = _serialize(result)
    manifest: Dict[str, Any] = {
        "prompt": prompt,
        "mode": mode,
        "architecture_type": serialized.get("architecture_type")
        or serialized.get("selected_patterns", {}).get("primary", "router"),
        "cell_count": len(serialized.get("generated_cells", []) or []),
        "plan_title": serialized.get("notebook_plan", {}).get("title"),
    }

    # Persist helpful artifacts for downstream consumers
    plan = serialized.get("notebook_plan")
    if plan:
        plan_path = target / "notebook_plan.json"
        _write_json(plan_path, plan)
        manifest["plan_path"] = str(plan_path)

    cells = serialized.get("generated_cells")
    if isinstance(cells, Iterable):
        cells_path = target / "generated_cells.json"
        _write_json(cells_path, cells)
        manifest["cells_path"] = str(cells_path)

    manifest_path = target / "manifest.json"
    _write_json(manifest_path, manifest)

    return GenerationArtifacts(
        mode=mode,
        prompt=prompt,
        manifest=manifest,
        manifest_path=str(manifest_path),
        output_dir=str(target),
        result=serialized,
    )


async def _handle_build_index(
    cache_path: str, store_path: str, use_openai: bool, chunk_size: int, chunk_overlap: int
) -> str:
    """Build a documentation index from cached docs."""

    embeddings = None if use_openai else FakeEmbeddings(size=32)
    manager = await build_index_from_cache(
        cache_path=cache_path,
        store_path=store_path,
        embeddings=embeddings,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    return manager.store_path


def _run_generate(args: argparse.Namespace) -> int:
    artifacts = asyncio.run(
        generate_artifacts(
            args.prompt,
            output_dir=args.output,
            mode=args.mode,
        )
    )

    print(f"✓ Generated artifacts in {artifacts['output_dir']}")
    print(f"  Manifest: {artifacts['manifest_path']}")
    if artifacts["manifest"].get("plan_path"):
        print(f"  Plan: {artifacts['manifest']['plan_path']}")
    if artifacts["manifest"].get("cells_path"):
        print(f"  Cells: {artifacts['manifest']['cells_path']}")
    return 0


def _run_build_index(args: argparse.Namespace) -> int:
    try:
        path = asyncio.run(
            _handle_build_index(
                cache_path=args.cache,
                store_path=args.store,
                use_openai=args.use_openai,
                chunk_size=args.chunk_size,
                chunk_overlap=args.chunk_overlap,
            )
        )
        print(f"✓ Vector index written to {path}")
        return 0
    except Exception as exc:  # pragma: no cover - defensive
        print(f"✗ Failed to build index: {exc}")
        return 1


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="LangGraph Notebook Foundry CLI")
    subparsers = parser.add_subparsers(dest="command", required=True)

    gen = subparsers.add_parser("generate", help="Generate notebook artifacts")
    gen.add_argument("prompt", type=str, help="User prompt describing the system to build")
    gen.add_argument(
        "-o",
        "--output",
        default="./output",
        help="Directory to write artifacts (default: ./output)",
    )
    gen.add_argument(
        "--mode",
        choices=["stub", "live"],
        default="stub",
        help="Generation mode. 'stub' avoids external API calls (default).",
    )
    gen.set_defaults(func=_run_generate)

    idx = subparsers.add_parser("build-index", help="Build vector index from cached docs")
    idx.add_argument("--cache", default="./data/cached_docs", help="Path to cached docs directory")
    idx.add_argument("--store", default=settings.vector_store_path, help="Path to save the vector index")
    idx.add_argument(
        "--use-openai",
        action="store_true",
        help="Use OpenAI embeddings instead of local fake embeddings.",
    )
    idx.add_argument("--chunk-size", type=int, default=500, help="Chunk size for document splitting")
    idx.add_argument("--chunk-overlap", type=int, default=50, help="Chunk overlap for document splitting")
    idx.set_defaults(func=_run_build_index)

    return parser


def main(argv: list[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    return args.func(args)


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
